// This file is @generated by prost-build.
/// Configuration for BigLake managed tables.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BigLakeConfiguration {
    /// Optional. The connection specifying the credentials to be used to read and
    /// write to external storage, such as Cloud Storage. The connection_id can
    /// have the form `{project}.{location}.{connection_id}` or
    /// `projects/{project}/locations/{location}/connections/{connection_id}".
    #[prost(string, tag = "1")]
    pub connection_id: ::prost::alloc::string::String,
    /// Optional. The fully qualified location prefix of the external folder where
    /// table data is stored. The '*' wildcard character is not allowed. The URI
    /// should be in the format `gs://bucket/path_to_table/`
    #[prost(string, tag = "2")]
    pub storage_uri: ::prost::alloc::string::String,
    /// Optional. The file format the table data is stored in.
    #[prost(enumeration = "big_lake_configuration::FileFormat", tag = "3")]
    pub file_format: i32,
    /// Optional. The table format the metadata only snapshots are stored in.
    #[prost(enumeration = "big_lake_configuration::TableFormat", tag = "4")]
    pub table_format: i32,
}
/// Nested message and enum types in `BigLakeConfiguration`.
pub mod big_lake_configuration {
    /// Supported file formats for BigLake tables.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum FileFormat {
        /// Default Value.
        Unspecified = 0,
        /// Apache Parquet format.
        Parquet = 1,
    }
    impl FileFormat {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "FILE_FORMAT_UNSPECIFIED",
                Self::Parquet => "PARQUET",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "FILE_FORMAT_UNSPECIFIED" => Some(Self::Unspecified),
                "PARQUET" => Some(Self::Parquet),
                _ => None,
            }
        }
    }
    /// Supported table formats for BigLake tables.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum TableFormat {
        /// Default Value.
        Unspecified = 0,
        /// Apache Iceberg format.
        Iceberg = 1,
    }
    impl TableFormat {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "TABLE_FORMAT_UNSPECIFIED",
                Self::Iceberg => "ICEBERG",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "TABLE_FORMAT_UNSPECIFIED" => Some(Self::Unspecified),
                "ICEBERG" => Some(Self::Iceberg),
                _ => None,
            }
        }
    }
}
/// Configures table clustering.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Clustering {
    /// One or more fields on which data should be clustered. Only top-level,
    /// non-repeated, simple-type fields are supported. The ordering of the
    /// clustering fields should be prioritized from most to least important
    /// for filtering purposes.
    ///
    /// Additional information on limitations can be found here:
    /// <https://cloud.google.com/bigquery/docs/creating-clustered-tables#limitations>
    #[prost(string, repeated, tag = "1")]
    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Options for data format adjustments.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct DataFormatOptions {
    /// Optional. Output timestamp as usec int64. Default is false.
    #[prost(bool, tag = "1")]
    pub use_int64_timestamp: bool,
}
/// Identifier for a dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DatasetReference {
    /// Required. A unique ID for this dataset, without the project name. The ID
    /// must contain only letters (a-z, A-Z), numbers (0-9), or underscores (_).
    /// The maximum length is 1,024 characters.
    #[prost(string, tag = "1")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Optional. The ID of the project containing this dataset.
    #[prost(string, tag = "2")]
    pub project_id: ::prost::alloc::string::String,
}
/// Configuration for Cloud KMS encryption settings.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EncryptionConfiguration {
    /// Optional. Describes the Cloud KMS encryption key that will be used to
    /// protect destination BigQuery table. The BigQuery Service Account associated
    /// with your project requires access to this encryption key.
    #[prost(message, optional, tag = "1")]
    pub kms_key_name: ::core::option::Option<::prost::alloc::string::String>,
}
/// Options defining open source compatible datasets living in the BigQuery
/// catalog. Contains metadata of open source database, schema
/// or namespace represented by the current dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExternalCatalogDatasetOptions {
    /// Optional. A map of key value pairs defining the parameters and properties
    /// of the open source schema. Maximum size of 2Mib.
    #[prost(map = "string, string", tag = "1")]
    pub parameters: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. The storage location URI for all tables in the dataset.
    /// Equivalent to hive metastore's database locationUri. Maximum length of 1024
    /// characters.
    #[prost(string, tag = "2")]
    pub default_storage_location_uri: ::prost::alloc::string::String,
}
/// Configures the access a dataset defined in an external metadata storage.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExternalDatasetReference {
    /// Required. External source that backs this dataset.
    #[prost(string, tag = "2")]
    pub external_source: ::prost::alloc::string::String,
    /// Required. The connection id that is used to access the external_source.
    ///
    /// Format:
    ///    projects/{project_id}/locations/{location_id}/connections/{connection_id}
    #[prost(string, tag = "3")]
    pub connection: ::prost::alloc::string::String,
}
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RestrictionConfig {
    /// Output only. Specifies the type of dataset/table restriction.
    #[prost(enumeration = "restriction_config::RestrictionType", tag = "1")]
    pub r#type: i32,
}
/// Nested message and enum types in `RestrictionConfig`.
pub mod restriction_config {
    /// RestrictionType specifies the type of dataset/table restriction.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum RestrictionType {
        /// Should never be used.
        Unspecified = 0,
        /// Restrict data egress. See [Data
        /// egress](<https://cloud.google.com/bigquery/docs/analytics-hub-introduction#data_egress>)
        /// for more details.
        RestrictedDataEgress = 1,
    }
    impl RestrictionType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "RESTRICTION_TYPE_UNSPECIFIED",
                Self::RestrictedDataEgress => "RESTRICTED_DATA_EGRESS",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "RESTRICTION_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "RESTRICTED_DATA_EGRESS" => Some(Self::RestrictedDataEgress),
                _ => None,
            }
        }
    }
}
/// Id path of a routine.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RoutineReference {
    /// Required. The ID of the project containing this routine.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. The ID of the dataset containing this routine.
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. The ID of the routine. The ID must contain only
    /// letters (a-z, A-Z), numbers (0-9), or underscores (_). The maximum
    /// length is 256 characters.
    #[prost(string, tag = "3")]
    pub routine_id: ::prost::alloc::string::String,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TableReference {
    /// Required. The ID of the project containing this table.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. The ID of the dataset containing this table.
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. The ID of the table. The ID can contain Unicode characters in
    /// category L (letter), M (mark), N (number), Pc (connector, including
    /// underscore), Pd (dash), and Zs (space). For more information, see [General
    /// Category](<https://wikipedia.org/wiki/Unicode_character_property#General_Category>).
    /// The maximum length is 1,024 characters.  Certain operations allow suffixing
    /// of the table ID with a partition decorator, such as
    /// `sample_table$20190123`.
    #[prost(string, tag = "3")]
    pub table_id: ::prost::alloc::string::String,
}
/// Schema of a table
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TableSchema {
    /// Describes the fields in a table.
    #[prost(message, repeated, tag = "1")]
    pub fields: ::prost::alloc::vec::Vec<TableFieldSchema>,
    /// Optional. Specifies metadata of the foreign data type definition in field
    /// schema
    /// ([TableFieldSchema.foreign_type_definition][google.cloud.bigquery.v2.TableFieldSchema.foreign_type_definition]).
    #[prost(message, optional, tag = "3")]
    pub foreign_type_info: ::core::option::Option<ForeignTypeInfo>,
}
/// Metadata about the foreign data type definition such as the system
/// in which the type is defined.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ForeignTypeInfo {
    /// Required. Specifies the system which defines the foreign data type.
    #[prost(enumeration = "foreign_type_info::TypeSystem", tag = "1")]
    pub type_system: i32,
}
/// Nested message and enum types in `ForeignTypeInfo`.
pub mod foreign_type_info {
    /// External systems, such as query engines or table formats, that have their
    /// own data types.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum TypeSystem {
        /// TypeSystem not specified.
        Unspecified = 0,
        /// Represents Hive data types.
        Hive = 1,
    }
    impl TypeSystem {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "TYPE_SYSTEM_UNSPECIFIED",
                Self::Hive => "HIVE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "TYPE_SYSTEM_UNSPECIFIED" => Some(Self::Unspecified),
                "HIVE" => Some(Self::Hive),
                _ => None,
            }
        }
    }
}
/// Data policy option proto, it currently supports name only, will support
/// precedence later.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DataPolicyOption {
    /// Data policy resource name in the form of
    /// projects/project_id/locations/location_id/dataPolicies/data_policy_id.
    #[prost(string, optional, tag = "1")]
    pub name: ::core::option::Option<::prost::alloc::string::String>,
}
/// A field in TableSchema
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TableFieldSchema {
    /// Required. The field name. The name must contain only letters (a-z, A-Z),
    /// numbers (0-9), or underscores (_), and must start with a letter or
    /// underscore. The maximum length is 300 characters.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The field data type. Possible values include:
    ///
    /// * STRING
    /// * BYTES
    /// * INTEGER (or INT64)
    /// * FLOAT (or FLOAT64)
    /// * BOOLEAN (or BOOL)
    /// * TIMESTAMP
    /// * DATE
    /// * TIME
    /// * DATETIME
    /// * GEOGRAPHY
    /// * NUMERIC
    /// * BIGNUMERIC
    /// * JSON
    /// * RECORD (or STRUCT)
    /// * RANGE
    ///
    /// Use of RECORD/STRUCT indicates that the field contains a nested schema.
    #[prost(string, tag = "2")]
    pub r#type: ::prost::alloc::string::String,
    /// Optional. The field mode. Possible values include NULLABLE, REQUIRED and
    /// REPEATED. The default value is NULLABLE.
    #[prost(string, tag = "3")]
    pub mode: ::prost::alloc::string::String,
    /// Optional. Describes the nested schema fields if the type property is set
    /// to RECORD.
    #[prost(message, repeated, tag = "4")]
    pub fields: ::prost::alloc::vec::Vec<TableFieldSchema>,
    /// Optional. The field description. The maximum length is 1,024 characters.
    #[prost(message, optional, tag = "6")]
    pub description: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. The policy tags attached to this field, used for field-level
    /// access control. If not set, defaults to empty policy_tags.
    #[prost(message, optional, tag = "9")]
    pub policy_tags: ::core::option::Option<table_field_schema::PolicyTagList>,
    /// Optional. Data policy options, will replace the data_policies.
    #[prost(message, repeated, tag = "21")]
    pub data_policies: ::prost::alloc::vec::Vec<DataPolicyOption>,
    /// Optional. Maximum length of values of this field for STRINGS or BYTES.
    ///
    /// If max_length is not specified, no maximum length constraint is imposed
    /// on this field.
    ///
    /// If type = "STRING", then max_length represents the maximum UTF-8
    /// length of strings in this field.
    ///
    /// If type = "BYTES", then max_length represents the maximum number of
    /// bytes in this field.
    ///
    /// It is invalid to set this field if type &ne; "STRING" and &ne; "BYTES".
    #[prost(int64, tag = "10")]
    pub max_length: i64,
    /// Optional. Precision (maximum number of total digits in base 10) and scale
    /// (maximum number of digits in the fractional part in base 10) constraints
    /// for values of this field for NUMERIC or BIGNUMERIC.
    ///
    /// It is invalid to set precision or scale if type &ne; "NUMERIC" and &ne;
    /// "BIGNUMERIC".
    ///
    /// If precision and scale are not specified, no value range constraint is
    /// imposed on this field insofar as values are permitted by the type.
    ///
    /// Values of this NUMERIC or BIGNUMERIC field must be in this range when:
    ///
    /// * Precision (<var>P</var>) and scale (<var>S</var>) are specified:
    ///    [-10<sup><var>P</var>-<var>S</var></sup> + 10<sup>-<var>S</var></sup>,
    ///     10<sup><var>P</var>-<var>S</var></sup> - 10<sup>-<var>S</var></sup>]
    /// * Precision (<var>P</var>) is specified but not scale (and thus scale is
    ///    interpreted to be equal to zero):
    ///    \[-10<sup><var>P</var></sup> + 1, 10<sup><var>P</var></sup> - 1\].
    ///
    /// Acceptable values for precision and scale if both are specified:
    ///
    /// * If type = "NUMERIC":
    ///    1 &le; precision - scale &le; 29 and 0 &le; scale &le; 9.
    /// * If type = "BIGNUMERIC":
    ///    1 &le; precision - scale &le; 38 and 0 &le; scale &le; 38.
    ///
    /// Acceptable values for precision if only precision is specified but not
    /// scale (and thus scale is interpreted to be equal to zero):
    ///
    /// * If type = "NUMERIC": 1 &le; precision &le; 29.
    /// * If type = "BIGNUMERIC": 1 &le; precision &le; 38.
    ///
    /// If scale is specified but not precision, then it is invalid.
    #[prost(int64, tag = "11")]
    pub precision: i64,
    /// Optional. See documentation for precision.
    #[prost(int64, tag = "12")]
    pub scale: i64,
    /// Optional. Specifies the rounding mode to be used when storing values of
    /// NUMERIC and BIGNUMERIC type.
    #[prost(enumeration = "table_field_schema::RoundingMode", tag = "15")]
    pub rounding_mode: i32,
    /// Optional. Field collation can be set only when the type of field is STRING.
    /// The following values are supported:
    ///
    /// * 'und:ci': undetermined locale, case insensitive.
    /// * '': empty string. Default to case-sensitive behavior.
    #[prost(message, optional, tag = "13")]
    pub collation: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. A SQL expression to specify the \[default value\]
    /// (<https://cloud.google.com/bigquery/docs/default-values>) for this field.
    #[prost(message, optional, tag = "14")]
    pub default_value_expression: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. The subtype of the RANGE, if the type of this field is RANGE. If
    /// the type is RANGE, this field is required. Values for the field element
    /// type can be the following:
    ///
    /// * DATE
    /// * DATETIME
    /// * TIMESTAMP
    #[prost(message, optional, tag = "18")]
    pub range_element_type: ::core::option::Option<table_field_schema::FieldElementType>,
    /// Optional. Definition of the foreign data type.
    /// Only valid for top-level schema fields (not nested fields).
    /// If the type is FOREIGN, this field is required.
    #[prost(string, tag = "23")]
    pub foreign_type_definition: ::prost::alloc::string::String,
}
/// Nested message and enum types in `TableFieldSchema`.
pub mod table_field_schema {
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct PolicyTagList {
        /// A list of policy tag resource names. For example,
        /// "projects/1/locations/eu/taxonomies/2/policyTags/3". At most 1 policy tag
        /// is currently allowed.
        #[prost(string, repeated, tag = "1")]
        pub names: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    }
    /// Represents the type of a field element.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct FieldElementType {
        /// Required. The type of a field element. For more information, see
        /// [TableFieldSchema.type][google.cloud.bigquery.v2.TableFieldSchema.type].
        #[prost(string, tag = "1")]
        pub r#type: ::prost::alloc::string::String,
    }
    /// Rounding mode options that can be used when storing NUMERIC
    /// or BIGNUMERIC values.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum RoundingMode {
        /// Unspecified will default to using ROUND_HALF_AWAY_FROM_ZERO.
        Unspecified = 0,
        /// ROUND_HALF_AWAY_FROM_ZERO rounds half values away from zero
        /// when applying precision and scale upon writing of NUMERIC and BIGNUMERIC
        /// values.
        /// For Scale: 0
        /// 1.1, 1.2, 1.3, 1.4 => 1
        /// 1.5, 1.6, 1.7, 1.8, 1.9 => 2
        RoundHalfAwayFromZero = 1,
        /// ROUND_HALF_EVEN rounds half values to the nearest even value
        /// when applying precision and scale upon writing of NUMERIC and BIGNUMERIC
        /// values.
        /// For Scale: 0
        /// 1.1, 1.2, 1.3, 1.4 => 1
        /// 1.5 => 2
        /// 1.6, 1.7, 1.8, 1.9 => 2
        /// 2.5 => 2
        RoundHalfEven = 2,
    }
    impl RoundingMode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "ROUNDING_MODE_UNSPECIFIED",
                Self::RoundHalfAwayFromZero => "ROUND_HALF_AWAY_FROM_ZERO",
                Self::RoundHalfEven => "ROUND_HALF_EVEN",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "ROUNDING_MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "ROUND_HALF_AWAY_FROM_ZERO" => Some(Self::RoundHalfAwayFromZero),
                "ROUND_HALF_EVEN" => Some(Self::RoundHalfEven),
                _ => None,
            }
        }
    }
}
/// Grants all resources of particular types in a particular dataset read access
/// to the current dataset.
///
/// Similar to how individually authorized views work, updates to any resource
/// granted through its dataset (including creation of new resources) requires
/// read permission to referenced resources, plus write permission to the
/// authorizing dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DatasetAccessEntry {
    /// The dataset this entry applies to
    #[prost(message, optional, tag = "1")]
    pub dataset: ::core::option::Option<DatasetReference>,
    /// Which resources in the dataset this entry applies to. Currently, only
    /// views are supported, but additional target types may be added in the
    /// future.
    #[prost(enumeration = "dataset_access_entry::TargetType", repeated, tag = "2")]
    pub target_types: ::prost::alloc::vec::Vec<i32>,
}
/// Nested message and enum types in `DatasetAccessEntry`.
pub mod dataset_access_entry {
    /// Indicates the type of resources in a dataset that the entry applies to.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum TargetType {
        /// Do not use. You must set a target type explicitly.
        Unspecified = 0,
        /// This entry applies to views in the dataset.
        Views = 1,
        /// This entry applies to routines in the dataset.
        Routines = 2,
    }
    impl TargetType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "TARGET_TYPE_UNSPECIFIED",
                Self::Views => "VIEWS",
                Self::Routines => "ROUTINES",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "TARGET_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "VIEWS" => Some(Self::Views),
                "ROUTINES" => Some(Self::Routines),
                _ => None,
            }
        }
    }
}
/// An object that defines dataset access for an entity.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Access {
    /// An IAM role ID that should be granted to the user, group,
    /// or domain specified in this access entry.
    /// The following legacy mappings will be applied:
    ///
    /// * `OWNER`: `roles/bigquery.dataOwner`
    /// * `WRITER`: `roles/bigquery.dataEditor`
    /// * `READER`: `roles/bigquery.dataViewer`
    ///
    /// This field will accept any of the above formats, but will return only
    /// the legacy format. For example, if you set this field to
    /// "roles/bigquery.dataOwner", it will be returned back as "OWNER".
    #[prost(string, tag = "1")]
    pub role: ::prost::alloc::string::String,
    /// \[Pick one\] An email address of a user to grant access to. For example:
    /// fred@example.com. Maps to IAM policy member "user:EMAIL" or
    /// "serviceAccount:EMAIL".
    #[prost(string, tag = "2")]
    pub user_by_email: ::prost::alloc::string::String,
    /// \[Pick one\] An email address of a Google Group to grant access to.
    /// Maps to IAM policy member "group:GROUP".
    #[prost(string, tag = "3")]
    pub group_by_email: ::prost::alloc::string::String,
    /// \[Pick one\] A domain to grant access to. Any users signed in with the domain
    /// specified will be granted the specified access. Example: "example.com".
    /// Maps to IAM policy member "domain:DOMAIN".
    #[prost(string, tag = "4")]
    pub domain: ::prost::alloc::string::String,
    /// \[Pick one\] A special group to grant access to. Possible values include:
    ///
    ///    * projectOwners: Owners of the enclosing project.
    ///    * projectReaders: Readers of the enclosing project.
    ///    * projectWriters: Writers of the enclosing project.
    ///    * allAuthenticatedUsers: All authenticated BigQuery users.
    ///
    /// Maps to similarly-named IAM members.
    #[prost(string, tag = "5")]
    pub special_group: ::prost::alloc::string::String,
    /// \[Pick one\] Some other type of member that appears in the IAM Policy but
    /// isn't a user, group, domain, or special group.
    #[prost(string, tag = "7")]
    pub iam_member: ::prost::alloc::string::String,
    /// \[Pick one\] A view from a different dataset to grant access to. Queries
    /// executed against that view will have read access to views/tables/routines
    /// in this dataset.
    /// The role field is not required when this field is set. If that view is
    /// updated by any user, access to the view needs to be granted again via an
    /// update operation.
    #[prost(message, optional, tag = "6")]
    pub view: ::core::option::Option<TableReference>,
    /// \[Pick one\] A routine from a different dataset to grant access to. Queries
    /// executed against that routine will have read access to
    /// views/tables/routines in this dataset. Only UDF is supported for now.
    /// The role field is not required when this field is set. If that routine is
    /// updated by any user, access to the routine needs to be granted again via
    /// an update operation.
    #[prost(message, optional, tag = "8")]
    pub routine: ::core::option::Option<RoutineReference>,
    /// \[Pick one\] A grant authorizing all resources of a particular type in a
    /// particular dataset access to this dataset. Only views are supported for
    /// now. The role field is not required when this field is set. If that dataset
    /// is deleted and re-created, its access needs to be granted again via an
    /// update operation.
    #[prost(message, optional, tag = "9")]
    pub dataset: ::core::option::Option<DatasetAccessEntry>,
    /// Optional. condition for the binding. If CEL expression in this field is
    /// true, this access binding will be considered
    #[prost(message, optional, tag = "10")]
    pub condition: ::core::option::Option<super::super::super::r#type::Expr>,
}
/// Represents a BigQuery dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Dataset {
    /// Output only. The resource type.
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// Output only. A hash of the resource.
    #[prost(string, tag = "2")]
    pub etag: ::prost::alloc::string::String,
    /// Output only. The fully-qualified unique name of the dataset in the format
    /// projectId:datasetId. The dataset name without the project name is given in
    /// the datasetId field. When creating a new dataset, leave this field blank,
    /// and instead specify the datasetId field.
    #[prost(string, tag = "3")]
    pub id: ::prost::alloc::string::String,
    /// Output only. A URL that can be used to access the resource again. You can
    /// use this URL in Get or Update requests to the resource.
    #[prost(string, tag = "4")]
    pub self_link: ::prost::alloc::string::String,
    /// Required. A reference that identifies the dataset.
    #[prost(message, optional, tag = "5")]
    pub dataset_reference: ::core::option::Option<DatasetReference>,
    /// Optional. A descriptive name for the dataset.
    #[prost(message, optional, tag = "6")]
    pub friendly_name: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. A user-friendly description of the dataset.
    #[prost(message, optional, tag = "7")]
    pub description: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. The default lifetime of all tables in the dataset, in
    /// milliseconds. The minimum lifetime value is 3600000 milliseconds (one
    /// hour). To clear an existing default expiration with a PATCH request, set to
    /// 0. Once this property is set, all newly-created tables in the dataset will
    /// have an expirationTime property set to the creation time plus the value in
    /// this property, and changing the value will only affect new tables, not
    /// existing ones. When the expirationTime for a given table is reached, that
    /// table will be deleted automatically.
    /// If a table's expirationTime is modified or removed before the table
    /// expires, or if you provide an explicit expirationTime when creating a
    /// table, that value takes precedence over the default expiration time
    /// indicated by this property.
    #[prost(message, optional, tag = "8")]
    pub default_table_expiration_ms: ::core::option::Option<i64>,
    /// This default partition expiration, expressed in milliseconds.
    ///
    /// When new time-partitioned tables are created in a dataset where this
    /// property is set, the table will inherit this value, propagated as the
    /// `TimePartitioning.expirationMs` property on the new table.  If you set
    /// `TimePartitioning.expirationMs` explicitly when creating a table,
    /// the `defaultPartitionExpirationMs` of the containing dataset is ignored.
    ///
    /// When creating a partitioned table, if `defaultPartitionExpirationMs`
    /// is set, the `defaultTableExpirationMs` value is ignored and the table
    /// will not be inherit a table expiration deadline.
    #[prost(message, optional, tag = "14")]
    pub default_partition_expiration_ms: ::core::option::Option<i64>,
    /// The labels associated with this dataset. You can use these
    /// to organize and group your datasets.
    /// You can set this property when inserting or updating a dataset.
    /// See [Creating and Updating Dataset
    /// Labels](<https://cloud.google.com/bigquery/docs/creating-managing-labels#creating_and_updating_dataset_labels>)
    /// for more information.
    #[prost(map = "string, string", tag = "9")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. An array of objects that define dataset access for one or more
    /// entities. You can set this property when inserting or updating a dataset in
    /// order to control who is allowed to access the data. If unspecified at
    /// dataset creation time, BigQuery adds default dataset access for the
    /// following entities: access.specialGroup: projectReaders; access.role:
    /// READER; access.specialGroup: projectWriters; access.role: WRITER;
    /// access.specialGroup: projectOwners; access.role: OWNER;
    /// access.userByEmail: \[dataset creator email\]; access.role: OWNER;
    /// If you patch a dataset, then this field is overwritten by the patched
    /// dataset's access field. To add entities, you must supply the entire
    /// existing access array in addition to any new entities that you want to add.
    #[prost(message, repeated, tag = "10")]
    pub access: ::prost::alloc::vec::Vec<Access>,
    /// Output only. The time when this dataset was created, in milliseconds since
    /// the epoch.
    #[prost(int64, tag = "11")]
    pub creation_time: i64,
    /// Output only. The date when this dataset was last modified, in milliseconds
    /// since the epoch.
    #[prost(int64, tag = "12")]
    pub last_modified_time: i64,
    /// The geographic location where the dataset should reside. See
    /// <https://cloud.google.com/bigquery/docs/locations> for supported
    /// locations.
    #[prost(string, tag = "13")]
    pub location: ::prost::alloc::string::String,
    /// The default encryption key for all tables in the dataset.
    /// After this property is set, the encryption key of all newly-created tables
    /// in the dataset is set to this value unless the table creation request or
    /// query explicitly overrides the key.
    #[prost(message, optional, tag = "16")]
    pub default_encryption_configuration: ::core::option::Option<
        EncryptionConfiguration,
    >,
    /// Output only. Reserved for future use.
    #[prost(message, optional, tag = "17")]
    pub satisfies_pzs: ::core::option::Option<bool>,
    /// Output only. Reserved for future use.
    #[prost(message, optional, tag = "31")]
    pub satisfies_pzi: ::core::option::Option<bool>,
    /// Output only. Same as `type` in `ListFormatDataset`.
    /// The type of the dataset, one of:
    ///
    /// * DEFAULT - only accessible by owner and authorized accounts,
    /// * PUBLIC - accessible by everyone,
    /// * LINKED - linked dataset,
    /// * EXTERNAL - dataset with definition in external metadata catalog.
    #[prost(string, tag = "18")]
    pub r#type: ::prost::alloc::string::String,
    /// Optional. The source dataset reference when the dataset is of type LINKED.
    /// For all other dataset types it is not set. This field cannot be updated
    /// once it is set. Any attempt to update this field using Update and Patch API
    /// Operations will be ignored.
    #[prost(message, optional, tag = "19")]
    pub linked_dataset_source: ::core::option::Option<LinkedDatasetSource>,
    /// Output only. Metadata about the LinkedDataset. Filled out when the dataset
    /// type is LINKED.
    #[prost(message, optional, tag = "29")]
    pub linked_dataset_metadata: ::core::option::Option<LinkedDatasetMetadata>,
    /// Optional. Reference to a read-only external dataset defined in data
    /// catalogs outside of BigQuery. Filled out when the dataset type is EXTERNAL.
    #[prost(message, optional, tag = "20")]
    pub external_dataset_reference: ::core::option::Option<ExternalDatasetReference>,
    /// Optional. Options defining open source compatible datasets living in the
    /// BigQuery catalog. Contains metadata of open source database, schema or
    /// namespace represented by the current dataset.
    #[prost(message, optional, tag = "32")]
    pub external_catalog_dataset_options: ::core::option::Option<
        ExternalCatalogDatasetOptions,
    >,
    /// Optional. TRUE if the dataset and its table names are case-insensitive,
    /// otherwise FALSE. By default, this is FALSE, which means the dataset and its
    /// table names are case-sensitive. This field does not affect routine
    /// references.
    #[prost(message, optional, tag = "21")]
    pub is_case_insensitive: ::core::option::Option<bool>,
    /// Optional. Defines the default collation specification of future tables
    /// created in the dataset. If a table is created in this dataset without
    /// table-level default collation, then the table inherits the dataset default
    /// collation, which is applied to the string fields that do not have explicit
    /// collation specified. A change to this field affects only tables created
    /// afterwards, and does not alter the existing tables.
    /// The following values are supported:
    ///
    /// * 'und:ci': undetermined locale, case insensitive.
    /// * '': empty string. Default to case-sensitive behavior.
    #[prost(message, optional, tag = "22")]
    pub default_collation: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Defines the default rounding mode specification of new tables
    /// created within this dataset. During table creation, if this field is
    /// specified, the table within this dataset will inherit the default rounding
    /// mode of the dataset. Setting the default rounding mode on a table overrides
    /// this option. Existing tables in the dataset are unaffected.
    /// If columns are defined during that table creation,
    /// they will immediately inherit the table's default rounding mode,
    /// unless otherwise specified.
    #[prost(enumeration = "table_field_schema::RoundingMode", tag = "26")]
    pub default_rounding_mode: i32,
    /// Optional. Defines the time travel window in hours. The value can be from 48
    /// to 168 hours (2 to 7 days). The default value is 168 hours if this is not
    /// set.
    #[prost(message, optional, tag = "23")]
    pub max_time_travel_hours: ::core::option::Option<i64>,
    /// Output only. Tags for the dataset. To provide tags as inputs, use the
    /// `resourceTags` field.
    #[deprecated]
    #[prost(message, repeated, tag = "24")]
    pub tags: ::prost::alloc::vec::Vec<GcpTag>,
    /// Optional. Updates storage_billing_model for the dataset.
    #[prost(enumeration = "dataset::StorageBillingModel", tag = "25")]
    pub storage_billing_model: i32,
    /// Optional. Output only. Restriction config for all tables and dataset. If
    /// set, restrict certain accesses on the dataset and all its tables based on
    /// the config. See [Data
    /// egress](<https://cloud.google.com/bigquery/docs/analytics-hub-introduction#data_egress>)
    /// for more details.
    #[prost(message, optional, tag = "27")]
    pub restrictions: ::core::option::Option<RestrictionConfig>,
    /// Optional. The [tags](<https://cloud.google.com/bigquery/docs/tags>) attached
    /// to this dataset. Tag keys are globally unique. Tag key is expected to be in
    /// the namespaced format, for example "123456789012/environment" where
    /// 123456789012 is the ID of the parent organization or project resource for
    /// this tag key. Tag value is expected to be the short name, for example
    /// "Production". See [Tag
    /// definitions](<https://cloud.google.com/iam/docs/tags-access-control#definitions>)
    /// for more details.
    #[prost(map = "string, string", tag = "30")]
    pub resource_tags: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
/// Nested message and enum types in `Dataset`.
pub mod dataset {
    /// Indicates the billing model that will be applied to the dataset.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum StorageBillingModel {
        /// Value not set.
        Unspecified = 0,
        /// Billing for logical bytes.
        Logical = 1,
        /// Billing for physical bytes.
        Physical = 2,
    }
    impl StorageBillingModel {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STORAGE_BILLING_MODEL_UNSPECIFIED",
                Self::Logical => "LOGICAL",
                Self::Physical => "PHYSICAL",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STORAGE_BILLING_MODEL_UNSPECIFIED" => Some(Self::Unspecified),
                "LOGICAL" => Some(Self::Logical),
                "PHYSICAL" => Some(Self::Physical),
                _ => None,
            }
        }
    }
}
/// A global tag managed by Resource Manager.
/// <https://cloud.google.com/iam/docs/tags-access-control#definitions>
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GcpTag {
    /// Required. The namespaced friendly name of the tag key, e.g.
    /// "12345/environment" where 12345 is org id.
    #[prost(string, tag = "1")]
    pub tag_key: ::prost::alloc::string::String,
    /// Required. The friendly short name of the tag value, e.g. "production".
    #[prost(string, tag = "2")]
    pub tag_value: ::prost::alloc::string::String,
}
/// A dataset source type which refers to another BigQuery dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct LinkedDatasetSource {
    /// The source dataset reference contains project numbers and not project ids.
    #[prost(message, optional, tag = "1")]
    pub source_dataset: ::core::option::Option<DatasetReference>,
}
/// Metadata about the Linked Dataset.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct LinkedDatasetMetadata {
    /// Output only. Specifies whether Linked Dataset is currently in a linked
    /// state or not.
    #[prost(enumeration = "linked_dataset_metadata::LinkState", tag = "1")]
    pub link_state: i32,
}
/// Nested message and enum types in `LinkedDatasetMetadata`.
pub mod linked_dataset_metadata {
    /// Specifies whether Linked Dataset is currently in a linked state or not.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum LinkState {
        /// The default value.
        /// Default to the LINKED state.
        Unspecified = 0,
        /// Normal Linked Dataset state. Data is queryable via the Linked Dataset.
        Linked = 1,
        /// Data publisher or owner has unlinked this Linked Dataset. It means you
        /// can no longer query or see the data in the Linked Dataset.
        Unlinked = 2,
    }
    impl LinkState {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "LINK_STATE_UNSPECIFIED",
                Self::Linked => "LINKED",
                Self::Unlinked => "UNLINKED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "LINK_STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "LINKED" => Some(Self::Linked),
                "UNLINKED" => Some(Self::Unlinked),
                _ => None,
            }
        }
    }
}
/// Request format for getting information about a dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetDatasetRequest {
    /// Required. Project ID of the requested dataset
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the requested dataset
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Optional. Specifies the view that determines which dataset information is
    /// returned. By default, metadata and ACL information are returned.
    #[prost(enumeration = "get_dataset_request::DatasetView", tag = "3")]
    pub dataset_view: i32,
    /// Optional. The version of the access policy schema to fetch.
    /// Valid values are 0, 1, and 3. Requests specifying an invalid value will be
    /// rejected.
    ///
    /// Requests for conditional access policy binding in datasets must specify
    /// version 3. Dataset with no conditional role bindings in access policy may
    /// specify any valid value or leave the field unset.
    ///
    /// This field will be maped to \[IAM Policy version\]
    /// (<https://cloud.google.com/iam/docs/policies#versions>) and will be used to
    /// fetch policy from IAM.
    ///
    /// If unset or if 0 or 1 value is used for dataset with conditional bindings,
    /// access entry with condition will have role string appended by
    /// 'withcond' string followed by a hash value. For example :
    /// {
    ///    "access": [
    ///       {
    ///          "role":
    ///          "roles/bigquery.dataViewer_with_conditionalbinding_7a34awqsda",
    ///          "userByEmail": "user@example.com",
    ///       }
    ///    ]
    /// }
    /// Please refer <https://cloud.google.com/iam/docs/troubleshooting-withcond> for
    /// more details.
    #[prost(int32, tag = "4")]
    pub access_policy_version: i32,
}
/// Nested message and enum types in `GetDatasetRequest`.
pub mod get_dataset_request {
    /// DatasetView specifies which dataset information is returned.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum DatasetView {
        /// The default value.
        /// Default to the FULL view.
        Unspecified = 0,
        /// Includes metadata information for the dataset, such as location,
        /// etag, lastModifiedTime, etc.
        Metadata = 1,
        /// Includes ACL information for the dataset, which defines dataset access
        /// for one or more entities.
        Acl = 2,
        /// Includes both dataset metadata and ACL information.
        Full = 3,
    }
    impl DatasetView {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "DATASET_VIEW_UNSPECIFIED",
                Self::Metadata => "METADATA",
                Self::Acl => "ACL",
                Self::Full => "FULL",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "DATASET_VIEW_UNSPECIFIED" => Some(Self::Unspecified),
                "METADATA" => Some(Self::Metadata),
                "ACL" => Some(Self::Acl),
                "FULL" => Some(Self::Full),
                _ => None,
            }
        }
    }
}
/// Request format for inserting a dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct InsertDatasetRequest {
    /// Required. Project ID of the new dataset
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Datasets resource to use for the new dataset
    #[prost(message, optional, tag = "2")]
    pub dataset: ::core::option::Option<Dataset>,
    /// Optional. The version of the provided access policy schema.
    /// Valid values are 0, 1, and 3. Requests specifying an invalid value will be
    /// rejected.
    ///
    /// This version refers to the schema version of the access policy and not the
    /// version of access policy. This field's value can be equal or more
    /// than the access policy schema provided in the request.
    /// For example,
    ///    * Requests with conditional access policy binding in datasets must
    ///    specify
    ///      version 3.
    ///    * But dataset with no conditional role bindings in access policy
    ///      may specify any valid value or leave the field unset.
    /// If unset or if 0 or 1 value is used for dataset with conditional
    /// bindings, request will be rejected.
    ///
    /// This field will be maped to IAM Policy version
    /// (<https://cloud.google.com/iam/docs/policies#versions>) and will be used to
    /// set policy in IAM.
    #[prost(int32, tag = "4")]
    pub access_policy_version: i32,
}
/// Message for updating or patching a dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateOrPatchDatasetRequest {
    /// Required. Project ID of the dataset being updated
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the dataset being updated
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. Datasets resource which will replace or patch the specified
    /// dataset.
    #[prost(message, optional, tag = "3")]
    pub dataset: ::core::option::Option<Dataset>,
    /// Optional. The version of the provided access policy schema.
    /// Valid values are 0, 1, and 3. Requests specifying an invalid value will be
    /// rejected.
    ///
    /// This version refers to the schema version of the access policy and not the
    /// version of access policy. This field's value can be equal or more
    /// than the access policy schema provided in the request.
    /// For example,
    ///    * Operations updating conditional access policy binding in datasets must
    ///    specify
    ///      version 3. Some of the operations are :
    ///        -  Adding a new access policy entry with condition.
    ///        -  Removing an access policy entry with condition.
    ///        -  Updating an access policy entry with condition.
    ///    * But dataset with no conditional role bindings in access policy
    ///      may specify any valid value or leave the field unset.
    /// If unset or if 0 or 1 value is used for dataset with conditional
    /// bindings, request will be rejected.
    ///
    /// This field will be maped to IAM Policy version
    /// (<https://cloud.google.com/iam/docs/policies#versions>) and will be used to
    /// set policy in IAM.
    #[prost(int32, tag = "5")]
    pub access_policy_version: i32,
}
/// Request format for deleting a dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteDatasetRequest {
    /// Required. Project ID of the dataset being deleted
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of dataset being deleted
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// If True, delete all the tables in the dataset.
    /// If False and the dataset contains tables, the request will fail.
    /// Default is False
    #[prost(bool, tag = "3")]
    pub delete_contents: bool,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListDatasetsRequest {
    /// Required. Project ID of the datasets to be listed
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// The maximum number of results to return in a single response page.
    /// Leverage the page tokens to iterate through the entire collection.
    #[prost(message, optional, tag = "2")]
    pub max_results: ::core::option::Option<u32>,
    /// Page token, returned by a previous call, to request the next page of
    /// results
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
    /// Whether to list all datasets, including hidden ones
    #[prost(bool, tag = "4")]
    pub all: bool,
    /// An expression for filtering the results of the request by label.
    /// The syntax is `labels.<name>\[:<value>\]`.
    /// Multiple filters can be ANDed together by connecting with a space.
    /// Example: `labels.department:receiving labels.active`.
    /// See [Filtering datasets using
    /// labels](<https://cloud.google.com/bigquery/docs/filtering-labels#filtering_datasets_using_labels>)
    /// for details.
    #[prost(string, tag = "5")]
    pub filter: ::prost::alloc::string::String,
}
/// A dataset resource with only a subset of fields, to be returned in a list of
/// datasets.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFormatDataset {
    /// The resource type.
    /// This property always returns the value "bigquery#dataset"
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// The fully-qualified, unique, opaque ID of the dataset.
    #[prost(string, tag = "2")]
    pub id: ::prost::alloc::string::String,
    /// The dataset reference.
    /// Use this property to access specific parts of the dataset's ID, such as
    /// project ID or dataset ID.
    #[prost(message, optional, tag = "3")]
    pub dataset_reference: ::core::option::Option<DatasetReference>,
    /// The labels associated with this dataset.
    /// You can use these to organize and group your datasets.
    #[prost(map = "string, string", tag = "4")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// An alternate name for the dataset.  The friendly name is purely
    /// decorative in nature.
    #[prost(message, optional, tag = "5")]
    pub friendly_name: ::core::option::Option<::prost::alloc::string::String>,
    /// The geographic location where the dataset resides.
    #[prost(string, tag = "6")]
    pub location: ::prost::alloc::string::String,
}
/// Response format for a page of results when listing datasets.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DatasetList {
    /// Output only. The resource type.
    /// This property always returns the value "bigquery#datasetList"
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// Output only. A hash value of the results page. You can use this property to
    /// determine if the page has changed since the last request.
    #[prost(string, tag = "2")]
    pub etag: ::prost::alloc::string::String,
    /// A token that can be used to request the next results page. This property is
    /// omitted on the final results page.
    #[prost(string, tag = "3")]
    pub next_page_token: ::prost::alloc::string::String,
    /// An array of the dataset resources in the project.
    /// Each resource contains basic information.
    /// For full information about a particular dataset resource, use the Datasets:
    /// get method. This property is omitted when there are no datasets in the
    /// project.
    #[prost(message, repeated, tag = "4")]
    pub datasets: ::prost::alloc::vec::Vec<ListFormatDataset>,
    /// A list of skipped locations that were unreachable. For more information
    /// about BigQuery locations, see:
    /// <https://cloud.google.com/bigquery/docs/locations.> Example: "europe-west5"
    #[prost(string, repeated, tag = "5")]
    pub unreachable: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Request format for undeleting a dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UndeleteDatasetRequest {
    /// Required. Project ID of the dataset to be undeleted
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of dataset being deleted
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Optional. The exact time when the dataset was deleted. If not specified,
    /// the most recently deleted version is undeleted. Undeleting a dataset
    /// using deletion time is not supported.
    #[prost(message, optional, tag = "3")]
    pub deletion_time: ::core::option::Option<::prost_types::Timestamp>,
}
/// Generated client implementations.
pub mod dataset_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// DatasetService provides methods for managing BigQuery datasets.
    #[derive(Debug, Clone)]
    pub struct DatasetServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl DatasetServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> DatasetServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> DatasetServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            DatasetServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Returns the dataset specified by datasetID.
        pub async fn get_dataset(
            &mut self,
            request: impl tonic::IntoRequest<super::GetDatasetRequest>,
        ) -> std::result::Result<tonic::Response<super::Dataset>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.DatasetService/GetDataset",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.DatasetService",
                        "GetDataset",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a new empty dataset.
        pub async fn insert_dataset(
            &mut self,
            request: impl tonic::IntoRequest<super::InsertDatasetRequest>,
        ) -> std::result::Result<tonic::Response<super::Dataset>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.DatasetService/InsertDataset",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.DatasetService",
                        "InsertDataset",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates information in an existing dataset. The update method replaces the
        /// entire dataset resource, whereas the patch method only replaces fields that
        /// are provided in the submitted dataset resource.
        /// This method supports RFC5789 patch semantics.
        pub async fn patch_dataset(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateOrPatchDatasetRequest>,
        ) -> std::result::Result<tonic::Response<super::Dataset>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.DatasetService/PatchDataset",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.DatasetService",
                        "PatchDataset",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates information in an existing dataset. The update method replaces the
        /// entire dataset resource, whereas the patch method only replaces fields that
        /// are provided in the submitted dataset resource.
        pub async fn update_dataset(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateOrPatchDatasetRequest>,
        ) -> std::result::Result<tonic::Response<super::Dataset>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.DatasetService/UpdateDataset",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.DatasetService",
                        "UpdateDataset",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes the dataset specified by the datasetId value. Before you can delete
        /// a dataset, you must delete all its tables, either manually or by specifying
        /// deleteContents. Immediately after deletion, you can create another dataset
        /// with the same name.
        pub async fn delete_dataset(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteDatasetRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.DatasetService/DeleteDataset",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.DatasetService",
                        "DeleteDataset",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists all datasets in the specified project to which the user has been
        /// granted the READER dataset role.
        pub async fn list_datasets(
            &mut self,
            request: impl tonic::IntoRequest<super::ListDatasetsRequest>,
        ) -> std::result::Result<tonic::Response<super::DatasetList>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.DatasetService/ListDatasets",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.DatasetService",
                        "ListDatasets",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Undeletes a dataset which is within time travel window based on datasetId.
        /// If a time is specified, the dataset version deleted at that time is
        /// undeleted, else the last live version is undeleted.
        pub async fn undelete_dataset(
            &mut self,
            request: impl tonic::IntoRequest<super::UndeleteDatasetRequest>,
        ) -> std::result::Result<tonic::Response<super::Dataset>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.DatasetService/UndeleteDataset",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.DatasetService",
                        "UndeleteDataset",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// The data types that could be used as a target type when converting decimal
/// values.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum DecimalTargetType {
    /// Invalid type.
    Unspecified = 0,
    /// Decimal values could be converted to NUMERIC
    /// type.
    Numeric = 1,
    /// Decimal values could be converted to BIGNUMERIC
    /// type.
    Bignumeric = 2,
    /// Decimal values could be converted to STRING type.
    String = 3,
}
impl DecimalTargetType {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "DECIMAL_TARGET_TYPE_UNSPECIFIED",
            Self::Numeric => "NUMERIC",
            Self::Bignumeric => "BIGNUMERIC",
            Self::String => "STRING",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "DECIMAL_TARGET_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
            "NUMERIC" => Some(Self::Numeric),
            "BIGNUMERIC" => Some(Self::Bignumeric),
            "STRING" => Some(Self::String),
            _ => None,
        }
    }
}
/// Error details.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ErrorProto {
    /// A short error code that summarizes the error.
    #[prost(string, tag = "1")]
    pub reason: ::prost::alloc::string::String,
    /// Specifies where the error occurred, if present.
    #[prost(string, tag = "2")]
    pub location: ::prost::alloc::string::String,
    /// Debugging information. This property is internal to Google and should not
    /// be used.
    #[prost(string, tag = "3")]
    pub debug_info: ::prost::alloc::string::String,
    /// A human-readable description of the error.
    #[prost(string, tag = "4")]
    pub message: ::prost::alloc::string::String,
}
/// Metadata about open source compatible table. The fields contained in
/// these options correspond to hive metastore's table level properties.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExternalCatalogTableOptions {
    /// Optional. A map of key value pairs defining the parameters and properties
    /// of the open source table. Corresponds with hive meta store table
    /// parameters. Maximum size of 4Mib.
    #[prost(map = "string, string", tag = "1")]
    pub parameters: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. A storage descriptor containing information about the physical
    /// storage of this table.
    #[prost(message, optional, tag = "2")]
    pub storage_descriptor: ::core::option::Option<StorageDescriptor>,
    /// Optional. The connection specifying the credentials to be used to read
    /// external storage, such as Azure Blob, Cloud Storage, or S3. The connection
    /// is needed to read the open source table from BigQuery Engine. The
    /// connection_id can have the form
    /// `<project_id>.<location_id>.<connection_id>` or
    /// `projects/<project_id>/locations/<location_id>/connections/<connection_id>`.
    #[prost(string, tag = "3")]
    pub connection_id: ::prost::alloc::string::String,
}
/// Contains information about how a table's data is stored and accessed by open
/// source query engines.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StorageDescriptor {
    /// Optional. The physical location of the table
    /// (e.g. `gs://spark-dataproc-data/pangea-data/case_sensitive/` or
    /// `gs://spark-dataproc-data/pangea-data/*`).
    /// The maximum length is 2056 bytes.
    #[prost(string, tag = "1")]
    pub location_uri: ::prost::alloc::string::String,
    /// Optional. Specifies the fully qualified class name of the InputFormat
    /// (e.g. "org.apache.hadoop.hive.ql.io.orc.OrcInputFormat").
    /// The maximum length is 128 characters.
    #[prost(string, tag = "2")]
    pub input_format: ::prost::alloc::string::String,
    /// Optional. Specifies the fully qualified class name of the OutputFormat
    /// (e.g. "org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat").
    /// The maximum length is 128 characters.
    #[prost(string, tag = "3")]
    pub output_format: ::prost::alloc::string::String,
    /// Optional. Serializer and deserializer information.
    #[prost(message, optional, tag = "4")]
    pub serde_info: ::core::option::Option<SerDeInfo>,
}
/// Serializer and deserializer information.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SerDeInfo {
    /// Optional. Name of the SerDe.
    /// The maximum length is 256 characters.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. Specifies a fully-qualified class name of the serialization
    /// library that is responsible for the translation of data between table
    /// representation and the underlying low-level input and output format
    /// structures. The maximum length is 256 characters.
    #[prost(string, tag = "2")]
    pub serialization_library: ::prost::alloc::string::String,
    /// Optional. Key-value pairs that define the initialization parameters for the
    /// serialization library.
    /// Maximum size 10 Kib.
    #[prost(map = "string, string", tag = "3")]
    pub parameters: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
/// This enum defines how to interpret source URIs for load jobs and external
/// tables.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum FileSetSpecType {
    /// This option expands source URIs by listing files from the object store. It
    /// is the default behavior if FileSetSpecType is not set.
    FileSystemMatch = 0,
    /// This option indicates that the provided URIs are newline-delimited manifest
    /// files, with one URI per line. Wildcard URIs are not supported.
    NewLineDelimitedManifest = 1,
}
impl FileSetSpecType {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::FileSystemMatch => "FILE_SET_SPEC_TYPE_FILE_SYSTEM_MATCH",
            Self::NewLineDelimitedManifest => {
                "FILE_SET_SPEC_TYPE_NEW_LINE_DELIMITED_MANIFEST"
            }
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "FILE_SET_SPEC_TYPE_FILE_SYSTEM_MATCH" => Some(Self::FileSystemMatch),
            "FILE_SET_SPEC_TYPE_NEW_LINE_DELIMITED_MANIFEST" => {
                Some(Self::NewLineDelimitedManifest)
            }
            _ => None,
        }
    }
}
/// Options for configuring hive partitioning detect.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct HivePartitioningOptions {
    /// Optional. When set, what mode of hive partitioning to use when reading
    /// data.  The following modes are supported:
    ///
    /// * AUTO: automatically infer partition key name(s) and type(s).
    ///
    /// * STRINGS: automatically infer partition key name(s).  All types are
    /// strings.
    ///
    /// * CUSTOM: partition key schema is encoded in the source URI prefix.
    ///
    /// Not all storage formats support hive partitioning. Requesting hive
    /// partitioning on an unsupported format will lead to an error.
    /// Currently supported formats are: JSON, CSV, ORC, Avro and Parquet.
    #[prost(string, tag = "1")]
    pub mode: ::prost::alloc::string::String,
    /// Optional. When hive partition detection is requested, a common prefix for
    /// all source uris must be required.  The prefix must end immediately before
    /// the partition key encoding begins. For example, consider files following
    /// this data layout:
    ///
    /// gs://bucket/path_to_table/dt=2019-06-01/country=USA/id=7/file.avro
    ///
    /// gs://bucket/path_to_table/dt=2019-05-31/country=CA/id=3/file.avro
    ///
    /// When hive partitioning is requested with either AUTO or STRINGS detection,
    /// the common prefix can be either of gs://bucket/path_to_table or
    /// gs://bucket/path_to_table/.
    ///
    /// CUSTOM detection requires encoding the partitioning schema immediately
    /// after the common prefix.  For CUSTOM, any of
    ///
    /// * gs://bucket/path_to_table/{dt:DATE}/{country:STRING}/{id:INTEGER}
    ///
    /// * gs://bucket/path_to_table/{dt:STRING}/{country:STRING}/{id:INTEGER}
    ///
    /// * gs://bucket/path_to_table/{dt:DATE}/{country:STRING}/{id:STRING}
    ///
    /// would all be valid source URI prefixes.
    #[prost(string, tag = "2")]
    pub source_uri_prefix: ::prost::alloc::string::String,
    /// Optional. If set to true, queries over this table require a partition
    /// filter that can be used for partition elimination to be specified.
    ///
    /// Note that this field should only be true when creating a permanent
    /// external table or querying a temporary external table.
    ///
    /// Hive-partitioned loads with require_partition_filter explicitly set to
    /// true will fail.
    #[prost(message, optional, tag = "3")]
    pub require_partition_filter: ::core::option::Option<bool>,
    /// Output only. For permanent external tables, this field is populated with
    /// the hive partition keys in the order they were inferred. The types of the
    /// partition keys can be deduced by checking the table schema (which will
    /// include the partition keys). Not every API will populate this field in the
    /// output. For example, Tables.Get will populate it, but Tables.List will not
    /// contain this field.
    #[prost(string, repeated, tag = "4")]
    pub fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Used to indicate that a JSON variant, rather than normal JSON, is being used
/// as the source_format. This should only be used in combination with the
/// JSON source format.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum JsonExtension {
    /// The default if provided value is not one included in the enum, or the value
    /// is not specified. The source formate is parsed without any modification.
    Unspecified = 0,
    /// Use GeoJSON variant of JSON. See <https://tools.ietf.org/html/rfc7946.>
    Geojson = 1,
}
impl JsonExtension {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "JSON_EXTENSION_UNSPECIFIED",
            Self::Geojson => "GEOJSON",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "JSON_EXTENSION_UNSPECIFIED" => Some(Self::Unspecified),
            "GEOJSON" => Some(Self::Geojson),
            _ => None,
        }
    }
}
/// Indicates the map target type. Only applies to parquet maps.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum MapTargetType {
    /// In this mode, the map will have the following schema:
    /// struct map_field_name {  repeated struct key_value {  key  value  } }.
    Unspecified = 0,
    /// In this mode, the map will have the following schema:
    /// repeated struct map_field_name {  key  value }.
    ArrayOfStruct = 1,
}
impl MapTargetType {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "MAP_TARGET_TYPE_UNSPECIFIED",
            Self::ArrayOfStruct => "ARRAY_OF_STRUCT",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "MAP_TARGET_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
            "ARRAY_OF_STRUCT" => Some(Self::ArrayOfStruct),
            _ => None,
        }
    }
}
/// Options for external data sources.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct AvroOptions {
    /// Optional. If sourceFormat is set to "AVRO", indicates whether to interpret
    /// logical types as the corresponding BigQuery data type (for example,
    /// TIMESTAMP), instead of using the raw type (for example, INTEGER).
    #[prost(message, optional, tag = "1")]
    pub use_avro_logical_types: ::core::option::Option<bool>,
}
/// Parquet Options for load and make external tables.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ParquetOptions {
    /// Optional. Indicates whether to infer Parquet ENUM logical type as STRING
    /// instead of BYTES by default.
    #[prost(message, optional, tag = "1")]
    pub enum_as_string: ::core::option::Option<bool>,
    /// Optional. Indicates whether to use schema inference specifically for
    /// Parquet LIST logical type.
    #[prost(message, optional, tag = "2")]
    pub enable_list_inference: ::core::option::Option<bool>,
    /// Optional. Indicates how to represent a Parquet map if present.
    #[prost(enumeration = "MapTargetType", tag = "3")]
    pub map_target_type: i32,
}
/// Information related to a CSV data source.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CsvOptions {
    /// Optional. The separator character for fields in a CSV file. The separator
    /// is interpreted as a single byte. For files encoded in ISO-8859-1, any
    /// single character can be used as a separator. For files encoded in UTF-8,
    /// characters represented in decimal range 1-127 (U+0001-U+007F) can be used
    /// without any modification. UTF-8 characters encoded with multiple bytes
    /// (i.e. U+0080 and above) will have only the first byte used for separating
    /// fields. The remaining bytes will be treated as a part of the field.
    /// BigQuery also supports the escape sequence "\t" (U+0009) to specify a tab
    /// separator. The default value is comma (",", U+002C).
    #[prost(string, tag = "1")]
    pub field_delimiter: ::prost::alloc::string::String,
    /// Optional. The number of rows at the top of a CSV file that BigQuery will
    /// skip when reading the data. The default value is 0. This property is
    /// useful if you have header rows in the file that should be skipped.
    /// When autodetect is on, the behavior is the following:
    ///
    /// * skipLeadingRows unspecified - Autodetect tries to detect headers in the
    ///    first row. If they are not detected, the row is read as data. Otherwise
    ///    data is read starting from the second row.
    /// * skipLeadingRows is 0 - Instructs autodetect that there are no headers and
    ///    data should be read starting from the first row.
    /// * skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect
    ///    headers in row N. If headers are not detected, row N is just skipped.
    ///    Otherwise row N is used to extract column names for the detected schema.
    #[prost(message, optional, tag = "2")]
    pub skip_leading_rows: ::core::option::Option<i64>,
    /// Optional. The value that is used to quote data sections in a CSV file.
    /// BigQuery converts the string to ISO-8859-1 encoding, and then uses the
    /// first byte of the encoded string to split the data in its raw, binary
    /// state.
    /// The default value is a double-quote (").
    /// If your data does not contain quoted sections,
    /// set the property value to an empty string.
    /// If your data contains quoted newline characters, you must also set the
    /// allowQuotedNewlines property to true.
    /// To include the specific quote character within a quoted value, precede it
    /// with an additional matching quote character. For example, if you want to
    /// escape the default character  ' " ', use ' "" '.
    #[prost(message, optional, tag = "3")]
    pub quote: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Indicates if BigQuery should allow quoted data sections that
    /// contain newline characters in a CSV file. The default value is false.
    #[prost(message, optional, tag = "4")]
    pub allow_quoted_newlines: ::core::option::Option<bool>,
    /// Optional. Indicates if BigQuery should accept rows that are missing
    /// trailing optional columns. If true, BigQuery treats missing trailing
    /// columns as null values.
    /// If false, records with missing trailing columns are treated as bad records,
    /// and if there are too many bad records, an invalid error is returned in the
    /// job result. The default value is false.
    #[prost(message, optional, tag = "5")]
    pub allow_jagged_rows: ::core::option::Option<bool>,
    /// Optional. The character encoding of the data.
    /// The supported values are UTF-8, ISO-8859-1, UTF-16BE, UTF-16LE, UTF-32BE,
    /// and UTF-32LE.  The default value is UTF-8.
    /// BigQuery decodes the data after the raw, binary data has been split using
    /// the values of the quote and fieldDelimiter properties.
    #[prost(string, tag = "6")]
    pub encoding: ::prost::alloc::string::String,
    /// Optional. Indicates if the embedded ASCII control characters (the first 32
    /// characters in the ASCII-table, from '\x00' to '\x1F') are preserved.
    #[prost(message, optional, tag = "7")]
    pub preserve_ascii_control_characters: ::core::option::Option<bool>,
    /// Optional. Specifies a string that represents a null value in a CSV file.
    /// For example, if you specify "\N", BigQuery interprets "\N" as a null value
    /// when querying a CSV file.
    /// The default value is the empty string. If you set this property to a custom
    /// value, BigQuery throws an error if an empty string is present for all data
    /// types except for STRING and BYTE. For STRING and BYTE columns, BigQuery
    /// interprets the empty string as an empty value.
    #[prost(message, optional, tag = "8")]
    pub null_marker: ::core::option::Option<::prost::alloc::string::String>,
}
/// Json Options for load and make external tables.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JsonOptions {
    /// Optional. The character encoding of the data.
    /// The supported values are UTF-8, UTF-16BE, UTF-16LE, UTF-32BE,
    /// and UTF-32LE.  The default value is UTF-8.
    #[prost(string, tag = "1")]
    pub encoding: ::prost::alloc::string::String,
}
/// Information related to a Bigtable column.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BigtableColumn {
    /// \[Required\] Qualifier of the column.
    /// Columns in the parent column family that has this exact qualifier are
    /// exposed as `<family field name>.<column field name>` field.
    /// If the qualifier is valid UTF-8 string, it can be specified in the
    /// qualifier_string field.  Otherwise, a base-64 encoded value must be set to
    /// qualifier_encoded.
    /// The column field name is the same as the column qualifier. However, if the
    /// qualifier is not a valid BigQuery field identifier i.e. does not match
    /// [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as field_name.
    #[prost(message, optional, tag = "1")]
    pub qualifier_encoded: ::core::option::Option<::prost::alloc::vec::Vec<u8>>,
    /// Qualifier string.
    #[prost(message, optional, tag = "2")]
    pub qualifier_string: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. If the qualifier is not a valid BigQuery field identifier i.e.
    /// does not match [a-zA-Z][a-zA-Z0-9_]*,  a valid identifier must be provided
    /// as the column field name and is used as field name in queries.
    #[prost(string, tag = "3")]
    pub field_name: ::prost::alloc::string::String,
    /// Optional. The type to convert the value in cells of this column.
    /// The values are expected to be encoded using HBase Bytes.toBytes function
    /// when using the BINARY encoding value.
    /// Following BigQuery types are allowed (case-sensitive):
    ///
    /// * BYTES
    /// * STRING
    /// * INTEGER
    /// * FLOAT
    /// * BOOLEAN
    /// * JSON
    ///
    /// Default type is BYTES.
    /// 'type' can also be set at the column family level. However, the setting at
    /// this level takes precedence if 'type' is set at both levels.
    #[prost(string, tag = "4")]
    pub r#type: ::prost::alloc::string::String,
    /// Optional. The encoding of the values when the type is not STRING.
    /// Acceptable encoding values are:
    ///    TEXT - indicates values are alphanumeric text strings.
    ///    BINARY - indicates values are encoded using HBase Bytes.toBytes family of
    ///             functions.
    /// 'encoding' can also be set at the column family level. However, the setting
    /// at this level takes precedence if 'encoding' is set at both levels.
    #[prost(string, tag = "5")]
    pub encoding: ::prost::alloc::string::String,
    /// Optional. If this is set, only the latest version of value in this column
    ///              are exposed.
    /// 'onlyReadLatest' can also be set at the column family level. However, the
    /// setting at this level takes precedence if 'onlyReadLatest' is set at both
    /// levels.
    #[prost(message, optional, tag = "6")]
    pub only_read_latest: ::core::option::Option<bool>,
}
/// Information related to a Bigtable column family.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BigtableColumnFamily {
    /// Identifier of the column family.
    #[prost(string, tag = "1")]
    pub family_id: ::prost::alloc::string::String,
    /// Optional. The type to convert the value in cells of this column family.
    /// The values are expected to be encoded using HBase Bytes.toBytes function
    /// when using the BINARY encoding value.
    /// Following BigQuery types are allowed (case-sensitive):
    ///
    /// * BYTES
    /// * STRING
    /// * INTEGER
    /// * FLOAT
    /// * BOOLEAN
    /// * JSON
    ///
    /// Default type is BYTES.
    /// This can be overridden for a specific column by listing that column in
    /// 'columns' and specifying a type for it.
    #[prost(string, tag = "2")]
    pub r#type: ::prost::alloc::string::String,
    /// Optional. The encoding of the values when the type is not STRING.
    /// Acceptable encoding values are:
    ///    TEXT - indicates values are alphanumeric text strings.
    ///    BINARY - indicates values are encoded using HBase Bytes.toBytes family of
    ///             functions.
    /// This can be overridden for a specific column by listing that column in
    /// 'columns' and specifying an encoding for it.
    #[prost(string, tag = "3")]
    pub encoding: ::prost::alloc::string::String,
    /// Optional. Lists of columns that should be exposed as individual fields as
    /// opposed to a list of (column name, value) pairs.
    /// All columns whose qualifier matches a qualifier in this list can be
    /// accessed as `<family field name>.<column field name>`.
    /// Other columns can be accessed as a list through
    /// the `<family field name>.Column` field.
    #[prost(message, repeated, tag = "4")]
    pub columns: ::prost::alloc::vec::Vec<BigtableColumn>,
    /// Optional. If this is set only the latest version of value are exposed for
    /// all columns in this column family.
    /// This can be overridden for a specific column by listing that column in
    /// 'columns' and specifying a different setting
    /// for that column.
    #[prost(message, optional, tag = "5")]
    pub only_read_latest: ::core::option::Option<bool>,
}
/// Options specific to Google Cloud Bigtable data sources.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BigtableOptions {
    /// Optional. List of column families to expose in the table schema along with
    /// their types.
    /// This list restricts the column families that can be referenced in queries
    /// and specifies their value types.
    /// You can use this list to do type conversions - see the 'type' field for
    /// more details.
    /// If you leave this list empty, all column families are present in the table
    /// schema and their values are read as BYTES.
    /// During a query only the column families referenced in that query are read
    /// from Bigtable.
    #[prost(message, repeated, tag = "1")]
    pub column_families: ::prost::alloc::vec::Vec<BigtableColumnFamily>,
    /// Optional. If field is true, then the column families that are not
    /// specified in columnFamilies list are not exposed in the table schema.
    /// Otherwise, they are read with BYTES type values.
    /// The default value is false.
    #[prost(message, optional, tag = "2")]
    pub ignore_unspecified_column_families: ::core::option::Option<bool>,
    /// Optional. If field is true, then the rowkey column families will be read
    /// and converted to string. Otherwise they are read with BYTES type values and
    /// users need to manually cast them with CAST if necessary.
    /// The default value is false.
    #[prost(message, optional, tag = "3")]
    pub read_rowkey_as_string: ::core::option::Option<bool>,
    /// Optional. If field is true, then each column family will be read as a
    /// single JSON column. Otherwise they are read as a repeated cell structure
    /// containing timestamp/value tuples. The default value is false.
    #[prost(message, optional, tag = "4")]
    pub output_column_families_as_json: ::core::option::Option<bool>,
}
/// Options specific to Google Sheets data sources.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GoogleSheetsOptions {
    /// Optional. The number of rows at the top of a sheet that BigQuery will skip
    /// when reading the data. The default value is 0. This property is useful if
    /// you have header rows that should be skipped. When autodetect is on,
    /// the behavior is the following:
    /// * skipLeadingRows unspecified - Autodetect tries to detect headers in the
    ///    first row. If they are not detected, the row is read as data. Otherwise
    ///    data is read starting from the second row.
    /// * skipLeadingRows is 0 - Instructs autodetect that there are no headers and
    ///    data should be read starting from the first row.
    /// * skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect
    ///    headers in row N. If headers are not detected, row N is just skipped.
    ///    Otherwise row N is used to extract column names for the detected schema.
    #[prost(message, optional, tag = "1")]
    pub skip_leading_rows: ::core::option::Option<i64>,
    /// Optional. Range of a sheet to query from. Only used when non-empty.
    /// Typical format: sheet_name!top_left_cell_id:bottom_right_cell_id
    /// For example: sheet1!A1:B20
    #[prost(string, tag = "2")]
    pub range: ::prost::alloc::string::String,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExternalDataConfiguration {
    /// \[Required\] The fully-qualified URIs that point to your data in Google
    /// Cloud. For Google Cloud Storage URIs:
    ///    Each URI can contain one '*' wildcard character and it must come after
    ///    the 'bucket' name.
    ///    Size limits related to load jobs apply to external data sources.
    /// For Google Cloud Bigtable URIs:
    ///    Exactly one URI can be specified and it has be a fully specified and
    ///    valid HTTPS URL for a Google Cloud Bigtable table.
    /// For Google Cloud Datastore backups, exactly one URI can be specified. Also,
    /// the '*' wildcard character is not allowed.
    #[prost(string, repeated, tag = "1")]
    pub source_uris: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. Specifies how source URIs are interpreted for constructing the
    /// file set to load.  By default source URIs are expanded against the
    /// underlying storage.  Other options include specifying manifest files. Only
    /// applicable to object storage systems.
    #[prost(enumeration = "FileSetSpecType", tag = "25")]
    pub file_set_spec_type: i32,
    /// Optional. The schema for the data.
    /// Schema is required for CSV and JSON formats if autodetect is not on.
    /// Schema is disallowed for Google Cloud Bigtable, Cloud Datastore backups,
    /// Avro, ORC and Parquet formats.
    #[prost(message, optional, tag = "2")]
    pub schema: ::core::option::Option<TableSchema>,
    /// \[Required\] The data format.
    /// For CSV files, specify "CSV".
    /// For Google sheets, specify "GOOGLE_SHEETS".
    /// For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON".
    /// For Avro files, specify "AVRO".
    /// For Google Cloud Datastore backups, specify "DATASTORE_BACKUP".
    /// For Apache Iceberg tables, specify "ICEBERG".
    /// For ORC files, specify "ORC".
    /// For Parquet files, specify "PARQUET".
    /// \[Beta\] For Google Cloud Bigtable, specify "BIGTABLE".
    #[prost(string, tag = "3")]
    pub source_format: ::prost::alloc::string::String,
    /// Optional. The maximum number of bad records that BigQuery can ignore when
    /// reading data. If the number of bad records exceeds this value, an invalid
    /// error is returned in the job result. The default value is 0, which requires
    /// that all records are valid. This setting is ignored for Google Cloud
    /// Bigtable, Google Cloud Datastore backups, Avro, ORC and Parquet formats.
    #[prost(message, optional, tag = "4")]
    pub max_bad_records: ::core::option::Option<i32>,
    /// Try to detect schema and format options automatically.
    /// Any option specified explicitly will be honored.
    #[prost(message, optional, tag = "5")]
    pub autodetect: ::core::option::Option<bool>,
    /// Optional. Indicates if BigQuery should allow extra values that are not
    /// represented in the table schema.
    /// If true, the extra values are ignored.
    /// If false, records with extra columns are treated as bad records, and if
    /// there are too many bad records, an invalid error is returned in the job
    /// result.
    /// The default value is false.
    /// The sourceFormat property determines what BigQuery treats as an extra
    /// value:
    ///    CSV: Trailing columns
    ///    JSON: Named values that don't match any column names
    ///    Google Cloud Bigtable: This setting is ignored.
    ///    Google Cloud Datastore backups: This setting is ignored.
    ///    Avro: This setting is ignored.
    ///    ORC: This setting is ignored.
    ///    Parquet: This setting is ignored.
    #[prost(message, optional, tag = "6")]
    pub ignore_unknown_values: ::core::option::Option<bool>,
    /// Optional. The compression type of the data source.
    /// Possible values include GZIP and NONE. The default value is NONE.
    /// This setting is ignored for Google Cloud Bigtable, Google Cloud Datastore
    /// backups, Avro, ORC and Parquet
    /// formats. An empty string is an invalid value.
    #[prost(string, tag = "7")]
    pub compression: ::prost::alloc::string::String,
    /// Optional. Additional properties to set if sourceFormat is set to CSV.
    #[prost(message, optional, tag = "8")]
    pub csv_options: ::core::option::Option<CsvOptions>,
    /// Optional. Additional properties to set if sourceFormat is set to JSON.
    #[prost(message, optional, tag = "26")]
    pub json_options: ::core::option::Option<JsonOptions>,
    /// Optional. Additional options if sourceFormat is set to BIGTABLE.
    #[prost(message, optional, tag = "9")]
    pub bigtable_options: ::core::option::Option<BigtableOptions>,
    /// Optional. Additional options if sourceFormat is set to GOOGLE_SHEETS.
    #[prost(message, optional, tag = "10")]
    pub google_sheets_options: ::core::option::Option<GoogleSheetsOptions>,
    /// Optional. When set, configures hive partitioning support. Not all storage
    /// formats support hive partitioning -- requesting hive partitioning on an
    /// unsupported format will lead to an error, as will providing an invalid
    /// specification.
    #[prost(message, optional, tag = "13")]
    pub hive_partitioning_options: ::core::option::Option<HivePartitioningOptions>,
    /// Optional. The connection specifying the credentials to be used to read
    /// external storage, such as Azure Blob, Cloud Storage, or S3. The
    /// connection_id can have the form
    /// `{project_id}.{location_id};{connection_id}` or
    /// `projects/{project_id}/locations/{location_id}/connections/{connection_id}`.
    #[prost(string, tag = "14")]
    pub connection_id: ::prost::alloc::string::String,
    /// Defines the list of possible SQL data types to which the source decimal
    /// values are converted. This list and the precision and the scale parameters
    /// of the decimal field determine the target type. In the order of NUMERIC,
    /// BIGNUMERIC, and STRING, a
    /// type is picked if it is in the specified list and if it supports the
    /// precision and the scale. STRING supports all precision and scale values.
    /// If none of the listed types supports the precision and the scale, the type
    /// supporting the widest range in the specified list is picked, and if a value
    /// exceeds the supported range when reading the data, an error will be thrown.
    ///
    /// Example: Suppose the value of this field is \["NUMERIC", "BIGNUMERIC"\].
    /// If (precision,scale) is:
    ///
    /// * (38,9) -> NUMERIC;
    /// * (39,9) -> BIGNUMERIC (NUMERIC cannot hold 30 integer digits);
    /// * (38,10) -> BIGNUMERIC (NUMERIC cannot hold 10 fractional digits);
    /// * (76,38) -> BIGNUMERIC;
    /// * (77,38) -> BIGNUMERIC (error if value exeeds supported range).
    ///
    /// This field cannot contain duplicate types. The order of the types in this
    /// field is ignored. For example, \["BIGNUMERIC", "NUMERIC"\] is the same as
    /// \["NUMERIC", "BIGNUMERIC"\] and NUMERIC always takes precedence over
    /// BIGNUMERIC.
    ///
    /// Defaults to \["NUMERIC", "STRING"\] for ORC and \["NUMERIC"\] for the other
    /// file formats.
    #[prost(enumeration = "DecimalTargetType", repeated, tag = "16")]
    pub decimal_target_types: ::prost::alloc::vec::Vec<i32>,
    /// Optional. Additional properties to set if sourceFormat is set to AVRO.
    #[prost(message, optional, tag = "17")]
    pub avro_options: ::core::option::Option<AvroOptions>,
    /// Optional. Load option to be used together with source_format
    /// newline-delimited JSON to indicate that a variant of JSON is being loaded.
    /// To load newline-delimited GeoJSON, specify GEOJSON (and source_format must
    /// be set to NEWLINE_DELIMITED_JSON).
    #[prost(enumeration = "JsonExtension", tag = "18")]
    pub json_extension: i32,
    /// Optional. Additional properties to set if sourceFormat is set to PARQUET.
    #[prost(message, optional, tag = "19")]
    pub parquet_options: ::core::option::Option<ParquetOptions>,
    /// Optional. ObjectMetadata is used to create Object Tables. Object Tables
    /// contain a listing of objects (with their metadata) found at the
    /// source_uris. If ObjectMetadata is set, source_format should be omitted.
    ///
    /// Currently SIMPLE is the only supported Object Metadata type.
    #[prost(
        enumeration = "external_data_configuration::ObjectMetadata",
        optional,
        tag = "22"
    )]
    pub object_metadata: ::core::option::Option<i32>,
    /// Optional. When creating an external table, the user can provide a reference
    /// file with the table schema. This is enabled for the following formats:
    /// AVRO, PARQUET, ORC.
    #[prost(message, optional, tag = "23")]
    pub reference_file_schema_uri: ::core::option::Option<
        ::prost::alloc::string::String,
    >,
    /// Optional. Metadata Cache Mode for the table. Set this to enable caching of
    /// metadata from external data source.
    #[prost(enumeration = "external_data_configuration::MetadataCacheMode", tag = "24")]
    pub metadata_cache_mode: i32,
}
/// Nested message and enum types in `ExternalDataConfiguration`.
pub mod external_data_configuration {
    /// Supported Object Metadata Types.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum ObjectMetadata {
        /// Unspecified by default.
        Unspecified = 0,
        /// A synonym for `SIMPLE`.
        Directory = 1,
        /// Directory listing of objects.
        Simple = 2,
    }
    impl ObjectMetadata {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "OBJECT_METADATA_UNSPECIFIED",
                Self::Directory => "DIRECTORY",
                Self::Simple => "SIMPLE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "OBJECT_METADATA_UNSPECIFIED" => Some(Self::Unspecified),
                "DIRECTORY" => Some(Self::Directory),
                "SIMPLE" => Some(Self::Simple),
                _ => None,
            }
        }
    }
    /// MetadataCacheMode identifies if the table should use metadata caching for
    /// files from external source (eg Google Cloud Storage).
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum MetadataCacheMode {
        /// Unspecified metadata cache mode.
        Unspecified = 0,
        /// Set this mode to trigger automatic background refresh of metadata cache
        /// from the external source. Queries will use the latest available cache
        /// version within the table's maxStaleness interval.
        Automatic = 1,
        /// Set this mode to enable triggering manual refresh of the metadata cache
        /// from external source. Queries will use the latest manually triggered
        /// cache version within the table's maxStaleness interval.
        Manual = 2,
    }
    impl MetadataCacheMode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "METADATA_CACHE_MODE_UNSPECIFIED",
                Self::Automatic => "AUTOMATIC",
                Self::Manual => "MANUAL",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "METADATA_CACHE_MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "AUTOMATIC" => Some(Self::Automatic),
                "MANUAL" => Some(Self::Manual),
                _ => None,
            }
        }
    }
}
/// Id path of a model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelReference {
    /// Required. The ID of the project containing this model.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. The ID of the dataset containing this model.
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. The ID of the model. The ID must contain only
    /// letters (a-z, A-Z), numbers (0-9), or underscores (_). The maximum
    /// length is 1,024 characters.
    #[prost(string, tag = "3")]
    pub model_id: ::prost::alloc::string::String,
}
/// The type of a struct parameter.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryParameterStructType {
    /// Optional. The name of this field.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The type of this field.
    #[prost(message, optional, tag = "2")]
    pub r#type: ::core::option::Option<QueryParameterType>,
    /// Optional. Human-oriented description of the field.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
}
/// The type of a query parameter.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryParameterType {
    /// Required. The top level type of this field.
    #[prost(string, tag = "1")]
    pub r#type: ::prost::alloc::string::String,
    /// Optional. The type of the array's elements, if this is an array.
    #[prost(message, optional, boxed, tag = "2")]
    pub array_type: ::core::option::Option<
        ::prost::alloc::boxed::Box<QueryParameterType>,
    >,
    /// Optional. The types of the fields of this struct, in order, if this is a
    /// struct.
    #[prost(message, repeated, tag = "3")]
    pub struct_types: ::prost::alloc::vec::Vec<QueryParameterStructType>,
    /// Optional. The element type of the range, if this is a range.
    #[prost(message, optional, boxed, tag = "4")]
    pub range_element_type: ::core::option::Option<
        ::prost::alloc::boxed::Box<QueryParameterType>,
    >,
}
/// Represents the value of a range.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RangeValue {
    /// Optional. The start value of the range. A missing value represents an
    /// unbounded start.
    #[prost(message, optional, boxed, tag = "1")]
    pub start: ::core::option::Option<::prost::alloc::boxed::Box<QueryParameterValue>>,
    /// Optional. The end value of the range. A missing value represents an
    /// unbounded end.
    #[prost(message, optional, boxed, tag = "2")]
    pub end: ::core::option::Option<::prost::alloc::boxed::Box<QueryParameterValue>>,
}
/// The value of a query parameter.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryParameterValue {
    /// Optional. The value of this value, if a simple scalar type.
    #[prost(message, optional, tag = "1")]
    pub value: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. The array values, if this is an array type.
    #[prost(message, repeated, tag = "2")]
    pub array_values: ::prost::alloc::vec::Vec<QueryParameterValue>,
    /// The struct field values.
    #[prost(map = "string, message", tag = "3")]
    pub struct_values: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        QueryParameterValue,
    >,
    /// Optional. The range value, if this is a range type.
    #[prost(message, optional, boxed, tag = "6")]
    pub range_value: ::core::option::Option<::prost::alloc::boxed::Box<RangeValue>>,
    /// This field should not be used.
    #[prost(message, repeated, tag = "5")]
    pub alt_struct_values: ::prost::alloc::vec::Vec<::prost_types::Value>,
}
/// A parameter given to a query.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryParameter {
    /// Optional. If unset, this is a positional parameter. Otherwise, should be
    /// unique within a query.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The type of this parameter.
    #[prost(message, optional, tag = "2")]
    pub parameter_type: ::core::option::Option<QueryParameterType>,
    /// Required. The value of this parameter.
    #[prost(message, optional, tag = "3")]
    pub parameter_value: ::core::option::Option<QueryParameterValue>,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RangePartitioning {
    /// Required. The name of the column to partition the table on. It must be a
    /// top-level, INT64 column whose mode is NULLABLE or REQUIRED.
    #[prost(string, tag = "1")]
    pub field: ::prost::alloc::string::String,
    /// Defines the ranges for range partitioning.
    #[prost(message, optional, tag = "2")]
    pub range: ::core::option::Option<range_partitioning::Range>,
}
/// Nested message and enum types in `RangePartitioning`.
pub mod range_partitioning {
    /// Defines the ranges for range partitioning.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Range {
        /// Required. The start of range partitioning, inclusive. This field is an
        /// INT64 value represented as a string.
        #[prost(string, tag = "1")]
        pub start: ::prost::alloc::string::String,
        /// Required. The end of range partitioning, exclusive. This field is an
        /// INT64 value represented as a string.
        #[prost(string, tag = "2")]
        pub end: ::prost::alloc::string::String,
        /// Required. The width of each interval. This field is an INT64 value
        /// represented as a string.
        #[prost(string, tag = "3")]
        pub interval: ::prost::alloc::string::String,
    }
}
/// The data type of a variable such as a function argument.
/// Examples include:
///
/// * INT64: `{"typeKind": "INT64"}`
///
/// * ARRAY<STRING>:
///
///      {
///        "typeKind": "ARRAY",
///        "arrayElementType": {"typeKind": "STRING"}
///      }
///
/// * STRUCT<x STRING, y ARRAY<DATE>>:
///
///      {
///        "typeKind": "STRUCT",
///        "structType":
///        {
///          "fields":
///          [
///            {
///              "name": "x",
///              "type": {"typeKind": "STRING"}
///            },
///            {
///              "name": "y",
///              "type":
///              {
///                "typeKind": "ARRAY",
///                "arrayElementType": {"typeKind": "DATE"}
///              }
///            }
///          ]
///        }
///      }
///
/// * RANGE<DATE>:
///
///      {
///        "typeKind": "RANGE",
///        "rangeElementType": {"typeKind": "DATE"}
///      }
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StandardSqlDataType {
    /// Required. The top level type of this field.
    /// Can be any GoogleSQL data type (e.g., "INT64", "DATE", "ARRAY").
    #[prost(enumeration = "standard_sql_data_type::TypeKind", tag = "1")]
    pub type_kind: i32,
    /// For complex types, the sub type information.
    #[prost(oneof = "standard_sql_data_type::SubType", tags = "2, 3, 4")]
    pub sub_type: ::core::option::Option<standard_sql_data_type::SubType>,
}
/// Nested message and enum types in `StandardSqlDataType`.
pub mod standard_sql_data_type {
    /// The kind of the datatype.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum TypeKind {
        /// Invalid type.
        Unspecified = 0,
        /// Encoded as a string in decimal format.
        Int64 = 2,
        /// Encoded as a boolean "false" or "true".
        Bool = 5,
        /// Encoded as a number, or string "NaN", "Infinity" or "-Infinity".
        Float64 = 7,
        /// Encoded as a string value.
        String = 8,
        /// Encoded as a base64 string per RFC 4648, section 4.
        Bytes = 9,
        /// Encoded as an RFC 3339 timestamp with mandatory "Z" time zone string:
        /// 1985-04-12T23:20:50.52Z
        Timestamp = 19,
        /// Encoded as RFC 3339 full-date format string: 1985-04-12
        Date = 10,
        /// Encoded as RFC 3339 partial-time format string: 23:20:50.52
        Time = 20,
        /// Encoded as RFC 3339 full-date "T" partial-time: 1985-04-12T23:20:50.52
        Datetime = 21,
        /// Encoded as fully qualified 3 part: 0-5 15 2:30:45.6
        Interval = 26,
        /// Encoded as WKT
        Geography = 22,
        /// Encoded as a decimal string.
        Numeric = 23,
        /// Encoded as a decimal string.
        Bignumeric = 24,
        /// Encoded as a string.
        Json = 25,
        /// Encoded as a list with types matching Type.array_type.
        Array = 16,
        /// Encoded as a list with fields of type Type.struct_type\[i\]. List is used
        /// because a JSON object cannot have duplicate field names.
        Struct = 17,
        /// Encoded as a pair with types matching range_element_type. Pairs must
        /// begin with "[", end with ")", and be separated by ", ".
        Range = 29,
    }
    impl TypeKind {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "TYPE_KIND_UNSPECIFIED",
                Self::Int64 => "INT64",
                Self::Bool => "BOOL",
                Self::Float64 => "FLOAT64",
                Self::String => "STRING",
                Self::Bytes => "BYTES",
                Self::Timestamp => "TIMESTAMP",
                Self::Date => "DATE",
                Self::Time => "TIME",
                Self::Datetime => "DATETIME",
                Self::Interval => "INTERVAL",
                Self::Geography => "GEOGRAPHY",
                Self::Numeric => "NUMERIC",
                Self::Bignumeric => "BIGNUMERIC",
                Self::Json => "JSON",
                Self::Array => "ARRAY",
                Self::Struct => "STRUCT",
                Self::Range => "RANGE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "TYPE_KIND_UNSPECIFIED" => Some(Self::Unspecified),
                "INT64" => Some(Self::Int64),
                "BOOL" => Some(Self::Bool),
                "FLOAT64" => Some(Self::Float64),
                "STRING" => Some(Self::String),
                "BYTES" => Some(Self::Bytes),
                "TIMESTAMP" => Some(Self::Timestamp),
                "DATE" => Some(Self::Date),
                "TIME" => Some(Self::Time),
                "DATETIME" => Some(Self::Datetime),
                "INTERVAL" => Some(Self::Interval),
                "GEOGRAPHY" => Some(Self::Geography),
                "NUMERIC" => Some(Self::Numeric),
                "BIGNUMERIC" => Some(Self::Bignumeric),
                "JSON" => Some(Self::Json),
                "ARRAY" => Some(Self::Array),
                "STRUCT" => Some(Self::Struct),
                "RANGE" => Some(Self::Range),
                _ => None,
            }
        }
    }
    /// For complex types, the sub type information.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum SubType {
        /// The type of the array's elements, if type_kind = "ARRAY".
        #[prost(message, tag = "2")]
        ArrayElementType(::prost::alloc::boxed::Box<super::StandardSqlDataType>),
        /// The fields of this struct, in order, if type_kind = "STRUCT".
        #[prost(message, tag = "3")]
        StructType(super::StandardSqlStructType),
        /// The type of the range's elements, if type_kind = "RANGE".
        #[prost(message, tag = "4")]
        RangeElementType(::prost::alloc::boxed::Box<super::StandardSqlDataType>),
    }
}
/// A field or a column.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StandardSqlField {
    /// Optional. The name of this field. Can be absent for struct fields.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The type of this parameter. Absent if not explicitly
    /// specified (e.g., CREATE FUNCTION statement can omit the return type;
    /// in this case the output parameter does not have this "type" field).
    #[prost(message, optional, tag = "2")]
    pub r#type: ::core::option::Option<StandardSqlDataType>,
}
/// The representation of a SQL STRUCT type.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StandardSqlStructType {
    /// Fields within the struct.
    #[prost(message, repeated, tag = "1")]
    pub fields: ::prost::alloc::vec::Vec<StandardSqlField>,
}
/// A table type
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StandardSqlTableType {
    /// The columns in this table type
    #[prost(message, repeated, tag = "1")]
    pub columns: ::prost::alloc::vec::Vec<StandardSqlField>,
}
/// System variables given to a query.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SystemVariables {
    /// Output only. Data type for each system variable.
    #[prost(map = "string, message", tag = "1")]
    pub types: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        StandardSqlDataType,
    >,
    /// Output only. Value for each system variable.
    #[prost(message, optional, tag = "2")]
    pub values: ::core::option::Option<::prost_types::Struct>,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TimePartitioning {
    /// Required. The supported types are DAY, HOUR, MONTH, and YEAR, which will
    /// generate one partition per day, hour, month, and year, respectively.
    #[prost(string, tag = "1")]
    pub r#type: ::prost::alloc::string::String,
    /// Optional. Number of milliseconds for which to keep the storage for a
    /// partition.
    /// A wrapper is used here because 0 is an invalid value.
    #[prost(message, optional, tag = "2")]
    pub expiration_ms: ::core::option::Option<i64>,
    /// Optional. If not set, the table is partitioned by pseudo
    /// column '_PARTITIONTIME'; if set, the table is partitioned by this field.
    /// The field must be a top-level TIMESTAMP or DATE field. Its mode must be
    /// NULLABLE or REQUIRED.
    /// A wrapper is used here because an empty string is an invalid value.
    #[prost(message, optional, tag = "3")]
    pub field: ::core::option::Option<::prost::alloc::string::String>,
}
///
/// This is used for defining User Defined Function (UDF) resources only when
/// using legacy SQL.  Users of GoogleSQL should leverage either DDL (e.g.
/// CREATE \[TEMPORARY\] FUNCTION ... ) or the Routines API to define UDF
/// resources.
///
/// For additional information on migrating, see:
/// <https://cloud.google.com/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql#differences_in_user-defined_javascript_functions>
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UserDefinedFunctionResource {
    /// \[Pick one\] A code resource to load from a Google Cloud Storage URI
    /// (gs://bucket/path).
    #[prost(message, optional, tag = "1")]
    pub resource_uri: ::core::option::Option<::prost::alloc::string::String>,
    /// \[Pick one\] An inline resource that contains code for a user-defined
    /// function (UDF). Providing a inline code resource is equivalent to providing
    /// a URI for a file containing the same code.
    #[prost(message, optional, tag = "2")]
    pub inline_code: ::core::option::Option<::prost::alloc::string::String>,
}
/// Properties for the destination table.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DestinationTableProperties {
    /// Optional. Friendly name for the destination table. If the table already
    /// exists, it should be same as the existing friendly name.
    #[prost(message, optional, tag = "1")]
    pub friendly_name: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. The description for the destination table.
    /// This will only be used if the destination table is newly created.
    /// If the table already exists and a value different than the current
    /// description is provided, the job will fail.
    #[prost(message, optional, tag = "2")]
    pub description: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. The labels associated with this table. You can use these to
    /// organize and group your tables. This will only be used if the destination
    /// table is newly created. If the table already exists and labels are
    /// different than the current labels are provided, the job will fail.
    #[prost(map = "string, string", tag = "3")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
/// A connection-level property to customize query behavior. Under JDBC, these
/// correspond directly to connection properties passed to the DriverManager.
/// Under ODBC, these correspond to properties in the connection string.
///
/// Currently supported connection properties:
///
/// * **dataset_project_id**: represents the default project for datasets that
/// are used in the query. Setting the
/// system variable `@@dataset_project_id` achieves the same behavior.  For
/// more information about system variables, see:
/// <https://cloud.google.com/bigquery/docs/reference/system-variables>
///
/// * **time_zone**: represents the default timezone used to run the query.
///
/// * **session_id**: associates the query with a given session.
///
/// * **query_label**: associates the query with a given job label. If set,
/// all subsequent queries in a script or session will have this label. For the
/// format in which a you can specify a query label, see labels
/// in the JobConfiguration resource type:
/// <https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfiguration>
///
/// * **service_account**: indicates the service account to use to run a
/// continuous query. If set, the query job uses the service account to access
/// Google Cloud resources. Service account access is bounded by the IAM
/// permissions that you have granted to the service account.
///
/// Additional properties are allowed, but ignored. Specifying multiple
/// connection properties with the same key returns an error.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ConnectionProperty {
    /// The key of the property to set.
    #[prost(string, tag = "1")]
    pub key: ::prost::alloc::string::String,
    /// The value of the property to set.
    #[prost(string, tag = "2")]
    pub value: ::prost::alloc::string::String,
}
/// JobConfigurationQuery configures a BigQuery query job.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobConfigurationQuery {
    /// \[Required\] SQL query text to execute. The useLegacySql field can be used
    /// to indicate whether the query uses legacy SQL or GoogleSQL.
    #[prost(string, tag = "1")]
    pub query: ::prost::alloc::string::String,
    /// Optional. Describes the table where the query results should be stored.
    /// This property must be set for large results that exceed the maximum
    /// response size.  For queries that produce anonymous (cached) results, this
    /// field will be populated by BigQuery.
    #[prost(message, optional, tag = "2")]
    pub destination_table: ::core::option::Option<TableReference>,
    /// Optional. You can specify external table definitions, which operate as
    /// ephemeral tables that can be queried.  These definitions are configured
    /// using a JSON map, where the string key represents the table identifier, and
    /// the value is the corresponding external data configuration object.
    #[prost(map = "string, message", tag = "23")]
    pub external_table_definitions: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ExternalDataConfiguration,
    >,
    /// Describes user-defined function resources used in the query.
    #[prost(message, repeated, tag = "4")]
    pub user_defined_function_resources: ::prost::alloc::vec::Vec<
        UserDefinedFunctionResource,
    >,
    /// Optional. Specifies whether the job is allowed to create new tables.
    /// The following values are supported:
    ///
    /// * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the
    /// table.
    /// * CREATE_NEVER: The table must already exist. If it does not,
    /// a 'notFound' error is returned in the job result.
    ///
    /// The default value is CREATE_IF_NEEDED.
    /// Creation, truncation and append actions occur as one atomic update
    /// upon job completion.
    #[prost(string, tag = "5")]
    pub create_disposition: ::prost::alloc::string::String,
    /// Optional. Specifies the action that occurs if the destination table
    /// already exists. The following values are supported:
    ///
    /// * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the
    /// data, removes the constraints, and uses the schema from the query result.
    /// * WRITE_APPEND: If the table already exists, BigQuery appends the data to
    /// the table.
    /// * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate'
    /// error is returned in the job result.
    ///
    /// The default value is WRITE_EMPTY. Each action is atomic and only occurs if
    /// BigQuery is able to complete the job successfully. Creation, truncation and
    /// append actions occur as one atomic update upon job completion.
    #[prost(string, tag = "6")]
    pub write_disposition: ::prost::alloc::string::String,
    /// Optional. Specifies the default dataset to use for unqualified
    /// table names in the query. This setting does not alter behavior of
    /// unqualified dataset names. Setting the system variable
    /// `@@dataset_id` achieves the same behavior.  See
    /// <https://cloud.google.com/bigquery/docs/reference/system-variables> for more
    /// information on system variables.
    #[prost(message, optional, tag = "7")]
    pub default_dataset: ::core::option::Option<DatasetReference>,
    /// Optional. Specifies a priority for the query. Possible values include
    /// INTERACTIVE and BATCH. The default value is INTERACTIVE.
    #[prost(string, tag = "8")]
    pub priority: ::prost::alloc::string::String,
    /// Optional. If true and query uses legacy SQL dialect, allows the query
    /// to produce arbitrarily large result tables at a slight cost in performance.
    /// Requires destinationTable to be set.
    /// For GoogleSQL queries, this flag is ignored and large results are
    /// always allowed.  However, you must still set destinationTable when result
    /// size exceeds the allowed maximum response size.
    #[prost(message, optional, tag = "10")]
    pub allow_large_results: ::core::option::Option<bool>,
    /// Optional. Whether to look for the result in the query cache. The query
    /// cache is a best-effort cache that will be flushed whenever tables in the
    /// query are modified. Moreover, the query cache is only available when a
    /// query does not have a destination table specified. The default value is
    /// true.
    #[prost(message, optional, tag = "11")]
    pub use_query_cache: ::core::option::Option<bool>,
    /// Optional. If true and query uses legacy SQL dialect, flattens all nested
    /// and repeated fields in the query results.
    /// allowLargeResults must be true if this is set to false.
    /// For GoogleSQL queries, this flag is ignored and results are never
    /// flattened.
    #[prost(message, optional, tag = "12")]
    pub flatten_results: ::core::option::Option<bool>,
    /// Limits the bytes billed for this job. Queries that will have
    /// bytes billed beyond this limit will fail (without incurring a charge).
    /// If unspecified, this will be set to your project default.
    #[prost(message, optional, tag = "14")]
    pub maximum_bytes_billed: ::core::option::Option<i64>,
    /// Optional. Specifies whether to use BigQuery's legacy SQL dialect for this
    /// query. The default value is true. If set to false, the query will use
    /// BigQuery's GoogleSQL:
    /// <https://cloud.google.com/bigquery/sql-reference/>
    ///
    /// When useLegacySql is set to false, the value of flattenResults is ignored;
    /// query will be run as if flattenResults is false.
    #[prost(message, optional, tag = "15")]
    pub use_legacy_sql: ::core::option::Option<bool>,
    /// GoogleSQL only. Set to POSITIONAL to use positional (?) query parameters
    /// or to NAMED to use named (@myparam) query parameters in this query.
    #[prost(string, tag = "16")]
    pub parameter_mode: ::prost::alloc::string::String,
    /// Query parameters for GoogleSQL queries.
    #[prost(message, repeated, tag = "17")]
    pub query_parameters: ::prost::alloc::vec::Vec<QueryParameter>,
    /// Output only. System variables for GoogleSQL queries. A system variable is
    /// output if the variable is settable and its value differs from the system
    /// default.
    /// "@@" prefix is not included in the name of the System variables.
    #[prost(message, optional, tag = "35")]
    pub system_variables: ::core::option::Option<SystemVariables>,
    /// Allows the schema of the destination table to be updated as a side effect
    /// of the query job. Schema update options are supported in two cases:
    /// when writeDisposition is WRITE_APPEND;
    /// when writeDisposition is WRITE_TRUNCATE and the destination table is a
    /// partition of a table, specified by partition decorators. For normal tables,
    /// WRITE_TRUNCATE will always overwrite the schema.
    /// One or more of the following values are specified:
    ///
    /// * ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
    /// * ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original
    /// schema to nullable.
    #[prost(string, repeated, tag = "18")]
    pub schema_update_options: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Time-based partitioning specification for the destination table. Only one
    /// of timePartitioning and rangePartitioning should be specified.
    #[prost(message, optional, tag = "19")]
    pub time_partitioning: ::core::option::Option<TimePartitioning>,
    /// Range partitioning specification for the destination table.
    /// Only one of timePartitioning and rangePartitioning should be specified.
    #[prost(message, optional, tag = "22")]
    pub range_partitioning: ::core::option::Option<RangePartitioning>,
    /// Clustering specification for the destination table.
    #[prost(message, optional, tag = "20")]
    pub clustering: ::core::option::Option<Clustering>,
    /// Custom encryption configuration (e.g., Cloud KMS keys)
    #[prost(message, optional, tag = "21")]
    pub destination_encryption_configuration: ::core::option::Option<
        EncryptionConfiguration,
    >,
    /// Options controlling the execution of scripts.
    #[prost(message, optional, tag = "24")]
    pub script_options: ::core::option::Option<ScriptOptions>,
    /// Connection properties which can modify the query behavior.
    #[prost(message, repeated, tag = "33")]
    pub connection_properties: ::prost::alloc::vec::Vec<ConnectionProperty>,
    /// If this property is true, the job creates a new session using a randomly
    /// generated session_id.  To continue using a created session with
    /// subsequent queries, pass the existing session identifier as a
    /// `ConnectionProperty` value.  The session identifier is returned as part of
    /// the `SessionInfo` message within the query statistics.
    ///
    /// The new session's location will be set to `Job.JobReference.location` if it
    /// is present, otherwise it's set to the default location based on existing
    /// routing logic.
    #[prost(message, optional, tag = "34")]
    pub create_session: ::core::option::Option<bool>,
    /// Optional. Whether to run the query as continuous or a regular query.
    /// Continuous query is currently in experimental stage and not ready for
    /// general usage.
    #[prost(message, optional, tag = "36")]
    pub continuous: ::core::option::Option<bool>,
}
/// Options related to script execution.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ScriptOptions {
    /// Timeout period for each statement in a script.
    #[prost(message, optional, tag = "1")]
    pub statement_timeout_ms: ::core::option::Option<i64>,
    /// Limit on the number of bytes billed per statement. Exceeding this budget
    /// results in an error.
    #[prost(message, optional, tag = "2")]
    pub statement_byte_budget: ::core::option::Option<i64>,
    /// Determines which statement in the script represents the "key result",
    /// used to populate the schema and query results of the script job.
    /// Default is LAST.
    #[prost(enumeration = "script_options::KeyResultStatementKind", tag = "4")]
    pub key_result_statement: i32,
}
/// Nested message and enum types in `ScriptOptions`.
pub mod script_options {
    /// KeyResultStatementKind controls how the key result is determined.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum KeyResultStatementKind {
        /// Default value.
        Unspecified = 0,
        /// The last result determines the key result.
        Last = 1,
        /// The first SELECT statement determines the key result.
        FirstSelect = 2,
    }
    impl KeyResultStatementKind {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "KEY_RESULT_STATEMENT_KIND_UNSPECIFIED",
                Self::Last => "LAST",
                Self::FirstSelect => "FIRST_SELECT",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "KEY_RESULT_STATEMENT_KIND_UNSPECIFIED" => Some(Self::Unspecified),
                "LAST" => Some(Self::Last),
                "FIRST_SELECT" => Some(Self::FirstSelect),
                _ => None,
            }
        }
    }
}
/// JobConfigurationLoad contains the configuration properties for loading data
/// into a destination table.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobConfigurationLoad {
    /// \[Required\] The fully-qualified URIs that point to your data in Google
    /// Cloud.
    /// For Google Cloud Storage URIs:
    ///    Each URI can contain one '*' wildcard character and it must come after
    ///    the 'bucket' name. Size limits related to load jobs apply to external
    ///    data sources.
    /// For Google Cloud Bigtable URIs:
    ///    Exactly one URI can be specified and it has be a fully specified and
    ///    valid HTTPS URL for a Google Cloud Bigtable table.
    /// For Google Cloud Datastore backups:
    ///   Exactly one URI can be specified. Also, the '*' wildcard character is not
    ///   allowed.
    #[prost(string, repeated, tag = "1")]
    pub source_uris: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. Specifies how source URIs are interpreted for constructing the
    /// file set to load. By default, source URIs are expanded against the
    /// underlying storage. You can also specify manifest files to control how the
    /// file set is constructed. This option is only applicable to object storage
    /// systems.
    #[prost(enumeration = "FileSetSpecType", tag = "49")]
    pub file_set_spec_type: i32,
    /// Optional. The schema for the destination table. The schema can be
    /// omitted if the destination table already exists, or if you're loading data
    /// from Google Cloud Datastore.
    #[prost(message, optional, tag = "2")]
    pub schema: ::core::option::Option<TableSchema>,
    /// \[Required\] The destination table to load the data into.
    #[prost(message, optional, tag = "3")]
    pub destination_table: ::core::option::Option<TableReference>,
    /// Optional. \[Experimental\] Properties with which to create the destination
    /// table if it is new.
    #[prost(message, optional, tag = "4")]
    pub destination_table_properties: ::core::option::Option<DestinationTableProperties>,
    /// Optional. Specifies whether the job is allowed to create new tables.
    /// The following values are supported:
    ///
    /// * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the
    /// table.
    /// * CREATE_NEVER: The table must already exist. If it does not,
    /// a 'notFound' error is returned in the job result.
    /// The default value is CREATE_IF_NEEDED.
    /// Creation, truncation and append actions occur as one atomic update
    /// upon job completion.
    #[prost(string, tag = "5")]
    pub create_disposition: ::prost::alloc::string::String,
    /// Optional. Specifies the action that occurs if the destination table
    /// already exists. The following values are supported:
    ///
    /// * WRITE_TRUNCATE:  If the table already exists, BigQuery overwrites the
    /// data, removes the constraints and uses the schema from the load job.
    /// * WRITE_APPEND: If the table already exists, BigQuery appends the data to
    /// the table.
    /// * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate'
    /// error is returned in the job result.
    ///
    /// The default value is WRITE_APPEND.
    /// Each action is atomic and only occurs if BigQuery is able to complete the
    /// job successfully.
    /// Creation, truncation and append actions occur as one atomic update
    /// upon job completion.
    #[prost(string, tag = "6")]
    pub write_disposition: ::prost::alloc::string::String,
    /// Optional. Specifies a string that represents a null value in a CSV file.
    /// For example, if you specify "\N", BigQuery interprets "\N" as a null value
    /// when loading a CSV file.
    /// The default value is the empty string. If you set this property to a custom
    /// value, BigQuery throws an error if an empty string is present for all data
    /// types except for STRING and BYTE. For STRING and BYTE columns, BigQuery
    /// interprets the empty string as an empty value.
    #[prost(message, optional, tag = "7")]
    pub null_marker: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. The separator character for fields in a CSV file. The separator
    /// is interpreted as a single byte. For files encoded in ISO-8859-1, any
    /// single character can be used as a separator. For files encoded in UTF-8,
    /// characters represented in decimal range 1-127 (U+0001-U+007F) can be used
    /// without any modification. UTF-8 characters encoded with multiple bytes
    /// (i.e. U+0080 and above) will have only the first byte used for separating
    /// fields. The remaining bytes will be treated as a part of the field.
    /// BigQuery also supports the escape sequence "\t" (U+0009) to specify a tab
    /// separator. The default value is comma (",", U+002C).
    #[prost(string, tag = "8")]
    pub field_delimiter: ::prost::alloc::string::String,
    /// Optional. The number of rows at the top of a CSV file that BigQuery will
    /// skip when loading the data. The default value is 0. This property is useful
    /// if you have header rows in the file that should be skipped. When autodetect
    /// is on, the behavior is the following:
    ///
    /// * skipLeadingRows unspecified - Autodetect tries to detect headers in the
    ///    first row. If they are not detected, the row is read as data. Otherwise
    ///    data is read starting from the second row.
    /// * skipLeadingRows is 0 - Instructs autodetect that there are no headers and
    ///    data should be read starting from the first row.
    /// * skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect
    ///    headers in row N. If headers are not detected, row N is just skipped.
    ///    Otherwise row N is used to extract column names for the detected schema.
    #[prost(message, optional, tag = "9")]
    pub skip_leading_rows: ::core::option::Option<i32>,
    /// Optional. The character encoding of the data.
    /// The supported values are UTF-8, ISO-8859-1, UTF-16BE, UTF-16LE, UTF-32BE,
    /// and UTF-32LE. The default value is UTF-8. BigQuery decodes the data after
    /// the raw, binary data has been split using the values of the `quote` and
    /// `fieldDelimiter` properties.
    ///
    /// If you don't specify an encoding, or if you specify a UTF-8 encoding when
    /// the CSV file is not UTF-8 encoded, BigQuery attempts to convert the data to
    /// UTF-8. Generally, your data loads successfully, but it may not match
    /// byte-for-byte what you expect. To avoid this, specify the correct encoding
    /// by using the `--encoding` flag.
    ///
    /// If BigQuery can't convert a character other than the ASCII `0` character,
    /// BigQuery converts the character to the standard Unicode replacement
    /// character: &#65533;.
    #[prost(string, tag = "10")]
    pub encoding: ::prost::alloc::string::String,
    /// Optional. The value that is used to quote data sections in a CSV file.
    /// BigQuery converts the string to ISO-8859-1 encoding, and then uses the
    /// first byte of the encoded string to split the data in its raw, binary
    /// state.
    /// The default value is a double-quote ('"').
    /// If your data does not contain quoted sections, set the property value to an
    /// empty string.
    /// If your data contains quoted newline characters, you must also set the
    /// allowQuotedNewlines property to true.
    /// To include the specific quote character within a quoted value, precede it
    /// with an additional matching quote character. For example, if you want to
    /// escape the default character  ' " ', use ' "" '.
    /// @default "
    #[prost(message, optional, tag = "11")]
    pub quote: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. The maximum number of bad records that BigQuery can ignore when
    /// running the job. If the number of bad records exceeds this value, an
    /// invalid error is returned in the job result.
    /// The default value is 0, which requires that all records are valid.
    /// This is only supported for CSV and NEWLINE_DELIMITED_JSON file formats.
    #[prost(message, optional, tag = "12")]
    pub max_bad_records: ::core::option::Option<i32>,
    /// Indicates if BigQuery should allow quoted data sections that contain
    /// newline characters in a CSV file. The default value is false.
    #[prost(message, optional, tag = "15")]
    pub allow_quoted_newlines: ::core::option::Option<bool>,
    /// Optional. The format of the data files.
    /// For CSV files, specify "CSV". For datastore backups,
    /// specify "DATASTORE_BACKUP". For newline-delimited JSON,
    /// specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO".
    /// For parquet, specify "PARQUET". For orc, specify "ORC".
    /// The default value is CSV.
    #[prost(string, tag = "16")]
    pub source_format: ::prost::alloc::string::String,
    /// Optional. Accept rows that are missing trailing optional columns.
    /// The missing values are treated as nulls.
    /// If false, records with missing trailing columns are treated as bad records,
    /// and if there are too many bad records, an invalid error is returned in the
    /// job result.
    /// The default value is false.
    /// Only applicable to CSV, ignored for other formats.
    #[prost(message, optional, tag = "17")]
    pub allow_jagged_rows: ::core::option::Option<bool>,
    /// Optional. Indicates if BigQuery should allow extra values that are not
    /// represented in the table schema.
    /// If true, the extra values are ignored.
    /// If false, records with extra columns are treated as bad records, and if
    /// there are too many bad records, an invalid error is returned in the job
    /// result. The default value is false.
    /// The sourceFormat property determines what BigQuery treats as an extra
    /// value:
    ///    CSV: Trailing columns
    ///    JSON: Named values that don't match any column names in the table schema
    ///    Avro, Parquet, ORC: Fields in the file schema that don't exist in the
    ///    table schema.
    #[prost(message, optional, tag = "18")]
    pub ignore_unknown_values: ::core::option::Option<bool>,
    /// If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity
    /// properties to load into BigQuery from a Cloud Datastore backup. Property
    /// names are case sensitive and must be top-level properties. If no properties
    /// are specified, BigQuery loads all properties. If any named property isn't
    /// found in the Cloud Datastore backup, an invalid error is returned in the
    /// job result.
    #[prost(string, repeated, tag = "19")]
    pub projection_fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. Indicates if we should automatically infer the options and
    /// schema for CSV and JSON sources.
    #[prost(message, optional, tag = "20")]
    pub autodetect: ::core::option::Option<bool>,
    /// Allows the schema of the destination table to be updated as a side effect
    /// of the load job if a schema is autodetected or supplied in the job
    /// configuration.
    /// Schema update options are supported in two cases:
    /// when writeDisposition is WRITE_APPEND;
    /// when writeDisposition is WRITE_TRUNCATE and the destination table is a
    /// partition of a table, specified by partition decorators. For normal tables,
    /// WRITE_TRUNCATE will always overwrite the schema.
    /// One or more of the following values are specified:
    ///
    /// * ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
    /// * ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original
    /// schema to nullable.
    #[prost(string, repeated, tag = "21")]
    pub schema_update_options: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Time-based partitioning specification for the destination table. Only one
    /// of timePartitioning and rangePartitioning should be specified.
    #[prost(message, optional, tag = "22")]
    pub time_partitioning: ::core::option::Option<TimePartitioning>,
    /// Range partitioning specification for the destination table.
    /// Only one of timePartitioning and rangePartitioning should be specified.
    #[prost(message, optional, tag = "26")]
    pub range_partitioning: ::core::option::Option<RangePartitioning>,
    /// Clustering specification for the destination table.
    #[prost(message, optional, tag = "23")]
    pub clustering: ::core::option::Option<Clustering>,
    /// Custom encryption configuration (e.g., Cloud KMS keys)
    #[prost(message, optional, tag = "24")]
    pub destination_encryption_configuration: ::core::option::Option<
        EncryptionConfiguration,
    >,
    /// Optional. If sourceFormat is set to "AVRO", indicates whether to interpret
    /// logical types as the corresponding BigQuery data type (for example,
    /// TIMESTAMP), instead of using the raw type (for example, INTEGER).
    #[prost(message, optional, tag = "25")]
    pub use_avro_logical_types: ::core::option::Option<bool>,
    /// Optional. The user can provide a reference file with the reader schema.
    /// This file is only loaded if it is part of source URIs, but is not loaded
    /// otherwise. It is enabled for the following formats: AVRO, PARQUET, ORC.
    #[prost(message, optional, tag = "45")]
    pub reference_file_schema_uri: ::core::option::Option<
        ::prost::alloc::string::String,
    >,
    /// Optional. When set, configures hive partitioning support.
    /// Not all storage formats support hive partitioning -- requesting hive
    /// partitioning on an unsupported format will lead to an error, as will
    /// providing an invalid specification.
    #[prost(message, optional, tag = "37")]
    pub hive_partitioning_options: ::core::option::Option<HivePartitioningOptions>,
    /// Defines the list of possible SQL data types to which the source decimal
    /// values are converted. This list and the precision and the scale parameters
    /// of the decimal field determine the target type. In the order of NUMERIC,
    /// BIGNUMERIC, and STRING, a
    /// type is picked if it is in the specified list and if it supports the
    /// precision and the scale. STRING supports all precision and scale values.
    /// If none of the listed types supports the precision and the scale, the type
    /// supporting the widest range in the specified list is picked, and if a value
    /// exceeds the supported range when reading the data, an error will be thrown.
    ///
    /// Example: Suppose the value of this field is \["NUMERIC", "BIGNUMERIC"\].
    /// If (precision,scale) is:
    ///
    /// * (38,9) -> NUMERIC;
    /// * (39,9) -> BIGNUMERIC (NUMERIC cannot hold 30 integer digits);
    /// * (38,10) -> BIGNUMERIC (NUMERIC cannot hold 10 fractional digits);
    /// * (76,38) -> BIGNUMERIC;
    /// * (77,38) -> BIGNUMERIC (error if value exeeds supported range).
    ///
    /// This field cannot contain duplicate types. The order of the types in this
    /// field is ignored. For example, \["BIGNUMERIC", "NUMERIC"\] is the same as
    /// \["NUMERIC", "BIGNUMERIC"\] and NUMERIC always takes precedence over
    /// BIGNUMERIC.
    ///
    /// Defaults to \["NUMERIC", "STRING"\] for ORC and \["NUMERIC"\] for the other
    /// file formats.
    #[prost(enumeration = "DecimalTargetType", repeated, tag = "39")]
    pub decimal_target_types: ::prost::alloc::vec::Vec<i32>,
    /// Optional. Load option to be used together with source_format
    /// newline-delimited JSON to indicate that a variant of JSON is being loaded.
    /// To load newline-delimited GeoJSON, specify GEOJSON (and source_format must
    /// be set to NEWLINE_DELIMITED_JSON).
    #[prost(enumeration = "JsonExtension", tag = "41")]
    pub json_extension: i32,
    /// Optional. Additional properties to set if sourceFormat is set to PARQUET.
    #[prost(message, optional, tag = "42")]
    pub parquet_options: ::core::option::Option<ParquetOptions>,
    /// Optional. When sourceFormat is set to "CSV", this indicates whether the
    /// embedded ASCII control characters (the first 32 characters in the
    /// ASCII-table, from
    /// '\x00' to '\x1F') are preserved.
    #[prost(message, optional, tag = "44")]
    pub preserve_ascii_control_characters: ::core::option::Option<bool>,
    /// Optional. Connection properties which can modify the load job behavior.
    /// Currently, only the 'session_id' connection property is supported, and is
    /// used to resolve _SESSION appearing as the dataset id.
    #[prost(message, repeated, tag = "46")]
    pub connection_properties: ::prost::alloc::vec::Vec<ConnectionProperty>,
    /// Optional. If this property is true, the job creates a new session using a
    /// randomly generated session_id.  To continue using a created session with
    /// subsequent queries, pass the existing session identifier as a
    /// `ConnectionProperty` value.  The session identifier is returned as part of
    /// the `SessionInfo` message within the query statistics.
    ///
    /// The new session's location will be set to `Job.JobReference.location` if it
    /// is present, otherwise it's set to the default location based on existing
    /// routing logic.
    #[prost(message, optional, tag = "47")]
    pub create_session: ::core::option::Option<bool>,
    /// Optional. Character map supported for column names in CSV/Parquet loads.
    /// Defaults to STRICT and can be overridden by Project Config Service. Using
    /// this option with unsupporting load formats will result in an error.
    #[prost(enumeration = "job_configuration_load::ColumnNameCharacterMap", tag = "50")]
    pub column_name_character_map: i32,
    /// Optional. \[Experimental\] Configures the load job to copy files directly to
    /// the destination BigLake managed table, bypassing file content reading and
    /// rewriting.
    ///
    /// Copying files only is supported when all the following are true:
    ///
    /// * `source_uris` are located in the same Cloud Storage location as the
    ///    destination table's `storage_uri` location.
    /// * `source_format` is `PARQUET`.
    /// * `destination_table` is an existing BigLake managed table. The table's
    ///    schema does not have flexible column names. The table's columns do not
    ///    have type parameters other than precision and scale.
    /// * No options other than the above are specified.
    #[prost(message, optional, tag = "51")]
    pub copy_files_only: ::core::option::Option<bool>,
}
/// Nested message and enum types in `JobConfigurationLoad`.
pub mod job_configuration_load {
    /// Indicates the character map used for column names.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum ColumnNameCharacterMap {
        /// Unspecified column name character map.
        Unspecified = 0,
        /// Support flexible column name and reject invalid column names.
        Strict = 1,
        /// Support alphanumeric + underscore characters and names must start with a
        /// letter or underscore. Invalid column names will be normalized.
        V1 = 2,
        /// Support flexible column name. Invalid column names will be normalized.
        V2 = 3,
    }
    impl ColumnNameCharacterMap {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "COLUMN_NAME_CHARACTER_MAP_UNSPECIFIED",
                Self::Strict => "STRICT",
                Self::V1 => "V1",
                Self::V2 => "V2",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "COLUMN_NAME_CHARACTER_MAP_UNSPECIFIED" => Some(Self::Unspecified),
                "STRICT" => Some(Self::Strict),
                "V1" => Some(Self::V1),
                "V2" => Some(Self::V2),
                _ => None,
            }
        }
    }
}
/// JobConfigurationTableCopy configures a job that copies data from one table
/// to another.
/// For more information on copying tables, see [Copy a
///   table](<https://cloud.google.com/bigquery/docs/managing-tables#copy-table>).
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobConfigurationTableCopy {
    /// \[Pick one\] Source table to copy.
    #[prost(message, optional, tag = "1")]
    pub source_table: ::core::option::Option<TableReference>,
    /// \[Pick one\] Source tables to copy.
    #[prost(message, repeated, tag = "2")]
    pub source_tables: ::prost::alloc::vec::Vec<TableReference>,
    /// \[Required\] The destination table.
    #[prost(message, optional, tag = "3")]
    pub destination_table: ::core::option::Option<TableReference>,
    /// Optional. Specifies whether the job is allowed to create new tables.
    /// The following values are supported:
    ///
    /// * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the
    /// table.
    /// * CREATE_NEVER: The table must already exist. If it does not,
    /// a 'notFound' error is returned in the job result.
    ///
    /// The default value is CREATE_IF_NEEDED.
    /// Creation, truncation and append actions occur as one atomic update
    /// upon job completion.
    #[prost(string, tag = "4")]
    pub create_disposition: ::prost::alloc::string::String,
    /// Optional. Specifies the action that occurs if the destination table
    /// already exists. The following values are supported:
    ///
    /// * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the
    /// table data and uses the schema and table constraints from the source table.
    /// * WRITE_APPEND: If the table already exists, BigQuery appends the data to
    /// the table.
    /// * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate'
    /// error is returned in the job result.
    ///
    /// The default value is WRITE_EMPTY. Each action is atomic and only occurs if
    /// BigQuery is able to complete the job successfully. Creation, truncation and
    /// append actions occur as one atomic update upon job completion.
    #[prost(string, tag = "5")]
    pub write_disposition: ::prost::alloc::string::String,
    /// Custom encryption configuration (e.g., Cloud KMS keys).
    #[prost(message, optional, tag = "6")]
    pub destination_encryption_configuration: ::core::option::Option<
        EncryptionConfiguration,
    >,
    /// Optional. Supported operation types in table copy job.
    #[prost(enumeration = "job_configuration_table_copy::OperationType", tag = "8")]
    pub operation_type: i32,
    /// Optional. The time when the destination table expires. Expired tables will
    /// be deleted and their storage reclaimed.
    #[prost(message, optional, tag = "9")]
    pub destination_expiration_time: ::core::option::Option<::prost_types::Timestamp>,
}
/// Nested message and enum types in `JobConfigurationTableCopy`.
pub mod job_configuration_table_copy {
    /// Indicates different operation types supported in table copy job.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum OperationType {
        /// Unspecified operation type.
        Unspecified = 0,
        /// The source and destination table have the same table type.
        Copy = 1,
        /// The source table type is TABLE and
        /// the destination table type is SNAPSHOT.
        Snapshot = 2,
        /// The source table type is SNAPSHOT and
        /// the destination table type is TABLE.
        Restore = 3,
        /// The source and destination table have the same table type,
        /// but only bill for unique data.
        Clone = 4,
    }
    impl OperationType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "OPERATION_TYPE_UNSPECIFIED",
                Self::Copy => "COPY",
                Self::Snapshot => "SNAPSHOT",
                Self::Restore => "RESTORE",
                Self::Clone => "CLONE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "OPERATION_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "COPY" => Some(Self::Copy),
                "SNAPSHOT" => Some(Self::Snapshot),
                "RESTORE" => Some(Self::Restore),
                "CLONE" => Some(Self::Clone),
                _ => None,
            }
        }
    }
}
/// JobConfigurationExtract configures a job that exports data from a BigQuery
/// table into Google Cloud Storage.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobConfigurationExtract {
    /// \[Pick one\] A list of fully-qualified Google Cloud Storage URIs where the
    /// extracted table should be written.
    #[prost(string, repeated, tag = "3")]
    pub destination_uris: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. Whether to print out a header row in the results.
    /// Default is true. Not applicable when extracting models.
    #[prost(message, optional, tag = "4")]
    pub print_header: ::core::option::Option<bool>,
    /// Optional. When extracting data in CSV format, this defines the
    /// delimiter to use between fields in the exported data.
    /// Default is ','. Not applicable when extracting models.
    #[prost(string, tag = "5")]
    pub field_delimiter: ::prost::alloc::string::String,
    /// Optional. The exported file format. Possible values include CSV,
    /// NEWLINE_DELIMITED_JSON, PARQUET, or AVRO for tables and ML_TF_SAVED_MODEL
    /// or ML_XGBOOST_BOOSTER for models. The default value for tables is CSV.
    /// Tables with nested or repeated fields cannot be exported as CSV. The
    /// default value for models is ML_TF_SAVED_MODEL.
    #[prost(string, tag = "6")]
    pub destination_format: ::prost::alloc::string::String,
    /// Optional. The compression type to use for exported files. Possible values
    /// include DEFLATE, GZIP, NONE, SNAPPY, and ZSTD. The default value is NONE.
    /// Not all compression formats are support for all file formats. DEFLATE is
    /// only supported for Avro. ZSTD is only supported for Parquet. Not applicable
    /// when extracting models.
    #[prost(string, tag = "7")]
    pub compression: ::prost::alloc::string::String,
    /// Whether to use logical types when extracting to AVRO format. Not applicable
    /// when extracting models.
    #[prost(message, optional, tag = "13")]
    pub use_avro_logical_types: ::core::option::Option<bool>,
    /// Optional. Model extract options only applicable when extracting models.
    #[prost(message, optional, tag = "14")]
    pub model_extract_options: ::core::option::Option<
        job_configuration_extract::ModelExtractOptions,
    >,
    /// Required. Source reference for the export.
    #[prost(oneof = "job_configuration_extract::Source", tags = "1, 9")]
    pub source: ::core::option::Option<job_configuration_extract::Source>,
}
/// Nested message and enum types in `JobConfigurationExtract`.
pub mod job_configuration_extract {
    /// Options related to model extraction.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct ModelExtractOptions {
        /// The 1-based ID of the trial to be exported from a hyperparameter tuning
        /// model. If not specified, the trial with id =
        /// [Model](<https://cloud.google.com/bigquery/docs/reference/rest/v2/models#resource:-model>).defaultTrialId
        /// is exported. This field is ignored for models not trained with
        /// hyperparameter tuning.
        #[prost(message, optional, tag = "1")]
        pub trial_id: ::core::option::Option<i64>,
    }
    /// Required. Source reference for the export.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Source {
        /// A reference to the table being exported.
        #[prost(message, tag = "1")]
        SourceTable(super::TableReference),
        /// A reference to the model being exported.
        #[prost(message, tag = "9")]
        SourceModel(super::ModelReference),
    }
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobConfiguration {
    /// Output only. The type of the job. Can be QUERY, LOAD, EXTRACT, COPY or
    /// UNKNOWN.
    #[prost(string, tag = "8")]
    pub job_type: ::prost::alloc::string::String,
    /// \[Pick one\] Configures a query job.
    #[prost(message, optional, tag = "1")]
    pub query: ::core::option::Option<JobConfigurationQuery>,
    /// \[Pick one\] Configures a load job.
    #[prost(message, optional, tag = "2")]
    pub load: ::core::option::Option<JobConfigurationLoad>,
    /// \[Pick one\] Copies a table.
    #[prost(message, optional, tag = "3")]
    pub copy: ::core::option::Option<JobConfigurationTableCopy>,
    /// \[Pick one\] Configures an extract job.
    #[prost(message, optional, tag = "4")]
    pub extract: ::core::option::Option<JobConfigurationExtract>,
    /// Optional. If set, don't actually run this job. A valid query will return
    /// a mostly empty response with some processing statistics, while an invalid
    /// query will return the same error it would if it wasn't a dry run. Behavior
    /// of non-query jobs is undefined.
    #[prost(message, optional, tag = "5")]
    pub dry_run: ::core::option::Option<bool>,
    /// Optional. Job timeout in milliseconds. If this time limit is exceeded,
    /// BigQuery will attempt to stop a longer job, but may not always succeed in
    /// canceling it before the job completes. For example, a job that takes more
    /// than 60 seconds to complete has a better chance of being stopped than a job
    /// that takes 10 seconds to complete.
    #[prost(message, optional, tag = "6")]
    pub job_timeout_ms: ::core::option::Option<i64>,
    /// The labels associated with this job. You can use these to organize and
    /// group your jobs.
    /// Label keys and values can be no longer than 63 characters, can only contain
    /// lowercase letters, numeric characters, underscores and dashes.
    /// International characters are allowed. Label values are optional.  Label
    /// keys must start with a letter and each label in the list must have a
    /// different key.
    #[prost(map = "string, string", tag = "7")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
/// Reason about why a Job was created from a
/// [`jobs.query`](<https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/query>)
/// method when used with `JOB_CREATION_OPTIONAL` Job creation mode.
///
/// For
/// [`jobs.insert`](<https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/insert>)
/// method calls it will always be `REQUESTED`.
///
/// [Preview](<https://cloud.google.com/products/#product-launch-stages>)
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct JobCreationReason {
    /// Output only. Specifies the high level reason why a Job was created.
    #[prost(enumeration = "job_creation_reason::Code", tag = "1")]
    pub code: i32,
}
/// Nested message and enum types in `JobCreationReason`.
pub mod job_creation_reason {
    /// Indicates the high level reason why a job was created.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Code {
        /// Reason is not specified.
        Unspecified = 0,
        /// Job creation was requested.
        Requested = 1,
        /// The query request ran beyond a system defined timeout specified by the
        /// [timeoutMs field in the
        /// QueryRequest](<https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/query#queryrequest>).
        /// As a result it was considered a long running operation for which a job
        /// was created.
        LongRunning = 2,
        /// The results from the query cannot fit in the response.
        LargeResults = 3,
        /// BigQuery has determined that the query needs to be executed as a Job.
        Other = 4,
    }
    impl Code {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "CODE_UNSPECIFIED",
                Self::Requested => "REQUESTED",
                Self::LongRunning => "LONG_RUNNING",
                Self::LargeResults => "LARGE_RESULTS",
                Self::Other => "OTHER",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "CODE_UNSPECIFIED" => Some(Self::Unspecified),
                "REQUESTED" => Some(Self::Requested),
                "LONG_RUNNING" => Some(Self::LongRunning),
                "LARGE_RESULTS" => Some(Self::LargeResults),
                "OTHER" => Some(Self::Other),
                _ => None,
            }
        }
    }
}
/// A job reference is a fully qualified identifier for referring to a job.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobReference {
    /// Required. The ID of the project containing this job.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. The ID of the job. The ID must contain only letters (a-z, A-Z),
    /// numbers (0-9), underscores (_), or dashes (-). The maximum length is 1,024
    /// characters.
    #[prost(string, tag = "2")]
    pub job_id: ::prost::alloc::string::String,
    /// Optional. The geographic location of the job. The default value is US.
    ///
    /// For more information about BigQuery locations, see:
    /// <https://cloud.google.com/bigquery/docs/locations>
    #[prost(message, optional, tag = "3")]
    pub location: ::core::option::Option<::prost::alloc::string::String>,
    /// This field should not be used.
    #[prost(string, repeated, tag = "5")]
    pub location_alternative: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Remote Model Info
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RemoteModelInfo {
    /// Output only. Fully qualified name of the user-provided connection object of
    /// the remote model. Format:
    /// ```"projects/{project_id}/locations/{location_id}/connections/{connection_id}"```
    #[prost(string, tag = "3")]
    pub connection: ::prost::alloc::string::String,
    /// Output only. Max number of rows in each batch sent to the remote service.
    /// If unset, the number of rows in each batch is set dynamically.
    #[prost(int64, tag = "4")]
    pub max_batching_rows: i64,
    /// Output only. The model version for LLM.
    #[prost(string, tag = "5")]
    pub remote_model_version: ::prost::alloc::string::String,
    /// Output only. The name of the speech recognizer to use for speech
    /// recognition. The expected format is
    /// `projects/{project}/locations/{location}/recognizers/{recognizer}`.
    /// Customers can specify this field at model creation. If not specified, a
    /// default recognizer `projects/{model
    /// project}/locations/global/recognizers/_` will be used. See more details at
    /// [recognizers](<https://cloud.google.com/speech-to-text/v2/docs/reference/rest/v2/projects.locations.recognizers>)
    #[prost(string, tag = "7")]
    pub speech_recognizer: ::prost::alloc::string::String,
    /// Remote services are services outside of BigQuery used by remote models for
    /// predictions. A remote service is backed by either an arbitrary endpoint or
    /// a selected remote service type, but not both.
    #[prost(oneof = "remote_model_info::RemoteService", tags = "1, 2")]
    pub remote_service: ::core::option::Option<remote_model_info::RemoteService>,
}
/// Nested message and enum types in `RemoteModelInfo`.
pub mod remote_model_info {
    /// Supported service type for remote model.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum RemoteServiceType {
        /// Unspecified remote service type.
        Unspecified = 0,
        /// V3 Cloud AI Translation API. See more details at \[Cloud Translation API\]
        /// (<https://cloud.google.com/translate/docs/reference/rest>).
        CloudAiTranslateV3 = 1,
        /// V1 Cloud AI Vision API See more details at \[Cloud Vision API\]
        /// (<https://cloud.google.com/vision/docs/reference/rest>).
        CloudAiVisionV1 = 2,
        /// V1 Cloud AI Natural Language API. See more details at [REST Resource:
        /// documents](<https://cloud.google.com/natural-language/docs/reference/rest/v1/documents>).
        CloudAiNaturalLanguageV1 = 3,
        /// V2 Speech-to-Text API. See more details at [Google Cloud Speech-to-Text
        /// V2 API](<https://cloud.google.com/speech-to-text/v2/docs>)
        CloudAiSpeechToTextV2 = 7,
    }
    impl RemoteServiceType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "REMOTE_SERVICE_TYPE_UNSPECIFIED",
                Self::CloudAiTranslateV3 => "CLOUD_AI_TRANSLATE_V3",
                Self::CloudAiVisionV1 => "CLOUD_AI_VISION_V1",
                Self::CloudAiNaturalLanguageV1 => "CLOUD_AI_NATURAL_LANGUAGE_V1",
                Self::CloudAiSpeechToTextV2 => "CLOUD_AI_SPEECH_TO_TEXT_V2",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "REMOTE_SERVICE_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "CLOUD_AI_TRANSLATE_V3" => Some(Self::CloudAiTranslateV3),
                "CLOUD_AI_VISION_V1" => Some(Self::CloudAiVisionV1),
                "CLOUD_AI_NATURAL_LANGUAGE_V1" => Some(Self::CloudAiNaturalLanguageV1),
                "CLOUD_AI_SPEECH_TO_TEXT_V2" => Some(Self::CloudAiSpeechToTextV2),
                _ => None,
            }
        }
    }
    /// Remote services are services outside of BigQuery used by remote models for
    /// predictions. A remote service is backed by either an arbitrary endpoint or
    /// a selected remote service type, but not both.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum RemoteService {
        /// Output only. The endpoint for remote model.
        #[prost(string, tag = "1")]
        Endpoint(::prost::alloc::string::String),
        /// Output only. The remote service type for remote model.
        #[prost(enumeration = "RemoteServiceType", tag = "2")]
        RemoteServiceType(i32),
    }
}
/// Information about a single transform column.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TransformColumn {
    /// Output only. Name of the column.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. Data type of the column after the transform.
    #[prost(message, optional, tag = "2")]
    pub r#type: ::core::option::Option<StandardSqlDataType>,
    /// Output only. The SQL expression used in the column transform.
    #[prost(string, tag = "3")]
    pub transform_sql: ::prost::alloc::string::String,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Model {
    /// Output only. A hash of this resource.
    #[prost(string, tag = "1")]
    pub etag: ::prost::alloc::string::String,
    /// Required. Unique identifier for this model.
    #[prost(message, optional, tag = "2")]
    pub model_reference: ::core::option::Option<ModelReference>,
    /// Output only. The time when this model was created, in millisecs since the
    /// epoch.
    #[prost(int64, tag = "5")]
    pub creation_time: i64,
    /// Output only. The time when this model was last modified, in millisecs since
    /// the epoch.
    #[prost(int64, tag = "6")]
    pub last_modified_time: i64,
    /// Optional. A user-friendly description of this model.
    #[prost(string, tag = "12")]
    pub description: ::prost::alloc::string::String,
    /// Optional. A descriptive name for this model.
    #[prost(string, tag = "14")]
    pub friendly_name: ::prost::alloc::string::String,
    /// The labels associated with this model. You can use these to organize
    /// and group your models. Label keys and values can be no longer
    /// than 63 characters, can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// Label values are optional. Label keys must start with a letter and each
    /// label in the list must have a different key.
    #[prost(map = "string, string", tag = "15")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. The time when this model expires, in milliseconds since the
    /// epoch. If not present, the model will persist indefinitely. Expired models
    /// will be deleted and their storage reclaimed.  The defaultTableExpirationMs
    /// property of the encapsulating dataset can be used to set a default
    /// expirationTime on newly created models.
    #[prost(int64, tag = "16")]
    pub expiration_time: i64,
    /// Output only. The geographic location where the model resides. This value
    /// is inherited from the dataset.
    #[prost(string, tag = "13")]
    pub location: ::prost::alloc::string::String,
    /// Custom encryption configuration (e.g., Cloud KMS keys). This shows the
    /// encryption configuration of the model data while stored in BigQuery
    /// storage. This field can be used with PatchModel to update encryption key
    /// for an already encrypted model.
    #[prost(message, optional, tag = "17")]
    pub encryption_configuration: ::core::option::Option<EncryptionConfiguration>,
    /// Output only. Type of the model resource.
    #[prost(enumeration = "model::ModelType", tag = "7")]
    pub model_type: i32,
    /// Information for all training runs in increasing order of start_time.
    #[prost(message, repeated, tag = "9")]
    pub training_runs: ::prost::alloc::vec::Vec<model::TrainingRun>,
    /// Output only. Input feature columns for the model inference. If the model is
    /// trained with TRANSFORM clause, these are the input of the TRANSFORM clause.
    #[prost(message, repeated, tag = "10")]
    pub feature_columns: ::prost::alloc::vec::Vec<StandardSqlField>,
    /// Output only. Label columns that were used to train this model.
    /// The output of the model will have a "predicted_" prefix to these columns.
    #[prost(message, repeated, tag = "11")]
    pub label_columns: ::prost::alloc::vec::Vec<StandardSqlField>,
    /// Output only. This field will be populated if a TRANSFORM clause was used to
    /// train a model. TRANSFORM clause (if used) takes feature_columns as input
    /// and outputs transform_columns. transform_columns then are used to train the
    /// model.
    #[prost(message, repeated, tag = "26")]
    pub transform_columns: ::prost::alloc::vec::Vec<TransformColumn>,
    /// Output only. All hyperparameter search spaces in this model.
    #[prost(message, optional, tag = "18")]
    pub hparam_search_spaces: ::core::option::Option<model::HparamSearchSpaces>,
    /// Output only. The default trial_id to use in TVFs when the trial_id is not
    /// passed in. For single-objective [hyperparameter
    /// tuning](<https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview>)
    /// models, this is the best trial ID. For multi-objective [hyperparameter
    /// tuning](<https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview>)
    /// models, this is the smallest trial ID among all Pareto optimal trials.
    #[prost(int64, tag = "21")]
    pub default_trial_id: i64,
    /// Output only. Trials of a [hyperparameter
    /// tuning](<https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview>)
    /// model sorted by trial_id.
    #[prost(message, repeated, tag = "20")]
    pub hparam_trials: ::prost::alloc::vec::Vec<model::HparamTuningTrial>,
    /// Output only. For single-objective [hyperparameter
    /// tuning](<https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview>)
    /// models, it only contains the best trial. For multi-objective
    /// [hyperparameter
    /// tuning](<https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview>)
    /// models, it contains all Pareto optimal trials sorted by trial_id.
    #[prost(int64, repeated, packed = "false", tag = "22")]
    pub optimal_trial_ids: ::prost::alloc::vec::Vec<i64>,
    /// Output only. Remote model info
    #[prost(message, optional, tag = "25")]
    pub remote_model_info: ::core::option::Option<RemoteModelInfo>,
}
/// Nested message and enum types in `Model`.
pub mod model {
    /// Enums for seasonal period.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct SeasonalPeriod {}
    /// Nested message and enum types in `SeasonalPeriod`.
    pub mod seasonal_period {
        /// Seasonal period type.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum SeasonalPeriodType {
            /// Unspecified seasonal period.
            Unspecified = 0,
            /// No seasonality
            NoSeasonality = 1,
            /// Daily period, 24 hours.
            Daily = 2,
            /// Weekly period, 7 days.
            Weekly = 3,
            /// Monthly period, 30 days or irregular.
            Monthly = 4,
            /// Quarterly period, 90 days or irregular.
            Quarterly = 5,
            /// Yearly period, 365 days or irregular.
            Yearly = 6,
        }
        impl SeasonalPeriodType {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "SEASONAL_PERIOD_TYPE_UNSPECIFIED",
                    Self::NoSeasonality => "NO_SEASONALITY",
                    Self::Daily => "DAILY",
                    Self::Weekly => "WEEKLY",
                    Self::Monthly => "MONTHLY",
                    Self::Quarterly => "QUARTERLY",
                    Self::Yearly => "YEARLY",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "SEASONAL_PERIOD_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                    "NO_SEASONALITY" => Some(Self::NoSeasonality),
                    "DAILY" => Some(Self::Daily),
                    "WEEKLY" => Some(Self::Weekly),
                    "MONTHLY" => Some(Self::Monthly),
                    "QUARTERLY" => Some(Self::Quarterly),
                    "YEARLY" => Some(Self::Yearly),
                    _ => None,
                }
            }
        }
    }
    /// Enums for kmeans model type.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct KmeansEnums {}
    /// Nested message and enum types in `KmeansEnums`.
    pub mod kmeans_enums {
        /// Indicates the method used to initialize the centroids for KMeans
        /// clustering algorithm.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum KmeansInitializationMethod {
            /// Unspecified initialization method.
            Unspecified = 0,
            /// Initializes the centroids randomly.
            Random = 1,
            /// Initializes the centroids using data specified in
            /// kmeans_initialization_column.
            Custom = 2,
            /// Initializes with kmeans++.
            KmeansPlusPlus = 3,
        }
        impl KmeansInitializationMethod {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "KMEANS_INITIALIZATION_METHOD_UNSPECIFIED",
                    Self::Random => "RANDOM",
                    Self::Custom => "CUSTOM",
                    Self::KmeansPlusPlus => "KMEANS_PLUS_PLUS",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "KMEANS_INITIALIZATION_METHOD_UNSPECIFIED" => Some(Self::Unspecified),
                    "RANDOM" => Some(Self::Random),
                    "CUSTOM" => Some(Self::Custom),
                    "KMEANS_PLUS_PLUS" => Some(Self::KmeansPlusPlus),
                    _ => None,
                }
            }
        }
    }
    /// Enums for XGBoost model type.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct BoostedTreeOptionEnums {}
    /// Nested message and enum types in `BoostedTreeOptionEnums`.
    pub mod boosted_tree_option_enums {
        /// Booster types supported. Refer to booster parameter in XGBoost.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum BoosterType {
            /// Unspecified booster type.
            Unspecified = 0,
            /// Gbtree booster.
            Gbtree = 1,
            /// Dart booster.
            Dart = 2,
        }
        impl BoosterType {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "BOOSTER_TYPE_UNSPECIFIED",
                    Self::Gbtree => "GBTREE",
                    Self::Dart => "DART",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "BOOSTER_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                    "GBTREE" => Some(Self::Gbtree),
                    "DART" => Some(Self::Dart),
                    _ => None,
                }
            }
        }
        /// Type of normalization algorithm for boosted tree models using dart
        /// booster. Refer to normalize_type in XGBoost.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum DartNormalizeType {
            /// Unspecified dart normalize type.
            Unspecified = 0,
            /// New trees have the same weight of each of dropped trees.
            Tree = 1,
            /// New trees have the same weight of sum of dropped trees.
            Forest = 2,
        }
        impl DartNormalizeType {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "DART_NORMALIZE_TYPE_UNSPECIFIED",
                    Self::Tree => "TREE",
                    Self::Forest => "FOREST",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "DART_NORMALIZE_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                    "TREE" => Some(Self::Tree),
                    "FOREST" => Some(Self::Forest),
                    _ => None,
                }
            }
        }
        /// Tree construction algorithm used in boosted tree models.
        /// Refer to tree_method in XGBoost.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum TreeMethod {
            /// Unspecified tree method.
            Unspecified = 0,
            /// Use heuristic to choose the fastest method.
            Auto = 1,
            /// Exact greedy algorithm.
            Exact = 2,
            /// Approximate greedy algorithm using quantile sketch and gradient
            /// histogram.
            Approx = 3,
            /// Fast histogram optimized approximate greedy algorithm.
            Hist = 4,
        }
        impl TreeMethod {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "TREE_METHOD_UNSPECIFIED",
                    Self::Auto => "AUTO",
                    Self::Exact => "EXACT",
                    Self::Approx => "APPROX",
                    Self::Hist => "HIST",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "TREE_METHOD_UNSPECIFIED" => Some(Self::Unspecified),
                    "AUTO" => Some(Self::Auto),
                    "EXACT" => Some(Self::Exact),
                    "APPROX" => Some(Self::Approx),
                    "HIST" => Some(Self::Hist),
                    _ => None,
                }
            }
        }
    }
    /// Enums for hyperparameter tuning.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct HparamTuningEnums {}
    /// Nested message and enum types in `HparamTuningEnums`.
    pub mod hparam_tuning_enums {
        /// Available evaluation metrics used as hyperparameter tuning objectives.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum HparamTuningObjective {
            /// Unspecified evaluation metric.
            Unspecified = 0,
            /// Mean absolute error.
            /// mean_absolute_error = AVG(ABS(label - predicted))
            MeanAbsoluteError = 1,
            /// Mean squared error.
            /// mean_squared_error = AVG(POW(label - predicted, 2))
            MeanSquaredError = 2,
            /// Mean squared log error.
            /// mean_squared_log_error = AVG(POW(LN(1 + label) - LN(1 + predicted), 2))
            MeanSquaredLogError = 3,
            /// Mean absolute error.
            /// median_absolute_error = APPROX_QUANTILES(absolute_error, 2)\[OFFSET(1)\]
            MedianAbsoluteError = 4,
            /// R^2 score. This corresponds to r2_score in ML.EVALUATE.
            /// r_squared = 1 - SUM(squared_error)/(COUNT(label)*VAR_POP(label))
            RSquared = 5,
            /// Explained variance.
            /// explained_variance = 1 - VAR_POP(label_error)/VAR_POP(label)
            ExplainedVariance = 6,
            /// Precision is the fraction of actual positive predictions that had
            /// positive actual labels. For multiclass this is a macro-averaged metric
            /// treating each class as a binary classifier.
            Precision = 7,
            /// Recall is the fraction of actual positive labels that were given a
            /// positive prediction. For multiclass this is a macro-averaged metric.
            Recall = 8,
            /// Accuracy is the fraction of predictions given the correct label. For
            /// multiclass this is a globally micro-averaged metric.
            Accuracy = 9,
            /// The F1 score is an average of recall and precision. For multiclass this
            /// is a macro-averaged metric.
            F1Score = 10,
            /// Logorithmic Loss. For multiclass this is a macro-averaged metric.
            LogLoss = 11,
            /// Area Under an ROC Curve. For multiclass this is a macro-averaged
            /// metric.
            RocAuc = 12,
            /// Davies-Bouldin Index.
            DaviesBouldinIndex = 13,
            /// Mean Average Precision.
            MeanAveragePrecision = 14,
            /// Normalized Discounted Cumulative Gain.
            NormalizedDiscountedCumulativeGain = 15,
            /// Average Rank.
            AverageRank = 16,
        }
        impl HparamTuningObjective {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "HPARAM_TUNING_OBJECTIVE_UNSPECIFIED",
                    Self::MeanAbsoluteError => "MEAN_ABSOLUTE_ERROR",
                    Self::MeanSquaredError => "MEAN_SQUARED_ERROR",
                    Self::MeanSquaredLogError => "MEAN_SQUARED_LOG_ERROR",
                    Self::MedianAbsoluteError => "MEDIAN_ABSOLUTE_ERROR",
                    Self::RSquared => "R_SQUARED",
                    Self::ExplainedVariance => "EXPLAINED_VARIANCE",
                    Self::Precision => "PRECISION",
                    Self::Recall => "RECALL",
                    Self::Accuracy => "ACCURACY",
                    Self::F1Score => "F1_SCORE",
                    Self::LogLoss => "LOG_LOSS",
                    Self::RocAuc => "ROC_AUC",
                    Self::DaviesBouldinIndex => "DAVIES_BOULDIN_INDEX",
                    Self::MeanAveragePrecision => "MEAN_AVERAGE_PRECISION",
                    Self::NormalizedDiscountedCumulativeGain => {
                        "NORMALIZED_DISCOUNTED_CUMULATIVE_GAIN"
                    }
                    Self::AverageRank => "AVERAGE_RANK",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "HPARAM_TUNING_OBJECTIVE_UNSPECIFIED" => Some(Self::Unspecified),
                    "MEAN_ABSOLUTE_ERROR" => Some(Self::MeanAbsoluteError),
                    "MEAN_SQUARED_ERROR" => Some(Self::MeanSquaredError),
                    "MEAN_SQUARED_LOG_ERROR" => Some(Self::MeanSquaredLogError),
                    "MEDIAN_ABSOLUTE_ERROR" => Some(Self::MedianAbsoluteError),
                    "R_SQUARED" => Some(Self::RSquared),
                    "EXPLAINED_VARIANCE" => Some(Self::ExplainedVariance),
                    "PRECISION" => Some(Self::Precision),
                    "RECALL" => Some(Self::Recall),
                    "ACCURACY" => Some(Self::Accuracy),
                    "F1_SCORE" => Some(Self::F1Score),
                    "LOG_LOSS" => Some(Self::LogLoss),
                    "ROC_AUC" => Some(Self::RocAuc),
                    "DAVIES_BOULDIN_INDEX" => Some(Self::DaviesBouldinIndex),
                    "MEAN_AVERAGE_PRECISION" => Some(Self::MeanAveragePrecision),
                    "NORMALIZED_DISCOUNTED_CUMULATIVE_GAIN" => {
                        Some(Self::NormalizedDiscountedCumulativeGain)
                    }
                    "AVERAGE_RANK" => Some(Self::AverageRank),
                    _ => None,
                }
            }
        }
    }
    /// Evaluation metrics for regression and explicit feedback type matrix
    /// factorization models.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct RegressionMetrics {
        /// Mean absolute error.
        #[prost(message, optional, tag = "1")]
        pub mean_absolute_error: ::core::option::Option<f64>,
        /// Mean squared error.
        #[prost(message, optional, tag = "2")]
        pub mean_squared_error: ::core::option::Option<f64>,
        /// Mean squared log error.
        #[prost(message, optional, tag = "3")]
        pub mean_squared_log_error: ::core::option::Option<f64>,
        /// Median absolute error.
        #[prost(message, optional, tag = "4")]
        pub median_absolute_error: ::core::option::Option<f64>,
        /// R^2 score. This corresponds to r2_score in ML.EVALUATE.
        #[prost(message, optional, tag = "5")]
        pub r_squared: ::core::option::Option<f64>,
    }
    /// Aggregate metrics for classification/classifier models. For multi-class
    /// models, the metrics are either macro-averaged or micro-averaged. When
    /// macro-averaged, the metrics are calculated for each label and then an
    /// unweighted average is taken of those values. When micro-averaged, the
    /// metric is calculated globally by counting the total number of correctly
    /// predicted rows.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct AggregateClassificationMetrics {
        /// Precision is the fraction of actual positive predictions that had
        /// positive actual labels. For multiclass this is a macro-averaged
        /// metric treating each class as a binary classifier.
        #[prost(message, optional, tag = "1")]
        pub precision: ::core::option::Option<f64>,
        /// Recall is the fraction of actual positive labels that were given a
        /// positive prediction. For multiclass this is a macro-averaged metric.
        #[prost(message, optional, tag = "2")]
        pub recall: ::core::option::Option<f64>,
        /// Accuracy is the fraction of predictions given the correct label. For
        /// multiclass this is a micro-averaged metric.
        #[prost(message, optional, tag = "3")]
        pub accuracy: ::core::option::Option<f64>,
        /// Threshold at which the metrics are computed. For binary
        /// classification models this is the positive class threshold.
        /// For multi-class classfication models this is the confidence
        /// threshold.
        #[prost(message, optional, tag = "4")]
        pub threshold: ::core::option::Option<f64>,
        /// The F1 score is an average of recall and precision. For multiclass
        /// this is a macro-averaged metric.
        #[prost(message, optional, tag = "5")]
        pub f1_score: ::core::option::Option<f64>,
        /// Logarithmic Loss. For multiclass this is a macro-averaged metric.
        #[prost(message, optional, tag = "6")]
        pub log_loss: ::core::option::Option<f64>,
        /// Area Under a ROC Curve. For multiclass this is a macro-averaged
        /// metric.
        #[prost(message, optional, tag = "7")]
        pub roc_auc: ::core::option::Option<f64>,
    }
    /// Evaluation metrics for binary classification/classifier models.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct BinaryClassificationMetrics {
        /// Aggregate classification metrics.
        #[prost(message, optional, tag = "1")]
        pub aggregate_classification_metrics: ::core::option::Option<
            AggregateClassificationMetrics,
        >,
        /// Binary confusion matrix at multiple thresholds.
        #[prost(message, repeated, tag = "2")]
        pub binary_confusion_matrix_list: ::prost::alloc::vec::Vec<
            binary_classification_metrics::BinaryConfusionMatrix,
        >,
        /// Label representing the positive class.
        #[prost(string, tag = "3")]
        pub positive_label: ::prost::alloc::string::String,
        /// Label representing the negative class.
        #[prost(string, tag = "4")]
        pub negative_label: ::prost::alloc::string::String,
    }
    /// Nested message and enum types in `BinaryClassificationMetrics`.
    pub mod binary_classification_metrics {
        /// Confusion matrix for binary classification models.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct BinaryConfusionMatrix {
            /// Threshold value used when computing each of the following metric.
            #[prost(message, optional, tag = "1")]
            pub positive_class_threshold: ::core::option::Option<f64>,
            /// Number of true samples predicted as true.
            #[prost(message, optional, tag = "2")]
            pub true_positives: ::core::option::Option<i64>,
            /// Number of false samples predicted as true.
            #[prost(message, optional, tag = "3")]
            pub false_positives: ::core::option::Option<i64>,
            /// Number of true samples predicted as false.
            #[prost(message, optional, tag = "4")]
            pub true_negatives: ::core::option::Option<i64>,
            /// Number of false samples predicted as false.
            #[prost(message, optional, tag = "5")]
            pub false_negatives: ::core::option::Option<i64>,
            /// The fraction of actual positive predictions that had positive actual
            /// labels.
            #[prost(message, optional, tag = "6")]
            pub precision: ::core::option::Option<f64>,
            /// The fraction of actual positive labels that were given a positive
            /// prediction.
            #[prost(message, optional, tag = "7")]
            pub recall: ::core::option::Option<f64>,
            /// The equally weighted average of recall and precision.
            #[prost(message, optional, tag = "8")]
            pub f1_score: ::core::option::Option<f64>,
            /// The fraction of predictions given the correct label.
            #[prost(message, optional, tag = "9")]
            pub accuracy: ::core::option::Option<f64>,
        }
    }
    /// Evaluation metrics for multi-class classification/classifier models.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct MultiClassClassificationMetrics {
        /// Aggregate classification metrics.
        #[prost(message, optional, tag = "1")]
        pub aggregate_classification_metrics: ::core::option::Option<
            AggregateClassificationMetrics,
        >,
        /// Confusion matrix at different thresholds.
        #[prost(message, repeated, tag = "2")]
        pub confusion_matrix_list: ::prost::alloc::vec::Vec<
            multi_class_classification_metrics::ConfusionMatrix,
        >,
    }
    /// Nested message and enum types in `MultiClassClassificationMetrics`.
    pub mod multi_class_classification_metrics {
        /// Confusion matrix for multi-class classification models.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct ConfusionMatrix {
            /// Confidence threshold used when computing the entries of the
            /// confusion matrix.
            #[prost(message, optional, tag = "1")]
            pub confidence_threshold: ::core::option::Option<f64>,
            /// One row per actual label.
            #[prost(message, repeated, tag = "2")]
            pub rows: ::prost::alloc::vec::Vec<confusion_matrix::Row>,
        }
        /// Nested message and enum types in `ConfusionMatrix`.
        pub mod confusion_matrix {
            /// A single entry in the confusion matrix.
            #[derive(Clone, PartialEq, ::prost::Message)]
            pub struct Entry {
                /// The predicted label. For confidence_threshold > 0, we will
                /// also add an entry indicating the number of items under the
                /// confidence threshold.
                #[prost(string, tag = "1")]
                pub predicted_label: ::prost::alloc::string::String,
                /// Number of items being predicted as this label.
                #[prost(message, optional, tag = "2")]
                pub item_count: ::core::option::Option<i64>,
            }
            /// A single row in the confusion matrix.
            #[derive(Clone, PartialEq, ::prost::Message)]
            pub struct Row {
                /// The original label of this row.
                #[prost(string, tag = "1")]
                pub actual_label: ::prost::alloc::string::String,
                /// Info describing predicted label distribution.
                #[prost(message, repeated, tag = "2")]
                pub entries: ::prost::alloc::vec::Vec<Entry>,
            }
        }
    }
    /// Evaluation metrics for clustering models.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ClusteringMetrics {
        /// Davies-Bouldin index.
        #[prost(message, optional, tag = "1")]
        pub davies_bouldin_index: ::core::option::Option<f64>,
        /// Mean of squared distances between each sample to its cluster centroid.
        #[prost(message, optional, tag = "2")]
        pub mean_squared_distance: ::core::option::Option<f64>,
        /// Information for all clusters.
        #[prost(message, repeated, tag = "3")]
        pub clusters: ::prost::alloc::vec::Vec<clustering_metrics::Cluster>,
    }
    /// Nested message and enum types in `ClusteringMetrics`.
    pub mod clustering_metrics {
        /// Message containing the information about one cluster.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct Cluster {
            /// Centroid id.
            #[prost(int64, tag = "1")]
            pub centroid_id: i64,
            /// Values of highly variant features for this cluster.
            #[prost(message, repeated, tag = "2")]
            pub feature_values: ::prost::alloc::vec::Vec<cluster::FeatureValue>,
            /// Count of training data rows that were assigned to this cluster.
            #[prost(message, optional, tag = "3")]
            pub count: ::core::option::Option<i64>,
        }
        /// Nested message and enum types in `Cluster`.
        pub mod cluster {
            /// Representative value of a single feature within the cluster.
            #[derive(Clone, PartialEq, ::prost::Message)]
            pub struct FeatureValue {
                /// The feature column name.
                #[prost(string, tag = "1")]
                pub feature_column: ::prost::alloc::string::String,
                /// Value.
                #[prost(oneof = "feature_value::Value", tags = "2, 3")]
                pub value: ::core::option::Option<feature_value::Value>,
            }
            /// Nested message and enum types in `FeatureValue`.
            pub mod feature_value {
                /// Representative value of a categorical feature.
                #[derive(Clone, PartialEq, ::prost::Message)]
                pub struct CategoricalValue {
                    /// Counts of all categories for the categorical feature. If there are
                    /// more than ten categories, we return top ten (by count) and return
                    /// one more CategoryCount with category "_OTHER_" and count as
                    /// aggregate counts of remaining categories.
                    #[prost(message, repeated, tag = "1")]
                    pub category_counts: ::prost::alloc::vec::Vec<
                        categorical_value::CategoryCount,
                    >,
                }
                /// Nested message and enum types in `CategoricalValue`.
                pub mod categorical_value {
                    /// Represents the count of a single category within the cluster.
                    #[derive(Clone, PartialEq, ::prost::Message)]
                    pub struct CategoryCount {
                        /// The name of category.
                        #[prost(string, tag = "1")]
                        pub category: ::prost::alloc::string::String,
                        /// The count of training samples matching the category within the
                        /// cluster.
                        #[prost(message, optional, tag = "2")]
                        pub count: ::core::option::Option<i64>,
                    }
                }
                /// Value.
                #[derive(Clone, PartialEq, ::prost::Oneof)]
                pub enum Value {
                    /// The numerical feature value. This is the centroid value for this
                    /// feature.
                    #[prost(message, tag = "2")]
                    NumericalValue(f64),
                    /// The categorical feature value.
                    #[prost(message, tag = "3")]
                    CategoricalValue(CategoricalValue),
                }
            }
        }
    }
    /// Evaluation metrics used by weighted-ALS models specified by
    /// feedback_type=implicit.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct RankingMetrics {
        /// Calculates a precision per user for all the items by ranking them and
        /// then averages all the precisions across all the users.
        #[prost(message, optional, tag = "1")]
        pub mean_average_precision: ::core::option::Option<f64>,
        /// Similar to the mean squared error computed in regression and explicit
        /// recommendation models except instead of computing the rating directly,
        /// the output from evaluate is computed against a preference which is 1 or 0
        /// depending on if the rating exists or not.
        #[prost(message, optional, tag = "2")]
        pub mean_squared_error: ::core::option::Option<f64>,
        /// A metric to determine the goodness of a ranking calculated from the
        /// predicted confidence by comparing it to an ideal rank measured by the
        /// original ratings.
        #[prost(message, optional, tag = "3")]
        pub normalized_discounted_cumulative_gain: ::core::option::Option<f64>,
        /// Determines the goodness of a ranking by computing the percentile rank
        /// from the predicted confidence and dividing it by the original rank.
        #[prost(message, optional, tag = "4")]
        pub average_rank: ::core::option::Option<f64>,
    }
    /// Model evaluation metrics for ARIMA forecasting models.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ArimaForecastingMetrics {
        /// Repeated as there can be many metric sets (one for each model) in
        /// auto-arima and the large-scale case.
        #[prost(message, repeated, tag = "6")]
        pub arima_single_model_forecasting_metrics: ::prost::alloc::vec::Vec<
            arima_forecasting_metrics::ArimaSingleModelForecastingMetrics,
        >,
    }
    /// Nested message and enum types in `ArimaForecastingMetrics`.
    pub mod arima_forecasting_metrics {
        /// Model evaluation metrics for a single ARIMA forecasting model.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct ArimaSingleModelForecastingMetrics {
            /// Non-seasonal order.
            #[prost(message, optional, tag = "1")]
            pub non_seasonal_order: ::core::option::Option<super::ArimaOrder>,
            /// Arima fitting metrics.
            #[prost(message, optional, tag = "2")]
            pub arima_fitting_metrics: ::core::option::Option<
                super::ArimaFittingMetrics,
            >,
            /// Is arima model fitted with drift or not. It is always false when d
            /// is not 1.
            #[prost(message, optional, tag = "3")]
            pub has_drift: ::core::option::Option<bool>,
            /// The time_series_id value for this time series. It will be one of
            /// the unique values from the time_series_id_column specified during
            /// ARIMA model training. Only present when time_series_id_column
            /// training option was used.
            #[prost(string, tag = "4")]
            pub time_series_id: ::prost::alloc::string::String,
            /// The tuple of time_series_ids identifying this time series. It will
            /// be one of the unique tuples of values present in the
            /// time_series_id_columns specified during ARIMA model training. Only
            /// present when time_series_id_columns training option was used and
            /// the order of values here are same as the order of
            /// time_series_id_columns.
            #[prost(string, repeated, tag = "9")]
            pub time_series_ids: ::prost::alloc::vec::Vec<
                ::prost::alloc::string::String,
            >,
            /// Seasonal periods. Repeated because multiple periods are supported
            /// for one time series.
            #[prost(
                enumeration = "super::seasonal_period::SeasonalPeriodType",
                repeated,
                tag = "5"
            )]
            pub seasonal_periods: ::prost::alloc::vec::Vec<i32>,
            /// If true, holiday_effect is a part of time series decomposition result.
            #[prost(message, optional, tag = "6")]
            pub has_holiday_effect: ::core::option::Option<bool>,
            /// If true, spikes_and_dips is a part of time series decomposition result.
            #[prost(message, optional, tag = "7")]
            pub has_spikes_and_dips: ::core::option::Option<bool>,
            /// If true, step_changes is a part of time series decomposition result.
            #[prost(message, optional, tag = "8")]
            pub has_step_changes: ::core::option::Option<bool>,
        }
    }
    /// Model evaluation metrics for dimensionality reduction models.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct DimensionalityReductionMetrics {
        /// Total percentage of variance explained by the selected principal
        /// components.
        #[prost(message, optional, tag = "1")]
        pub total_explained_variance_ratio: ::core::option::Option<f64>,
    }
    /// Evaluation metrics of a model. These are either computed on all training
    /// data or just the eval data based on whether eval data was used during
    /// training. These are not present for imported models.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct EvaluationMetrics {
        /// Metrics.
        #[prost(oneof = "evaluation_metrics::Metrics", tags = "1, 2, 3, 4, 5, 6, 7")]
        pub metrics: ::core::option::Option<evaluation_metrics::Metrics>,
    }
    /// Nested message and enum types in `EvaluationMetrics`.
    pub mod evaluation_metrics {
        /// Metrics.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum Metrics {
            /// Populated for regression models and explicit feedback type matrix
            /// factorization models.
            #[prost(message, tag = "1")]
            RegressionMetrics(super::RegressionMetrics),
            /// Populated for binary classification/classifier models.
            #[prost(message, tag = "2")]
            BinaryClassificationMetrics(super::BinaryClassificationMetrics),
            /// Populated for multi-class classification/classifier models.
            #[prost(message, tag = "3")]
            MultiClassClassificationMetrics(super::MultiClassClassificationMetrics),
            /// Populated for clustering models.
            #[prost(message, tag = "4")]
            ClusteringMetrics(super::ClusteringMetrics),
            /// Populated for implicit feedback type matrix factorization models.
            #[prost(message, tag = "5")]
            RankingMetrics(super::RankingMetrics),
            /// Populated for ARIMA models.
            #[prost(message, tag = "6")]
            ArimaForecastingMetrics(super::ArimaForecastingMetrics),
            /// Evaluation metrics when the model is a dimensionality reduction model,
            /// which currently includes PCA.
            #[prost(message, tag = "7")]
            DimensionalityReductionMetrics(super::DimensionalityReductionMetrics),
        }
    }
    /// Data split result. This contains references to the training and evaluation
    /// data tables that were used to train the model.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct DataSplitResult {
        /// Table reference of the training data after split.
        #[prost(message, optional, tag = "1")]
        pub training_table: ::core::option::Option<super::TableReference>,
        /// Table reference of the evaluation data after split.
        #[prost(message, optional, tag = "2")]
        pub evaluation_table: ::core::option::Option<super::TableReference>,
        /// Table reference of the test data after split.
        #[prost(message, optional, tag = "3")]
        pub test_table: ::core::option::Option<super::TableReference>,
    }
    /// Arima order, can be used for both non-seasonal and seasonal parts.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct ArimaOrder {
        /// Order of the autoregressive part.
        #[prost(message, optional, tag = "1")]
        pub p: ::core::option::Option<i64>,
        /// Order of the differencing part.
        #[prost(message, optional, tag = "2")]
        pub d: ::core::option::Option<i64>,
        /// Order of the moving-average part.
        #[prost(message, optional, tag = "3")]
        pub q: ::core::option::Option<i64>,
    }
    /// ARIMA model fitting metrics.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct ArimaFittingMetrics {
        /// Log-likelihood.
        #[prost(message, optional, tag = "1")]
        pub log_likelihood: ::core::option::Option<f64>,
        /// AIC.
        #[prost(message, optional, tag = "2")]
        pub aic: ::core::option::Option<f64>,
        /// Variance.
        #[prost(message, optional, tag = "3")]
        pub variance: ::core::option::Option<f64>,
    }
    /// Global explanations containing the top most important features
    /// after training.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct GlobalExplanation {
        /// A list of the top global explanations. Sorted by absolute value of
        /// attribution in descending order.
        #[prost(message, repeated, tag = "1")]
        pub explanations: ::prost::alloc::vec::Vec<global_explanation::Explanation>,
        /// Class label for this set of global explanations. Will be empty/null for
        /// binary logistic and linear regression models. Sorted alphabetically in
        /// descending order.
        #[prost(string, tag = "2")]
        pub class_label: ::prost::alloc::string::String,
    }
    /// Nested message and enum types in `GlobalExplanation`.
    pub mod global_explanation {
        /// Explanation for a single feature.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct Explanation {
            /// The full feature name. For non-numerical features, will be formatted
            /// like `<column_name>.<encoded_feature_name>`. Overall size of feature
            /// name will always be truncated to first 120 characters.
            #[prost(string, tag = "1")]
            pub feature_name: ::prost::alloc::string::String,
            /// Attribution of feature.
            #[prost(message, optional, tag = "2")]
            pub attribution: ::core::option::Option<f64>,
        }
    }
    /// Encoding methods for categorical features.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct CategoryEncodingMethod {}
    /// Nested message and enum types in `CategoryEncodingMethod`.
    pub mod category_encoding_method {
        /// Supported encoding methods for categorical features.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum EncodingMethod {
            /// Unspecified encoding method.
            Unspecified = 0,
            /// Applies one-hot encoding.
            OneHotEncoding = 1,
            /// Applies label encoding.
            LabelEncoding = 2,
            /// Applies dummy encoding.
            DummyEncoding = 3,
        }
        impl EncodingMethod {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "ENCODING_METHOD_UNSPECIFIED",
                    Self::OneHotEncoding => "ONE_HOT_ENCODING",
                    Self::LabelEncoding => "LABEL_ENCODING",
                    Self::DummyEncoding => "DUMMY_ENCODING",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "ENCODING_METHOD_UNSPECIFIED" => Some(Self::Unspecified),
                    "ONE_HOT_ENCODING" => Some(Self::OneHotEncoding),
                    "LABEL_ENCODING" => Some(Self::LabelEncoding),
                    "DUMMY_ENCODING" => Some(Self::DummyEncoding),
                    _ => None,
                }
            }
        }
    }
    /// PCA solver options.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct PcaSolverOptionEnums {}
    /// Nested message and enum types in `PcaSolverOptionEnums`.
    pub mod pca_solver_option_enums {
        /// Enums for supported PCA solvers.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum PcaSolver {
            /// Default value.
            Unspecified = 0,
            /// Full eigen-decoposition.
            Full = 1,
            /// Randomized SVD.
            Randomized = 2,
            /// Auto.
            Auto = 3,
        }
        impl PcaSolver {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "UNSPECIFIED",
                    Self::Full => "FULL",
                    Self::Randomized => "RANDOMIZED",
                    Self::Auto => "AUTO",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "UNSPECIFIED" => Some(Self::Unspecified),
                    "FULL" => Some(Self::Full),
                    "RANDOMIZED" => Some(Self::Randomized),
                    "AUTO" => Some(Self::Auto),
                    _ => None,
                }
            }
        }
    }
    /// Model registry options.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct ModelRegistryOptionEnums {}
    /// Nested message and enum types in `ModelRegistryOptionEnums`.
    pub mod model_registry_option_enums {
        /// Enums for supported model registries.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum ModelRegistry {
            /// Default value.
            Unspecified = 0,
            /// Vertex AI.
            VertexAi = 1,
        }
        impl ModelRegistry {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "MODEL_REGISTRY_UNSPECIFIED",
                    Self::VertexAi => "VERTEX_AI",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "MODEL_REGISTRY_UNSPECIFIED" => Some(Self::Unspecified),
                    "VERTEX_AI" => Some(Self::VertexAi),
                    _ => None,
                }
            }
        }
    }
    /// Information about a single training query run for the model.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct TrainingRun {
        /// Output only. Options that were used for this training run, includes
        /// user specified and default options that were used.
        #[prost(message, optional, tag = "1")]
        pub training_options: ::core::option::Option<training_run::TrainingOptions>,
        /// Output only. The start time of this training run.
        #[prost(message, optional, tag = "8")]
        pub start_time: ::core::option::Option<::prost_types::Timestamp>,
        /// Output only. Output of each iteration run, results.size() <=
        /// max_iterations.
        #[prost(message, repeated, tag = "6")]
        pub results: ::prost::alloc::vec::Vec<training_run::IterationResult>,
        /// Output only. The evaluation metrics over training/eval data that were
        /// computed at the end of training.
        #[prost(message, optional, tag = "7")]
        pub evaluation_metrics: ::core::option::Option<EvaluationMetrics>,
        /// Output only. Data split result of the training run. Only set when the
        /// input data is actually split.
        #[prost(message, optional, tag = "9")]
        pub data_split_result: ::core::option::Option<DataSplitResult>,
        /// Output only. Global explanation contains the explanation of top features
        /// on the model level. Applies to both regression and classification models.
        #[prost(message, optional, tag = "11")]
        pub model_level_global_explanation: ::core::option::Option<GlobalExplanation>,
        /// Output only. Global explanation contains the explanation of top features
        /// on the class level. Applies to classification models only.
        #[prost(message, repeated, tag = "12")]
        pub class_level_global_explanations: ::prost::alloc::vec::Vec<GlobalExplanation>,
        /// The model id in the [Vertex AI Model
        /// Registry](<https://cloud.google.com/vertex-ai/docs/model-registry/introduction>)
        /// for this training run.
        #[prost(string, tag = "14")]
        pub vertex_ai_model_id: ::prost::alloc::string::String,
        /// Output only. The model version in the [Vertex AI Model
        /// Registry](<https://cloud.google.com/vertex-ai/docs/model-registry/introduction>)
        /// for this training run.
        #[prost(string, tag = "15")]
        pub vertex_ai_model_version: ::prost::alloc::string::String,
    }
    /// Nested message and enum types in `TrainingRun`.
    pub mod training_run {
        /// Options used in model training.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct TrainingOptions {
            /// The maximum number of iterations in training. Used only for iterative
            /// training algorithms.
            #[prost(int64, tag = "1")]
            pub max_iterations: i64,
            /// Type of loss function used during training run.
            #[prost(enumeration = "super::LossType", tag = "2")]
            pub loss_type: i32,
            /// Learning rate in training. Used only for iterative training algorithms.
            #[prost(double, tag = "3")]
            pub learn_rate: f64,
            /// L1 regularization coefficient.
            #[prost(message, optional, tag = "4")]
            pub l1_regularization: ::core::option::Option<f64>,
            /// L2 regularization coefficient.
            #[prost(message, optional, tag = "5")]
            pub l2_regularization: ::core::option::Option<f64>,
            /// When early_stop is true, stops training when accuracy improvement is
            /// less than 'min_relative_progress'. Used only for iterative training
            /// algorithms.
            #[prost(message, optional, tag = "6")]
            pub min_relative_progress: ::core::option::Option<f64>,
            /// Whether to train a model from the last checkpoint.
            #[prost(message, optional, tag = "7")]
            pub warm_start: ::core::option::Option<bool>,
            /// Whether to stop early when the loss doesn't improve significantly
            /// any more (compared to min_relative_progress). Used only for iterative
            /// training algorithms.
            #[prost(message, optional, tag = "8")]
            pub early_stop: ::core::option::Option<bool>,
            /// Name of input label columns in training data.
            #[prost(string, repeated, tag = "9")]
            pub input_label_columns: ::prost::alloc::vec::Vec<
                ::prost::alloc::string::String,
            >,
            /// The data split type for training and evaluation, e.g. RANDOM.
            #[prost(enumeration = "super::DataSplitMethod", tag = "10")]
            pub data_split_method: i32,
            /// The fraction of evaluation data over the whole input data. The rest
            /// of data will be used as training data. The format should be double.
            /// Accurate to two decimal places.
            /// Default value is 0.2.
            #[prost(double, tag = "11")]
            pub data_split_eval_fraction: f64,
            /// The column to split data with. This column won't be used as a
            /// feature.
            /// 1. When data_split_method is CUSTOM, the corresponding column should
            /// be boolean. The rows with true value tag are eval data, and the false
            /// are training data.
            /// 2. When data_split_method is SEQ, the first DATA_SPLIT_EVAL_FRACTION
            /// rows (from smallest to largest) in the corresponding column are used
            /// as training data, and the rest are eval data. It respects the order
            /// in Orderable data types:
            /// <https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#data-type-properties>
            #[prost(string, tag = "12")]
            pub data_split_column: ::prost::alloc::string::String,
            /// The strategy to determine learn rate for the current iteration.
            #[prost(enumeration = "super::LearnRateStrategy", tag = "13")]
            pub learn_rate_strategy: i32,
            /// Specifies the initial learning rate for the line search learn rate
            /// strategy.
            #[prost(double, tag = "16")]
            pub initial_learn_rate: f64,
            /// Weights associated with each label class, for rebalancing the
            /// training data. Only applicable for classification models.
            #[prost(map = "string, double", tag = "17")]
            pub label_class_weights: ::std::collections::HashMap<
                ::prost::alloc::string::String,
                f64,
            >,
            /// User column specified for matrix factorization models.
            #[prost(string, tag = "18")]
            pub user_column: ::prost::alloc::string::String,
            /// Item column specified for matrix factorization models.
            #[prost(string, tag = "19")]
            pub item_column: ::prost::alloc::string::String,
            /// Distance type for clustering models.
            #[prost(enumeration = "super::DistanceType", tag = "20")]
            pub distance_type: i32,
            /// Number of clusters for clustering models.
            #[prost(int64, tag = "21")]
            pub num_clusters: i64,
            /// Google Cloud Storage URI from which the model was imported. Only
            /// applicable for imported models.
            #[prost(string, tag = "22")]
            pub model_uri: ::prost::alloc::string::String,
            /// Optimization strategy for training linear regression models.
            #[prost(enumeration = "super::OptimizationStrategy", tag = "23")]
            pub optimization_strategy: i32,
            /// Hidden units for dnn models.
            #[prost(int64, repeated, tag = "24")]
            pub hidden_units: ::prost::alloc::vec::Vec<i64>,
            /// Batch size for dnn models.
            #[prost(int64, tag = "25")]
            pub batch_size: i64,
            /// Dropout probability for dnn models.
            #[prost(message, optional, tag = "26")]
            pub dropout: ::core::option::Option<f64>,
            /// Maximum depth of a tree for boosted tree models.
            #[prost(int64, tag = "27")]
            pub max_tree_depth: i64,
            /// Subsample fraction of the training data to grow tree to prevent
            /// overfitting for boosted tree models.
            #[prost(double, tag = "28")]
            pub subsample: f64,
            /// Minimum split loss for boosted tree models.
            #[prost(message, optional, tag = "29")]
            pub min_split_loss: ::core::option::Option<f64>,
            /// Booster type for boosted tree models.
            #[prost(
                enumeration = "super::boosted_tree_option_enums::BoosterType",
                tag = "60"
            )]
            pub booster_type: i32,
            /// Number of parallel trees constructed during each iteration for boosted
            /// tree models.
            #[prost(message, optional, tag = "61")]
            pub num_parallel_tree: ::core::option::Option<i64>,
            /// Type of normalization algorithm for boosted tree models using
            /// dart booster.
            #[prost(
                enumeration = "super::boosted_tree_option_enums::DartNormalizeType",
                tag = "62"
            )]
            pub dart_normalize_type: i32,
            /// Tree construction algorithm for boosted tree models.
            #[prost(
                enumeration = "super::boosted_tree_option_enums::TreeMethod",
                tag = "63"
            )]
            pub tree_method: i32,
            /// Minimum sum of instance weight needed in a child for boosted tree
            /// models.
            #[prost(message, optional, tag = "64")]
            pub min_tree_child_weight: ::core::option::Option<i64>,
            /// Subsample ratio of columns when constructing each tree for boosted tree
            /// models.
            #[prost(message, optional, tag = "65")]
            pub colsample_bytree: ::core::option::Option<f64>,
            /// Subsample ratio of columns for each level for boosted tree models.
            #[prost(message, optional, tag = "66")]
            pub colsample_bylevel: ::core::option::Option<f64>,
            /// Subsample ratio of columns for each node(split) for boosted tree
            /// models.
            #[prost(message, optional, tag = "67")]
            pub colsample_bynode: ::core::option::Option<f64>,
            /// Num factors specified for matrix factorization models.
            #[prost(int64, tag = "30")]
            pub num_factors: i64,
            /// Feedback type that specifies which algorithm to run for matrix
            /// factorization.
            #[prost(enumeration = "super::FeedbackType", tag = "31")]
            pub feedback_type: i32,
            /// Hyperparameter for matrix factoration when implicit feedback type is
            /// specified.
            #[prost(message, optional, tag = "32")]
            pub wals_alpha: ::core::option::Option<f64>,
            /// The method used to initialize the centroids for kmeans algorithm.
            #[prost(
                enumeration = "super::kmeans_enums::KmeansInitializationMethod",
                tag = "33"
            )]
            pub kmeans_initialization_method: i32,
            /// The column used to provide the initial centroids for kmeans algorithm
            /// when kmeans_initialization_method is CUSTOM.
            #[prost(string, tag = "34")]
            pub kmeans_initialization_column: ::prost::alloc::string::String,
            /// Column to be designated as time series timestamp for ARIMA model.
            #[prost(string, tag = "35")]
            pub time_series_timestamp_column: ::prost::alloc::string::String,
            /// Column to be designated as time series data for ARIMA model.
            #[prost(string, tag = "36")]
            pub time_series_data_column: ::prost::alloc::string::String,
            /// Whether to enable auto ARIMA or not.
            #[prost(message, optional, tag = "37")]
            pub auto_arima: ::core::option::Option<bool>,
            /// A specification of the non-seasonal part of the ARIMA model: the three
            /// components (p, d, q) are the AR order, the degree of differencing, and
            /// the MA order.
            #[prost(message, optional, tag = "38")]
            pub non_seasonal_order: ::core::option::Option<super::ArimaOrder>,
            /// The data frequency of a time series.
            #[prost(enumeration = "super::DataFrequency", tag = "39")]
            pub data_frequency: i32,
            /// Whether or not p-value test should be computed for this model. Only
            /// available for linear and logistic regression models.
            #[prost(message, optional, tag = "40")]
            pub calculate_p_values: ::core::option::Option<bool>,
            /// Include drift when fitting an ARIMA model.
            #[prost(message, optional, tag = "41")]
            pub include_drift: ::core::option::Option<bool>,
            /// The geographical region based on which the holidays are considered in
            /// time series modeling. If a valid value is specified, then holiday
            /// effects modeling is enabled.
            #[prost(enumeration = "super::HolidayRegion", tag = "42")]
            pub holiday_region: i32,
            /// A list of geographical regions that are used for time series modeling.
            #[prost(enumeration = "super::HolidayRegion", repeated, tag = "71")]
            pub holiday_regions: ::prost::alloc::vec::Vec<i32>,
            /// The time series id column that was used during ARIMA model training.
            #[prost(string, tag = "43")]
            pub time_series_id_column: ::prost::alloc::string::String,
            /// The time series id columns that were used during ARIMA model training.
            #[prost(string, repeated, tag = "51")]
            pub time_series_id_columns: ::prost::alloc::vec::Vec<
                ::prost::alloc::string::String,
            >,
            /// The number of periods ahead that need to be forecasted.
            #[prost(int64, tag = "44")]
            pub horizon: i64,
            /// The max value of the sum of non-seasonal p and q.
            #[prost(int64, tag = "46")]
            pub auto_arima_max_order: i64,
            /// The min value of the sum of non-seasonal p and q.
            #[prost(int64, tag = "83")]
            pub auto_arima_min_order: i64,
            /// Number of trials to run this hyperparameter tuning job.
            #[prost(int64, tag = "47")]
            pub num_trials: i64,
            /// Maximum number of trials to run in parallel.
            #[prost(int64, tag = "48")]
            pub max_parallel_trials: i64,
            /// The target evaluation metrics to optimize the hyperparameters for.
            #[prost(
                enumeration = "super::hparam_tuning_enums::HparamTuningObjective",
                repeated,
                tag = "54"
            )]
            pub hparam_tuning_objectives: ::prost::alloc::vec::Vec<i32>,
            /// If true, perform decompose time series and save the results.
            #[prost(message, optional, tag = "50")]
            pub decompose_time_series: ::core::option::Option<bool>,
            /// If true, clean spikes and dips in the input time series.
            #[prost(message, optional, tag = "52")]
            pub clean_spikes_and_dips: ::core::option::Option<bool>,
            /// If true, detect step changes and make data adjustment in the input time
            /// series.
            #[prost(message, optional, tag = "53")]
            pub adjust_step_changes: ::core::option::Option<bool>,
            /// If true, enable global explanation during training.
            #[prost(message, optional, tag = "55")]
            pub enable_global_explain: ::core::option::Option<bool>,
            /// Number of paths for the sampled Shapley explain method.
            #[prost(int64, tag = "56")]
            pub sampled_shapley_num_paths: i64,
            /// Number of integral steps for the integrated gradients explain method.
            #[prost(int64, tag = "57")]
            pub integrated_gradients_num_steps: i64,
            /// Categorical feature encoding method.
            #[prost(
                enumeration = "super::category_encoding_method::EncodingMethod",
                tag = "58"
            )]
            pub category_encoding_method: i32,
            /// Based on the selected TF version, the corresponding docker image is
            /// used to train external models.
            #[prost(string, tag = "70")]
            pub tf_version: ::prost::alloc::string::String,
            /// Enums for color space, used for processing images in Object Table.
            /// See more details at
            /// <https://www.tensorflow.org/io/tutorials/colorspace.>
            #[prost(enumeration = "super::ColorSpace", tag = "72")]
            pub color_space: i32,
            /// Name of the instance weight column for training data.
            /// This column isn't be used as a feature.
            #[prost(string, tag = "73")]
            pub instance_weight_column: ::prost::alloc::string::String,
            /// Smoothing window size for the trend component. When a positive value is
            /// specified, a center moving average smoothing is applied on the history
            /// trend. When the smoothing window is out of the boundary at the
            /// beginning or the end of the trend, the first element or the last
            /// element is padded to fill the smoothing window before the average is
            /// applied.
            #[prost(int64, tag = "74")]
            pub trend_smoothing_window_size: i64,
            /// The fraction of the interpolated length of the time series that's used
            /// to model the time series trend component. All of the time points of the
            /// time series are used to model the non-trend component. This training
            /// option accelerates modeling training without sacrificing much
            /// forecasting accuracy. You can use this option with
            /// `minTimeSeriesLength` but not with `maxTimeSeriesLength`.
            #[prost(double, tag = "75")]
            pub time_series_length_fraction: f64,
            /// The minimum number of time points in a time series that are used in
            /// modeling the trend component of the time series. If you use this option
            /// you must also set the `timeSeriesLengthFraction` option. This training
            /// option ensures that enough time points are available when you use
            /// `timeSeriesLengthFraction` in trend modeling. This is particularly
            /// important when forecasting multiple time series in a single query using
            /// `timeSeriesIdColumn`. If the total number of time points is less than
            /// the `minTimeSeriesLength` value, then the query uses all available time
            /// points.
            #[prost(int64, tag = "76")]
            pub min_time_series_length: i64,
            /// The maximum number of time points in a time series that can be used in
            /// modeling the trend component of the time series. Don't use this option
            /// with the `timeSeriesLengthFraction` or `minTimeSeriesLength` options.
            #[prost(int64, tag = "77")]
            pub max_time_series_length: i64,
            /// User-selected XGBoost versions for training of XGBoost models.
            #[prost(string, tag = "78")]
            pub xgboost_version: ::prost::alloc::string::String,
            /// Whether to use approximate feature contribution method in XGBoost model
            /// explanation for global explain.
            #[prost(message, optional, tag = "84")]
            pub approx_global_feature_contrib: ::core::option::Option<bool>,
            /// Whether the model should include intercept during model training.
            #[prost(message, optional, tag = "85")]
            pub fit_intercept: ::core::option::Option<bool>,
            /// Number of principal components to keep in the PCA model. Must be <= the
            /// number of features.
            #[prost(int64, tag = "86")]
            pub num_principal_components: i64,
            /// The minimum ratio of cumulative explained variance that needs to be
            /// given by the PCA model.
            #[prost(double, tag = "87")]
            pub pca_explained_variance_ratio: f64,
            /// If true, scale the feature values by dividing the feature standard
            /// deviation. Currently only apply to PCA.
            #[prost(message, optional, tag = "88")]
            pub scale_features: ::core::option::Option<bool>,
            /// The solver for PCA.
            #[prost(
                enumeration = "super::pca_solver_option_enums::PcaSolver",
                tag = "89"
            )]
            pub pca_solver: i32,
            /// Whether to calculate class weights automatically based on the
            /// popularity of each label.
            #[prost(message, optional, tag = "90")]
            pub auto_class_weights: ::core::option::Option<bool>,
            /// Activation function of the neural nets.
            #[prost(string, tag = "91")]
            pub activation_fn: ::prost::alloc::string::String,
            /// Optimizer used for training the neural nets.
            #[prost(string, tag = "92")]
            pub optimizer: ::prost::alloc::string::String,
            /// Budget in hours for AutoML training.
            #[prost(double, tag = "93")]
            pub budget_hours: f64,
            /// Whether to standardize numerical features. Default to true.
            #[prost(message, optional, tag = "94")]
            pub standardize_features: ::core::option::Option<bool>,
            /// L1 regularization coefficient to activations.
            #[prost(double, tag = "95")]
            pub l1_reg_activation: f64,
            /// The model registry.
            #[prost(
                enumeration = "super::model_registry_option_enums::ModelRegistry",
                tag = "96"
            )]
            pub model_registry: i32,
            /// The version aliases to apply in Vertex AI model registry. Always
            /// overwrite if the version aliases exists in a existing model.
            #[prost(string, repeated, tag = "97")]
            pub vertex_ai_model_version_aliases: ::prost::alloc::vec::Vec<
                ::prost::alloc::string::String,
            >,
            /// Optional. Names of the columns to slice on. Applies to contribution
            /// analysis models.
            #[prost(string, repeated, tag = "104")]
            pub dimension_id_columns: ::prost::alloc::vec::Vec<
                ::prost::alloc::string::String,
            >,
            /// The contribution metric. Applies to contribution analysis models.
            /// Allowed formats supported are for summable and summable ratio
            /// contribution metrics. These include expressions such as `SUM(x)` or
            /// `SUM(x)/SUM(y)`, where x and y are column names from the base table.
            #[prost(string, optional, tag = "105")]
            pub contribution_metric: ::core::option::Option<
                ::prost::alloc::string::String,
            >,
            /// Name of the column used to determine the rows corresponding to control
            /// and test. Applies to contribution analysis models.
            #[prost(string, optional, tag = "106")]
            pub is_test_column: ::core::option::Option<::prost::alloc::string::String>,
            /// The apriori support minimum. Applies to contribution analysis models.
            #[prost(double, optional, tag = "107")]
            pub min_apriori_support: ::core::option::Option<f64>,
        }
        /// Information about a single iteration of the training run.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct IterationResult {
            /// Index of the iteration, 0 based.
            #[prost(message, optional, tag = "1")]
            pub index: ::core::option::Option<i32>,
            /// Time taken to run the iteration in milliseconds.
            #[prost(message, optional, tag = "4")]
            pub duration_ms: ::core::option::Option<i64>,
            /// Loss computed on the training data at the end of iteration.
            #[prost(message, optional, tag = "5")]
            pub training_loss: ::core::option::Option<f64>,
            /// Loss computed on the eval data at the end of iteration.
            #[prost(message, optional, tag = "6")]
            pub eval_loss: ::core::option::Option<f64>,
            /// Learn rate used for this iteration.
            #[prost(double, tag = "7")]
            pub learn_rate: f64,
            /// Information about top clusters for clustering models.
            #[prost(message, repeated, tag = "8")]
            pub cluster_infos: ::prost::alloc::vec::Vec<iteration_result::ClusterInfo>,
            /// Arima result.
            #[prost(message, optional, tag = "9")]
            pub arima_result: ::core::option::Option<iteration_result::ArimaResult>,
            /// The information of the principal components.
            #[prost(message, repeated, tag = "10")]
            pub principal_component_infos: ::prost::alloc::vec::Vec<
                iteration_result::PrincipalComponentInfo,
            >,
        }
        /// Nested message and enum types in `IterationResult`.
        pub mod iteration_result {
            /// Information about a single cluster for clustering model.
            #[derive(Clone, Copy, PartialEq, ::prost::Message)]
            pub struct ClusterInfo {
                /// Centroid id.
                #[prost(int64, tag = "1")]
                pub centroid_id: i64,
                /// Cluster radius, the average distance from centroid
                /// to each point assigned to the cluster.
                #[prost(message, optional, tag = "2")]
                pub cluster_radius: ::core::option::Option<f64>,
                /// Cluster size, the total number of points assigned to the cluster.
                #[prost(message, optional, tag = "3")]
                pub cluster_size: ::core::option::Option<i64>,
            }
            /// (Auto-)arima fitting result. Wrap everything in ArimaResult for easier
            /// refactoring if we want to use model-specific iteration results.
            #[derive(Clone, PartialEq, ::prost::Message)]
            pub struct ArimaResult {
                /// This message is repeated because there are multiple arima models
                /// fitted in auto-arima. For non-auto-arima model, its size is one.
                #[prost(message, repeated, tag = "1")]
                pub arima_model_info: ::prost::alloc::vec::Vec<
                    arima_result::ArimaModelInfo,
                >,
                /// Seasonal periods. Repeated because multiple periods are supported for
                /// one time series.
                #[prost(
                    enumeration = "super::super::seasonal_period::SeasonalPeriodType",
                    repeated,
                    tag = "2"
                )]
                pub seasonal_periods: ::prost::alloc::vec::Vec<i32>,
            }
            /// Nested message and enum types in `ArimaResult`.
            pub mod arima_result {
                /// Arima coefficients.
                #[derive(Clone, PartialEq, ::prost::Message)]
                pub struct ArimaCoefficients {
                    /// Auto-regressive coefficients, an array of double.
                    #[prost(double, repeated, tag = "1")]
                    pub auto_regressive_coefficients: ::prost::alloc::vec::Vec<f64>,
                    /// Moving-average coefficients, an array of double.
                    #[prost(double, repeated, tag = "2")]
                    pub moving_average_coefficients: ::prost::alloc::vec::Vec<f64>,
                    /// Intercept coefficient, just a double not an array.
                    #[prost(message, optional, tag = "3")]
                    pub intercept_coefficient: ::core::option::Option<f64>,
                }
                /// Arima model information.
                #[derive(Clone, PartialEq, ::prost::Message)]
                pub struct ArimaModelInfo {
                    /// Non-seasonal order.
                    #[prost(message, optional, tag = "1")]
                    pub non_seasonal_order: ::core::option::Option<
                        super::super::super::ArimaOrder,
                    >,
                    /// Arima coefficients.
                    #[prost(message, optional, tag = "2")]
                    pub arima_coefficients: ::core::option::Option<ArimaCoefficients>,
                    /// Arima fitting metrics.
                    #[prost(message, optional, tag = "3")]
                    pub arima_fitting_metrics: ::core::option::Option<
                        super::super::super::ArimaFittingMetrics,
                    >,
                    /// Whether Arima model fitted with drift or not. It is always false
                    /// when d is not 1.
                    #[prost(message, optional, tag = "4")]
                    pub has_drift: ::core::option::Option<bool>,
                    /// The time_series_id value for this time series. It will be one of
                    /// the unique values from the time_series_id_column specified during
                    /// ARIMA model training. Only present when time_series_id_column
                    /// training option was used.
                    #[prost(string, tag = "5")]
                    pub time_series_id: ::prost::alloc::string::String,
                    /// The tuple of time_series_ids identifying this time series. It will
                    /// be one of the unique tuples of values present in the
                    /// time_series_id_columns specified during ARIMA model training. Only
                    /// present when time_series_id_columns training option was used and
                    /// the order of values here are same as the order of
                    /// time_series_id_columns.
                    #[prost(string, repeated, tag = "10")]
                    pub time_series_ids: ::prost::alloc::vec::Vec<
                        ::prost::alloc::string::String,
                    >,
                    /// Seasonal periods. Repeated because multiple periods are supported
                    /// for one time series.
                    #[prost(
                        enumeration = "super::super::super::seasonal_period::SeasonalPeriodType",
                        repeated,
                        tag = "6"
                    )]
                    pub seasonal_periods: ::prost::alloc::vec::Vec<i32>,
                    /// If true, holiday_effect is a part of time series decomposition
                    /// result.
                    #[prost(message, optional, tag = "7")]
                    pub has_holiday_effect: ::core::option::Option<bool>,
                    /// If true, spikes_and_dips is a part of time series decomposition
                    /// result.
                    #[prost(message, optional, tag = "8")]
                    pub has_spikes_and_dips: ::core::option::Option<bool>,
                    /// If true, step_changes is a part of time series decomposition
                    /// result.
                    #[prost(message, optional, tag = "9")]
                    pub has_step_changes: ::core::option::Option<bool>,
                }
            }
            /// Principal component infos, used only for eigen decomposition based
            /// models, e.g., PCA. Ordered by explained_variance in the descending
            /// order.
            #[derive(Clone, Copy, PartialEq, ::prost::Message)]
            pub struct PrincipalComponentInfo {
                /// Id of the principal component.
                #[prost(message, optional, tag = "1")]
                pub principal_component_id: ::core::option::Option<i64>,
                /// Explained variance by this principal component, which is simply the
                /// eigenvalue.
                #[prost(message, optional, tag = "2")]
                pub explained_variance: ::core::option::Option<f64>,
                /// Explained_variance over the total explained variance.
                #[prost(message, optional, tag = "3")]
                pub explained_variance_ratio: ::core::option::Option<f64>,
                /// The explained_variance is pre-ordered in the descending order to
                /// compute the cumulative explained variance ratio.
                #[prost(message, optional, tag = "4")]
                pub cumulative_explained_variance_ratio: ::core::option::Option<f64>,
            }
        }
    }
    /// Search space for a double hyperparameter.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct DoubleHparamSearchSpace {
        /// Search space.
        #[prost(oneof = "double_hparam_search_space::SearchSpace", tags = "1, 2")]
        pub search_space: ::core::option::Option<
            double_hparam_search_space::SearchSpace,
        >,
    }
    /// Nested message and enum types in `DoubleHparamSearchSpace`.
    pub mod double_hparam_search_space {
        /// Range of a double hyperparameter.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct DoubleRange {
            /// Min value of the double parameter.
            #[prost(message, optional, tag = "1")]
            pub min: ::core::option::Option<f64>,
            /// Max value of the double parameter.
            #[prost(message, optional, tag = "2")]
            pub max: ::core::option::Option<f64>,
        }
        /// Discrete candidates of a double hyperparameter.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct DoubleCandidates {
            /// Candidates for the double parameter in increasing order.
            #[prost(message, repeated, tag = "1")]
            pub candidates: ::prost::alloc::vec::Vec<f64>,
        }
        /// Search space.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum SearchSpace {
            /// Range of the double hyperparameter.
            #[prost(message, tag = "1")]
            Range(DoubleRange),
            /// Candidates of the double hyperparameter.
            #[prost(message, tag = "2")]
            Candidates(DoubleCandidates),
        }
    }
    /// Search space for an int hyperparameter.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct IntHparamSearchSpace {
        /// Search space.
        #[prost(oneof = "int_hparam_search_space::SearchSpace", tags = "1, 2")]
        pub search_space: ::core::option::Option<int_hparam_search_space::SearchSpace>,
    }
    /// Nested message and enum types in `IntHparamSearchSpace`.
    pub mod int_hparam_search_space {
        /// Range of an int hyperparameter.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct IntRange {
            /// Min value of the int parameter.
            #[prost(message, optional, tag = "1")]
            pub min: ::core::option::Option<i64>,
            /// Max value of the int parameter.
            #[prost(message, optional, tag = "2")]
            pub max: ::core::option::Option<i64>,
        }
        /// Discrete candidates of an int hyperparameter.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct IntCandidates {
            /// Candidates for the int parameter in increasing order.
            #[prost(message, repeated, tag = "1")]
            pub candidates: ::prost::alloc::vec::Vec<i64>,
        }
        /// Search space.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum SearchSpace {
            /// Range of the int hyperparameter.
            #[prost(message, tag = "1")]
            Range(IntRange),
            /// Candidates of the int hyperparameter.
            #[prost(message, tag = "2")]
            Candidates(IntCandidates),
        }
    }
    /// Search space for string and enum.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct StringHparamSearchSpace {
        /// Canididates for the string or enum parameter in lower case.
        #[prost(string, repeated, tag = "1")]
        pub candidates: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    }
    /// Search space for int array.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct IntArrayHparamSearchSpace {
        /// Candidates for the int array parameter.
        #[prost(message, repeated, tag = "1")]
        pub candidates: ::prost::alloc::vec::Vec<
            int_array_hparam_search_space::IntArray,
        >,
    }
    /// Nested message and enum types in `IntArrayHparamSearchSpace`.
    pub mod int_array_hparam_search_space {
        /// An array of int.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct IntArray {
            /// Elements in the int array.
            #[prost(int64, repeated, tag = "1")]
            pub elements: ::prost::alloc::vec::Vec<i64>,
        }
    }
    /// Hyperparameter search spaces.
    /// These should be a subset of training_options.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct HparamSearchSpaces {
        /// Learning rate of training jobs.
        #[prost(message, optional, tag = "2")]
        pub learn_rate: ::core::option::Option<DoubleHparamSearchSpace>,
        /// L1 regularization coefficient.
        #[prost(message, optional, tag = "3")]
        pub l1_reg: ::core::option::Option<DoubleHparamSearchSpace>,
        /// L2 regularization coefficient.
        #[prost(message, optional, tag = "4")]
        pub l2_reg: ::core::option::Option<DoubleHparamSearchSpace>,
        /// Number of clusters for k-means.
        #[prost(message, optional, tag = "26")]
        pub num_clusters: ::core::option::Option<IntHparamSearchSpace>,
        /// Number of latent factors to train on.
        #[prost(message, optional, tag = "31")]
        pub num_factors: ::core::option::Option<IntHparamSearchSpace>,
        /// Hidden units for neural network models.
        #[prost(message, optional, tag = "34")]
        pub hidden_units: ::core::option::Option<IntArrayHparamSearchSpace>,
        /// Mini batch sample size.
        #[prost(message, optional, tag = "37")]
        pub batch_size: ::core::option::Option<IntHparamSearchSpace>,
        /// Dropout probability for dnn model training and boosted tree models
        /// using dart booster.
        #[prost(message, optional, tag = "38")]
        pub dropout: ::core::option::Option<DoubleHparamSearchSpace>,
        /// Maximum depth of a tree for boosted tree models.
        #[prost(message, optional, tag = "41")]
        pub max_tree_depth: ::core::option::Option<IntHparamSearchSpace>,
        /// Subsample the training data to grow tree to prevent overfitting for
        /// boosted tree models.
        #[prost(message, optional, tag = "42")]
        pub subsample: ::core::option::Option<DoubleHparamSearchSpace>,
        /// Minimum split loss for boosted tree models.
        #[prost(message, optional, tag = "43")]
        pub min_split_loss: ::core::option::Option<DoubleHparamSearchSpace>,
        /// Hyperparameter for matrix factoration when implicit feedback type is
        /// specified.
        #[prost(message, optional, tag = "49")]
        pub wals_alpha: ::core::option::Option<DoubleHparamSearchSpace>,
        /// Booster type for boosted tree models.
        #[prost(message, optional, tag = "56")]
        pub booster_type: ::core::option::Option<StringHparamSearchSpace>,
        /// Number of parallel trees for boosted tree models.
        #[prost(message, optional, tag = "57")]
        pub num_parallel_tree: ::core::option::Option<IntHparamSearchSpace>,
        /// Dart normalization type for boosted tree models.
        #[prost(message, optional, tag = "58")]
        pub dart_normalize_type: ::core::option::Option<StringHparamSearchSpace>,
        /// Tree construction algorithm for boosted tree models.
        #[prost(message, optional, tag = "59")]
        pub tree_method: ::core::option::Option<StringHparamSearchSpace>,
        /// Minimum sum of instance weight needed in a child for boosted tree models.
        #[prost(message, optional, tag = "60")]
        pub min_tree_child_weight: ::core::option::Option<IntHparamSearchSpace>,
        /// Subsample ratio of columns when constructing each tree for boosted tree
        /// models.
        #[prost(message, optional, tag = "61")]
        pub colsample_bytree: ::core::option::Option<DoubleHparamSearchSpace>,
        /// Subsample ratio of columns for each level for boosted tree models.
        #[prost(message, optional, tag = "62")]
        pub colsample_bylevel: ::core::option::Option<DoubleHparamSearchSpace>,
        /// Subsample ratio of columns for each node(split) for boosted tree models.
        #[prost(message, optional, tag = "63")]
        pub colsample_bynode: ::core::option::Option<DoubleHparamSearchSpace>,
        /// Activation functions of neural network models.
        #[prost(message, optional, tag = "67")]
        pub activation_fn: ::core::option::Option<StringHparamSearchSpace>,
        /// Optimizer of TF models.
        #[prost(message, optional, tag = "68")]
        pub optimizer: ::core::option::Option<StringHparamSearchSpace>,
    }
    /// Training info of a trial in [hyperparameter
    /// tuning](<https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview>)
    /// models.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct HparamTuningTrial {
        /// 1-based index of the trial.
        #[prost(int64, tag = "1")]
        pub trial_id: i64,
        /// Starting time of the trial.
        #[prost(int64, tag = "2")]
        pub start_time_ms: i64,
        /// Ending time of the trial.
        #[prost(int64, tag = "3")]
        pub end_time_ms: i64,
        /// The hyperprameters selected for this trial.
        #[prost(message, optional, tag = "4")]
        pub hparams: ::core::option::Option<training_run::TrainingOptions>,
        /// Evaluation metrics of this trial calculated on the test data.
        /// Empty in Job API.
        #[prost(message, optional, tag = "5")]
        pub evaluation_metrics: ::core::option::Option<EvaluationMetrics>,
        /// The status of the trial.
        #[prost(enumeration = "hparam_tuning_trial::TrialStatus", tag = "6")]
        pub status: i32,
        /// Error message for FAILED and INFEASIBLE trial.
        #[prost(string, tag = "7")]
        pub error_message: ::prost::alloc::string::String,
        /// Loss computed on the training data at the end of trial.
        #[prost(message, optional, tag = "8")]
        pub training_loss: ::core::option::Option<f64>,
        /// Loss computed on the eval data at the end of trial.
        #[prost(message, optional, tag = "9")]
        pub eval_loss: ::core::option::Option<f64>,
        /// Hyperparameter tuning evaluation metrics of this trial calculated on the
        /// eval data. Unlike evaluation_metrics, only the fields corresponding to
        /// the hparam_tuning_objectives are set.
        #[prost(message, optional, tag = "10")]
        pub hparam_tuning_evaluation_metrics: ::core::option::Option<EvaluationMetrics>,
    }
    /// Nested message and enum types in `HparamTuningTrial`.
    pub mod hparam_tuning_trial {
        /// Current status of the trial.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum TrialStatus {
            /// Default value.
            Unspecified = 0,
            /// Scheduled but not started.
            NotStarted = 1,
            /// Running state.
            Running = 2,
            /// The trial succeeded.
            Succeeded = 3,
            /// The trial failed.
            Failed = 4,
            /// The trial is infeasible due to the invalid params.
            Infeasible = 5,
            /// Trial stopped early because it's not promising.
            StoppedEarly = 6,
        }
        impl TrialStatus {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "TRIAL_STATUS_UNSPECIFIED",
                    Self::NotStarted => "NOT_STARTED",
                    Self::Running => "RUNNING",
                    Self::Succeeded => "SUCCEEDED",
                    Self::Failed => "FAILED",
                    Self::Infeasible => "INFEASIBLE",
                    Self::StoppedEarly => "STOPPED_EARLY",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "TRIAL_STATUS_UNSPECIFIED" => Some(Self::Unspecified),
                    "NOT_STARTED" => Some(Self::NotStarted),
                    "RUNNING" => Some(Self::Running),
                    "SUCCEEDED" => Some(Self::Succeeded),
                    "FAILED" => Some(Self::Failed),
                    "INFEASIBLE" => Some(Self::Infeasible),
                    "STOPPED_EARLY" => Some(Self::StoppedEarly),
                    _ => None,
                }
            }
        }
    }
    /// Indicates the type of the Model.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum ModelType {
        /// Default value.
        Unspecified = 0,
        /// Linear regression model.
        LinearRegression = 1,
        /// Logistic regression based classification model.
        LogisticRegression = 2,
        /// K-means clustering model.
        Kmeans = 3,
        /// Matrix factorization model.
        MatrixFactorization = 4,
        /// DNN classifier model.
        DnnClassifier = 5,
        /// An imported TensorFlow model.
        Tensorflow = 6,
        /// DNN regressor model.
        DnnRegressor = 7,
        /// An imported XGBoost model.
        Xgboost = 8,
        /// Boosted tree regressor model.
        BoostedTreeRegressor = 9,
        /// Boosted tree classifier model.
        BoostedTreeClassifier = 10,
        /// ARIMA model.
        Arima = 11,
        /// AutoML Tables regression model.
        AutomlRegressor = 12,
        /// AutoML Tables classification model.
        AutomlClassifier = 13,
        /// Prinpical Component Analysis model.
        Pca = 14,
        /// Wide-and-deep classifier model.
        DnnLinearCombinedClassifier = 16,
        /// Wide-and-deep regressor model.
        DnnLinearCombinedRegressor = 17,
        /// Autoencoder model.
        Autoencoder = 18,
        /// New name for the ARIMA model.
        ArimaPlus = 19,
        /// ARIMA with external regressors.
        ArimaPlusXreg = 23,
        /// Random forest regressor model.
        RandomForestRegressor = 24,
        /// Random forest classifier model.
        RandomForestClassifier = 25,
        /// An imported TensorFlow Lite model.
        TensorflowLite = 26,
        /// An imported ONNX model.
        Onnx = 28,
        /// Model to capture the columns and logic in the TRANSFORM clause along with
        /// statistics useful for ML analytic functions.
        TransformOnly = 29,
        /// The contribution analysis model.
        ContributionAnalysis = 37,
    }
    impl ModelType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "MODEL_TYPE_UNSPECIFIED",
                Self::LinearRegression => "LINEAR_REGRESSION",
                Self::LogisticRegression => "LOGISTIC_REGRESSION",
                Self::Kmeans => "KMEANS",
                Self::MatrixFactorization => "MATRIX_FACTORIZATION",
                Self::DnnClassifier => "DNN_CLASSIFIER",
                Self::Tensorflow => "TENSORFLOW",
                Self::DnnRegressor => "DNN_REGRESSOR",
                Self::Xgboost => "XGBOOST",
                Self::BoostedTreeRegressor => "BOOSTED_TREE_REGRESSOR",
                Self::BoostedTreeClassifier => "BOOSTED_TREE_CLASSIFIER",
                Self::Arima => "ARIMA",
                Self::AutomlRegressor => "AUTOML_REGRESSOR",
                Self::AutomlClassifier => "AUTOML_CLASSIFIER",
                Self::Pca => "PCA",
                Self::DnnLinearCombinedClassifier => "DNN_LINEAR_COMBINED_CLASSIFIER",
                Self::DnnLinearCombinedRegressor => "DNN_LINEAR_COMBINED_REGRESSOR",
                Self::Autoencoder => "AUTOENCODER",
                Self::ArimaPlus => "ARIMA_PLUS",
                Self::ArimaPlusXreg => "ARIMA_PLUS_XREG",
                Self::RandomForestRegressor => "RANDOM_FOREST_REGRESSOR",
                Self::RandomForestClassifier => "RANDOM_FOREST_CLASSIFIER",
                Self::TensorflowLite => "TENSORFLOW_LITE",
                Self::Onnx => "ONNX",
                Self::TransformOnly => "TRANSFORM_ONLY",
                Self::ContributionAnalysis => "CONTRIBUTION_ANALYSIS",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "MODEL_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "LINEAR_REGRESSION" => Some(Self::LinearRegression),
                "LOGISTIC_REGRESSION" => Some(Self::LogisticRegression),
                "KMEANS" => Some(Self::Kmeans),
                "MATRIX_FACTORIZATION" => Some(Self::MatrixFactorization),
                "DNN_CLASSIFIER" => Some(Self::DnnClassifier),
                "TENSORFLOW" => Some(Self::Tensorflow),
                "DNN_REGRESSOR" => Some(Self::DnnRegressor),
                "XGBOOST" => Some(Self::Xgboost),
                "BOOSTED_TREE_REGRESSOR" => Some(Self::BoostedTreeRegressor),
                "BOOSTED_TREE_CLASSIFIER" => Some(Self::BoostedTreeClassifier),
                "ARIMA" => Some(Self::Arima),
                "AUTOML_REGRESSOR" => Some(Self::AutomlRegressor),
                "AUTOML_CLASSIFIER" => Some(Self::AutomlClassifier),
                "PCA" => Some(Self::Pca),
                "DNN_LINEAR_COMBINED_CLASSIFIER" => {
                    Some(Self::DnnLinearCombinedClassifier)
                }
                "DNN_LINEAR_COMBINED_REGRESSOR" => Some(Self::DnnLinearCombinedRegressor),
                "AUTOENCODER" => Some(Self::Autoencoder),
                "ARIMA_PLUS" => Some(Self::ArimaPlus),
                "ARIMA_PLUS_XREG" => Some(Self::ArimaPlusXreg),
                "RANDOM_FOREST_REGRESSOR" => Some(Self::RandomForestRegressor),
                "RANDOM_FOREST_CLASSIFIER" => Some(Self::RandomForestClassifier),
                "TENSORFLOW_LITE" => Some(Self::TensorflowLite),
                "ONNX" => Some(Self::Onnx),
                "TRANSFORM_ONLY" => Some(Self::TransformOnly),
                "CONTRIBUTION_ANALYSIS" => Some(Self::ContributionAnalysis),
                _ => None,
            }
        }
    }
    /// Loss metric to evaluate model training performance.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum LossType {
        /// Default value.
        Unspecified = 0,
        /// Mean squared loss, used for linear regression.
        MeanSquaredLoss = 1,
        /// Mean log loss, used for logistic regression.
        MeanLogLoss = 2,
    }
    impl LossType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "LOSS_TYPE_UNSPECIFIED",
                Self::MeanSquaredLoss => "MEAN_SQUARED_LOSS",
                Self::MeanLogLoss => "MEAN_LOG_LOSS",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "LOSS_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "MEAN_SQUARED_LOSS" => Some(Self::MeanSquaredLoss),
                "MEAN_LOG_LOSS" => Some(Self::MeanLogLoss),
                _ => None,
            }
        }
    }
    /// Distance metric used to compute the distance between two points.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum DistanceType {
        /// Default value.
        Unspecified = 0,
        /// Eculidean distance.
        Euclidean = 1,
        /// Cosine distance.
        Cosine = 2,
    }
    impl DistanceType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "DISTANCE_TYPE_UNSPECIFIED",
                Self::Euclidean => "EUCLIDEAN",
                Self::Cosine => "COSINE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "DISTANCE_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "EUCLIDEAN" => Some(Self::Euclidean),
                "COSINE" => Some(Self::Cosine),
                _ => None,
            }
        }
    }
    /// Indicates the method to split input data into multiple tables.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum DataSplitMethod {
        /// Default value.
        Unspecified = 0,
        /// Splits data randomly.
        Random = 1,
        /// Splits data with the user provided tags.
        Custom = 2,
        /// Splits data sequentially.
        Sequential = 3,
        /// Data split will be skipped.
        NoSplit = 4,
        /// Splits data automatically: Uses NO_SPLIT if the data size is small.
        /// Otherwise uses RANDOM.
        AutoSplit = 5,
    }
    impl DataSplitMethod {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "DATA_SPLIT_METHOD_UNSPECIFIED",
                Self::Random => "RANDOM",
                Self::Custom => "CUSTOM",
                Self::Sequential => "SEQUENTIAL",
                Self::NoSplit => "NO_SPLIT",
                Self::AutoSplit => "AUTO_SPLIT",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "DATA_SPLIT_METHOD_UNSPECIFIED" => Some(Self::Unspecified),
                "RANDOM" => Some(Self::Random),
                "CUSTOM" => Some(Self::Custom),
                "SEQUENTIAL" => Some(Self::Sequential),
                "NO_SPLIT" => Some(Self::NoSplit),
                "AUTO_SPLIT" => Some(Self::AutoSplit),
                _ => None,
            }
        }
    }
    /// Type of supported data frequency for time series forecasting models.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum DataFrequency {
        /// Default value.
        Unspecified = 0,
        /// Automatically inferred from timestamps.
        AutoFrequency = 1,
        /// Yearly data.
        Yearly = 2,
        /// Quarterly data.
        Quarterly = 3,
        /// Monthly data.
        Monthly = 4,
        /// Weekly data.
        Weekly = 5,
        /// Daily data.
        Daily = 6,
        /// Hourly data.
        Hourly = 7,
        /// Per-minute data.
        PerMinute = 8,
    }
    impl DataFrequency {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "DATA_FREQUENCY_UNSPECIFIED",
                Self::AutoFrequency => "AUTO_FREQUENCY",
                Self::Yearly => "YEARLY",
                Self::Quarterly => "QUARTERLY",
                Self::Monthly => "MONTHLY",
                Self::Weekly => "WEEKLY",
                Self::Daily => "DAILY",
                Self::Hourly => "HOURLY",
                Self::PerMinute => "PER_MINUTE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "DATA_FREQUENCY_UNSPECIFIED" => Some(Self::Unspecified),
                "AUTO_FREQUENCY" => Some(Self::AutoFrequency),
                "YEARLY" => Some(Self::Yearly),
                "QUARTERLY" => Some(Self::Quarterly),
                "MONTHLY" => Some(Self::Monthly),
                "WEEKLY" => Some(Self::Weekly),
                "DAILY" => Some(Self::Daily),
                "HOURLY" => Some(Self::Hourly),
                "PER_MINUTE" => Some(Self::PerMinute),
                _ => None,
            }
        }
    }
    /// Type of supported holiday regions for time series forecasting models.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum HolidayRegion {
        /// Holiday region unspecified.
        Unspecified = 0,
        /// Global.
        Global = 1,
        /// North America.
        Na = 2,
        /// Japan and Asia Pacific: Korea, Greater China, India, Australia, and New
        /// Zealand.
        Japac = 3,
        /// Europe, the Middle East and Africa.
        Emea = 4,
        /// Latin America and the Caribbean.
        Lac = 5,
        /// United Arab Emirates
        Ae = 6,
        /// Argentina
        Ar = 7,
        /// Austria
        At = 8,
        /// Australia
        Au = 9,
        /// Belgium
        Be = 10,
        /// Brazil
        Br = 11,
        /// Canada
        Ca = 12,
        /// Switzerland
        Ch = 13,
        /// Chile
        Cl = 14,
        /// China
        Cn = 15,
        /// Colombia
        Co = 16,
        /// Czechoslovakia
        Cs = 17,
        /// Czech Republic
        Cz = 18,
        /// Germany
        De = 19,
        /// Denmark
        Dk = 20,
        /// Algeria
        Dz = 21,
        /// Ecuador
        Ec = 22,
        /// Estonia
        Ee = 23,
        /// Egypt
        Eg = 24,
        /// Spain
        Es = 25,
        /// Finland
        Fi = 26,
        /// France
        Fr = 27,
        /// Great Britain (United Kingdom)
        Gb = 28,
        /// Greece
        Gr = 29,
        /// Hong Kong
        Hk = 30,
        /// Hungary
        Hu = 31,
        /// Indonesia
        Id = 32,
        /// Ireland
        Ie = 33,
        /// Israel
        Il = 34,
        /// India
        In = 35,
        /// Iran
        Ir = 36,
        /// Italy
        It = 37,
        /// Japan
        Jp = 38,
        /// Korea (South)
        Kr = 39,
        /// Latvia
        Lv = 40,
        /// Morocco
        Ma = 41,
        /// Mexico
        Mx = 42,
        /// Malaysia
        My = 43,
        /// Nigeria
        Ng = 44,
        /// Netherlands
        Nl = 45,
        /// Norway
        No = 46,
        /// New Zealand
        Nz = 47,
        /// Peru
        Pe = 48,
        /// Philippines
        Ph = 49,
        /// Pakistan
        Pk = 50,
        /// Poland
        Pl = 51,
        /// Portugal
        Pt = 52,
        /// Romania
        Ro = 53,
        /// Serbia
        Rs = 54,
        /// Russian Federation
        Ru = 55,
        /// Saudi Arabia
        Sa = 56,
        /// Sweden
        Se = 57,
        /// Singapore
        Sg = 58,
        /// Slovenia
        Si = 59,
        /// Slovakia
        Sk = 60,
        /// Thailand
        Th = 61,
        /// Turkey
        Tr = 62,
        /// Taiwan
        Tw = 63,
        /// Ukraine
        Ua = 64,
        /// United States
        Us = 65,
        /// Venezuela
        Ve = 66,
        /// Viet Nam
        Vn = 67,
        /// South Africa
        Za = 68,
    }
    impl HolidayRegion {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "HOLIDAY_REGION_UNSPECIFIED",
                Self::Global => "GLOBAL",
                Self::Na => "NA",
                Self::Japac => "JAPAC",
                Self::Emea => "EMEA",
                Self::Lac => "LAC",
                Self::Ae => "AE",
                Self::Ar => "AR",
                Self::At => "AT",
                Self::Au => "AU",
                Self::Be => "BE",
                Self::Br => "BR",
                Self::Ca => "CA",
                Self::Ch => "CH",
                Self::Cl => "CL",
                Self::Cn => "CN",
                Self::Co => "CO",
                Self::Cs => "CS",
                Self::Cz => "CZ",
                Self::De => "DE",
                Self::Dk => "DK",
                Self::Dz => "DZ",
                Self::Ec => "EC",
                Self::Ee => "EE",
                Self::Eg => "EG",
                Self::Es => "ES",
                Self::Fi => "FI",
                Self::Fr => "FR",
                Self::Gb => "GB",
                Self::Gr => "GR",
                Self::Hk => "HK",
                Self::Hu => "HU",
                Self::Id => "ID",
                Self::Ie => "IE",
                Self::Il => "IL",
                Self::In => "IN",
                Self::Ir => "IR",
                Self::It => "IT",
                Self::Jp => "JP",
                Self::Kr => "KR",
                Self::Lv => "LV",
                Self::Ma => "MA",
                Self::Mx => "MX",
                Self::My => "MY",
                Self::Ng => "NG",
                Self::Nl => "NL",
                Self::No => "NO",
                Self::Nz => "NZ",
                Self::Pe => "PE",
                Self::Ph => "PH",
                Self::Pk => "PK",
                Self::Pl => "PL",
                Self::Pt => "PT",
                Self::Ro => "RO",
                Self::Rs => "RS",
                Self::Ru => "RU",
                Self::Sa => "SA",
                Self::Se => "SE",
                Self::Sg => "SG",
                Self::Si => "SI",
                Self::Sk => "SK",
                Self::Th => "TH",
                Self::Tr => "TR",
                Self::Tw => "TW",
                Self::Ua => "UA",
                Self::Us => "US",
                Self::Ve => "VE",
                Self::Vn => "VN",
                Self::Za => "ZA",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "HOLIDAY_REGION_UNSPECIFIED" => Some(Self::Unspecified),
                "GLOBAL" => Some(Self::Global),
                "NA" => Some(Self::Na),
                "JAPAC" => Some(Self::Japac),
                "EMEA" => Some(Self::Emea),
                "LAC" => Some(Self::Lac),
                "AE" => Some(Self::Ae),
                "AR" => Some(Self::Ar),
                "AT" => Some(Self::At),
                "AU" => Some(Self::Au),
                "BE" => Some(Self::Be),
                "BR" => Some(Self::Br),
                "CA" => Some(Self::Ca),
                "CH" => Some(Self::Ch),
                "CL" => Some(Self::Cl),
                "CN" => Some(Self::Cn),
                "CO" => Some(Self::Co),
                "CS" => Some(Self::Cs),
                "CZ" => Some(Self::Cz),
                "DE" => Some(Self::De),
                "DK" => Some(Self::Dk),
                "DZ" => Some(Self::Dz),
                "EC" => Some(Self::Ec),
                "EE" => Some(Self::Ee),
                "EG" => Some(Self::Eg),
                "ES" => Some(Self::Es),
                "FI" => Some(Self::Fi),
                "FR" => Some(Self::Fr),
                "GB" => Some(Self::Gb),
                "GR" => Some(Self::Gr),
                "HK" => Some(Self::Hk),
                "HU" => Some(Self::Hu),
                "ID" => Some(Self::Id),
                "IE" => Some(Self::Ie),
                "IL" => Some(Self::Il),
                "IN" => Some(Self::In),
                "IR" => Some(Self::Ir),
                "IT" => Some(Self::It),
                "JP" => Some(Self::Jp),
                "KR" => Some(Self::Kr),
                "LV" => Some(Self::Lv),
                "MA" => Some(Self::Ma),
                "MX" => Some(Self::Mx),
                "MY" => Some(Self::My),
                "NG" => Some(Self::Ng),
                "NL" => Some(Self::Nl),
                "NO" => Some(Self::No),
                "NZ" => Some(Self::Nz),
                "PE" => Some(Self::Pe),
                "PH" => Some(Self::Ph),
                "PK" => Some(Self::Pk),
                "PL" => Some(Self::Pl),
                "PT" => Some(Self::Pt),
                "RO" => Some(Self::Ro),
                "RS" => Some(Self::Rs),
                "RU" => Some(Self::Ru),
                "SA" => Some(Self::Sa),
                "SE" => Some(Self::Se),
                "SG" => Some(Self::Sg),
                "SI" => Some(Self::Si),
                "SK" => Some(Self::Sk),
                "TH" => Some(Self::Th),
                "TR" => Some(Self::Tr),
                "TW" => Some(Self::Tw),
                "UA" => Some(Self::Ua),
                "US" => Some(Self::Us),
                "VE" => Some(Self::Ve),
                "VN" => Some(Self::Vn),
                "ZA" => Some(Self::Za),
                _ => None,
            }
        }
    }
    /// Enums for color space, used for processing images in Object Table.
    /// See more details at
    /// <https://www.tensorflow.org/io/tutorials/colorspace.>
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum ColorSpace {
        /// Unspecified color space
        Unspecified = 0,
        /// RGB
        Rgb = 1,
        /// HSV
        Hsv = 2,
        /// YIQ
        Yiq = 3,
        /// YUV
        Yuv = 4,
        /// GRAYSCALE
        Grayscale = 5,
    }
    impl ColorSpace {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "COLOR_SPACE_UNSPECIFIED",
                Self::Rgb => "RGB",
                Self::Hsv => "HSV",
                Self::Yiq => "YIQ",
                Self::Yuv => "YUV",
                Self::Grayscale => "GRAYSCALE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "COLOR_SPACE_UNSPECIFIED" => Some(Self::Unspecified),
                "RGB" => Some(Self::Rgb),
                "HSV" => Some(Self::Hsv),
                "YIQ" => Some(Self::Yiq),
                "YUV" => Some(Self::Yuv),
                "GRAYSCALE" => Some(Self::Grayscale),
                _ => None,
            }
        }
    }
    /// Indicates the learning rate optimization strategy to use.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum LearnRateStrategy {
        /// Default value.
        Unspecified = 0,
        /// Use line search to determine learning rate.
        LineSearch = 1,
        /// Use a constant learning rate.
        Constant = 2,
    }
    impl LearnRateStrategy {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "LEARN_RATE_STRATEGY_UNSPECIFIED",
                Self::LineSearch => "LINE_SEARCH",
                Self::Constant => "CONSTANT",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "LEARN_RATE_STRATEGY_UNSPECIFIED" => Some(Self::Unspecified),
                "LINE_SEARCH" => Some(Self::LineSearch),
                "CONSTANT" => Some(Self::Constant),
                _ => None,
            }
        }
    }
    /// Indicates the optimization strategy used for training.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum OptimizationStrategy {
        /// Default value.
        Unspecified = 0,
        /// Uses an iterative batch gradient descent algorithm.
        BatchGradientDescent = 1,
        /// Uses a normal equation to solve linear regression problem.
        NormalEquation = 2,
    }
    impl OptimizationStrategy {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "OPTIMIZATION_STRATEGY_UNSPECIFIED",
                Self::BatchGradientDescent => "BATCH_GRADIENT_DESCENT",
                Self::NormalEquation => "NORMAL_EQUATION",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "OPTIMIZATION_STRATEGY_UNSPECIFIED" => Some(Self::Unspecified),
                "BATCH_GRADIENT_DESCENT" => Some(Self::BatchGradientDescent),
                "NORMAL_EQUATION" => Some(Self::NormalEquation),
                _ => None,
            }
        }
    }
    /// Indicates the training algorithm to use for matrix factorization models.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum FeedbackType {
        /// Default value.
        Unspecified = 0,
        /// Use weighted-als for implicit feedback problems.
        Implicit = 1,
        /// Use nonweighted-als for explicit feedback problems.
        Explicit = 2,
    }
    impl FeedbackType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "FEEDBACK_TYPE_UNSPECIFIED",
                Self::Implicit => "IMPLICIT",
                Self::Explicit => "EXPLICIT",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "FEEDBACK_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "IMPLICIT" => Some(Self::Implicit),
                "EXPLICIT" => Some(Self::Explicit),
                _ => None,
            }
        }
    }
}
/// Request format for getting information about a BigQuery ML model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetModelRequest {
    /// Required. Project ID of the requested model.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the requested model.
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. Model ID of the requested model.
    #[prost(string, tag = "3")]
    pub model_id: ::prost::alloc::string::String,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PatchModelRequest {
    /// Required. Project ID of the model to patch.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the model to patch.
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. Model ID of the model to patch.
    #[prost(string, tag = "3")]
    pub model_id: ::prost::alloc::string::String,
    /// Required. Patched model.
    /// Follows RFC5789 patch semantics. Missing fields are not updated.
    /// To clear a field, explicitly set to default value.
    #[prost(message, optional, tag = "4")]
    pub model: ::core::option::Option<Model>,
}
/// Request format for deleting BigQuery ML models.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteModelRequest {
    /// Required. Project ID of the model to delete.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the model to delete.
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. Model ID of the model to delete.
    #[prost(string, tag = "3")]
    pub model_id: ::prost::alloc::string::String,
}
/// Request format for listing BigQuery ML models.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsRequest {
    /// Required. Project ID of the models to list.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the models to list.
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// The maximum number of results to return in a single response page.
    /// Leverage the page tokens to iterate through the entire collection.
    #[prost(message, optional, tag = "3")]
    pub max_results: ::core::option::Option<u32>,
    /// Page token, returned by a previous call to request the next page of
    /// results
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response format for a single page when listing BigQuery ML models.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsResponse {
    /// Models in the requested dataset. Only the following fields are populated:
    /// model_reference, model_type, creation_time, last_modified_time and
    /// labels.
    #[prost(message, repeated, tag = "1")]
    pub models: ::prost::alloc::vec::Vec<Model>,
    /// A token to request the next page of results.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod model_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// Model Service for BigQuery ML
    #[derive(Debug, Clone)]
    pub struct ModelServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl ModelServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> ModelServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> ModelServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            ModelServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Gets the specified model resource by model ID.
        pub async fn get_model(
            &mut self,
            request: impl tonic::IntoRequest<super::GetModelRequest>,
        ) -> std::result::Result<tonic::Response<super::Model>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.ModelService/GetModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("google.cloud.bigquery.v2.ModelService", "GetModel"),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists all models in the specified dataset. Requires the READER dataset
        /// role. After retrieving the list of models, you can get information about a
        /// particular model by calling the models.get method.
        pub async fn list_models(
            &mut self,
            request: impl tonic::IntoRequest<super::ListModelsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListModelsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.ModelService/ListModels",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.ModelService",
                        "ListModels",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Patch specific fields in the specified model.
        pub async fn patch_model(
            &mut self,
            request: impl tonic::IntoRequest<super::PatchModelRequest>,
        ) -> std::result::Result<tonic::Response<super::Model>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.ModelService/PatchModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.ModelService",
                        "PatchModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes the model specified by modelId from the dataset.
        pub async fn delete_model(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteModelRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.ModelService/DeleteModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.ModelService",
                        "DeleteModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Id path of a row access policy.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RowAccessPolicyReference {
    /// Required. The ID of the project containing this row access policy.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. The ID of the dataset containing this row access policy.
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. The ID of the table containing this row access policy.
    #[prost(string, tag = "3")]
    pub table_id: ::prost::alloc::string::String,
    /// Required. The ID of the row access policy. The ID must contain only
    /// letters (a-z, A-Z), numbers (0-9), or underscores (_). The maximum
    /// length is 256 characters.
    #[prost(string, tag = "4")]
    pub policy_id: ::prost::alloc::string::String,
}
/// \[Preview\] Information related to sessions.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SessionInfo {
    /// Output only. The id of the session.
    #[prost(string, tag = "1")]
    pub session_id: ::prost::alloc::string::String,
}
/// An operation within a stage.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExplainQueryStep {
    /// Machine-readable operation type.
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// Human-readable description of the step(s).
    #[prost(string, repeated, tag = "2")]
    pub substeps: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// A single stage of query execution.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExplainQueryStage {
    /// Human-readable name for the stage.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Unique ID for the stage within the plan.
    #[prost(message, optional, tag = "2")]
    pub id: ::core::option::Option<i64>,
    /// Stage start time represented as milliseconds since the epoch.
    #[prost(int64, tag = "3")]
    pub start_ms: i64,
    /// Stage end time represented as milliseconds since the epoch.
    #[prost(int64, tag = "4")]
    pub end_ms: i64,
    /// IDs for stages that are inputs to this stage.
    #[prost(int64, repeated, tag = "5")]
    pub input_stages: ::prost::alloc::vec::Vec<i64>,
    /// Relative amount of time the average shard spent waiting to be
    /// scheduled.
    #[prost(message, optional, tag = "6")]
    pub wait_ratio_avg: ::core::option::Option<f64>,
    /// Milliseconds the average shard spent waiting to be scheduled.
    #[prost(message, optional, tag = "7")]
    pub wait_ms_avg: ::core::option::Option<i64>,
    /// Relative amount of time the slowest shard spent waiting to be
    /// scheduled.
    #[prost(message, optional, tag = "8")]
    pub wait_ratio_max: ::core::option::Option<f64>,
    /// Milliseconds the slowest shard spent waiting to be scheduled.
    #[prost(message, optional, tag = "9")]
    pub wait_ms_max: ::core::option::Option<i64>,
    /// Relative amount of time the average shard spent reading input.
    #[prost(message, optional, tag = "10")]
    pub read_ratio_avg: ::core::option::Option<f64>,
    /// Milliseconds the average shard spent reading input.
    #[prost(message, optional, tag = "11")]
    pub read_ms_avg: ::core::option::Option<i64>,
    /// Relative amount of time the slowest shard spent reading input.
    #[prost(message, optional, tag = "12")]
    pub read_ratio_max: ::core::option::Option<f64>,
    /// Milliseconds the slowest shard spent reading input.
    #[prost(message, optional, tag = "13")]
    pub read_ms_max: ::core::option::Option<i64>,
    /// Relative amount of time the average shard spent on CPU-bound tasks.
    #[prost(message, optional, tag = "14")]
    pub compute_ratio_avg: ::core::option::Option<f64>,
    /// Milliseconds the average shard spent on CPU-bound tasks.
    #[prost(message, optional, tag = "15")]
    pub compute_ms_avg: ::core::option::Option<i64>,
    /// Relative amount of time the slowest shard spent on CPU-bound tasks.
    #[prost(message, optional, tag = "16")]
    pub compute_ratio_max: ::core::option::Option<f64>,
    /// Milliseconds the slowest shard spent on CPU-bound tasks.
    #[prost(message, optional, tag = "17")]
    pub compute_ms_max: ::core::option::Option<i64>,
    /// Relative amount of time the average shard spent on writing output.
    #[prost(message, optional, tag = "18")]
    pub write_ratio_avg: ::core::option::Option<f64>,
    /// Milliseconds the average shard spent on writing output.
    #[prost(message, optional, tag = "19")]
    pub write_ms_avg: ::core::option::Option<i64>,
    /// Relative amount of time the slowest shard spent on writing output.
    #[prost(message, optional, tag = "20")]
    pub write_ratio_max: ::core::option::Option<f64>,
    /// Milliseconds the slowest shard spent on writing output.
    #[prost(message, optional, tag = "21")]
    pub write_ms_max: ::core::option::Option<i64>,
    /// Total number of bytes written to shuffle.
    #[prost(message, optional, tag = "22")]
    pub shuffle_output_bytes: ::core::option::Option<i64>,
    /// Total number of bytes written to shuffle and spilled to disk.
    #[prost(message, optional, tag = "23")]
    pub shuffle_output_bytes_spilled: ::core::option::Option<i64>,
    /// Number of records read into the stage.
    #[prost(message, optional, tag = "24")]
    pub records_read: ::core::option::Option<i64>,
    /// Number of records written by the stage.
    #[prost(message, optional, tag = "25")]
    pub records_written: ::core::option::Option<i64>,
    /// Number of parallel input segments to be processed
    #[prost(message, optional, tag = "26")]
    pub parallel_inputs: ::core::option::Option<i64>,
    /// Number of parallel input segments completed.
    #[prost(message, optional, tag = "27")]
    pub completed_parallel_inputs: ::core::option::Option<i64>,
    /// Current status for this stage.
    #[prost(string, tag = "28")]
    pub status: ::prost::alloc::string::String,
    /// List of operations within the stage in dependency order (approximately
    /// chronological).
    #[prost(message, repeated, tag = "29")]
    pub steps: ::prost::alloc::vec::Vec<ExplainQueryStep>,
    /// Slot-milliseconds used by the stage.
    #[prost(message, optional, tag = "30")]
    pub slot_ms: ::core::option::Option<i64>,
    /// Output only. Compute mode for this stage.
    #[prost(enumeration = "explain_query_stage::ComputeMode", tag = "31")]
    pub compute_mode: i32,
}
/// Nested message and enum types in `ExplainQueryStage`.
pub mod explain_query_stage {
    /// Indicates the type of compute mode.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum ComputeMode {
        /// ComputeMode type not specified.
        Unspecified = 0,
        /// This stage was processed using BigQuery slots.
        Bigquery = 1,
        /// This stage was processed using BI Engine compute.
        BiEngine = 2,
    }
    impl ComputeMode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "COMPUTE_MODE_UNSPECIFIED",
                Self::Bigquery => "BIGQUERY",
                Self::BiEngine => "BI_ENGINE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "COMPUTE_MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "BIGQUERY" => Some(Self::Bigquery),
                "BI_ENGINE" => Some(Self::BiEngine),
                _ => None,
            }
        }
    }
}
/// Summary of the state of query execution at a given time.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct QueryTimelineSample {
    /// Milliseconds elapsed since the start of query execution.
    #[prost(message, optional, tag = "1")]
    pub elapsed_ms: ::core::option::Option<i64>,
    /// Cumulative slot-ms consumed by the query.
    #[prost(message, optional, tag = "2")]
    pub total_slot_ms: ::core::option::Option<i64>,
    /// Total units of work remaining for the query. This number can be revised
    /// (increased or decreased) while the query is running.
    #[prost(message, optional, tag = "3")]
    pub pending_units: ::core::option::Option<i64>,
    /// Total parallel units of work completed by this query.
    #[prost(message, optional, tag = "4")]
    pub completed_units: ::core::option::Option<i64>,
    /// Total number of active workers. This does not correspond directly to
    /// slot usage. This is the largest value observed since the last sample.
    #[prost(message, optional, tag = "5")]
    pub active_units: ::core::option::Option<i64>,
    /// Units of work that can be scheduled immediately. Providing additional slots
    /// for these units of work will accelerate the query, if no other query in
    /// the reservation needs additional slots.
    #[prost(message, optional, tag = "7")]
    pub estimated_runnable_units: ::core::option::Option<i64>,
}
/// The external service cost is a portion of the total cost, these costs are not
/// additive with total_bytes_billed. Moreover, this field only track external
/// service costs that will show up as BigQuery costs (e.g. training BigQuery
/// ML job with google cloud CAIP or Automl Tables services), not other costs
/// which may be accrued by running the query (e.g. reading from Bigtable or
/// Cloud Storage). The external service costs with different billing sku (e.g.
/// CAIP job is charged based on VM usage) are converted to BigQuery
/// billed_bytes and slot_ms with equivalent amount of US dollars. Services may
/// not directly correlate to these metrics, but these are the equivalents for
/// billing purposes.
/// Output only.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExternalServiceCost {
    /// External service name.
    #[prost(string, tag = "1")]
    pub external_service: ::prost::alloc::string::String,
    /// External service cost in terms of bigquery bytes processed.
    #[prost(message, optional, tag = "2")]
    pub bytes_processed: ::core::option::Option<i64>,
    /// External service cost in terms of bigquery bytes billed.
    #[prost(message, optional, tag = "3")]
    pub bytes_billed: ::core::option::Option<i64>,
    /// External service cost in terms of bigquery slot milliseconds.
    #[prost(message, optional, tag = "4")]
    pub slot_ms: ::core::option::Option<i64>,
    /// Non-preemptable reserved slots used for external job.
    /// For example, reserved slots for Cloua AI Platform job are the VM usages
    /// converted to BigQuery slot with equivalent mount of price.
    #[prost(int64, tag = "5")]
    pub reserved_slot_count: i64,
}
/// Statistics for the EXPORT DATA statement as part of Query Job. EXTRACT
/// JOB statistics are populated in JobStatistics4.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ExportDataStatistics {
    /// Number of destination files generated in case of EXPORT DATA
    /// statement only.
    #[prost(message, optional, tag = "1")]
    pub file_count: ::core::option::Option<i64>,
    /// \[Alpha\] Number of destination rows generated in case of EXPORT DATA
    /// statement only.
    #[prost(message, optional, tag = "2")]
    pub row_count: ::core::option::Option<i64>,
}
/// Reason why BI Engine didn't accelerate the query (or sub-query).
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BiEngineReason {
    /// Output only. High-level BI Engine reason for partial or disabled
    /// acceleration
    #[prost(enumeration = "bi_engine_reason::Code", tag = "1")]
    pub code: i32,
    /// Output only. Free form human-readable reason for partial or disabled
    /// acceleration.
    #[prost(string, tag = "2")]
    pub message: ::prost::alloc::string::String,
}
/// Nested message and enum types in `BiEngineReason`.
pub mod bi_engine_reason {
    /// Indicates the high-level reason for no/partial acceleration
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Code {
        /// BiEngineReason not specified.
        Unspecified = 0,
        /// No reservation available for BI Engine acceleration.
        NoReservation = 1,
        /// Not enough memory available for BI Engine acceleration.
        InsufficientReservation = 2,
        /// This particular SQL text is not supported for acceleration by BI Engine.
        UnsupportedSqlText = 4,
        /// Input too large for acceleration by BI Engine.
        InputTooLarge = 5,
        /// Catch-all code for all other cases for partial or disabled acceleration.
        OtherReason = 6,
        /// One or more tables were not eligible for BI Engine acceleration.
        TableExcluded = 7,
    }
    impl Code {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "CODE_UNSPECIFIED",
                Self::NoReservation => "NO_RESERVATION",
                Self::InsufficientReservation => "INSUFFICIENT_RESERVATION",
                Self::UnsupportedSqlText => "UNSUPPORTED_SQL_TEXT",
                Self::InputTooLarge => "INPUT_TOO_LARGE",
                Self::OtherReason => "OTHER_REASON",
                Self::TableExcluded => "TABLE_EXCLUDED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "CODE_UNSPECIFIED" => Some(Self::Unspecified),
                "NO_RESERVATION" => Some(Self::NoReservation),
                "INSUFFICIENT_RESERVATION" => Some(Self::InsufficientReservation),
                "UNSUPPORTED_SQL_TEXT" => Some(Self::UnsupportedSqlText),
                "INPUT_TOO_LARGE" => Some(Self::InputTooLarge),
                "OTHER_REASON" => Some(Self::OtherReason),
                "TABLE_EXCLUDED" => Some(Self::TableExcluded),
                _ => None,
            }
        }
    }
}
/// Statistics for a BI Engine specific query.
/// Populated as part of JobStatistics2
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BiEngineStatistics {
    /// Output only. Specifies which mode of BI Engine acceleration was performed
    /// (if any).
    #[prost(enumeration = "bi_engine_statistics::BiEngineMode", tag = "1")]
    pub bi_engine_mode: i32,
    /// Output only. Specifies which mode of BI Engine acceleration was performed
    /// (if any).
    #[prost(enumeration = "bi_engine_statistics::BiEngineAccelerationMode", tag = "3")]
    pub acceleration_mode: i32,
    /// In case of DISABLED or PARTIAL bi_engine_mode, these contain the
    /// explanatory reasons as to why BI Engine could not accelerate.
    /// In case the full query was accelerated, this field is not populated.
    #[prost(message, repeated, tag = "2")]
    pub bi_engine_reasons: ::prost::alloc::vec::Vec<BiEngineReason>,
}
/// Nested message and enum types in `BiEngineStatistics`.
pub mod bi_engine_statistics {
    /// Indicates the type of BI Engine acceleration.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum BiEngineMode {
        /// BiEngineMode type not specified.
        AccelerationModeUnspecified = 0,
        /// BI Engine disabled the acceleration. bi_engine_reasons
        /// specifies a more detailed reason.
        Disabled = 1,
        /// Part of the query was accelerated using BI Engine.
        /// See bi_engine_reasons for why parts of the query were not
        /// accelerated.
        Partial = 2,
        /// All of the query was accelerated using BI Engine.
        Full = 3,
    }
    impl BiEngineMode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::AccelerationModeUnspecified => "ACCELERATION_MODE_UNSPECIFIED",
                Self::Disabled => "DISABLED",
                Self::Partial => "PARTIAL",
                Self::Full => "FULL",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "ACCELERATION_MODE_UNSPECIFIED" => {
                    Some(Self::AccelerationModeUnspecified)
                }
                "DISABLED" => Some(Self::Disabled),
                "PARTIAL" => Some(Self::Partial),
                "FULL" => Some(Self::Full),
                _ => None,
            }
        }
    }
    /// Indicates the type of BI Engine acceleration.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum BiEngineAccelerationMode {
        /// BiEngineMode type not specified.
        Unspecified = 0,
        /// BI Engine acceleration was attempted but disabled. bi_engine_reasons
        /// specifies a more detailed reason.
        BiEngineDisabled = 1,
        /// Some inputs were accelerated using BI Engine.
        /// See bi_engine_reasons for why parts of the query were not
        /// accelerated.
        PartialInput = 2,
        /// All of the query inputs were accelerated using BI Engine.
        FullInput = 3,
        /// All of the query was accelerated using BI Engine.
        FullQuery = 4,
    }
    impl BiEngineAccelerationMode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "BI_ENGINE_ACCELERATION_MODE_UNSPECIFIED",
                Self::BiEngineDisabled => "BI_ENGINE_DISABLED",
                Self::PartialInput => "PARTIAL_INPUT",
                Self::FullInput => "FULL_INPUT",
                Self::FullQuery => "FULL_QUERY",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "BI_ENGINE_ACCELERATION_MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "BI_ENGINE_DISABLED" => Some(Self::BiEngineDisabled),
                "PARTIAL_INPUT" => Some(Self::PartialInput),
                "FULL_INPUT" => Some(Self::FullInput),
                "FULL_QUERY" => Some(Self::FullQuery),
                _ => None,
            }
        }
    }
}
/// Reason about why no search index was used in the search query (or
/// sub-query).
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct IndexUnusedReason {
    /// Specifies the high-level reason for the scenario when no search index was
    /// used.
    #[prost(enumeration = "index_unused_reason::Code", optional, tag = "1")]
    pub code: ::core::option::Option<i32>,
    /// Free form human-readable reason for the scenario when no search index was
    /// used.
    #[prost(string, optional, tag = "2")]
    pub message: ::core::option::Option<::prost::alloc::string::String>,
    /// Specifies the base table involved in the reason that no search index was
    /// used.
    #[prost(message, optional, tag = "3")]
    pub base_table: ::core::option::Option<TableReference>,
    /// Specifies the name of the unused search index, if available.
    #[prost(string, optional, tag = "4")]
    pub index_name: ::core::option::Option<::prost::alloc::string::String>,
}
/// Nested message and enum types in `IndexUnusedReason`.
pub mod index_unused_reason {
    /// Indicates the high-level reason for the scenario when no search index was
    /// used.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Code {
        /// Code not specified.
        Unspecified = 0,
        /// Indicates the search index configuration has not been created.
        IndexConfigNotAvailable = 1,
        /// Indicates the search index creation has not been completed.
        PendingIndexCreation = 2,
        /// Indicates the base table has been truncated (rows have been removed
        /// from table with TRUNCATE TABLE statement) since the last time the search
        /// index was refreshed.
        BaseTableTruncated = 3,
        /// Indicates the search index configuration has been changed since the last
        /// time the search index was refreshed.
        IndexConfigModified = 4,
        /// Indicates the search query accesses data at a timestamp before the last
        /// time the search index was refreshed.
        TimeTravelQuery = 5,
        /// Indicates the usage of search index will not contribute to any pruning
        /// improvement for the search function, e.g. when the search predicate is in
        /// a disjunction with other non-search predicates.
        NoPruningPower = 6,
        /// Indicates the search index does not cover all fields in the search
        /// function.
        UnindexedSearchFields = 7,
        /// Indicates the search index does not support the given search query
        /// pattern.
        UnsupportedSearchPattern = 8,
        /// Indicates the query has been optimized by using a materialized view.
        OptimizedWithMaterializedView = 9,
        /// Indicates the query has been secured by data masking, and thus search
        /// indexes are not applicable.
        SecuredByDataMasking = 11,
        /// Indicates that the search index and the search function call do not
        /// have the same text analyzer.
        MismatchedTextAnalyzer = 12,
        /// Indicates the base table is too small (below a certain threshold).
        /// The index does not provide noticeable search performance gains
        /// when the base table is too small.
        BaseTableTooSmall = 13,
        /// Indicates that the total size of indexed base tables in your organization
        /// exceeds your region's limit and the index is not used in the query. To
        /// index larger base tables, you can
        /// <a
        /// href="<https://cloud.google.com/bigquery/docs/search-index#use_your_own_reservation">use>
        /// your own reservation</a> for index-management jobs.
        BaseTableTooLarge = 14,
        /// Indicates that the estimated performance gain from using the search index
        /// is too low for the given search query.
        EstimatedPerformanceGainTooLow = 15,
        /// Indicates that search indexes can not be used for search query with
        /// STANDARD edition.
        NotSupportedInStandardEdition = 17,
        /// Indicates that an option in the search function that cannot make use of
        /// the index has been selected.
        IndexSuppressedByFunctionOption = 18,
        /// Indicates that the query was cached, and thus the search index was not
        /// used.
        QueryCacheHit = 19,
        /// The index cannot be used in the search query because it is stale.
        StaleIndex = 20,
        /// Indicates an internal error that causes the search index to be unused.
        InternalError = 10,
        /// Indicates that the reason search indexes cannot be used in the query is
        /// not covered by any of the other IndexUnusedReason options.
        OtherReason = 16,
    }
    impl Code {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "CODE_UNSPECIFIED",
                Self::IndexConfigNotAvailable => "INDEX_CONFIG_NOT_AVAILABLE",
                Self::PendingIndexCreation => "PENDING_INDEX_CREATION",
                Self::BaseTableTruncated => "BASE_TABLE_TRUNCATED",
                Self::IndexConfigModified => "INDEX_CONFIG_MODIFIED",
                Self::TimeTravelQuery => "TIME_TRAVEL_QUERY",
                Self::NoPruningPower => "NO_PRUNING_POWER",
                Self::UnindexedSearchFields => "UNINDEXED_SEARCH_FIELDS",
                Self::UnsupportedSearchPattern => "UNSUPPORTED_SEARCH_PATTERN",
                Self::OptimizedWithMaterializedView => "OPTIMIZED_WITH_MATERIALIZED_VIEW",
                Self::SecuredByDataMasking => "SECURED_BY_DATA_MASKING",
                Self::MismatchedTextAnalyzer => "MISMATCHED_TEXT_ANALYZER",
                Self::BaseTableTooSmall => "BASE_TABLE_TOO_SMALL",
                Self::BaseTableTooLarge => "BASE_TABLE_TOO_LARGE",
                Self::EstimatedPerformanceGainTooLow => {
                    "ESTIMATED_PERFORMANCE_GAIN_TOO_LOW"
                }
                Self::NotSupportedInStandardEdition => {
                    "NOT_SUPPORTED_IN_STANDARD_EDITION"
                }
                Self::IndexSuppressedByFunctionOption => {
                    "INDEX_SUPPRESSED_BY_FUNCTION_OPTION"
                }
                Self::QueryCacheHit => "QUERY_CACHE_HIT",
                Self::StaleIndex => "STALE_INDEX",
                Self::InternalError => "INTERNAL_ERROR",
                Self::OtherReason => "OTHER_REASON",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "CODE_UNSPECIFIED" => Some(Self::Unspecified),
                "INDEX_CONFIG_NOT_AVAILABLE" => Some(Self::IndexConfigNotAvailable),
                "PENDING_INDEX_CREATION" => Some(Self::PendingIndexCreation),
                "BASE_TABLE_TRUNCATED" => Some(Self::BaseTableTruncated),
                "INDEX_CONFIG_MODIFIED" => Some(Self::IndexConfigModified),
                "TIME_TRAVEL_QUERY" => Some(Self::TimeTravelQuery),
                "NO_PRUNING_POWER" => Some(Self::NoPruningPower),
                "UNINDEXED_SEARCH_FIELDS" => Some(Self::UnindexedSearchFields),
                "UNSUPPORTED_SEARCH_PATTERN" => Some(Self::UnsupportedSearchPattern),
                "OPTIMIZED_WITH_MATERIALIZED_VIEW" => {
                    Some(Self::OptimizedWithMaterializedView)
                }
                "SECURED_BY_DATA_MASKING" => Some(Self::SecuredByDataMasking),
                "MISMATCHED_TEXT_ANALYZER" => Some(Self::MismatchedTextAnalyzer),
                "BASE_TABLE_TOO_SMALL" => Some(Self::BaseTableTooSmall),
                "BASE_TABLE_TOO_LARGE" => Some(Self::BaseTableTooLarge),
                "ESTIMATED_PERFORMANCE_GAIN_TOO_LOW" => {
                    Some(Self::EstimatedPerformanceGainTooLow)
                }
                "NOT_SUPPORTED_IN_STANDARD_EDITION" => {
                    Some(Self::NotSupportedInStandardEdition)
                }
                "INDEX_SUPPRESSED_BY_FUNCTION_OPTION" => {
                    Some(Self::IndexSuppressedByFunctionOption)
                }
                "QUERY_CACHE_HIT" => Some(Self::QueryCacheHit),
                "STALE_INDEX" => Some(Self::StaleIndex),
                "INTERNAL_ERROR" => Some(Self::InternalError),
                "OTHER_REASON" => Some(Self::OtherReason),
                _ => None,
            }
        }
    }
}
/// Statistics for a search query.
/// Populated as part of JobStatistics2.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchStatistics {
    /// Specifies the index usage mode for the query.
    #[prost(enumeration = "search_statistics::IndexUsageMode", tag = "1")]
    pub index_usage_mode: i32,
    /// When `indexUsageMode` is `UNUSED` or `PARTIALLY_USED`, this field explains
    /// why indexes were not used in all or part of the search query. If
    /// `indexUsageMode` is `FULLY_USED`, this field is not populated.
    #[prost(message, repeated, tag = "2")]
    pub index_unused_reasons: ::prost::alloc::vec::Vec<IndexUnusedReason>,
}
/// Nested message and enum types in `SearchStatistics`.
pub mod search_statistics {
    /// Indicates the type of search index usage in the entire search query.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum IndexUsageMode {
        /// Index usage mode not specified.
        Unspecified = 0,
        /// No search indexes were used in the search query. See
        /// \[`indexUnusedReasons`\]
        /// (/bigquery/docs/reference/rest/v2/Job#IndexUnusedReason)
        /// for detailed reasons.
        Unused = 1,
        /// Part of the search query used search indexes. See \[`indexUnusedReasons`\]
        /// (/bigquery/docs/reference/rest/v2/Job#IndexUnusedReason)
        /// for why other parts of the query did not use search indexes.
        PartiallyUsed = 2,
        /// The entire search query used search indexes.
        FullyUsed = 4,
    }
    impl IndexUsageMode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "INDEX_USAGE_MODE_UNSPECIFIED",
                Self::Unused => "UNUSED",
                Self::PartiallyUsed => "PARTIALLY_USED",
                Self::FullyUsed => "FULLY_USED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "INDEX_USAGE_MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "UNUSED" => Some(Self::Unused),
                "PARTIALLY_USED" => Some(Self::PartiallyUsed),
                "FULLY_USED" => Some(Self::FullyUsed),
                _ => None,
            }
        }
    }
}
/// Statistics for a vector search query.
/// Populated as part of JobStatistics2.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct VectorSearchStatistics {
    /// Specifies the index usage mode for the query.
    #[prost(enumeration = "vector_search_statistics::IndexUsageMode", tag = "1")]
    pub index_usage_mode: i32,
    /// When `indexUsageMode` is `UNUSED` or `PARTIALLY_USED`, this field explains
    /// why indexes were not used in all or part of the vector search query. If
    /// `indexUsageMode` is `FULLY_USED`, this field is not populated.
    #[prost(message, repeated, tag = "2")]
    pub index_unused_reasons: ::prost::alloc::vec::Vec<IndexUnusedReason>,
}
/// Nested message and enum types in `VectorSearchStatistics`.
pub mod vector_search_statistics {
    /// Indicates the type of vector index usage in the entire vector search query.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum IndexUsageMode {
        /// Index usage mode not specified.
        Unspecified = 0,
        /// No vector indexes were used in the vector search query. See
        /// \[`indexUnusedReasons`\]
        /// (/bigquery/docs/reference/rest/v2/Job#IndexUnusedReason)
        /// for detailed reasons.
        Unused = 1,
        /// Part of the vector search query used vector indexes. See
        /// \[`indexUnusedReasons`\]
        /// (/bigquery/docs/reference/rest/v2/Job#IndexUnusedReason)
        /// for why other parts of the query did not use vector indexes.
        PartiallyUsed = 2,
        /// The entire vector search query used vector indexes.
        FullyUsed = 4,
    }
    impl IndexUsageMode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "INDEX_USAGE_MODE_UNSPECIFIED",
                Self::Unused => "UNUSED",
                Self::PartiallyUsed => "PARTIALLY_USED",
                Self::FullyUsed => "FULLY_USED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "INDEX_USAGE_MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "UNUSED" => Some(Self::Unused),
                "PARTIALLY_USED" => Some(Self::PartiallyUsed),
                "FULLY_USED" => Some(Self::FullyUsed),
                _ => None,
            }
        }
    }
}
/// Query optimization information for a QUERY job.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryInfo {
    /// Output only. Information about query optimizations.
    #[prost(message, optional, tag = "2")]
    pub optimization_details: ::core::option::Option<::prost_types::Struct>,
}
/// Statistics for a LOAD query.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct LoadQueryStatistics {
    /// Output only. Number of source files in a LOAD query.
    #[prost(message, optional, tag = "1")]
    pub input_files: ::core::option::Option<i64>,
    /// Output only. Number of bytes of source data in a LOAD query.
    #[prost(message, optional, tag = "2")]
    pub input_file_bytes: ::core::option::Option<i64>,
    /// Output only. Number of rows imported in a LOAD query.
    /// Note that while a LOAD query is in the running state, this value may
    /// change.
    #[prost(message, optional, tag = "3")]
    pub output_rows: ::core::option::Option<i64>,
    /// Output only. Size of the loaded data in bytes. Note that while a LOAD query
    /// is in the running state, this value may change.
    #[prost(message, optional, tag = "4")]
    pub output_bytes: ::core::option::Option<i64>,
    /// Output only. The number of bad records encountered while processing a LOAD
    /// query. Note that if the job has failed because of more bad records
    /// encountered than the maximum allowed in the load job configuration, then
    /// this number can be less than the total number of bad records present in the
    /// input data.
    #[prost(message, optional, tag = "5")]
    pub bad_records: ::core::option::Option<i64>,
}
/// Statistics for a query job.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobStatistics2 {
    /// Output only. Describes execution plan for the query.
    #[prost(message, repeated, tag = "1")]
    pub query_plan: ::prost::alloc::vec::Vec<ExplainQueryStage>,
    /// Output only. The original estimate of bytes processed for the job.
    #[prost(message, optional, tag = "2")]
    pub estimated_bytes_processed: ::core::option::Option<i64>,
    /// Output only. Describes a timeline of job execution.
    #[prost(message, repeated, tag = "3")]
    pub timeline: ::prost::alloc::vec::Vec<QueryTimelineSample>,
    /// Output only. Total number of partitions processed from all partitioned
    /// tables referenced in the job.
    #[prost(message, optional, tag = "4")]
    pub total_partitions_processed: ::core::option::Option<i64>,
    /// Output only. Total bytes processed for the job.
    #[prost(message, optional, tag = "5")]
    pub total_bytes_processed: ::core::option::Option<i64>,
    /// Output only. For dry-run jobs, totalBytesProcessed is an estimate and this
    /// field specifies the accuracy of the estimate. Possible values can be:
    /// UNKNOWN: accuracy of the estimate is unknown.
    /// PRECISE: estimate is precise.
    /// LOWER_BOUND: estimate is lower bound of what the query would cost.
    /// UPPER_BOUND: estimate is upper bound of what the query would cost.
    #[prost(string, tag = "21")]
    pub total_bytes_processed_accuracy: ::prost::alloc::string::String,
    /// Output only. If the project is configured to use on-demand pricing,
    /// then this field contains the total bytes billed for the job.
    /// If the project is configured to use flat-rate pricing, then you are
    /// not billed for bytes and this field is informational only.
    #[prost(message, optional, tag = "6")]
    pub total_bytes_billed: ::core::option::Option<i64>,
    /// Output only. Billing tier for the job. This is a BigQuery-specific concept
    /// which is not related to the Google Cloud notion of "free tier". The value
    /// here is a measure of the query's resource consumption relative to the
    /// amount of data scanned. For on-demand queries, the limit is 100, and all
    /// queries within this limit are billed at the standard on-demand rates.
    /// On-demand queries that exceed this limit will fail with a
    /// billingTierLimitExceeded error.
    #[prost(message, optional, tag = "7")]
    pub billing_tier: ::core::option::Option<i32>,
    /// Output only. Slot-milliseconds for the job.
    #[prost(message, optional, tag = "8")]
    pub total_slot_ms: ::core::option::Option<i64>,
    /// Output only. Whether the query result was fetched from the query cache.
    #[prost(message, optional, tag = "9")]
    pub cache_hit: ::core::option::Option<bool>,
    /// Output only. Referenced tables for the job. Queries that reference more
    /// than 50 tables will not have a complete list.
    #[prost(message, repeated, tag = "10")]
    pub referenced_tables: ::prost::alloc::vec::Vec<TableReference>,
    /// Output only. Referenced routines for the job.
    #[prost(message, repeated, tag = "24")]
    pub referenced_routines: ::prost::alloc::vec::Vec<RoutineReference>,
    /// Output only. The schema of the results. Present only for successful dry
    /// run of non-legacy SQL queries.
    #[prost(message, optional, tag = "11")]
    pub schema: ::core::option::Option<TableSchema>,
    /// Output only. The number of rows affected by a DML statement. Present
    /// only for DML statements INSERT, UPDATE or DELETE.
    #[prost(message, optional, tag = "12")]
    pub num_dml_affected_rows: ::core::option::Option<i64>,
    /// Output only. Detailed statistics for DML statements INSERT, UPDATE, DELETE,
    /// MERGE or TRUNCATE.
    #[prost(message, optional, tag = "32")]
    pub dml_stats: ::core::option::Option<DmlStats>,
    /// Output only. GoogleSQL only: list of undeclared query
    /// parameters detected during a dry run validation.
    #[prost(message, repeated, tag = "13")]
    pub undeclared_query_parameters: ::prost::alloc::vec::Vec<QueryParameter>,
    /// Output only. The type of query statement, if valid.
    /// Possible values:
    ///
    /// * `SELECT`:
    /// [`SELECT`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#select_list>)
    /// statement.
    /// * `ASSERT`:
    /// [`ASSERT`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/debugging-statements#assert>)
    /// statement.
    /// * `INSERT`:
    /// [`INSERT`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#insert_statement>)
    /// statement.
    /// * `UPDATE`:
    /// [`UPDATE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#update_statement>)
    /// statement.
    /// * `DELETE`:
    /// [`DELETE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language>)
    /// statement.
    /// * `MERGE`:
    /// [`MERGE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language>)
    /// statement.
    /// * `CREATE_TABLE`: [`CREATE
    /// TABLE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_table_statement>)
    /// statement, without `AS SELECT`.
    /// * `CREATE_TABLE_AS_SELECT`: [`CREATE TABLE AS
    /// SELECT`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#query_statement>)
    /// statement.
    /// * `CREATE_VIEW`: [`CREATE
    /// VIEW`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_view_statement>)
    /// statement.
    /// * `CREATE_MODEL`: [`CREATE
    /// MODEL`](<https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create#create_model_statement>)
    /// statement.
    /// * `CREATE_MATERIALIZED_VIEW`: [`CREATE MATERIALIZED
    /// VIEW`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_materialized_view_statement>)
    /// statement.
    /// * `CREATE_FUNCTION`: [`CREATE
    /// FUNCTION`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_function_statement>)
    /// statement.
    /// * `CREATE_TABLE_FUNCTION`: [`CREATE TABLE
    /// FUNCTION`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_table_function_statement>)
    /// statement.
    /// * `CREATE_PROCEDURE`: [`CREATE
    /// PROCEDURE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_procedure>)
    /// statement.
    /// * `CREATE_ROW_ACCESS_POLICY`: [`CREATE ROW ACCESS
    /// POLICY`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_row_access_policy_statement>)
    /// statement.
    /// * `CREATE_SCHEMA`: [`CREATE
    /// SCHEMA`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_schema_statement>)
    /// statement.
    /// * `CREATE_SNAPSHOT_TABLE`: [`CREATE SNAPSHOT
    /// TABLE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_snapshot_table_statement>)
    /// statement.
    /// * `CREATE_SEARCH_INDEX`: [`CREATE SEARCH
    /// INDEX`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_search_index_statement>)
    /// statement.
    /// * `DROP_TABLE`: [`DROP
    /// TABLE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_table_statement>)
    /// statement.
    /// * `DROP_EXTERNAL_TABLE`: [`DROP EXTERNAL
    /// TABLE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_external_table_statement>)
    /// statement.
    /// * `DROP_VIEW`: [`DROP
    /// VIEW`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_view_statement>)
    /// statement.
    /// * `DROP_MODEL`: [`DROP
    /// MODEL`](<https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-drop-model>)
    /// statement.
    /// * `DROP_MATERIALIZED_VIEW`: [`DROP MATERIALIZED
    ///   VIEW`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_materialized_view_statement>)
    /// statement.
    /// * `DROP_FUNCTION` : [`DROP
    /// FUNCTION`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_function_statement>)
    /// statement.
    /// * `DROP_TABLE_FUNCTION` : [`DROP TABLE
    /// FUNCTION`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_table_function>)
    /// statement.
    /// * `DROP_PROCEDURE`: [`DROP
    /// PROCEDURE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_procedure_statement>)
    /// statement.
    /// * `DROP_SEARCH_INDEX`: [`DROP SEARCH
    /// INDEX`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_search_index>)
    /// statement.
    /// * `DROP_SCHEMA`: [`DROP
    /// SCHEMA`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_schema_statement>)
    /// statement.
    /// * `DROP_SNAPSHOT_TABLE`: [`DROP SNAPSHOT
    /// TABLE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_snapshot_table_statement>)
    /// statement.
    /// * `DROP_ROW_ACCESS_POLICY`: \[`DROP [ALL\] ROW ACCESS
    /// POLICY|POLICIES`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#drop_row_access_policy_statement>)
    /// statement.
    /// * `ALTER_TABLE`: [`ALTER
    /// TABLE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#alter_table_set_options_statement>)
    /// statement.
    /// * `ALTER_VIEW`: [`ALTER
    /// VIEW`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#alter_view_set_options_statement>)
    /// statement.
    /// * `ALTER_MATERIALIZED_VIEW`: [`ALTER MATERIALIZED
    /// VIEW`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#alter_materialized_view_set_options_statement>)
    /// statement.
    /// * `ALTER_SCHEMA`: [`ALTER
    /// SCHEMA`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#aalter_schema_set_options_statement>)
    /// statement.
    /// * `SCRIPT`:
    /// [`SCRIPT`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/procedural-language>).
    /// * `TRUNCATE_TABLE`: [`TRUNCATE
    /// TABLE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#truncate_table_statement>)
    /// statement.
    /// * `CREATE_EXTERNAL_TABLE`: [`CREATE EXTERNAL
    /// TABLE`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#create_external_table_statement>)
    /// statement.
    /// * `EXPORT_DATA`: [`EXPORT
    /// DATA`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/other-statements#export_data_statement>)
    /// statement.
    /// * `EXPORT_MODEL`: [`EXPORT
    /// MODEL`](<https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-export-model>)
    /// statement.
    /// * `LOAD_DATA`: [`LOAD
    /// DATA`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/other-statements#load_data_statement>)
    /// statement.
    /// * `CALL`:
    /// [`CALL`](<https://cloud.google.com/bigquery/docs/reference/standard-sql/procedural-language#call>)
    /// statement.
    #[prost(string, tag = "14")]
    pub statement_type: ::prost::alloc::string::String,
    /// Output only. The DDL operation performed, possibly
    /// dependent on the pre-existence of the DDL target.
    #[prost(string, tag = "15")]
    pub ddl_operation_performed: ::prost::alloc::string::String,
    /// Output only. The DDL target table. Present only for
    /// CREATE/DROP TABLE/VIEW and DROP ALL ROW ACCESS POLICIES queries.
    #[prost(message, optional, tag = "16")]
    pub ddl_target_table: ::core::option::Option<TableReference>,
    /// Output only. The table after rename. Present only for ALTER TABLE RENAME TO
    /// query.
    #[prost(message, optional, tag = "31")]
    pub ddl_destination_table: ::core::option::Option<TableReference>,
    /// Output only. The DDL target row access policy. Present only for
    /// CREATE/DROP ROW ACCESS POLICY queries.
    #[prost(message, optional, tag = "26")]
    pub ddl_target_row_access_policy: ::core::option::Option<RowAccessPolicyReference>,
    /// Output only. The number of row access policies affected by a DDL statement.
    /// Present only for DROP ALL ROW ACCESS POLICIES queries.
    #[prost(message, optional, tag = "27")]
    pub ddl_affected_row_access_policy_count: ::core::option::Option<i64>,
    /// Output only. \[Beta\] The DDL target routine. Present only for
    /// CREATE/DROP FUNCTION/PROCEDURE queries.
    #[prost(message, optional, tag = "22")]
    pub ddl_target_routine: ::core::option::Option<RoutineReference>,
    /// Output only. The DDL target dataset. Present only for CREATE/ALTER/DROP
    /// SCHEMA(dataset) queries.
    #[prost(message, optional, tag = "30")]
    pub ddl_target_dataset: ::core::option::Option<DatasetReference>,
    /// Output only. Statistics of a BigQuery ML training job.
    #[prost(message, optional, tag = "23")]
    pub ml_statistics: ::core::option::Option<MlStatistics>,
    /// Output only. Stats for EXPORT DATA statement.
    #[prost(message, optional, tag = "25")]
    pub export_data_statistics: ::core::option::Option<ExportDataStatistics>,
    /// Output only. Job cost breakdown as bigquery internal cost and external
    /// service costs.
    #[prost(message, repeated, tag = "28")]
    pub external_service_costs: ::prost::alloc::vec::Vec<ExternalServiceCost>,
    /// Output only. BI Engine specific Statistics.
    #[prost(message, optional, tag = "29")]
    pub bi_engine_statistics: ::core::option::Option<BiEngineStatistics>,
    /// Output only. Statistics for a LOAD query.
    #[prost(message, optional, tag = "33")]
    pub load_query_statistics: ::core::option::Option<LoadQueryStatistics>,
    /// Output only. Referenced table for DCL statement.
    #[prost(message, optional, tag = "34")]
    pub dcl_target_table: ::core::option::Option<TableReference>,
    /// Output only. Referenced view for DCL statement.
    #[prost(message, optional, tag = "35")]
    pub dcl_target_view: ::core::option::Option<TableReference>,
    /// Output only. Referenced dataset for DCL statement.
    #[prost(message, optional, tag = "36")]
    pub dcl_target_dataset: ::core::option::Option<DatasetReference>,
    /// Output only. Search query specific statistics.
    #[prost(message, optional, tag = "37")]
    pub search_statistics: ::core::option::Option<SearchStatistics>,
    /// Output only. Vector Search query specific statistics.
    #[prost(message, optional, tag = "44")]
    pub vector_search_statistics: ::core::option::Option<VectorSearchStatistics>,
    /// Output only. Performance insights.
    #[prost(message, optional, tag = "38")]
    pub performance_insights: ::core::option::Option<PerformanceInsights>,
    /// Output only. Query optimization information for a QUERY job.
    #[prost(message, optional, tag = "39")]
    pub query_info: ::core::option::Option<QueryInfo>,
    /// Output only. Statistics of a Spark procedure job.
    #[prost(message, optional, tag = "40")]
    pub spark_statistics: ::core::option::Option<SparkStatistics>,
    /// Output only. Total bytes transferred for cross-cloud queries such as Cross
    /// Cloud Transfer and CREATE TABLE AS SELECT (CTAS).
    #[prost(message, optional, tag = "41")]
    pub transferred_bytes: ::core::option::Option<i64>,
    /// Output only. Statistics of materialized views of a query job.
    #[prost(message, optional, tag = "42")]
    pub materialized_view_statistics: ::core::option::Option<MaterializedViewStatistics>,
    /// Output only. Statistics of metadata cache usage in a query for BigLake
    /// tables.
    #[prost(message, optional, tag = "43")]
    pub metadata_cache_statistics: ::core::option::Option<MetadataCacheStatistics>,
}
/// Statistics for a load job.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobStatistics3 {
    /// Output only. Number of source files in a load job.
    #[prost(message, optional, tag = "1")]
    pub input_files: ::core::option::Option<i64>,
    /// Output only. Number of bytes of source data in a load job.
    #[prost(message, optional, tag = "2")]
    pub input_file_bytes: ::core::option::Option<i64>,
    /// Output only. Number of rows imported in a load job.
    /// Note that while an import job is in the running state, this
    /// value may change.
    #[prost(message, optional, tag = "3")]
    pub output_rows: ::core::option::Option<i64>,
    /// Output only. Size of the loaded data in bytes. Note
    /// that while a load job is in the running state, this value may change.
    #[prost(message, optional, tag = "4")]
    pub output_bytes: ::core::option::Option<i64>,
    /// Output only. The number of bad records encountered. Note that if the job
    /// has failed because of more bad records encountered than the maximum
    /// allowed in the load job configuration, then this number can be less than
    /// the total number of bad records present in the input data.
    #[prost(message, optional, tag = "5")]
    pub bad_records: ::core::option::Option<i64>,
    /// Output only. Describes a timeline of job execution.
    #[prost(message, repeated, tag = "7")]
    pub timeline: ::prost::alloc::vec::Vec<QueryTimelineSample>,
}
/// Statistics for an extract job.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobStatistics4 {
    /// Output only. Number of files per destination URI or URI pattern
    /// specified in the extract configuration. These values will be in the same
    /// order as the URIs specified in the 'destinationUris' field.
    #[prost(int64, repeated, packed = "false", tag = "1")]
    pub destination_uri_file_counts: ::prost::alloc::vec::Vec<i64>,
    /// Output only. Number of user bytes extracted into the result. This is the
    /// byte count as computed by BigQuery for billing purposes
    /// and doesn't have any relationship with the number of actual
    /// result bytes extracted in the desired format.
    #[prost(message, optional, tag = "2")]
    pub input_bytes: ::core::option::Option<i64>,
    /// Output only. Describes a timeline of job execution.
    #[prost(message, repeated, tag = "3")]
    pub timeline: ::prost::alloc::vec::Vec<QueryTimelineSample>,
}
/// Statistics for a copy job.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct CopyJobStatistics {
    /// Output only. Number of rows copied to the destination table.
    #[prost(message, optional, tag = "1")]
    pub copied_rows: ::core::option::Option<i64>,
    /// Output only. Number of logical bytes copied to the destination table.
    #[prost(message, optional, tag = "2")]
    pub copied_logical_bytes: ::core::option::Option<i64>,
}
/// Job statistics specific to a BigQuery ML training job.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MlStatistics {
    /// Output only. Maximum number of iterations specified as max_iterations in
    /// the 'CREATE MODEL' query. The actual number of iterations may be less than
    /// this number due to early stop.
    #[prost(int64, tag = "1")]
    pub max_iterations: i64,
    /// Results for all completed iterations.
    /// Empty for [hyperparameter tuning
    /// jobs](<https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview>).
    #[prost(message, repeated, tag = "2")]
    pub iteration_results: ::prost::alloc::vec::Vec<
        model::training_run::IterationResult,
    >,
    /// Output only. The type of the model that is being trained.
    #[prost(enumeration = "model::ModelType", tag = "3")]
    pub model_type: i32,
    /// Output only. Training type of the job.
    #[prost(enumeration = "ml_statistics::TrainingType", tag = "4")]
    pub training_type: i32,
    /// Output only. Trials of a [hyperparameter tuning
    /// job](<https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview>)
    /// sorted by trial_id.
    #[prost(message, repeated, tag = "5")]
    pub hparam_trials: ::prost::alloc::vec::Vec<model::HparamTuningTrial>,
}
/// Nested message and enum types in `MlStatistics`.
pub mod ml_statistics {
    /// Training type.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum TrainingType {
        /// Unspecified training type.
        Unspecified = 0,
        /// Single training with fixed parameter space.
        SingleTraining = 1,
        /// [Hyperparameter tuning
        /// training](<https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-hp-tuning-overview>).
        HparamTuning = 2,
    }
    impl TrainingType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "TRAINING_TYPE_UNSPECIFIED",
                Self::SingleTraining => "SINGLE_TRAINING",
                Self::HparamTuning => "HPARAM_TUNING",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "TRAINING_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "SINGLE_TRAINING" => Some(Self::SingleTraining),
                "HPARAM_TUNING" => Some(Self::HparamTuning),
                _ => None,
            }
        }
    }
}
/// Job statistics specific to the child job of a script.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ScriptStatistics {
    /// Whether this child job was a statement or expression.
    #[prost(enumeration = "script_statistics::EvaluationKind", tag = "1")]
    pub evaluation_kind: i32,
    /// Stack trace showing the line/column/procedure name of each frame on the
    /// stack at the point where the current evaluation happened. The leaf frame
    /// is first, the primary script is last. Never empty.
    #[prost(message, repeated, tag = "2")]
    pub stack_frames: ::prost::alloc::vec::Vec<script_statistics::ScriptStackFrame>,
}
/// Nested message and enum types in `ScriptStatistics`.
pub mod script_statistics {
    /// Represents the location of the statement/expression being evaluated.
    /// Line and column numbers are defined as follows:
    ///
    /// - Line and column numbers start with one.  That is, line 1 column 1 denotes
    ///    the start of the script.
    /// - When inside a stored procedure, all line/column numbers are relative
    ///    to the procedure body, not the script in which the procedure was defined.
    /// - Start/end positions exclude leading/trailing comments and whitespace.
    ///    The end position always ends with a ";", when present.
    /// - Multi-byte Unicode characters are treated as just one column.
    /// - If the original script (or procedure definition) contains TAB characters,
    ///    a tab "snaps" the indentation forward to the nearest multiple of 8
    ///    characters, plus 1. For example, a TAB on column 1, 2, 3, 4, 5, 6 , or 8
    ///    will advance the next character to column 9.  A TAB on column 9, 10, 11,
    ///    12, 13, 14, 15, or 16 will advance the next character to column 17.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ScriptStackFrame {
        /// Output only. One-based start line.
        #[prost(int32, tag = "1")]
        pub start_line: i32,
        /// Output only. One-based start column.
        #[prost(int32, tag = "2")]
        pub start_column: i32,
        /// Output only. One-based end line.
        #[prost(int32, tag = "3")]
        pub end_line: i32,
        /// Output only. One-based end column.
        #[prost(int32, tag = "4")]
        pub end_column: i32,
        /// Output only. Name of the active procedure, empty if in a top-level
        /// script.
        #[prost(string, tag = "5")]
        pub procedure_id: ::prost::alloc::string::String,
        /// Output only. Text of the current statement/expression.
        #[prost(string, tag = "6")]
        pub text: ::prost::alloc::string::String,
    }
    /// Describes how the job is evaluated.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum EvaluationKind {
        /// Default value.
        Unspecified = 0,
        /// The statement appears directly in the script.
        Statement = 1,
        /// The statement evaluates an expression that appears in the script.
        Expression = 2,
    }
    impl EvaluationKind {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "EVALUATION_KIND_UNSPECIFIED",
                Self::Statement => "STATEMENT",
                Self::Expression => "EXPRESSION",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "EVALUATION_KIND_UNSPECIFIED" => Some(Self::Unspecified),
                "STATEMENT" => Some(Self::Statement),
                "EXPRESSION" => Some(Self::Expression),
                _ => None,
            }
        }
    }
}
/// Statistics for row-level security.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RowLevelSecurityStatistics {
    /// Whether any accessed data was protected by row access policies.
    #[prost(bool, tag = "1")]
    pub row_level_security_applied: bool,
}
/// Statistics for data-masking.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct DataMaskingStatistics {
    /// Whether any accessed data was protected by the data masking.
    #[prost(bool, tag = "1")]
    pub data_masking_applied: bool,
}
/// Statistics for a single job execution.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobStatistics {
    /// Output only. Creation time of this job, in milliseconds since the epoch.
    /// This field will be present on all jobs.
    #[prost(int64, tag = "1")]
    pub creation_time: i64,
    /// Output only. Start time of this job, in milliseconds since the epoch.
    /// This field will be present when the job transitions from the PENDING state
    /// to either RUNNING or DONE.
    #[prost(int64, tag = "2")]
    pub start_time: i64,
    /// Output only. End time of this job, in milliseconds since the epoch. This
    /// field will be present whenever a job is in the DONE state.
    #[prost(int64, tag = "3")]
    pub end_time: i64,
    /// Output only. Total bytes processed for the job.
    #[prost(message, optional, tag = "4")]
    pub total_bytes_processed: ::core::option::Option<i64>,
    /// Output only. \[TrustedTester\] Job progress (0.0 -> 1.0) for LOAD and
    /// EXTRACT jobs.
    #[prost(message, optional, tag = "5")]
    pub completion_ratio: ::core::option::Option<f64>,
    /// Output only. Quotas which delayed this job's start time.
    #[prost(string, repeated, tag = "9")]
    pub quota_deferments: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Output only. Statistics for a query job.
    #[prost(message, optional, tag = "6")]
    pub query: ::core::option::Option<JobStatistics2>,
    /// Output only. Statistics for a load job.
    #[prost(message, optional, tag = "7")]
    pub load: ::core::option::Option<JobStatistics3>,
    /// Output only. Statistics for an extract job.
    #[prost(message, optional, tag = "8")]
    pub extract: ::core::option::Option<JobStatistics4>,
    /// Output only. Statistics for a copy job.
    #[prost(message, optional, tag = "21")]
    pub copy: ::core::option::Option<CopyJobStatistics>,
    /// Output only. Slot-milliseconds for the job.
    #[prost(message, optional, tag = "10")]
    pub total_slot_ms: ::core::option::Option<i64>,
    /// Output only. Name of the primary reservation assigned to this job. Note
    /// that this could be different than reservations reported in the reservation
    /// usage field if parent reservations were used to execute this job.
    #[prost(string, tag = "15")]
    pub reservation_id: ::prost::alloc::string::String,
    /// Output only. Number of child jobs executed.
    #[prost(int64, tag = "12")]
    pub num_child_jobs: i64,
    /// Output only. If this is a child job, specifies the job ID of the parent.
    #[prost(string, tag = "13")]
    pub parent_job_id: ::prost::alloc::string::String,
    /// Output only. If this a child job of a script, specifies information about
    /// the context of this job within the script.
    #[prost(message, optional, tag = "14")]
    pub script_statistics: ::core::option::Option<ScriptStatistics>,
    /// Output only. Statistics for row-level security. Present only for query and
    /// extract jobs.
    #[prost(message, optional, tag = "16")]
    pub row_level_security_statistics: ::core::option::Option<
        RowLevelSecurityStatistics,
    >,
    /// Output only. Statistics for data-masking. Present only for query and
    /// extract jobs.
    #[prost(message, optional, tag = "20")]
    pub data_masking_statistics: ::core::option::Option<DataMaskingStatistics>,
    /// Output only. \[Alpha\] Information of the multi-statement transaction if this
    /// job is part of one.
    ///
    /// This property is only expected on a child job or a job that is in a
    /// session. A script parent job is not part of the transaction started in the
    /// script.
    #[prost(message, optional, tag = "17")]
    pub transaction_info: ::core::option::Option<job_statistics::TransactionInfo>,
    /// Output only. Information of the session if this job is part of one.
    #[prost(message, optional, tag = "18")]
    pub session_info: ::core::option::Option<SessionInfo>,
    /// Output only. The duration in milliseconds of the execution of the final
    /// attempt of this job, as BigQuery may internally re-attempt to execute the
    /// job.
    #[prost(int64, tag = "22")]
    pub final_execution_duration_ms: i64,
    /// Output only. Name of edition corresponding to the reservation for this job
    /// at the time of this update.
    #[prost(enumeration = "ReservationEdition", tag = "24")]
    pub edition: i32,
}
/// Nested message and enum types in `JobStatistics`.
pub mod job_statistics {
    /// \[Alpha\] Information of a multi-statement transaction.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct TransactionInfo {
        /// Output only. \[Alpha\] Id of the transaction.
        #[prost(string, tag = "1")]
        pub transaction_id: ::prost::alloc::string::String,
    }
}
/// Detailed statistics for DML statements
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct DmlStats {
    /// Output only. Number of inserted Rows. Populated by DML INSERT and MERGE
    /// statements
    #[prost(message, optional, tag = "1")]
    pub inserted_row_count: ::core::option::Option<i64>,
    /// Output only. Number of deleted Rows. populated by DML DELETE, MERGE and
    /// TRUNCATE statements.
    #[prost(message, optional, tag = "2")]
    pub deleted_row_count: ::core::option::Option<i64>,
    /// Output only. Number of updated Rows. Populated by DML UPDATE and MERGE
    /// statements.
    #[prost(message, optional, tag = "3")]
    pub updated_row_count: ::core::option::Option<i64>,
}
/// Performance insights for the job.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PerformanceInsights {
    /// Output only. Average execution ms of previous runs. Indicates the job ran
    /// slow compared to previous executions. To find previous executions, use
    /// INFORMATION_SCHEMA tables and filter jobs with same query hash.
    #[prost(int64, tag = "1")]
    pub avg_previous_execution_ms: i64,
    /// Output only. Standalone query stage performance insights, for exploring
    /// potential improvements.
    #[prost(message, repeated, tag = "2")]
    pub stage_performance_standalone_insights: ::prost::alloc::vec::Vec<
        StagePerformanceStandaloneInsight,
    >,
    /// Output only. Query stage performance insights compared to previous runs,
    /// for diagnosing performance regression.
    #[prost(message, repeated, tag = "3")]
    pub stage_performance_change_insights: ::prost::alloc::vec::Vec<
        StagePerformanceChangeInsight,
    >,
}
/// Performance insights compared to the previous executions for a specific
/// stage.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct StagePerformanceChangeInsight {
    /// Output only. The stage id that the insight mapped to.
    #[prost(int64, tag = "1")]
    pub stage_id: i64,
    /// Output only. Input data change insight of the query stage.
    #[prost(message, optional, tag = "2")]
    pub input_data_change: ::core::option::Option<InputDataChange>,
}
/// Details about the input data change insight.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct InputDataChange {
    /// Output only. Records read difference percentage compared to a previous run.
    #[prost(float, tag = "1")]
    pub records_read_diff_percentage: f32,
}
/// Standalone performance insights for a specific stage.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StagePerformanceStandaloneInsight {
    /// Output only. The stage id that the insight mapped to.
    #[prost(int64, tag = "1")]
    pub stage_id: i64,
    /// Output only. True if the stage has a slot contention issue.
    #[prost(bool, optional, tag = "2")]
    pub slot_contention: ::core::option::Option<bool>,
    /// Output only. True if the stage has insufficient shuffle quota.
    #[prost(bool, optional, tag = "3")]
    pub insufficient_shuffle_quota: ::core::option::Option<bool>,
    /// Output only. If present, the stage had the following reasons for being
    /// disqualified from BI Engine execution.
    #[prost(message, repeated, tag = "5")]
    pub bi_engine_reasons: ::prost::alloc::vec::Vec<BiEngineReason>,
    /// Output only. High cardinality joins in the stage.
    #[prost(message, repeated, tag = "6")]
    pub high_cardinality_joins: ::prost::alloc::vec::Vec<HighCardinalityJoin>,
    /// Output only. Partition skew in the stage.
    #[prost(message, optional, tag = "7")]
    pub partition_skew: ::core::option::Option<PartitionSkew>,
}
/// High cardinality join detailed information.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct HighCardinalityJoin {
    /// Output only. Count of left input rows.
    #[prost(int64, tag = "1")]
    pub left_rows: i64,
    /// Output only. Count of right input rows.
    #[prost(int64, tag = "2")]
    pub right_rows: i64,
    /// Output only. Count of the output rows.
    #[prost(int64, tag = "3")]
    pub output_rows: i64,
    /// Output only. The index of the join operator in the ExplainQueryStep lists.
    #[prost(int32, tag = "4")]
    pub step_index: i32,
}
/// Partition skew detailed information.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PartitionSkew {
    /// Output only. Source stages which produce skewed data.
    #[prost(message, repeated, tag = "1")]
    pub skew_sources: ::prost::alloc::vec::Vec<partition_skew::SkewSource>,
}
/// Nested message and enum types in `PartitionSkew`.
pub mod partition_skew {
    /// Details about source stages which produce skewed data.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct SkewSource {
        /// Output only. Stage id of the skew source stage.
        #[prost(int64, tag = "1")]
        pub stage_id: i64,
    }
}
/// Statistics for a BigSpark query.
/// Populated as part of JobStatistics2
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SparkStatistics {
    /// Output only. Spark job ID if a Spark job is created successfully.
    #[prost(string, optional, tag = "1")]
    pub spark_job_id: ::core::option::Option<::prost::alloc::string::String>,
    /// Output only. Location where the Spark job is executed.
    /// A location is selected by BigQueury for jobs configured to run in a
    /// multi-region.
    #[prost(string, optional, tag = "2")]
    pub spark_job_location: ::core::option::Option<::prost::alloc::string::String>,
    /// Output only. Endpoints returned from Dataproc.
    /// Key list:
    ///   - history_server_endpoint: A link to Spark job UI.
    #[prost(map = "string, string", tag = "3")]
    pub endpoints: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Output only. Logging info is used to generate a link to Cloud Logging.
    #[prost(message, optional, tag = "4")]
    pub logging_info: ::core::option::Option<spark_statistics::LoggingInfo>,
    /// Output only. The Cloud KMS encryption key that is used to protect the
    /// resources created by the Spark job. If the Spark procedure uses the invoker
    /// security mode, the Cloud KMS encryption key is either inferred from the
    /// provided system variable,
    /// `@@spark_proc_properties.kms_key_name`, or the default key of the BigQuery
    /// job's project (if the CMEK organization policy is enforced). Otherwise, the
    /// Cloud KMS key is either inferred from the Spark connection associated with
    /// the procedure (if it is provided), or from the default key of the Spark
    /// connection's project if the CMEK organization policy is enforced.
    ///
    /// Example:
    ///
    /// * `projects/\[kms_project_id\]/locations/\[region\]/keyRings/\[key_region\]/cryptoKeys/\[key\]`
    #[prost(string, optional, tag = "5")]
    pub kms_key_name: ::core::option::Option<::prost::alloc::string::String>,
    /// Output only. The Google Cloud Storage bucket that is used as the default
    /// file system by the Spark application. This field is only filled when the
    /// Spark procedure uses the invoker security mode. The `gcsStagingBucket`
    /// bucket is inferred from the `@@spark_proc_properties.staging_bucket` system
    /// variable (if it is provided). Otherwise, BigQuery creates a default staging
    /// bucket for the job and returns the bucket name in this field.
    ///
    /// Example:
    ///
    /// * `gs://\[bucket_name\]`
    #[prost(string, optional, tag = "6")]
    pub gcs_staging_bucket: ::core::option::Option<::prost::alloc::string::String>,
}
/// Nested message and enum types in `SparkStatistics`.
pub mod spark_statistics {
    /// Spark job logs can be filtered by these fields in Cloud Logging.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct LoggingInfo {
        /// Output only. Resource type used for logging.
        #[prost(string, tag = "1")]
        pub resource_type: ::prost::alloc::string::String,
        /// Output only. Project ID where the Spark logs were written.
        #[prost(string, tag = "2")]
        pub project_id: ::prost::alloc::string::String,
    }
}
/// Statistics of materialized views considered in a query job.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MaterializedViewStatistics {
    /// Materialized views considered for the query job. Only certain materialized
    /// views are used. For a detailed list, see the child message.
    ///
    /// If many materialized views are considered, then the list might be
    /// incomplete.
    #[prost(message, repeated, tag = "1")]
    pub materialized_view: ::prost::alloc::vec::Vec<MaterializedView>,
}
/// A materialized view considered for a query job.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MaterializedView {
    /// The candidate materialized view.
    #[prost(message, optional, tag = "1")]
    pub table_reference: ::core::option::Option<TableReference>,
    /// Whether the materialized view is chosen for the query.
    ///
    /// A materialized view can be chosen to rewrite multiple parts of the same
    /// query. If a materialized view is chosen to rewrite any part of the query,
    /// then this field is true, even if the materialized view was not chosen to
    /// rewrite others parts.
    #[prost(bool, optional, tag = "2")]
    pub chosen: ::core::option::Option<bool>,
    /// If present, specifies a best-effort estimation of the bytes saved by using
    /// the materialized view rather than its base tables.
    #[prost(int64, optional, tag = "3")]
    pub estimated_bytes_saved: ::core::option::Option<i64>,
    /// If present, specifies the reason why the materialized view was not chosen
    /// for the query.
    #[prost(enumeration = "materialized_view::RejectedReason", optional, tag = "4")]
    pub rejected_reason: ::core::option::Option<i32>,
}
/// Nested message and enum types in `MaterializedView`.
pub mod materialized_view {
    /// Reason why a materialized view was not chosen for a query. For more
    /// information, see [Understand why materialized views were
    /// rejected](<https://cloud.google.com/bigquery/docs/materialized-views-use#understand-rejected>).
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum RejectedReason {
        /// Default unspecified value.
        Unspecified = 0,
        /// View has no cached data because it has not refreshed yet.
        NoData = 1,
        /// The estimated cost of the view is more expensive than another view or the
        /// base table.
        ///
        /// Note: The estimate cost might not match the billed cost.
        Cost = 2,
        /// View has no cached data because a base table is truncated.
        BaseTableTruncated = 3,
        /// View is invalidated because of a data change in one or more base tables.
        /// It could be any recent change if the
        /// [`max_staleness`](<https://cloud.google.com/bigquery/docs/materialized-views-create#max_staleness>)
        /// option is not set for the view, or otherwise any change outside of the
        /// staleness window.
        BaseTableDataChange = 4,
        /// View is invalidated because a base table's partition expiration has
        /// changed.
        BaseTablePartitionExpirationChange = 5,
        /// View is invalidated because a base table's partition has expired.
        BaseTableExpiredPartition = 6,
        /// View is invalidated because a base table has an incompatible metadata
        /// change.
        BaseTableIncompatibleMetadataChange = 7,
        /// View is invalidated because it was refreshed with a time zone other than
        /// that of the current job.
        TimeZone = 8,
        /// View is outside the time travel window.
        OutOfTimeTravelWindow = 9,
        /// View is inaccessible to the user because of a fine-grained security
        /// policy on one of its base tables.
        BaseTableFineGrainedSecurityPolicy = 10,
        /// One of the view's base tables is too stale. For example, the cached
        /// metadata of a BigLake external table needs to be updated.
        BaseTableTooStale = 11,
    }
    impl RejectedReason {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "REJECTED_REASON_UNSPECIFIED",
                Self::NoData => "NO_DATA",
                Self::Cost => "COST",
                Self::BaseTableTruncated => "BASE_TABLE_TRUNCATED",
                Self::BaseTableDataChange => "BASE_TABLE_DATA_CHANGE",
                Self::BaseTablePartitionExpirationChange => {
                    "BASE_TABLE_PARTITION_EXPIRATION_CHANGE"
                }
                Self::BaseTableExpiredPartition => "BASE_TABLE_EXPIRED_PARTITION",
                Self::BaseTableIncompatibleMetadataChange => {
                    "BASE_TABLE_INCOMPATIBLE_METADATA_CHANGE"
                }
                Self::TimeZone => "TIME_ZONE",
                Self::OutOfTimeTravelWindow => "OUT_OF_TIME_TRAVEL_WINDOW",
                Self::BaseTableFineGrainedSecurityPolicy => {
                    "BASE_TABLE_FINE_GRAINED_SECURITY_POLICY"
                }
                Self::BaseTableTooStale => "BASE_TABLE_TOO_STALE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "REJECTED_REASON_UNSPECIFIED" => Some(Self::Unspecified),
                "NO_DATA" => Some(Self::NoData),
                "COST" => Some(Self::Cost),
                "BASE_TABLE_TRUNCATED" => Some(Self::BaseTableTruncated),
                "BASE_TABLE_DATA_CHANGE" => Some(Self::BaseTableDataChange),
                "BASE_TABLE_PARTITION_EXPIRATION_CHANGE" => {
                    Some(Self::BaseTablePartitionExpirationChange)
                }
                "BASE_TABLE_EXPIRED_PARTITION" => Some(Self::BaseTableExpiredPartition),
                "BASE_TABLE_INCOMPATIBLE_METADATA_CHANGE" => {
                    Some(Self::BaseTableIncompatibleMetadataChange)
                }
                "TIME_ZONE" => Some(Self::TimeZone),
                "OUT_OF_TIME_TRAVEL_WINDOW" => Some(Self::OutOfTimeTravelWindow),
                "BASE_TABLE_FINE_GRAINED_SECURITY_POLICY" => {
                    Some(Self::BaseTableFineGrainedSecurityPolicy)
                }
                "BASE_TABLE_TOO_STALE" => Some(Self::BaseTableTooStale),
                _ => None,
            }
        }
    }
}
/// Table level detail on the usage of metadata caching. Only set for Metadata
/// caching eligible tables referenced in the query.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TableMetadataCacheUsage {
    /// Metadata caching eligible table referenced in the query.
    #[prost(message, optional, tag = "1")]
    pub table_reference: ::core::option::Option<TableReference>,
    /// Reason for not using metadata caching for the table.
    #[prost(
        enumeration = "table_metadata_cache_usage::UnusedReason",
        optional,
        tag = "2"
    )]
    pub unused_reason: ::core::option::Option<i32>,
    /// Free form human-readable reason metadata caching was unused for
    /// the job.
    #[prost(string, optional, tag = "3")]
    pub explanation: ::core::option::Option<::prost::alloc::string::String>,
    /// Duration since last refresh as of this job for managed tables (indicates
    /// metadata cache staleness as seen by this job).
    #[prost(message, optional, tag = "5")]
    pub staleness: ::core::option::Option<::prost_types::Duration>,
    /// [Table
    /// type](<https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#Table.FIELDS.type>).
    #[prost(string, tag = "6")]
    pub table_type: ::prost::alloc::string::String,
}
/// Nested message and enum types in `TableMetadataCacheUsage`.
pub mod table_metadata_cache_usage {
    /// Reasons for not using metadata caching.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum UnusedReason {
        /// Unused reasons not specified.
        Unspecified = 0,
        /// Metadata cache was outside the table's maxStaleness.
        ExceededMaxStaleness = 1,
        /// Metadata caching feature is not enabled. \[Update BigLake tables\]
        /// (/bigquery/docs/create-cloud-storage-table-biglake#update-biglake-tables)
        /// to enable the metadata caching.
        MetadataCachingNotEnabled = 3,
        /// Other unknown reason.
        OtherReason = 2,
    }
    impl UnusedReason {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "UNUSED_REASON_UNSPECIFIED",
                Self::ExceededMaxStaleness => "EXCEEDED_MAX_STALENESS",
                Self::MetadataCachingNotEnabled => "METADATA_CACHING_NOT_ENABLED",
                Self::OtherReason => "OTHER_REASON",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "UNUSED_REASON_UNSPECIFIED" => Some(Self::Unspecified),
                "EXCEEDED_MAX_STALENESS" => Some(Self::ExceededMaxStaleness),
                "METADATA_CACHING_NOT_ENABLED" => Some(Self::MetadataCachingNotEnabled),
                "OTHER_REASON" => Some(Self::OtherReason),
                _ => None,
            }
        }
    }
}
/// Statistics for metadata caching in BigLake tables.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MetadataCacheStatistics {
    /// Set for the Metadata caching eligible tables referenced in the query.
    #[prost(message, repeated, tag = "1")]
    pub table_metadata_cache_usage: ::prost::alloc::vec::Vec<TableMetadataCacheUsage>,
}
/// The type of editions.
/// Different features and behaviors are provided to different editions
/// Capacity commitments and reservations are linked to editions.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum ReservationEdition {
    /// Default value, which will be treated as ENTERPRISE.
    Unspecified = 0,
    /// Standard edition.
    Standard = 1,
    /// Enterprise edition.
    Enterprise = 2,
    /// Enterprise plus edition.
    EnterprisePlus = 3,
}
impl ReservationEdition {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "RESERVATION_EDITION_UNSPECIFIED",
            Self::Standard => "STANDARD",
            Self::Enterprise => "ENTERPRISE",
            Self::EnterprisePlus => "ENTERPRISE_PLUS",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "RESERVATION_EDITION_UNSPECIFIED" => Some(Self::Unspecified),
            "STANDARD" => Some(Self::Standard),
            "ENTERPRISE" => Some(Self::Enterprise),
            "ENTERPRISE_PLUS" => Some(Self::EnterprisePlus),
            _ => None,
        }
    }
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobStatus {
    /// Output only. Final error result of the job. If present, indicates that the
    /// job has completed and was unsuccessful.
    #[prost(message, optional, tag = "1")]
    pub error_result: ::core::option::Option<ErrorProto>,
    /// Output only. The first errors encountered during the running of the job.
    /// The final message includes the number of errors that caused the process to
    /// stop. Errors here do not necessarily mean that the job has not completed or
    /// was unsuccessful.
    #[prost(message, repeated, tag = "2")]
    pub errors: ::prost::alloc::vec::Vec<ErrorProto>,
    /// Output only. Running state of the job.  Valid states include 'PENDING',
    /// 'RUNNING', and 'DONE'.
    #[prost(string, tag = "3")]
    pub state: ::prost::alloc::string::String,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Job {
    /// Output only. The type of the resource.
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// Output only. A hash of this resource.
    #[prost(string, tag = "2")]
    pub etag: ::prost::alloc::string::String,
    /// Output only. Opaque ID field of the job.
    #[prost(string, tag = "3")]
    pub id: ::prost::alloc::string::String,
    /// Output only. A URL that can be used to access the resource again.
    #[prost(string, tag = "4")]
    pub self_link: ::prost::alloc::string::String,
    /// Output only. Email address of the user who ran the job.
    #[prost(string, tag = "5")]
    pub user_email: ::prost::alloc::string::String,
    /// Required. Describes the job configuration.
    #[prost(message, optional, tag = "6")]
    pub configuration: ::core::option::Option<JobConfiguration>,
    /// Optional. Reference describing the unique-per-user name of the job.
    #[prost(message, optional, tag = "7")]
    pub job_reference: ::core::option::Option<JobReference>,
    /// Output only. Information about the job, including starting time and ending
    /// time of the job.
    #[prost(message, optional, tag = "8")]
    pub statistics: ::core::option::Option<JobStatistics>,
    /// Output only. The status of this job. Examine this value when polling an
    /// asynchronous job to see if the job is complete.
    #[prost(message, optional, tag = "9")]
    pub status: ::core::option::Option<JobStatus>,
    /// Output only. \[Full-projection-only\] String representation of identity of
    /// requesting party. Populated for both first- and third-party identities.
    /// Only present for APIs that support third-party identities.
    #[prost(string, tag = "13")]
    pub principal_subject: ::prost::alloc::string::String,
    /// Output only. The reason why a Job was created.
    /// [Preview](<https://cloud.google.com/products/#product-launch-stages>)
    #[prost(message, optional, tag = "14")]
    pub job_creation_reason: ::core::option::Option<JobCreationReason>,
}
/// Describes format of a jobs cancellation request.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CancelJobRequest {
    /// Required. Project ID of the job to cancel
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Job ID of the job to cancel
    #[prost(string, tag = "2")]
    pub job_id: ::prost::alloc::string::String,
    /// The geographic location of the job. You must specify the location to run
    /// the job for the following scenarios:
    ///
    /// * If the location to run a job is not in the `us` or
    ///    the `eu` multi-regional location
    /// * If the job's location is in a single region (for example,
    ///    `us-central1`)
    ///
    /// For more information, see
    /// <https://cloud.google.com/bigquery/docs/locations#specifying_your_location.>
    #[prost(string, tag = "3")]
    pub location: ::prost::alloc::string::String,
}
/// Describes format of a jobs cancellation response.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobCancelResponse {
    /// The resource type of the response.
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// The final state of the job.
    #[prost(message, optional, tag = "2")]
    pub job: ::core::option::Option<Job>,
}
/// Describes format of a jobs get request.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetJobRequest {
    /// Required. Project ID of the requested job.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Job ID of the requested job.
    #[prost(string, tag = "2")]
    pub job_id: ::prost::alloc::string::String,
    /// The geographic location of the job. You must specify the location to run
    /// the job for the following scenarios:
    ///
    /// * If the location to run a job is not in the `us` or
    ///    the `eu` multi-regional location
    /// * If the job's location is in a single region (for example,
    ///    `us-central1`)
    ///
    /// For more information, see
    /// <https://cloud.google.com/bigquery/docs/locations#specifying_your_location.>
    #[prost(string, tag = "3")]
    pub location: ::prost::alloc::string::String,
}
/// Describes format of a job insertion request.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct InsertJobRequest {
    /// Project ID of project that will be billed for the job.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Jobs resource to insert.
    #[prost(message, optional, tag = "3")]
    pub job: ::core::option::Option<Job>,
}
/// Describes the format of a jobs deletion request.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteJobRequest {
    /// Required. Project ID of the job for which metadata is to be deleted.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Job ID of the job for which metadata is to be deleted. If this is
    /// a parent job which has child jobs, the metadata from all child jobs will be
    /// deleted as well. Direct deletion of the metadata of child jobs is not
    /// allowed.
    #[prost(string, tag = "2")]
    pub job_id: ::prost::alloc::string::String,
    /// The geographic location of the job. Required.
    /// See details at:
    /// <https://cloud.google.com/bigquery/docs/locations#specifying_your_location.>
    #[prost(string, tag = "3")]
    pub location: ::prost::alloc::string::String,
}
/// Describes the format of the list jobs request.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListJobsRequest {
    /// Project ID of the jobs to list.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Whether to display jobs owned by all users in the project. Default False.
    #[prost(bool, tag = "2")]
    pub all_users: bool,
    /// The maximum number of results to return in a single response page.
    /// Leverage the page tokens to iterate through the entire collection.
    #[prost(message, optional, tag = "3")]
    pub max_results: ::core::option::Option<i32>,
    /// Min value for job creation time, in milliseconds since the POSIX epoch.
    /// If set, only jobs created after or at this timestamp are returned.
    #[prost(uint64, tag = "4")]
    pub min_creation_time: u64,
    /// Max value for job creation time, in milliseconds since the POSIX epoch.
    /// If set, only jobs created before or at this timestamp are returned.
    #[prost(message, optional, tag = "5")]
    pub max_creation_time: ::core::option::Option<u64>,
    /// Page token, returned by a previous call, to request the next page of
    /// results.
    #[prost(string, tag = "6")]
    pub page_token: ::prost::alloc::string::String,
    /// Restrict information returned to a set of selected fields
    #[prost(enumeration = "list_jobs_request::Projection", tag = "7")]
    pub projection: i32,
    /// Filter for job state
    #[prost(enumeration = "list_jobs_request::StateFilter", repeated, tag = "8")]
    pub state_filter: ::prost::alloc::vec::Vec<i32>,
    /// If set, show only child jobs of the specified parent.  Otherwise, show all
    /// top-level jobs.
    #[prost(string, tag = "9")]
    pub parent_job_id: ::prost::alloc::string::String,
}
/// Nested message and enum types in `ListJobsRequest`.
pub mod list_jobs_request {
    /// Projection is used to control what job information is returned.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Projection {
        /// Does not include the job configuration
        Minimal = 0,
        /// Includes all job data
        Full = 1,
    }
    impl Projection {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Minimal => "minimal",
                Self::Full => "full",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "minimal" => Some(Self::Minimal),
                "full" => Some(Self::Full),
                _ => None,
            }
        }
    }
    /// StateFilter allows filtration by job execution state.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum StateFilter {
        /// Finished jobs
        Done = 0,
        /// Pending jobs
        Pending = 1,
        /// Running jobs
        Running = 2,
    }
    impl StateFilter {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Done => "done",
                Self::Pending => "pending",
                Self::Running => "running",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "done" => Some(Self::Done),
                "pending" => Some(Self::Pending),
                "running" => Some(Self::Running),
                _ => None,
            }
        }
    }
}
/// ListFormatJob is a partial projection of job information returned as part
/// of a jobs.list response.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFormatJob {
    /// Unique opaque ID of the job.
    #[prost(string, tag = "1")]
    pub id: ::prost::alloc::string::String,
    /// The resource type.
    #[prost(string, tag = "2")]
    pub kind: ::prost::alloc::string::String,
    /// Unique opaque ID of the job.
    #[prost(message, optional, tag = "3")]
    pub job_reference: ::core::option::Option<JobReference>,
    /// Running state of the job. When the state is DONE, errorResult can be
    /// checked to determine whether the job succeeded or failed.
    #[prost(string, tag = "4")]
    pub state: ::prost::alloc::string::String,
    /// A result object that will be present only if the job has failed.
    #[prost(message, optional, tag = "5")]
    pub error_result: ::core::option::Option<ErrorProto>,
    /// Output only. Information about the job, including starting time and ending
    /// time of the job.
    #[prost(message, optional, tag = "6")]
    pub statistics: ::core::option::Option<JobStatistics>,
    /// Required. Describes the job configuration.
    #[prost(message, optional, tag = "7")]
    pub configuration: ::core::option::Option<JobConfiguration>,
    /// \[Full-projection-only\] Describes the status of this job.
    #[prost(message, optional, tag = "8")]
    pub status: ::core::option::Option<JobStatus>,
    /// \[Full-projection-only\] Email address of the user who ran the job.
    #[prost(string, tag = "9")]
    pub user_email: ::prost::alloc::string::String,
    /// \[Full-projection-only\] String representation of identity of requesting
    /// party. Populated for both first- and third-party identities. Only present
    /// for APIs that support third-party identities.
    #[prost(string, tag = "10")]
    pub principal_subject: ::prost::alloc::string::String,
}
/// JobList is the response format for a jobs.list call.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JobList {
    /// A hash of this page of results.
    #[prost(string, tag = "1")]
    pub etag: ::prost::alloc::string::String,
    /// The resource type of the response.
    #[prost(string, tag = "2")]
    pub kind: ::prost::alloc::string::String,
    /// A token to request the next page of results.
    #[prost(string, tag = "3")]
    pub next_page_token: ::prost::alloc::string::String,
    /// List of jobs that were requested.
    #[prost(message, repeated, tag = "4")]
    pub jobs: ::prost::alloc::vec::Vec<ListFormatJob>,
    /// A list of skipped locations that were unreachable. For more information
    /// about BigQuery locations, see:
    /// <https://cloud.google.com/bigquery/docs/locations.> Example: "europe-west5"
    #[prost(string, repeated, tag = "5")]
    pub unreachable: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Request object of GetQueryResults.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetQueryResultsRequest {
    /// Required. Project ID of the query job.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Job ID of the query job.
    #[prost(string, tag = "2")]
    pub job_id: ::prost::alloc::string::String,
    /// Zero-based index of the starting row.
    #[prost(message, optional, tag = "3")]
    pub start_index: ::core::option::Option<u64>,
    /// Page token, returned by a previous call, to request the next page of
    /// results.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Maximum number of results to read.
    #[prost(message, optional, tag = "5")]
    pub max_results: ::core::option::Option<u32>,
    /// Optional: Specifies the maximum amount of time, in milliseconds, that the
    /// client is willing to wait for the query to complete. By default, this limit
    /// is 10 seconds (10,000 milliseconds). If the query is complete, the
    /// jobComplete field in the response is true. If the query has not yet
    /// completed, jobComplete is false.
    ///
    /// You can request a longer timeout period in the timeoutMs field.  However,
    /// the call is not guaranteed to wait for the specified timeout; it typically
    /// returns after around 200 seconds (200,000 milliseconds), even if the query
    /// is not complete.
    ///
    /// If jobComplete is false, you can continue to wait for the query to complete
    /// by calling the getQueryResults method until the jobComplete field in the
    /// getQueryResults response is true.
    #[prost(message, optional, tag = "6")]
    pub timeout_ms: ::core::option::Option<u32>,
    /// The geographic location of the job. You must specify the location to run
    /// the job for the following scenarios:
    ///
    /// * If the location to run a job is not in the `us` or
    ///    the `eu` multi-regional location
    /// * If the job's location is in a single region (for example,
    /// `us-central1`)
    ///
    /// For more information, see
    /// <https://cloud.google.com/bigquery/docs/locations#specifying_your_location.>
    #[prost(string, tag = "7")]
    pub location: ::prost::alloc::string::String,
    /// Optional. Output format adjustments.
    #[prost(message, optional, tag = "8")]
    pub format_options: ::core::option::Option<DataFormatOptions>,
}
/// Response object of GetQueryResults.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetQueryResultsResponse {
    /// The resource type of the response.
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// A hash of this response.
    #[prost(string, tag = "2")]
    pub etag: ::prost::alloc::string::String,
    /// The schema of the results. Present only when the query completes
    /// successfully.
    #[prost(message, optional, tag = "3")]
    pub schema: ::core::option::Option<TableSchema>,
    /// Reference to the BigQuery Job that was created to run the query. This field
    /// will be present even if the original request timed out, in which case
    /// GetQueryResults can be used to read the results once the query has
    /// completed. Since this API only returns the first page of results,
    /// subsequent pages can be fetched via the same mechanism (GetQueryResults).
    #[prost(message, optional, tag = "4")]
    pub job_reference: ::core::option::Option<JobReference>,
    /// The total number of rows in the complete query result set, which can be
    /// more than the number of rows in this single page of results. Present only
    /// when the query completes successfully.
    #[prost(message, optional, tag = "5")]
    pub total_rows: ::core::option::Option<u64>,
    /// A token used for paging results.  When this token is non-empty, it
    /// indicates additional results are available.
    #[prost(string, tag = "6")]
    pub page_token: ::prost::alloc::string::String,
    /// An object with as many results as can be contained within the maximum
    /// permitted reply size. To get any additional rows, you can call
    /// GetQueryResults and specify the jobReference returned above. Present only
    /// when the query completes successfully.
    ///
    /// The REST-based representation of this data leverages a series of
    /// JSON f,v objects for indicating fields and values.
    #[prost(message, repeated, tag = "7")]
    pub rows: ::prost::alloc::vec::Vec<::prost_types::Struct>,
    /// The total number of bytes processed for this query.
    #[prost(message, optional, tag = "8")]
    pub total_bytes_processed: ::core::option::Option<i64>,
    /// Whether the query has completed or not. If rows or totalRows are present,
    /// this will always be true. If this is false, totalRows will not be
    /// available.
    #[prost(message, optional, tag = "9")]
    pub job_complete: ::core::option::Option<bool>,
    /// Output only. The first errors or warnings encountered during the running
    /// of the job. The final message includes the number of errors that caused the
    /// process to stop. Errors here do not necessarily mean that the job has
    /// completed or was unsuccessful. For more information about error messages,
    /// see [Error
    /// messages](<https://cloud.google.com/bigquery/docs/error-messages>).
    #[prost(message, repeated, tag = "10")]
    pub errors: ::prost::alloc::vec::Vec<ErrorProto>,
    /// Whether the query result was fetched from the query cache.
    #[prost(message, optional, tag = "11")]
    pub cache_hit: ::core::option::Option<bool>,
    /// Output only. The number of rows affected by a DML statement. Present only
    /// for DML statements INSERT, UPDATE or DELETE.
    #[prost(message, optional, tag = "12")]
    pub num_dml_affected_rows: ::core::option::Option<i64>,
}
/// Request format for the query request.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PostQueryRequest {
    /// Required. Project ID of the query request.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// The query request body.
    #[prost(message, optional, tag = "2")]
    pub query_request: ::core::option::Option<QueryRequest>,
}
/// Describes the format of the jobs.query request.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryRequest {
    /// The resource type of the request.
    #[prost(string, tag = "2")]
    pub kind: ::prost::alloc::string::String,
    /// Required. A query string to execute, using Google Standard SQL or legacy
    /// SQL syntax. Example: "SELECT COUNT(f1) FROM
    /// myProjectId.myDatasetId.myTableId".
    #[prost(string, tag = "3")]
    pub query: ::prost::alloc::string::String,
    /// Optional. The maximum number of rows of data to return per page of
    /// results. Setting this flag to a small value such as 1000 and then paging
    /// through results might improve reliability when the query result set is
    /// large. In addition to this limit, responses are also limited to 10 MB. By
    /// default, there is no maximum row count, and only the byte limit applies.
    #[prost(message, optional, tag = "4")]
    pub max_results: ::core::option::Option<u32>,
    /// Optional. Specifies the default datasetId and projectId to assume for any
    /// unqualified table names in the query. If not set, all table names in the
    /// query string must be qualified in the format 'datasetId.tableId'.
    #[prost(message, optional, tag = "5")]
    pub default_dataset: ::core::option::Option<DatasetReference>,
    /// Optional. Optional: Specifies the maximum amount of time, in milliseconds,
    /// that the client is willing to wait for the query to complete. By default,
    /// this limit is 10 seconds (10,000 milliseconds). If the query is complete,
    /// the jobComplete field in the response is true. If the query has not yet
    /// completed, jobComplete is false.
    ///
    /// You can request a longer timeout period in the timeoutMs field.  However,
    /// the call is not guaranteed to wait for the specified timeout; it typically
    /// returns after around 200 seconds (200,000 milliseconds), even if the query
    /// is not complete.
    ///
    /// If jobComplete is false, you can continue to wait for the query to complete
    /// by calling the getQueryResults method until the jobComplete field in the
    /// getQueryResults response is true.
    #[prost(message, optional, tag = "6")]
    pub timeout_ms: ::core::option::Option<u32>,
    /// Optional. If set to true, BigQuery doesn't run the job. Instead, if the
    /// query is valid, BigQuery returns statistics about the job such as how many
    /// bytes would be processed. If the query is invalid, an error returns. The
    /// default value is false.
    #[prost(bool, tag = "7")]
    pub dry_run: bool,
    /// Optional. Whether to look for the result in the query cache. The query
    /// cache is a best-effort cache that will be flushed whenever tables in the
    /// query are modified. The default value is true.
    #[prost(message, optional, tag = "9")]
    pub use_query_cache: ::core::option::Option<bool>,
    /// Specifies whether to use BigQuery's legacy SQL dialect for this query. The
    /// default value is true. If set to false, the query will use BigQuery's
    /// GoogleSQL: <https://cloud.google.com/bigquery/sql-reference/> When
    /// useLegacySql is set to false, the value of flattenResults is ignored; query
    /// will be run as if flattenResults is false.
    #[prost(message, optional, tag = "10")]
    pub use_legacy_sql: ::core::option::Option<bool>,
    /// GoogleSQL only. Set to POSITIONAL to use positional (?) query parameters
    /// or to NAMED to use named (@myparam) query parameters in this query.
    #[prost(string, tag = "11")]
    pub parameter_mode: ::prost::alloc::string::String,
    /// Query parameters for GoogleSQL queries.
    #[prost(message, repeated, tag = "12")]
    pub query_parameters: ::prost::alloc::vec::Vec<QueryParameter>,
    /// The geographic location where the job should run. See details at
    /// <https://cloud.google.com/bigquery/docs/locations#specifying_your_location.>
    #[prost(string, tag = "13")]
    pub location: ::prost::alloc::string::String,
    /// Optional. Output format adjustments.
    #[prost(message, optional, tag = "15")]
    pub format_options: ::core::option::Option<DataFormatOptions>,
    /// Optional. Connection properties which can modify the query behavior.
    #[prost(message, repeated, tag = "16")]
    pub connection_properties: ::prost::alloc::vec::Vec<ConnectionProperty>,
    /// Optional. The labels associated with this query.
    /// Labels can be used to organize and group query jobs.
    /// Label keys and values can be no longer than 63 characters, can only contain
    /// lowercase letters, numeric characters, underscores and dashes.
    /// International characters are allowed. Label keys must start with a letter
    /// and each label in the list must have a different key.
    #[prost(map = "string, string", tag = "17")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. Limits the bytes billed for this query. Queries with
    /// bytes billed above this limit will fail (without incurring a charge).
    /// If unspecified, the project default is used.
    #[prost(message, optional, tag = "18")]
    pub maximum_bytes_billed: ::core::option::Option<i64>,
    /// Optional. A unique user provided identifier to ensure idempotent behavior
    /// for queries. Note that this is different from the job_id. It has the
    /// following properties:
    ///
    /// 1. It is case-sensitive, limited to up to 36 ASCII characters. A UUID is
    ///     recommended.
    ///
    /// 2. Read only queries can ignore this token since they are nullipotent by
    ///     definition.
    ///
    /// 3. For the purposes of idempotency ensured by the request_id, a request
    ///     is considered duplicate of another only if they have the same request_id
    ///     and are actually duplicates. When determining whether a request is a
    ///     duplicate of another request, all parameters in the request that
    ///     may affect the result are considered. For example, query,
    ///     connection_properties, query_parameters, use_legacy_sql are parameters
    ///     that affect the result and are considered when determining whether a
    ///     request is a duplicate, but properties like timeout_ms don't
    ///     affect the result and are thus not considered. Dry run query
    ///     requests are never considered duplicate of another request.
    ///
    /// 4. When a duplicate mutating query request is detected, it returns:
    ///     a. the results of the mutation if it completes successfully within
    ///        the timeout.
    ///     b. the running operation if it is still in progress at the end of the
    ///         timeout.
    ///
    /// 5. Its lifetime is limited to 15 minutes. In other words, if two
    ///     requests are sent with the same request_id, but more than 15 minutes
    ///     apart, idempotency is not guaranteed.
    #[prost(string, tag = "19")]
    pub request_id: ::prost::alloc::string::String,
    /// Optional. If true, creates a new session using a randomly generated
    /// session_id. If false, runs query with an existing session_id passed in
    /// ConnectionProperty, otherwise runs query in non-session mode.
    ///
    /// The session location will be set to QueryRequest.location if it is present,
    /// otherwise it's set to the default location based on existing routing logic.
    #[prost(message, optional, tag = "20")]
    pub create_session: ::core::option::Option<bool>,
    /// Optional. If not set, jobs are always required.
    ///
    /// If set, the query request will follow the behavior described
    /// JobCreationMode.
    /// [Preview](<https://cloud.google.com/products/#product-launch-stages>)
    #[prost(enumeration = "query_request::JobCreationMode", tag = "22")]
    pub job_creation_mode: i32,
}
/// Nested message and enum types in `QueryRequest`.
pub mod query_request {
    /// Job Creation Mode provides different options on job creation.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum JobCreationMode {
        /// If unspecified JOB_CREATION_REQUIRED is the default.
        Unspecified = 0,
        /// Default. Job creation is always required.
        JobCreationRequired = 1,
        /// Job creation is optional. Returning immediate results is prioritized.
        /// BigQuery will automatically determine if a Job needs to be created.
        /// The conditions under which BigQuery can decide to not create a Job are
        /// subject to change. If Job creation is required, JOB_CREATION_REQUIRED
        /// mode should be used, which is the default.
        JobCreationOptional = 2,
    }
    impl JobCreationMode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "JOB_CREATION_MODE_UNSPECIFIED",
                Self::JobCreationRequired => "JOB_CREATION_REQUIRED",
                Self::JobCreationOptional => "JOB_CREATION_OPTIONAL",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "JOB_CREATION_MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "JOB_CREATION_REQUIRED" => Some(Self::JobCreationRequired),
                "JOB_CREATION_OPTIONAL" => Some(Self::JobCreationOptional),
                _ => None,
            }
        }
    }
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryResponse {
    /// The resource type.
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// The schema of the results. Present only when the query completes
    /// successfully.
    #[prost(message, optional, tag = "2")]
    pub schema: ::core::option::Option<TableSchema>,
    /// Reference to the Job that was created to run the query. This field will be
    /// present even if the original request timed out, in which case
    /// GetQueryResults can be used to read the results once the query has
    /// completed. Since this API only returns the first page of results,
    /// subsequent pages can be fetched via the same mechanism (GetQueryResults).
    ///
    /// If job_creation_mode was set to `JOB_CREATION_OPTIONAL` and the query
    /// completes without creating a job, this field will be empty.
    #[prost(message, optional, tag = "3")]
    pub job_reference: ::core::option::Option<JobReference>,
    /// Optional. The reason why a Job was created.
    ///
    /// Only relevant when a job_reference is present in the response.
    /// If job_reference is not present it will always be unset.
    /// [Preview](<https://cloud.google.com/products/#product-launch-stages>)
    #[prost(message, optional, tag = "15")]
    pub job_creation_reason: ::core::option::Option<JobCreationReason>,
    /// Auto-generated ID for the query.
    /// [Preview](<https://cloud.google.com/products/#product-launch-stages>)
    #[prost(string, tag = "14")]
    pub query_id: ::prost::alloc::string::String,
    /// The total number of rows in the complete query result set, which can be
    /// more than the number of rows in this single page of results.
    #[prost(message, optional, tag = "4")]
    pub total_rows: ::core::option::Option<u64>,
    /// A token used for paging results. A non-empty token indicates that
    /// additional results are available. To see additional results,
    /// query the
    /// [`jobs.getQueryResults`](<https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/getQueryResults>)
    /// method. For more information, see [Paging through table
    /// data](<https://cloud.google.com/bigquery/docs/paging-results>).
    #[prost(string, tag = "5")]
    pub page_token: ::prost::alloc::string::String,
    /// An object with as many results as can be contained within the maximum
    /// permitted reply size. To get any additional rows, you can call
    /// GetQueryResults and specify the jobReference returned above.
    #[prost(message, repeated, tag = "6")]
    pub rows: ::prost::alloc::vec::Vec<::prost_types::Struct>,
    /// The total number of bytes processed for this query. If this query was a dry
    /// run, this is the number of bytes that would be processed if the query were
    /// run.
    #[prost(message, optional, tag = "7")]
    pub total_bytes_processed: ::core::option::Option<i64>,
    /// Whether the query has completed or not. If rows or totalRows are present,
    /// this will always be true. If this is false, totalRows will not be
    /// available.
    #[prost(message, optional, tag = "8")]
    pub job_complete: ::core::option::Option<bool>,
    /// Output only. The first errors or warnings encountered during the running of
    /// the job. The final message includes the number of errors that caused the
    /// process to stop. Errors here do not necessarily mean that the job has
    /// completed or was unsuccessful. For more information about error messages,
    /// see [Error
    /// messages](<https://cloud.google.com/bigquery/docs/error-messages>).
    #[prost(message, repeated, tag = "9")]
    pub errors: ::prost::alloc::vec::Vec<ErrorProto>,
    /// Whether the query result was fetched from the query cache.
    #[prost(message, optional, tag = "10")]
    pub cache_hit: ::core::option::Option<bool>,
    /// Output only. The number of rows affected by a DML statement. Present only
    /// for DML statements INSERT, UPDATE or DELETE.
    #[prost(message, optional, tag = "11")]
    pub num_dml_affected_rows: ::core::option::Option<i64>,
    /// Output only. Information of the session if this job is part of one.
    #[prost(message, optional, tag = "12")]
    pub session_info: ::core::option::Option<SessionInfo>,
    /// Output only. Detailed statistics for DML statements INSERT, UPDATE, DELETE,
    /// MERGE or TRUNCATE.
    #[prost(message, optional, tag = "13")]
    pub dml_stats: ::core::option::Option<DmlStats>,
}
/// Generated client implementations.
pub mod job_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    #[derive(Debug, Clone)]
    pub struct JobServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl JobServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> JobServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> JobServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            JobServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Requests that a job be cancelled. This call will return immediately, and
        /// the client will need to poll for the job status to see if the cancel
        /// completed successfully. Cancelled jobs may still incur costs.
        pub async fn cancel_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CancelJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::JobCancelResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.JobService/CancelJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("google.cloud.bigquery.v2.JobService", "CancelJob"),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Returns information about a specific job. Job information is available for
        /// a six month period after creation. Requires that you're the person who ran
        /// the job, or have the Is Owner project role.
        pub async fn get_job(
            &mut self,
            request: impl tonic::IntoRequest<super::GetJobRequest>,
        ) -> std::result::Result<tonic::Response<super::Job>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.JobService/GetJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("google.cloud.bigquery.v2.JobService", "GetJob"),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Starts a new asynchronous job.
        ///
        /// This API has two different kinds of endpoint URIs, as this method supports
        /// a variety of use cases.
        ///
        /// * The *Metadata* URI is used for most interactions, as it accepts the job
        ///   configuration directly.
        /// * The *Upload* URI is ONLY for the case when you're sending both a load job
        ///   configuration and a data stream together.  In this case, the Upload URI
        ///   accepts the job configuration and the data as two distinct multipart MIME
        ///   parts.
        pub async fn insert_job(
            &mut self,
            request: impl tonic::IntoRequest<super::InsertJobRequest>,
        ) -> std::result::Result<tonic::Response<super::Job>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.JobService/InsertJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("google.cloud.bigquery.v2.JobService", "InsertJob"),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Requests the deletion of the metadata of a job. This call returns when the
        /// job's metadata is deleted.
        pub async fn delete_job(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteJobRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.JobService/DeleteJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("google.cloud.bigquery.v2.JobService", "DeleteJob"),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists all jobs that you started in the specified project. Job information
        /// is available for a six month period after creation. The job list is sorted
        /// in reverse chronological order, by job creation time. Requires the Can View
        /// project role, or the Is Owner project role if you set the allUsers
        /// property.
        pub async fn list_jobs(
            &mut self,
            request: impl tonic::IntoRequest<super::ListJobsRequest>,
        ) -> std::result::Result<tonic::Response<super::JobList>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.JobService/ListJobs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("google.cloud.bigquery.v2.JobService", "ListJobs"),
                );
            self.inner.unary(req, path, codec).await
        }
        /// RPC to get the results of a query job.
        pub async fn get_query_results(
            &mut self,
            request: impl tonic::IntoRequest<super::GetQueryResultsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::GetQueryResultsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.JobService/GetQueryResults",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.JobService",
                        "GetQueryResults",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Runs a BigQuery SQL query synchronously and returns query results if the
        /// query completes within a specified timeout.
        pub async fn query(
            &mut self,
            request: impl tonic::IntoRequest<super::PostQueryRequest>,
        ) -> std::result::Result<tonic::Response<super::QueryResponse>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.JobService/Query",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("google.cloud.bigquery.v2.JobService", "Query"));
            self.inner.unary(req, path, codec).await
        }
    }
}
/// BigQuery-specific metadata about a location. This will be set on
/// google.cloud.location.Location.metadata in Cloud Location API
/// responses.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct LocationMetadata {
    /// The legacy BigQuery location ID, e.g. EU for the europe location.
    /// This is for any API consumers that need the legacy US and EU locations.
    #[prost(string, tag = "1")]
    pub legacy_location_id: ::prost::alloc::string::String,
}
/// The partitioning information, which includes managed table, external table
/// and metastore partitioned table partition information.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PartitioningDefinition {
    /// Optional. Details about each partitioning column. This field is output only
    /// for all partitioning types other than metastore partitioned tables.
    /// BigQuery native tables only support 1 partitioning column. Other table
    /// types may support 0, 1 or more partitioning columns.
    /// For metastore partitioned tables, the order must match the definition order
    /// in the Hive Metastore, where it must match the physical layout of the
    /// table. For example,
    ///
    /// CREATE TABLE a_table(id BIGINT, name STRING)
    /// PARTITIONED BY (city STRING, state STRING).
    ///
    /// In this case the values must be \['city', 'state'\] in that order.
    #[prost(message, repeated, tag = "1")]
    pub partitioned_column: ::prost::alloc::vec::Vec<PartitionedColumn>,
}
/// The partitioning column information.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PartitionedColumn {
    /// Required. The name of the partition column.
    #[prost(string, optional, tag = "1")]
    pub field: ::core::option::Option<::prost::alloc::string::String>,
}
/// Represents privacy policy associated with "aggregation threshold" method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AggregationThresholdPolicy {
    /// Optional. The threshold for the "aggregation threshold" policy.
    #[prost(int64, optional, tag = "1")]
    pub threshold: ::core::option::Option<i64>,
    /// Optional. The privacy unit column(s) associated with this policy.
    /// For now, only one column per data source object (table, view) is allowed as
    /// a privacy unit column.
    /// Representing as a repeated field in metadata for extensibility to
    /// multiple columns in future.
    /// Duplicates and Repeated struct fields are not allowed.
    /// For nested fields, use dot notation ("outer.inner")
    #[prost(string, repeated, tag = "2")]
    pub privacy_unit_columns: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Represents privacy policy associated with "differential privacy" method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DifferentialPrivacyPolicy {
    /// Optional. The maximum epsilon value that a query can consume. If the
    /// subscriber specifies epsilon as a parameter in a SELECT query, it must be
    /// less than or equal to this value. The epsilon parameter controls the amount
    /// of noise that is added to the groups  a higher epsilon means less noise.
    #[prost(double, optional, tag = "1")]
    pub max_epsilon_per_query: ::core::option::Option<f64>,
    /// Optional. The delta value that is used per query. Delta represents the
    /// probability that any row will fail to be epsilon differentially private.
    /// Indicates the risk associated with exposing aggregate rows in the result of
    /// a query.
    #[prost(double, optional, tag = "2")]
    pub delta_per_query: ::core::option::Option<f64>,
    /// Optional. The maximum groups contributed value that is used per query.
    /// Represents the maximum number of groups to which each protected entity can
    /// contribute. Changing this value does not improve or worsen privacy. The
    /// best value for accuracy and utility depends on the query and data.
    #[prost(int64, optional, tag = "3")]
    pub max_groups_contributed: ::core::option::Option<i64>,
    /// Optional. The privacy unit column associated with this policy. Differential
    /// privacy policies can only have one privacy unit column per data source
    /// object (table, view).
    #[prost(string, optional, tag = "4")]
    pub privacy_unit_column: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. The total epsilon budget for all queries against the
    /// privacy-protected view. Each subscriber query against this view charges the
    /// amount of epsilon they request in their query. If there is sufficient
    /// budget, then the subscriber query attempts to complete. It might still fail
    /// due to other reasons, in which case the charge is refunded. If there is
    /// insufficient budget the query is rejected. There might be multiple charge
    /// attempts if a single query references multiple views. In this case there
    /// must be sufficient budget for all charges or the query is rejected and
    /// charges are refunded in best effort. The budget does not have a refresh
    /// policy and can only be updated via ALTER VIEW or circumvented by creating a
    /// new view that can be queried with a fresh budget.
    #[prost(double, optional, tag = "5")]
    pub epsilon_budget: ::core::option::Option<f64>,
    /// Optional. The total delta budget for all queries against the
    /// privacy-protected view. Each subscriber query against this view charges the
    /// amount of delta that is pre-defined by the contributor through the privacy
    /// policy delta_per_query field. If there is sufficient budget, then the
    /// subscriber query attempts to complete. It might still fail due to other
    /// reasons, in which case the charge is refunded. If there is insufficient
    /// budget the query is rejected. There might be multiple charge attempts if a
    /// single query references multiple views. In this case there must be
    /// sufficient budget for all charges or the query is rejected and charges are
    /// refunded in best effort. The budget does not have a refresh policy and can
    /// only be updated via ALTER VIEW or circumvented by creating a new view that
    /// can be queried with a fresh budget.
    #[prost(double, optional, tag = "6")]
    pub delta_budget: ::core::option::Option<f64>,
    /// Output only. The epsilon budget remaining. If budget is exhausted, no more
    /// queries are allowed. Note that the budget for queries that are in progress
    /// is deducted before the query executes. If the query fails or is cancelled
    /// then the budget is refunded. In this case the amount of budget remaining
    /// can increase.
    #[prost(double, optional, tag = "7")]
    pub epsilon_budget_remaining: ::core::option::Option<f64>,
    /// Output only. The delta budget remaining. If budget is exhausted, no more
    /// queries are allowed. Note that the budget for queries that are in progress
    /// is deducted before the query executes. If the query fails or is cancelled
    /// then the budget is refunded. In this case the amount of budget remaining
    /// can increase.
    #[prost(double, optional, tag = "8")]
    pub delta_budget_remaining: ::core::option::Option<f64>,
}
/// Represents privacy policy associated with "join restrictions". Join
/// restriction gives data providers the ability to enforce joins on the
/// 'join_allowed_columns' when data is queried from a privacy protected view.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JoinRestrictionPolicy {
    /// Optional. Specifies if a join is required or not on queries for the view.
    /// Default is JOIN_CONDITION_UNSPECIFIED.
    #[prost(enumeration = "join_restriction_policy::JoinCondition", optional, tag = "1")]
    pub join_condition: ::core::option::Option<i32>,
    /// Optional. The only columns that joins are allowed on.
    /// This field is must be specified for join_conditions JOIN_ANY and JOIN_ALL
    /// and it cannot be set for JOIN_BLOCKED.
    #[prost(string, repeated, tag = "2")]
    pub join_allowed_columns: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Nested message and enum types in `JoinRestrictionPolicy`.
pub mod join_restriction_policy {
    /// Enum for Join Restrictions policy.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum JoinCondition {
        /// A join is neither required nor restricted on any column. Default value.
        Unspecified = 0,
        /// A join is required on at least one of the specified columns.
        JoinAny = 1,
        /// A join is required on all specified columns.
        JoinAll = 2,
        /// A join is not required, but if present it is only permitted on
        /// 'join_allowed_columns'
        JoinNotRequired = 3,
        /// Joins are blocked for all queries.
        JoinBlocked = 4,
    }
    impl JoinCondition {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "JOIN_CONDITION_UNSPECIFIED",
                Self::JoinAny => "JOIN_ANY",
                Self::JoinAll => "JOIN_ALL",
                Self::JoinNotRequired => "JOIN_NOT_REQUIRED",
                Self::JoinBlocked => "JOIN_BLOCKED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "JOIN_CONDITION_UNSPECIFIED" => Some(Self::Unspecified),
                "JOIN_ANY" => Some(Self::JoinAny),
                "JOIN_ALL" => Some(Self::JoinAll),
                "JOIN_NOT_REQUIRED" => Some(Self::JoinNotRequired),
                "JOIN_BLOCKED" => Some(Self::JoinBlocked),
                _ => None,
            }
        }
    }
}
/// Represents privacy policy that contains the privacy requirements specified by
/// the data owner. Currently, this is only supported on views.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PrivacyPolicy {
    /// Optional. Join restriction policy is outside of the one of policies, since
    /// this policy can be set along with other policies. This policy gives data
    /// providers the ability to enforce joins on the 'join_allowed_columns' when
    /// data is queried from a privacy protected view.
    #[prost(message, optional, tag = "1")]
    pub join_restriction_policy: ::core::option::Option<JoinRestrictionPolicy>,
    /// Privacy policy associated with this requirement specification. Only one of
    /// the privacy methods is allowed per data source object.
    #[prost(oneof = "privacy_policy::PrivacyPolicy", tags = "2, 3")]
    pub privacy_policy: ::core::option::Option<privacy_policy::PrivacyPolicy>,
}
/// Nested message and enum types in `PrivacyPolicy`.
pub mod privacy_policy {
    /// Privacy policy associated with this requirement specification. Only one of
    /// the privacy methods is allowed per data source object.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum PrivacyPolicy {
        /// Optional. Policy used for aggregation thresholds.
        #[prost(message, tag = "2")]
        AggregationThresholdPolicy(super::AggregationThresholdPolicy),
        /// Optional. Policy used for differential privacy.
        #[prost(message, tag = "3")]
        DifferentialPrivacyPolicy(super::DifferentialPrivacyPolicy),
    }
}
/// Request object of GetServiceAccount
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetServiceAccountRequest {
    /// Required. ID of the project.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
}
/// Response object of GetServiceAccount
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetServiceAccountResponse {
    /// The resource type of the response.
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// The service account email address.
    #[prost(string, tag = "2")]
    pub email: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod project_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// This service provides access to BigQuery functionality related to projects.
    #[derive(Debug, Clone)]
    pub struct ProjectServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl ProjectServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> ProjectServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> ProjectServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            ProjectServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// RPC to get the service account for a project used for interactions with
        /// Google Cloud KMS
        pub async fn get_service_account(
            &mut self,
            request: impl tonic::IntoRequest<super::GetServiceAccountRequest>,
        ) -> std::result::Result<
            tonic::Response<super::GetServiceAccountResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.ProjectService/GetServiceAccount",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.ProjectService",
                        "GetServiceAccount",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// A user-defined function or a stored procedure.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Routine {
    /// Output only. A hash of this resource.
    #[prost(string, tag = "1")]
    pub etag: ::prost::alloc::string::String,
    /// Required. Reference describing the ID of this routine.
    #[prost(message, optional, tag = "2")]
    pub routine_reference: ::core::option::Option<RoutineReference>,
    /// Required. The type of routine.
    #[prost(enumeration = "routine::RoutineType", tag = "3")]
    pub routine_type: i32,
    /// Output only. The time when this routine was created, in milliseconds since
    /// the epoch.
    #[prost(int64, tag = "4")]
    pub creation_time: i64,
    /// Output only. The time when this routine was last modified, in milliseconds
    /// since the epoch.
    #[prost(int64, tag = "5")]
    pub last_modified_time: i64,
    /// Optional. Defaults to "SQL" if remote_function_options field is absent, not
    /// set otherwise.
    #[prost(enumeration = "routine::Language", tag = "6")]
    pub language: i32,
    /// Optional.
    #[prost(message, repeated, tag = "7")]
    pub arguments: ::prost::alloc::vec::Vec<routine::Argument>,
    /// Optional if language = "SQL"; required otherwise.
    /// Cannot be set if routine_type = "TABLE_VALUED_FUNCTION".
    ///
    /// If absent, the return type is inferred from definition_body at query time
    /// in each query that references this routine. If present, then the evaluated
    /// result will be cast to the specified returned type at query time.
    ///
    /// For example, for the functions created with the following statements:
    ///
    /// * `CREATE FUNCTION Add(x FLOAT64, y FLOAT64) RETURNS FLOAT64 AS (x + y);`
    ///
    /// * `CREATE FUNCTION Increment(x FLOAT64) AS (Add(x, 1));`
    ///
    /// * `CREATE FUNCTION Decrement(x FLOAT64) RETURNS FLOAT64 AS (Add(x, -1));`
    ///
    /// The return_type is `{type_kind: "FLOAT64"}` for `Add` and `Decrement`, and
    /// is absent for `Increment` (inferred as FLOAT64 at query time).
    ///
    /// Suppose the function `Add` is replaced by
    ///    `CREATE OR REPLACE FUNCTION Add(x INT64, y INT64) AS (x + y);`
    ///
    /// Then the inferred return type of `Increment` is automatically changed to
    /// INT64 at query time, while the return type of `Decrement` remains FLOAT64.
    #[prost(message, optional, tag = "10")]
    pub return_type: ::core::option::Option<StandardSqlDataType>,
    /// Optional. Can be set only if routine_type = "TABLE_VALUED_FUNCTION".
    ///
    /// If absent, the return table type is inferred from definition_body at query
    /// time in each query that references this routine. If present, then the
    /// columns in the evaluated table result will be cast to match the column
    /// types specified in return table type, at query time.
    #[prost(message, optional, tag = "13")]
    pub return_table_type: ::core::option::Option<StandardSqlTableType>,
    /// Optional. If language = "JAVASCRIPT", this field stores the path of the
    /// imported JAVASCRIPT libraries.
    #[prost(string, repeated, tag = "8")]
    pub imported_libraries: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Required. The body of the routine.
    ///
    /// For functions, this is the expression in the AS clause.
    ///
    /// If language=SQL, it is the substring inside (but excluding) the
    /// parentheses. For example, for the function created with the following
    /// statement:
    ///
    /// `CREATE FUNCTION JoinLines(x string, y string) as (concat(x, "\n", y))`
    ///
    /// The definition_body is `concat(x, "\n", y)` (\n is not replaced with
    /// linebreak).
    ///
    /// If language=JAVASCRIPT, it is the evaluated string in the AS clause.
    /// For example, for the function created with the following statement:
    ///
    /// `CREATE FUNCTION f() RETURNS STRING LANGUAGE js AS 'return "\n";\n'`
    ///
    /// The definition_body is
    ///
    /// `return "\n";\n`
    ///
    /// Note that both \n are replaced with linebreaks.
    #[prost(string, tag = "9")]
    pub definition_body: ::prost::alloc::string::String,
    /// Optional. The description of the routine, if defined.
    #[prost(string, tag = "11")]
    pub description: ::prost::alloc::string::String,
    /// Optional. The determinism level of the JavaScript UDF, if defined.
    #[prost(enumeration = "routine::DeterminismLevel", tag = "12")]
    pub determinism_level: i32,
    /// Optional. The security mode of the routine, if defined. If not defined, the
    /// security mode is automatically determined from the routine's configuration.
    #[prost(enumeration = "routine::SecurityMode", tag = "18")]
    pub security_mode: i32,
    /// Optional. Use this option to catch many common errors. Error checking is
    /// not exhaustive, and successfully creating a procedure doesn't guarantee
    /// that the procedure will successfully execute at runtime. If `strictMode` is
    /// set to `TRUE`, the procedure body is further checked for errors such as
    /// non-existent tables or columns. The `CREATE PROCEDURE` statement fails if
    /// the body fails any of these checks.
    ///
    /// If `strictMode` is set to `FALSE`, the procedure body is checked only for
    /// syntax. For procedures that invoke themselves recursively, specify
    /// `strictMode=FALSE` to avoid non-existent procedure errors during
    /// validation.
    ///
    /// Default value is `TRUE`.
    #[prost(message, optional, tag = "14")]
    pub strict_mode: ::core::option::Option<bool>,
    /// Optional. Remote function specific options.
    #[prost(message, optional, tag = "15")]
    pub remote_function_options: ::core::option::Option<routine::RemoteFunctionOptions>,
    /// Optional. Spark specific options.
    #[prost(message, optional, tag = "16")]
    pub spark_options: ::core::option::Option<SparkOptions>,
    /// Optional. If set to `DATA_MASKING`, the function is validated and made
    /// available as a masking function. For more information, see [Create custom
    /// masking
    /// routines](<https://cloud.google.com/bigquery/docs/user-defined-functions#custom-mask>).
    #[prost(enumeration = "routine::DataGovernanceType", tag = "17")]
    pub data_governance_type: i32,
}
/// Nested message and enum types in `Routine`.
pub mod routine {
    /// Input/output argument of a function or a stored procedure.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Argument {
        /// Optional. The name of this argument. Can be absent for function return
        /// argument.
        #[prost(string, tag = "1")]
        pub name: ::prost::alloc::string::String,
        /// Optional. Defaults to FIXED_TYPE.
        #[prost(enumeration = "argument::ArgumentKind", tag = "2")]
        pub argument_kind: i32,
        /// Optional. Specifies whether the argument is input or output.
        /// Can be set for procedures only.
        #[prost(enumeration = "argument::Mode", tag = "3")]
        pub mode: i32,
        /// Set if argument_kind == FIXED_TYPE.
        #[prost(message, optional, tag = "4")]
        pub data_type: ::core::option::Option<super::StandardSqlDataType>,
        /// Optional. Whether the argument is an aggregate function parameter.
        /// Must be Unset for routine types other than AGGREGATE_FUNCTION.
        /// For AGGREGATE_FUNCTION, if set to false, it is equivalent to adding "NOT
        /// AGGREGATE" clause in DDL; Otherwise, it is equivalent to omitting "NOT
        /// AGGREGATE" clause in DDL.
        #[prost(message, optional, tag = "6")]
        pub is_aggregate: ::core::option::Option<bool>,
    }
    /// Nested message and enum types in `Argument`.
    pub mod argument {
        /// Represents the kind of a given argument.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum ArgumentKind {
            /// Default value.
            Unspecified = 0,
            /// The argument is a variable with fully specified type, which can be a
            /// struct or an array, but not a table.
            FixedType = 1,
            /// The argument is any type, including struct or array, but not a table.
            AnyType = 2,
        }
        impl ArgumentKind {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "ARGUMENT_KIND_UNSPECIFIED",
                    Self::FixedType => "FIXED_TYPE",
                    Self::AnyType => "ANY_TYPE",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "ARGUMENT_KIND_UNSPECIFIED" => Some(Self::Unspecified),
                    "FIXED_TYPE" => Some(Self::FixedType),
                    "ANY_TYPE" => Some(Self::AnyType),
                    _ => None,
                }
            }
        }
        /// The input/output mode of the argument.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum Mode {
            /// Default value.
            Unspecified = 0,
            /// The argument is input-only.
            In = 1,
            /// The argument is output-only.
            Out = 2,
            /// The argument is both an input and an output.
            Inout = 3,
        }
        impl Mode {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "MODE_UNSPECIFIED",
                    Self::In => "IN",
                    Self::Out => "OUT",
                    Self::Inout => "INOUT",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "MODE_UNSPECIFIED" => Some(Self::Unspecified),
                    "IN" => Some(Self::In),
                    "OUT" => Some(Self::Out),
                    "INOUT" => Some(Self::Inout),
                    _ => None,
                }
            }
        }
    }
    /// Options for a remote user-defined function.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct RemoteFunctionOptions {
        /// Endpoint of the user-provided remote service, e.g.
        /// ```<https://us-east1-my_gcf_project.cloudfunctions.net/remote_add```>
        #[prost(string, tag = "1")]
        pub endpoint: ::prost::alloc::string::String,
        /// Fully qualified name of the user-provided connection object which holds
        /// the authentication information to send requests to the remote service.
        /// Format:
        /// ```"projects/{projectId}/locations/{locationId}/connections/{connectionId}"```
        #[prost(string, tag = "2")]
        pub connection: ::prost::alloc::string::String,
        /// User-defined context as a set of key/value pairs, which will be sent as
        /// function invocation context together with batched arguments in the
        /// requests to the remote service. The total number of bytes of keys and
        /// values must be less than 8KB.
        #[prost(map = "string, string", tag = "3")]
        pub user_defined_context: ::std::collections::HashMap<
            ::prost::alloc::string::String,
            ::prost::alloc::string::String,
        >,
        /// Max number of rows in each batch sent to the remote service.
        /// If absent or if 0, BigQuery dynamically decides the number of rows in a
        /// batch.
        #[prost(int64, tag = "4")]
        pub max_batching_rows: i64,
    }
    /// The fine-grained type of the routine.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum RoutineType {
        /// Default value.
        Unspecified = 0,
        /// Non-built-in persistent scalar function.
        ScalarFunction = 1,
        /// Stored procedure.
        Procedure = 2,
        /// Non-built-in persistent TVF.
        TableValuedFunction = 3,
        /// Non-built-in persistent aggregate function.
        AggregateFunction = 4,
    }
    impl RoutineType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "ROUTINE_TYPE_UNSPECIFIED",
                Self::ScalarFunction => "SCALAR_FUNCTION",
                Self::Procedure => "PROCEDURE",
                Self::TableValuedFunction => "TABLE_VALUED_FUNCTION",
                Self::AggregateFunction => "AGGREGATE_FUNCTION",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "ROUTINE_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "SCALAR_FUNCTION" => Some(Self::ScalarFunction),
                "PROCEDURE" => Some(Self::Procedure),
                "TABLE_VALUED_FUNCTION" => Some(Self::TableValuedFunction),
                "AGGREGATE_FUNCTION" => Some(Self::AggregateFunction),
                _ => None,
            }
        }
    }
    /// The language of the routine.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Language {
        /// Default value.
        Unspecified = 0,
        /// SQL language.
        Sql = 1,
        /// JavaScript language.
        Javascript = 2,
        /// Python language.
        Python = 3,
        /// Java language.
        Java = 4,
        /// Scala language.
        Scala = 5,
    }
    impl Language {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "LANGUAGE_UNSPECIFIED",
                Self::Sql => "SQL",
                Self::Javascript => "JAVASCRIPT",
                Self::Python => "PYTHON",
                Self::Java => "JAVA",
                Self::Scala => "SCALA",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "LANGUAGE_UNSPECIFIED" => Some(Self::Unspecified),
                "SQL" => Some(Self::Sql),
                "JAVASCRIPT" => Some(Self::Javascript),
                "PYTHON" => Some(Self::Python),
                "JAVA" => Some(Self::Java),
                "SCALA" => Some(Self::Scala),
                _ => None,
            }
        }
    }
    /// JavaScript UDF determinism levels.
    ///
    /// If all JavaScript UDFs are DETERMINISTIC, the query result is
    /// potentially cachable (see below). If any JavaScript UDF is
    /// NOT_DETERMINISTIC, the query result is not cacheable.
    ///
    /// Even if a JavaScript UDF is deterministic, many other factors can prevent
    /// usage of cached query results. Example factors include but not limited to:
    /// DDL/DML, non-deterministic SQL function calls, update of referenced
    /// tables/views/UDFs or imported JavaScript libraries.
    ///
    /// SQL UDFs cannot have determinism specified. Their determinism is
    /// automatically determined.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum DeterminismLevel {
        /// The determinism of the UDF is unspecified.
        Unspecified = 0,
        /// The UDF is deterministic, meaning that 2 function calls with the same
        /// inputs always produce the same result, even across 2 query runs.
        Deterministic = 1,
        /// The UDF is not deterministic.
        NotDeterministic = 2,
    }
    impl DeterminismLevel {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "DETERMINISM_LEVEL_UNSPECIFIED",
                Self::Deterministic => "DETERMINISTIC",
                Self::NotDeterministic => "NOT_DETERMINISTIC",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "DETERMINISM_LEVEL_UNSPECIFIED" => Some(Self::Unspecified),
                "DETERMINISTIC" => Some(Self::Deterministic),
                "NOT_DETERMINISTIC" => Some(Self::NotDeterministic),
                _ => None,
            }
        }
    }
    /// Security mode.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum SecurityMode {
        /// The security mode of the routine is unspecified.
        Unspecified = 0,
        /// The routine is to be executed with the privileges of the user who
        /// defines it.
        Definer = 1,
        /// The routine is to be executed with the privileges of the user who
        /// invokes it.
        Invoker = 2,
    }
    impl SecurityMode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "SECURITY_MODE_UNSPECIFIED",
                Self::Definer => "DEFINER",
                Self::Invoker => "INVOKER",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "SECURITY_MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "DEFINER" => Some(Self::Definer),
                "INVOKER" => Some(Self::Invoker),
                _ => None,
            }
        }
    }
    /// Data governance type values. Only supports `DATA_MASKING`.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum DataGovernanceType {
        /// The data governance type is unspecified.
        Unspecified = 0,
        /// The data governance type is data masking.
        DataMasking = 1,
    }
    impl DataGovernanceType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "DATA_GOVERNANCE_TYPE_UNSPECIFIED",
                Self::DataMasking => "DATA_MASKING",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "DATA_GOVERNANCE_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "DATA_MASKING" => Some(Self::DataMasking),
                _ => None,
            }
        }
    }
}
/// Options for a user-defined Spark routine.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SparkOptions {
    /// Fully qualified name of the user-provided Spark connection object. Format:
    /// ```"projects/{project_id}/locations/{location_id}/connections/{connection_id}"```
    #[prost(string, tag = "1")]
    pub connection: ::prost::alloc::string::String,
    /// Runtime version. If not specified, the default runtime version is used.
    #[prost(string, tag = "2")]
    pub runtime_version: ::prost::alloc::string::String,
    /// Custom container image for the runtime environment.
    #[prost(string, tag = "3")]
    pub container_image: ::prost::alloc::string::String,
    /// Configuration properties as a set of key/value pairs, which will be passed
    /// on to the Spark application. For more information, see
    /// [Apache Spark](<https://spark.apache.org/docs/latest/index.html>) and the
    /// [procedure option
    /// list](<https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#procedure_option_list>).
    #[prost(map = "string, string", tag = "4")]
    pub properties: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// The main file/jar URI of the Spark application. Exactly one of the
    /// definition_body field and the main_file_uri field must be set for Python.
    /// Exactly one of main_class and main_file_uri field
    /// should be set for Java/Scala language type.
    #[prost(string, tag = "5")]
    pub main_file_uri: ::prost::alloc::string::String,
    /// Python files to be placed on the PYTHONPATH for PySpark application.
    /// Supported file types: `.py`, `.egg`, and `.zip`. For more information
    /// about Apache Spark, see
    /// [Apache Spark](<https://spark.apache.org/docs/latest/index.html>).
    #[prost(string, repeated, tag = "6")]
    pub py_file_uris: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// JARs to include on the driver and executor CLASSPATH.
    /// For more information about Apache Spark, see
    /// [Apache Spark](<https://spark.apache.org/docs/latest/index.html>).
    #[prost(string, repeated, tag = "7")]
    pub jar_uris: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Files to be placed in the working directory of each executor.
    /// For more information about Apache Spark, see
    /// [Apache Spark](<https://spark.apache.org/docs/latest/index.html>).
    #[prost(string, repeated, tag = "8")]
    pub file_uris: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Archive files to be extracted into the working directory of each executor.
    /// For more information about Apache Spark, see
    /// [Apache Spark](<https://spark.apache.org/docs/latest/index.html>).
    #[prost(string, repeated, tag = "9")]
    pub archive_uris: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// The fully qualified name of a class in jar_uris, for example,
    /// com.example.wordcount. Exactly one of main_class and main_jar_uri field
    ///   should be set for Java/Scala language type.
    #[prost(string, tag = "10")]
    pub main_class: ::prost::alloc::string::String,
}
/// Describes the format for getting information about a routine.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetRoutineRequest {
    /// Required. Project ID of the requested routine
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the requested routine
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. Routine ID of the requested routine
    #[prost(string, tag = "3")]
    pub routine_id: ::prost::alloc::string::String,
}
/// Describes the format for inserting a routine.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct InsertRoutineRequest {
    /// Required. Project ID of the new routine
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the new routine
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. A routine resource to insert
    #[prost(message, optional, tag = "3")]
    pub routine: ::core::option::Option<Routine>,
}
/// Describes the format for updating a routine.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateRoutineRequest {
    /// Required. Project ID of the routine to update
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the routine to update
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. Routine ID of the routine to update
    #[prost(string, tag = "3")]
    pub routine_id: ::prost::alloc::string::String,
    /// Required. A routine resource which will replace the specified routine
    #[prost(message, optional, tag = "4")]
    pub routine: ::core::option::Option<Routine>,
}
/// Describes the format for the partial update (patch) of a routine.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PatchRoutineRequest {
    /// Required. Project ID of the routine to update
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the routine to update
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. Routine ID of the routine to update
    #[prost(string, tag = "3")]
    pub routine_id: ::prost::alloc::string::String,
    /// Required. A routine resource which will be used to partially
    /// update the specified routine
    #[prost(message, optional, tag = "4")]
    pub routine: ::core::option::Option<Routine>,
    /// Only the Routine fields in the field mask are updated
    /// by the given routine. Repeated routine fields will be fully replaced
    /// if contained in the field mask.
    #[prost(message, optional, tag = "5")]
    pub field_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Describes the format for deleting a routine.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteRoutineRequest {
    /// Required. Project ID of the routine to delete
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the routine to delete
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. Routine ID of the routine to delete
    #[prost(string, tag = "3")]
    pub routine_id: ::prost::alloc::string::String,
}
/// Describes the format for listing routines.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListRoutinesRequest {
    /// Required. Project ID of the routines to list
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the routines to list
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// The maximum number of results to return in a single response page.
    /// Leverage the page tokens to iterate through the entire collection.
    #[prost(message, optional, tag = "3")]
    pub max_results: ::core::option::Option<u32>,
    /// Page token, returned by a previous call, to request the next page of
    /// results
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// If set, then only the Routines matching this filter are returned.
    /// The supported format is `routineType:{RoutineType}`, where `{RoutineType}`
    /// is a RoutineType enum. For example: `routineType:SCALAR_FUNCTION`.
    #[prost(string, tag = "6")]
    pub filter: ::prost::alloc::string::String,
}
/// Describes the format of a single result page when listing routines.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListRoutinesResponse {
    /// Routines in the requested dataset. Unless read_mask is set in the request,
    /// only the following fields are populated:
    /// etag, project_id, dataset_id, routine_id, routine_type, creation_time,
    /// last_modified_time, language, and remote_function_options.
    #[prost(message, repeated, tag = "1")]
    pub routines: ::prost::alloc::vec::Vec<Routine>,
    /// A token to request the next page of results.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod routine_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// RoutineService provides management access to BigQuery routines.
    #[derive(Debug, Clone)]
    pub struct RoutineServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl RoutineServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> RoutineServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> RoutineServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            RoutineServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Gets the specified routine resource by routine ID.
        pub async fn get_routine(
            &mut self,
            request: impl tonic::IntoRequest<super::GetRoutineRequest>,
        ) -> std::result::Result<tonic::Response<super::Routine>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.RoutineService/GetRoutine",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.RoutineService",
                        "GetRoutine",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a new routine in the dataset.
        pub async fn insert_routine(
            &mut self,
            request: impl tonic::IntoRequest<super::InsertRoutineRequest>,
        ) -> std::result::Result<tonic::Response<super::Routine>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.RoutineService/InsertRoutine",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.RoutineService",
                        "InsertRoutine",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates information in an existing routine. The update method replaces the
        /// entire Routine resource.
        pub async fn update_routine(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateRoutineRequest>,
        ) -> std::result::Result<tonic::Response<super::Routine>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.RoutineService/UpdateRoutine",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.RoutineService",
                        "UpdateRoutine",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Patches information in an existing routine. The patch method does a partial
        /// update to an existing Routine resource.
        pub async fn patch_routine(
            &mut self,
            request: impl tonic::IntoRequest<super::PatchRoutineRequest>,
        ) -> std::result::Result<tonic::Response<super::Routine>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.RoutineService/PatchRoutine",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.RoutineService",
                        "PatchRoutine",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes the routine specified by routineId from the dataset.
        pub async fn delete_routine(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteRoutineRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.RoutineService/DeleteRoutine",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.RoutineService",
                        "DeleteRoutine",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists all routines in the specified dataset. Requires the READER dataset
        /// role.
        pub async fn list_routines(
            &mut self,
            request: impl tonic::IntoRequest<super::ListRoutinesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListRoutinesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.RoutineService/ListRoutines",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.RoutineService",
                        "ListRoutines",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Request message for the ListRowAccessPolicies method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListRowAccessPoliciesRequest {
    /// Required. Project ID of the row access policies to list.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of row access policies to list.
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. Table ID of the table to list row access policies.
    #[prost(string, tag = "3")]
    pub table_id: ::prost::alloc::string::String,
    /// Page token, returned by a previous call, to request the next page of
    /// results.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// The maximum number of results to return in a single response page. Leverage
    /// the page tokens to iterate through the entire collection.
    #[prost(int32, tag = "5")]
    pub page_size: i32,
}
/// Response message for the ListRowAccessPolicies method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListRowAccessPoliciesResponse {
    /// Row access policies on the requested table.
    #[prost(message, repeated, tag = "1")]
    pub row_access_policies: ::prost::alloc::vec::Vec<RowAccessPolicy>,
    /// A token to request the next page of results.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Represents access on a subset of rows on the specified table, defined by its
/// filter predicate. Access to the subset of rows is controlled by its IAM
/// policy.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RowAccessPolicy {
    /// Output only. A hash of this resource.
    #[prost(string, tag = "1")]
    pub etag: ::prost::alloc::string::String,
    /// Required. Reference describing the ID of this row access policy.
    #[prost(message, optional, tag = "2")]
    pub row_access_policy_reference: ::core::option::Option<RowAccessPolicyReference>,
    /// Required. A SQL boolean expression that represents the rows defined by this
    /// row access policy, similar to the boolean expression in a WHERE clause of a
    /// SELECT query on a table.
    /// References to other tables, routines, and temporary functions are not
    /// supported.
    ///
    /// Examples: region="EU"
    ///            date_field = CAST('2019-9-27' as DATE)
    ///            nullable_field is not NULL
    ///            numeric_field BETWEEN 1.0 AND 5.0
    #[prost(string, tag = "3")]
    pub filter_predicate: ::prost::alloc::string::String,
    /// Output only. The time when this row access policy was created, in
    /// milliseconds since the epoch.
    #[prost(message, optional, tag = "4")]
    pub creation_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The time when this row access policy was last modified, in
    /// milliseconds since the epoch.
    #[prost(message, optional, tag = "5")]
    pub last_modified_time: ::core::option::Option<::prost_types::Timestamp>,
}
/// Generated client implementations.
pub mod row_access_policy_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// Service for interacting with row access policies.
    #[derive(Debug, Clone)]
    pub struct RowAccessPolicyServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl RowAccessPolicyServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> RowAccessPolicyServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> RowAccessPolicyServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            RowAccessPolicyServiceClient::new(
                InterceptedService::new(inner, interceptor),
            )
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Lists all row access policies on the specified table.
        pub async fn list_row_access_policies(
            &mut self,
            request: impl tonic::IntoRequest<super::ListRowAccessPoliciesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListRowAccessPoliciesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.RowAccessPolicyService/ListRowAccessPolicies",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.RowAccessPolicyService",
                        "ListRowAccessPolicies",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Represents the primary key constraint on a table's columns.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PrimaryKey {
    /// Required. The columns that are composed of the primary key constraint.
    #[prost(string, repeated, tag = "1")]
    pub columns: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// The pair of the foreign key column and primary key column.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ColumnReference {
    /// Required. The column that composes the foreign key.
    #[prost(string, tag = "1")]
    pub referencing_column: ::prost::alloc::string::String,
    /// Required. The column in the primary key that are referenced by the
    /// referencing_column.
    #[prost(string, tag = "2")]
    pub referenced_column: ::prost::alloc::string::String,
}
/// Represents a foreign key constraint on a table's columns.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ForeignKey {
    /// Optional. Set only if the foreign key constraint is named.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The table that holds the primary key and is referenced by this
    /// foreign key.
    #[prost(message, optional, tag = "2")]
    pub referenced_table: ::core::option::Option<TableReference>,
    /// Required. The columns that compose the foreign key.
    #[prost(message, repeated, tag = "3")]
    pub column_references: ::prost::alloc::vec::Vec<ColumnReference>,
}
/// The TableConstraints defines the primary key and foreign key.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TableConstraints {
    /// Optional. Represents a primary key constraint on a table's columns.
    /// Present only if the table has a primary key.
    /// The primary key is not enforced.
    #[prost(message, optional, tag = "1")]
    pub primary_key: ::core::option::Option<PrimaryKey>,
    /// Optional. Present only if the table has a foreign key.
    /// The foreign key is not enforced.
    #[prost(message, repeated, tag = "2")]
    pub foreign_keys: ::prost::alloc::vec::Vec<ForeignKey>,
}
/// Replication info of a table created using `AS REPLICA` DDL like:
/// `CREATE MATERIALIZED VIEW mv1 AS REPLICA OF src_mv`
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TableReplicationInfo {
    /// Required. Source table reference that is replicated.
    #[prost(message, optional, tag = "1")]
    pub source_table: ::core::option::Option<TableReference>,
    /// Optional. Specifies the interval at which the source table is polled for
    /// updates.
    /// It's Optional. If not specified, default replication interval would be
    /// applied.
    #[prost(int64, tag = "2")]
    pub replication_interval_ms: i64,
    /// Optional. Output only. If source is a materialized view, this field
    /// signifies the last refresh time of the source.
    #[prost(int64, tag = "3")]
    pub replicated_source_last_refresh_time: i64,
    /// Optional. Output only. Replication status of configured replication.
    #[prost(enumeration = "table_replication_info::ReplicationStatus", tag = "4")]
    pub replication_status: i32,
    /// Optional. Output only. Replication error that will permanently stopped
    /// table replication.
    #[prost(message, optional, tag = "5")]
    pub replication_error: ::core::option::Option<ErrorProto>,
}
/// Nested message and enum types in `TableReplicationInfo`.
pub mod table_replication_info {
    /// Replication status of the table created using `AS REPLICA` like:
    /// `CREATE MATERIALIZED VIEW mv1 AS REPLICA OF src_mv`
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum ReplicationStatus {
        /// Default value.
        Unspecified = 0,
        /// Replication is Active with no errors.
        Active = 1,
        /// Source object is deleted.
        SourceDeleted = 2,
        /// Source revoked replication permissions.
        PermissionDenied = 3,
        /// Source configuration doesnt allow replication.
        UnsupportedConfiguration = 4,
    }
    impl ReplicationStatus {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "REPLICATION_STATUS_UNSPECIFIED",
                Self::Active => "ACTIVE",
                Self::SourceDeleted => "SOURCE_DELETED",
                Self::PermissionDenied => "PERMISSION_DENIED",
                Self::UnsupportedConfiguration => "UNSUPPORTED_CONFIGURATION",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "REPLICATION_STATUS_UNSPECIFIED" => Some(Self::Unspecified),
                "ACTIVE" => Some(Self::Active),
                "SOURCE_DELETED" => Some(Self::SourceDeleted),
                "PERMISSION_DENIED" => Some(Self::PermissionDenied),
                "UNSUPPORTED_CONFIGURATION" => Some(Self::UnsupportedConfiguration),
                _ => None,
            }
        }
    }
}
/// Describes the definition of a logical view.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ViewDefinition {
    /// Required. A query that BigQuery executes when the view is referenced.
    #[prost(string, tag = "1")]
    pub query: ::prost::alloc::string::String,
    /// Describes user-defined function resources used in the query.
    #[prost(message, repeated, tag = "2")]
    pub user_defined_function_resources: ::prost::alloc::vec::Vec<
        UserDefinedFunctionResource,
    >,
    /// Specifies whether to use BigQuery's legacy SQL for this view.
    /// The default value is true. If set to false, the view will use
    /// BigQuery's GoogleSQL:
    /// <https://cloud.google.com/bigquery/sql-reference/>
    ///
    /// Queries and views that reference this view must use the same flag value.
    /// A wrapper is used here because the default value is True.
    #[prost(message, optional, tag = "3")]
    pub use_legacy_sql: ::core::option::Option<bool>,
    /// True if the column names are explicitly specified. For example by using the
    /// 'CREATE VIEW v(c1, c2) AS ...' syntax.
    /// Can only be set for GoogleSQL views.
    #[prost(bool, tag = "4")]
    pub use_explicit_column_names: bool,
    /// Optional. Specifices the privacy policy for the view.
    #[prost(message, optional, tag = "5")]
    pub privacy_policy: ::core::option::Option<PrivacyPolicy>,
    /// Optional. Foreign view representations.
    #[prost(message, repeated, tag = "6")]
    pub foreign_definitions: ::prost::alloc::vec::Vec<ForeignViewDefinition>,
}
/// A view can be represented in multiple ways. Each representation has its own
/// dialect. This message stores the metadata required for these representations.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ForeignViewDefinition {
    /// Required. The query that defines the view.
    #[prost(string, tag = "1")]
    pub query: ::prost::alloc::string::String,
    /// Optional. Represents the dialect of the query.
    #[prost(string, tag = "7")]
    pub dialect: ::prost::alloc::string::String,
}
/// Definition and configuration of a materialized view.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MaterializedViewDefinition {
    /// Required. A query whose results are persisted.
    #[prost(string, tag = "1")]
    pub query: ::prost::alloc::string::String,
    /// Output only. The time when this materialized view was last refreshed, in
    /// milliseconds since the epoch.
    #[prost(int64, tag = "2")]
    pub last_refresh_time: i64,
    /// Optional. Enable automatic refresh of the materialized view when the base
    /// table is updated. The default value is "true".
    #[prost(message, optional, tag = "3")]
    pub enable_refresh: ::core::option::Option<bool>,
    /// Optional. The maximum frequency at which this materialized view will be
    /// refreshed. The default value is "1800000" (30 minutes).
    #[prost(message, optional, tag = "4")]
    pub refresh_interval_ms: ::core::option::Option<u64>,
    /// Optional. This option declares the intention to construct a materialized
    /// view that isn't refreshed incrementally.
    #[prost(message, optional, tag = "6")]
    pub allow_non_incremental_definition: ::core::option::Option<bool>,
}
/// Status of a materialized view.
/// The last refresh timestamp status is omitted here, but is present in the
/// MaterializedViewDefinition message.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MaterializedViewStatus {
    /// Output only. Refresh watermark of materialized view. The base tables' data
    /// were collected into the materialized view cache until this time.
    #[prost(message, optional, tag = "1")]
    pub refresh_watermark: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Error result of the last automatic refresh. If present,
    /// indicates that the last automatic refresh was unsuccessful.
    #[prost(message, optional, tag = "2")]
    pub last_refresh_status: ::core::option::Option<ErrorProto>,
}
/// Information about base table and snapshot time of the snapshot.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SnapshotDefinition {
    /// Required. Reference describing the ID of the table that was snapshot.
    #[prost(message, optional, tag = "1")]
    pub base_table_reference: ::core::option::Option<TableReference>,
    /// Required. The time at which the base table was snapshot. This value is
    /// reported in the JSON response using RFC3339 format.
    #[prost(message, optional, tag = "2")]
    pub snapshot_time: ::core::option::Option<::prost_types::Timestamp>,
}
/// Information about base table and clone time of a table clone.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CloneDefinition {
    /// Required. Reference describing the ID of the table that was cloned.
    #[prost(message, optional, tag = "1")]
    pub base_table_reference: ::core::option::Option<TableReference>,
    /// Required. The time at which the base table was cloned. This value is
    /// reported in the JSON response using RFC3339 format.
    #[prost(message, optional, tag = "2")]
    pub clone_time: ::core::option::Option<::prost_types::Timestamp>,
}
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct Streamingbuffer {
    /// Output only. A lower-bound estimate of the number of bytes currently in
    /// the streaming buffer.
    #[prost(uint64, tag = "1")]
    pub estimated_bytes: u64,
    /// Output only. A lower-bound estimate of the number of rows currently in the
    /// streaming buffer.
    #[prost(uint64, tag = "2")]
    pub estimated_rows: u64,
    /// Output only. Contains the timestamp of the oldest entry in the streaming
    /// buffer, in milliseconds since the epoch, if the streaming buffer is
    /// available.
    #[prost(fixed64, tag = "3")]
    pub oldest_entry_time: u64,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Table {
    /// The type of resource ID.
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// Output only. A hash of this resource.
    #[prost(string, tag = "2")]
    pub etag: ::prost::alloc::string::String,
    /// Output only. An opaque ID uniquely identifying the table.
    #[prost(string, tag = "3")]
    pub id: ::prost::alloc::string::String,
    /// Output only. A URL that can be used to access this resource again.
    #[prost(string, tag = "4")]
    pub self_link: ::prost::alloc::string::String,
    /// Required. Reference describing the ID of this table.
    #[prost(message, optional, tag = "5")]
    pub table_reference: ::core::option::Option<TableReference>,
    /// Optional. A descriptive name for this table.
    #[prost(message, optional, tag = "6")]
    pub friendly_name: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. A user-friendly description of this table.
    #[prost(message, optional, tag = "7")]
    pub description: ::core::option::Option<::prost::alloc::string::String>,
    /// The labels associated with this table. You can use these to organize and
    /// group your tables. Label keys and values can be no longer than 63
    /// characters, can only contain lowercase letters, numeric characters,
    /// underscores and dashes. International characters are allowed. Label values
    /// are optional. Label keys must start with a letter and each label in the
    /// list must have a different key.
    #[prost(map = "string, string", tag = "8")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. Describes the schema of this table.
    #[prost(message, optional, tag = "9")]
    pub schema: ::core::option::Option<TableSchema>,
    /// If specified, configures time-based partitioning for this table.
    #[prost(message, optional, tag = "10")]
    pub time_partitioning: ::core::option::Option<TimePartitioning>,
    /// If specified, configures range partitioning for this table.
    #[prost(message, optional, tag = "27")]
    pub range_partitioning: ::core::option::Option<RangePartitioning>,
    /// Clustering specification for the table. Must be specified with time-based
    /// partitioning, data in the table will be first partitioned and subsequently
    /// clustered.
    #[prost(message, optional, tag = "23")]
    pub clustering: ::core::option::Option<Clustering>,
    /// Optional. If set to true, queries over this table require
    /// a partition filter that can be used for partition elimination to be
    /// specified.
    #[prost(message, optional, tag = "28")]
    pub require_partition_filter: ::core::option::Option<bool>,
    /// Optional. The partition information for all table formats, including
    /// managed partitioned tables, hive partitioned tables, iceberg partitioned,
    /// and metastore partitioned tables. This field is only populated for
    /// metastore partitioned tables. For other table formats, this is an output
    /// only field.
    #[prost(message, optional, tag = "51")]
    pub partition_definition: ::core::option::Option<PartitioningDefinition>,
    /// Output only. The size of this table in logical bytes, excluding any data in
    /// the streaming buffer.
    #[prost(message, optional, tag = "11")]
    pub num_bytes: ::core::option::Option<i64>,
    /// Output only. The physical size of this table in bytes. This includes
    /// storage used for time travel.
    #[prost(message, optional, tag = "26")]
    pub num_physical_bytes: ::core::option::Option<i64>,
    /// Output only. The number of logical bytes in the table that are considered
    /// "long-term storage".
    #[prost(message, optional, tag = "12")]
    pub num_long_term_bytes: ::core::option::Option<i64>,
    /// Output only. The number of rows of data in this table, excluding any data
    /// in the streaming buffer.
    #[prost(message, optional, tag = "13")]
    pub num_rows: ::core::option::Option<u64>,
    /// Output only. The time when this table was created, in milliseconds since
    /// the epoch.
    #[prost(int64, tag = "14")]
    pub creation_time: i64,
    /// Optional. The time when this table expires, in milliseconds since the
    /// epoch. If not present, the table will persist indefinitely. Expired tables
    /// will be deleted and their storage reclaimed.  The defaultTableExpirationMs
    /// property of the encapsulating dataset can be used to set a default
    /// expirationTime on newly created tables.
    #[prost(message, optional, tag = "15")]
    pub expiration_time: ::core::option::Option<i64>,
    /// Output only. The time when this table was last modified, in milliseconds
    /// since the epoch.
    #[prost(fixed64, tag = "16")]
    pub last_modified_time: u64,
    /// Output only. Describes the table type. The following values are supported:
    ///
    /// * `TABLE`: A normal BigQuery table.
    /// * `VIEW`: A virtual table defined by a SQL query.
    /// * `EXTERNAL`: A table that references data stored in an external storage
    ///    system, such as Google Cloud Storage.
    /// * `MATERIALIZED_VIEW`: A precomputed view defined by a SQL query.
    /// * `SNAPSHOT`: An immutable BigQuery table that preserves the contents of a
    ///    base table at a particular time. See additional information on
    ///    [table
    ///    snapshots](<https://cloud.google.com/bigquery/docs/table-snapshots-intro>).
    ///
    /// The default value is `TABLE`.
    #[prost(string, tag = "17")]
    pub r#type: ::prost::alloc::string::String,
    /// Optional. The view definition.
    #[prost(message, optional, tag = "18")]
    pub view: ::core::option::Option<ViewDefinition>,
    /// Optional. The materialized view definition.
    #[prost(message, optional, tag = "25")]
    pub materialized_view: ::core::option::Option<MaterializedViewDefinition>,
    /// Output only. The materialized view status.
    #[prost(message, optional, tag = "42")]
    pub materialized_view_status: ::core::option::Option<MaterializedViewStatus>,
    /// Optional. Describes the data format, location, and other properties of
    /// a table stored outside of BigQuery. By defining these properties, the data
    /// source can then be queried as if it were a standard BigQuery table.
    #[prost(message, optional, tag = "19")]
    pub external_data_configuration: ::core::option::Option<ExternalDataConfiguration>,
    /// Optional. Specifies the configuration of a BigLake managed table.
    #[prost(message, optional, tag = "45")]
    pub biglake_configuration: ::core::option::Option<BigLakeConfiguration>,
    /// Output only. The geographic location where the table resides. This value
    /// is inherited from the dataset.
    #[prost(string, tag = "20")]
    pub location: ::prost::alloc::string::String,
    /// Output only. Contains information regarding this table's streaming buffer,
    /// if one is present. This field will be absent if the table is not being
    /// streamed to or if there is no data in the streaming buffer.
    #[prost(message, optional, tag = "21")]
    pub streaming_buffer: ::core::option::Option<Streamingbuffer>,
    /// Custom encryption configuration (e.g., Cloud KMS keys).
    #[prost(message, optional, tag = "22")]
    pub encryption_configuration: ::core::option::Option<EncryptionConfiguration>,
    /// Output only. Contains information about the snapshot. This value is set via
    /// snapshot creation.
    #[prost(message, optional, tag = "29")]
    pub snapshot_definition: ::core::option::Option<SnapshotDefinition>,
    /// Optional. Defines the default collation specification of new STRING fields
    /// in the table. During table creation or update, if a STRING field is added
    /// to this table without explicit collation specified, then the table inherits
    /// the table default collation. A change to this field affects only fields
    /// added afterwards, and does not alter the existing fields.
    /// The following values are supported:
    ///
    /// * 'und:ci': undetermined locale, case insensitive.
    /// * '': empty string. Default to case-sensitive behavior.
    #[prost(message, optional, tag = "30")]
    pub default_collation: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Defines the default rounding mode specification of new decimal
    /// fields (NUMERIC OR BIGNUMERIC) in the table. During table creation or
    /// update, if a decimal field is added to this table without an explicit
    /// rounding mode specified, then the field inherits the table default
    /// rounding mode. Changing this field doesn't affect existing fields.
    #[prost(enumeration = "table_field_schema::RoundingMode", tag = "44")]
    pub default_rounding_mode: i32,
    /// Output only. Contains information about the clone. This value is set via
    /// the clone operation.
    #[prost(message, optional, tag = "31")]
    pub clone_definition: ::core::option::Option<CloneDefinition>,
    /// Output only. Number of physical bytes used by time travel storage (deleted
    /// or changed data). This data is not kept in real time, and might be delayed
    /// by a few seconds to a few minutes.
    #[prost(message, optional, tag = "33")]
    pub num_time_travel_physical_bytes: ::core::option::Option<i64>,
    /// Output only. Total number of logical bytes in the table or materialized
    /// view.
    #[prost(message, optional, tag = "34")]
    pub num_total_logical_bytes: ::core::option::Option<i64>,
    /// Output only. Number of logical bytes that are less than 90 days old.
    #[prost(message, optional, tag = "35")]
    pub num_active_logical_bytes: ::core::option::Option<i64>,
    /// Output only. Number of logical bytes that are more than 90 days old.
    #[prost(message, optional, tag = "36")]
    pub num_long_term_logical_bytes: ::core::option::Option<i64>,
    /// Output only. Number of physical bytes used by current live data storage.
    /// This data is not kept in real time, and might be delayed by a few seconds
    /// to a few minutes.
    #[prost(message, optional, tag = "53")]
    pub num_current_physical_bytes: ::core::option::Option<i64>,
    /// Output only. The physical size of this table in bytes. This also includes
    /// storage used for time travel. This data is not kept in real time, and might
    /// be delayed by a few seconds to a few minutes.
    #[prost(message, optional, tag = "37")]
    pub num_total_physical_bytes: ::core::option::Option<i64>,
    /// Output only. Number of physical bytes less than 90 days old. This data is
    /// not kept in real time, and might be delayed by a few seconds to a few
    /// minutes.
    #[prost(message, optional, tag = "38")]
    pub num_active_physical_bytes: ::core::option::Option<i64>,
    /// Output only. Number of physical bytes more than 90 days old.
    /// This data is not kept in real time, and might be delayed by a few seconds
    /// to a few minutes.
    #[prost(message, optional, tag = "39")]
    pub num_long_term_physical_bytes: ::core::option::Option<i64>,
    /// Output only. The number of partitions present in the table or materialized
    /// view. This data is not kept in real time, and might be delayed by a few
    /// seconds to a few minutes.
    #[prost(message, optional, tag = "40")]
    pub num_partitions: ::core::option::Option<i64>,
    /// Optional. The maximum staleness of data that could be returned when the
    /// table (or stale MV) is queried. Staleness encoded as a string encoding
    /// of sql IntervalValue type.
    #[prost(string, tag = "41")]
    pub max_staleness: ::prost::alloc::string::String,
    /// Optional. Output only. Restriction config for table. If set, restrict
    /// certain accesses on the table based on the config. See [Data
    /// egress](<https://cloud.google.com/bigquery/docs/analytics-hub-introduction#data_egress>)
    /// for more details.
    #[prost(message, optional, tag = "46")]
    pub restrictions: ::core::option::Option<RestrictionConfig>,
    /// Optional. Tables Primary Key and Foreign Key information
    #[prost(message, optional, tag = "47")]
    pub table_constraints: ::core::option::Option<TableConstraints>,
    /// Optional. The [tags](<https://cloud.google.com/bigquery/docs/tags>) attached
    /// to this table. Tag keys are globally unique. Tag key is expected to be in
    /// the namespaced format, for example "123456789012/environment" where
    /// 123456789012 is the ID of the parent organization or project resource for
    /// this tag key. Tag value is expected to be the short name, for example
    /// "Production". See [Tag
    /// definitions](<https://cloud.google.com/iam/docs/tags-access-control#definitions>)
    /// for more details.
    #[prost(map = "string, string", tag = "48")]
    pub resource_tags: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. Table replication info for table created `AS REPLICA` DDL like:
    /// `CREATE MATERIALIZED VIEW mv1 AS REPLICA OF src_mv`
    #[prost(message, optional, tag = "49")]
    pub table_replication_info: ::core::option::Option<TableReplicationInfo>,
    /// Optional. Output only. Table references of all replicas currently active on
    /// the table.
    #[prost(message, repeated, tag = "50")]
    pub replicas: ::prost::alloc::vec::Vec<TableReference>,
    /// Optional. Options defining open source compatible table.
    #[prost(message, optional, tag = "54")]
    pub external_catalog_table_options: ::core::option::Option<
        ExternalCatalogTableOptions,
    >,
}
/// Request format for getting table metadata.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetTableRequest {
    /// Required. Project ID of the requested table
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the requested table
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. Table ID of the requested table
    #[prost(string, tag = "3")]
    pub table_id: ::prost::alloc::string::String,
    /// List of table schema fields to return (comma-separated).
    /// If unspecified, all fields are returned.
    /// A fieldMask cannot be used here because the fields will automatically be
    /// converted from camelCase to snake_case and the conversion will fail if
    /// there are underscores. Since these are fields in BigQuery table schemas,
    /// underscores are allowed.
    #[prost(string, tag = "4")]
    pub selected_fields: ::prost::alloc::string::String,
    /// Optional. Specifies the view that determines which table information is
    /// returned. By default, basic table information and storage statistics
    /// (STORAGE_STATS) are returned.
    #[prost(enumeration = "get_table_request::TableMetadataView", tag = "5")]
    pub view: i32,
}
/// Nested message and enum types in `GetTableRequest`.
pub mod get_table_request {
    /// TableMetadataView specifies which table information is returned.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum TableMetadataView {
        /// The default value.
        /// Default to the STORAGE_STATS view.
        Unspecified = 0,
        /// Includes basic table information including schema and
        /// partitioning specification. This view does not include storage statistics
        /// such as numRows or numBytes. This view is significantly more efficient
        /// and should be used to support high query rates.
        Basic = 1,
        /// Includes all information in the BASIC view as well as storage statistics
        /// (numBytes, numLongTermBytes, numRows and lastModifiedTime).
        StorageStats = 2,
        /// Includes all table information, including storage statistics.
        /// It returns same information as STORAGE_STATS view, but may contain
        /// additional information in the future.
        Full = 3,
    }
    impl TableMetadataView {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "TABLE_METADATA_VIEW_UNSPECIFIED",
                Self::Basic => "BASIC",
                Self::StorageStats => "STORAGE_STATS",
                Self::Full => "FULL",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "TABLE_METADATA_VIEW_UNSPECIFIED" => Some(Self::Unspecified),
                "BASIC" => Some(Self::Basic),
                "STORAGE_STATS" => Some(Self::StorageStats),
                "FULL" => Some(Self::Full),
                _ => None,
            }
        }
    }
}
/// Request format for inserting table metadata.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct InsertTableRequest {
    /// Required. Project ID of the new table
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the new table
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. A tables resource to insert
    #[prost(message, optional, tag = "4")]
    pub table: ::core::option::Option<Table>,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateOrPatchTableRequest {
    /// Required. Project ID of the table to update
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the table to update
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. Table ID of the table to update
    #[prost(string, tag = "3")]
    pub table_id: ::prost::alloc::string::String,
    /// Required. A tables resource which will replace or patch the specified table
    #[prost(message, optional, tag = "4")]
    pub table: ::core::option::Option<Table>,
    /// Optional. When true will autodetect schema, else will keep original schema.
    #[prost(bool, tag = "5")]
    pub autodetect_schema: bool,
}
/// Request format for deleting a table.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteTableRequest {
    /// Required. Project ID of the table to delete
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the table to delete
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Required. Table ID of the table to delete
    #[prost(string, tag = "3")]
    pub table_id: ::prost::alloc::string::String,
}
/// Request format for enumerating tables.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTablesRequest {
    /// Required. Project ID of the tables to list
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. Dataset ID of the tables to list
    #[prost(string, tag = "2")]
    pub dataset_id: ::prost::alloc::string::String,
    /// The maximum number of results to return in a single response page.
    /// Leverage the page tokens to iterate through the entire collection.
    #[prost(message, optional, tag = "3")]
    pub max_results: ::core::option::Option<u32>,
    /// Page token, returned by a previous call, to request the next page of
    /// results
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
}
/// Information about a logical view.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFormatView {
    /// True if view is defined in legacy SQL dialect,
    /// false if in GoogleSQL.
    #[prost(message, optional, tag = "1")]
    pub use_legacy_sql: ::core::option::Option<bool>,
    /// Specifices the privacy policy for the view.
    #[prost(message, optional, tag = "2")]
    pub privacy_policy: ::core::option::Option<PrivacyPolicy>,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFormatTable {
    /// The resource type.
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// An opaque ID of the table.
    #[prost(string, tag = "2")]
    pub id: ::prost::alloc::string::String,
    /// A reference uniquely identifying table.
    #[prost(message, optional, tag = "3")]
    pub table_reference: ::core::option::Option<TableReference>,
    /// The user-friendly name for this table.
    #[prost(message, optional, tag = "4")]
    pub friendly_name: ::core::option::Option<::prost::alloc::string::String>,
    /// The type of table.
    #[prost(string, tag = "5")]
    pub r#type: ::prost::alloc::string::String,
    /// The time-based partitioning for this table.
    #[prost(message, optional, tag = "6")]
    pub time_partitioning: ::core::option::Option<TimePartitioning>,
    /// The range partitioning for this table.
    #[prost(message, optional, tag = "12")]
    pub range_partitioning: ::core::option::Option<RangePartitioning>,
    /// Clustering specification for this table, if configured.
    #[prost(message, optional, tag = "11")]
    pub clustering: ::core::option::Option<Clustering>,
    /// The labels associated with this table. You can use these to organize
    /// and group your tables.
    #[prost(map = "string, string", tag = "7")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Additional details for a view.
    #[prost(message, optional, tag = "8")]
    pub view: ::core::option::Option<ListFormatView>,
    /// Output only. The time when this table was created, in milliseconds since
    /// the epoch.
    #[prost(int64, tag = "9")]
    pub creation_time: i64,
    /// The time when this table expires, in milliseconds since the
    /// epoch. If not present, the table will persist indefinitely. Expired tables
    /// will be deleted and their storage reclaimed.
    #[prost(int64, tag = "10")]
    pub expiration_time: i64,
    /// Optional. If set to true, queries including this table must specify a
    /// partition filter. This filter is used for partition elimination.
    #[prost(message, optional, tag = "14")]
    pub require_partition_filter: ::core::option::Option<bool>,
}
/// Partial projection of the metadata for a given table in a list response.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TableList {
    /// The type of list.
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// A hash of this page of results.
    #[prost(string, tag = "2")]
    pub etag: ::prost::alloc::string::String,
    /// A token to request the next page of results.
    #[prost(string, tag = "3")]
    pub next_page_token: ::prost::alloc::string::String,
    /// Tables in the requested dataset.
    #[prost(message, repeated, tag = "4")]
    pub tables: ::prost::alloc::vec::Vec<ListFormatTable>,
    /// The total number of tables in the dataset.
    #[prost(message, optional, tag = "5")]
    pub total_items: ::core::option::Option<i32>,
}
/// Generated client implementations.
pub mod table_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// TableService provides methods for managing BigQuery tables and table-like
    /// entities such as views and snapshots.
    #[derive(Debug, Clone)]
    pub struct TableServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl TableServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> TableServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> TableServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            TableServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Gets the specified table resource by table ID.
        /// This method does not return the data in the table, it only returns the
        /// table resource, which describes the structure of this table.
        pub async fn get_table(
            &mut self,
            request: impl tonic::IntoRequest<super::GetTableRequest>,
        ) -> std::result::Result<tonic::Response<super::Table>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.TableService/GetTable",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("google.cloud.bigquery.v2.TableService", "GetTable"),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a new, empty table in the dataset.
        pub async fn insert_table(
            &mut self,
            request: impl tonic::IntoRequest<super::InsertTableRequest>,
        ) -> std::result::Result<tonic::Response<super::Table>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.TableService/InsertTable",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.TableService",
                        "InsertTable",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates information in an existing table. The update method replaces the
        /// entire table resource, whereas the patch method only replaces fields that
        /// are provided in the submitted table resource.
        /// This method supports RFC5789 patch semantics.
        pub async fn patch_table(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateOrPatchTableRequest>,
        ) -> std::result::Result<tonic::Response<super::Table>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.TableService/PatchTable",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.TableService",
                        "PatchTable",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates information in an existing table. The update method replaces the
        /// entire Table resource, whereas the patch method only replaces fields that
        /// are provided in the submitted Table resource.
        pub async fn update_table(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateOrPatchTableRequest>,
        ) -> std::result::Result<tonic::Response<super::Table>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.TableService/UpdateTable",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.TableService",
                        "UpdateTable",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes the table specified by tableId from the dataset.
        /// If the table contains data, all the data will be deleted.
        pub async fn delete_table(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteTableRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.TableService/DeleteTable",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.TableService",
                        "DeleteTable",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists all tables in the specified dataset. Requires the READER dataset
        /// role.
        pub async fn list_tables(
            &mut self,
            request: impl tonic::IntoRequest<super::ListTablesRequest>,
        ) -> std::result::Result<tonic::Response<super::TableList>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.bigquery.v2.TableService/ListTables",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.bigquery.v2.TableService",
                        "ListTables",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
