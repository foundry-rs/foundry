// This file is @generated by prost-build.
/// Represents a hardware accelerator type.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum AcceleratorType {
    /// Unspecified accelerator type, which means no accelerator.
    Unspecified = 0,
    /// Deprecated: Nvidia Tesla K80 GPU has reached end of support,
    /// see <https://cloud.google.com/compute/docs/eol/k80-eol.>
    NvidiaTeslaK80 = 1,
    /// Nvidia Tesla P100 GPU.
    NvidiaTeslaP100 = 2,
    /// Nvidia Tesla V100 GPU.
    NvidiaTeslaV100 = 3,
    /// Nvidia Tesla P4 GPU.
    NvidiaTeslaP4 = 4,
    /// Nvidia Tesla T4 GPU.
    NvidiaTeslaT4 = 5,
    /// Nvidia Tesla A100 GPU.
    NvidiaTeslaA100 = 8,
    /// Nvidia A100 80GB GPU.
    NvidiaA10080gb = 9,
    /// Nvidia L4 GPU.
    NvidiaL4 = 11,
    /// Nvidia H100 80Gb GPU.
    NvidiaH10080gb = 13,
    /// Nvidia H100 Mega 80Gb GPU.
    NvidiaH100Mega80gb = 14,
    /// TPU v2.
    TpuV2 = 6,
    /// TPU v3.
    TpuV3 = 7,
    /// TPU v4.
    TpuV4Pod = 10,
    /// TPU v5.
    TpuV5Litepod = 12,
}
impl AcceleratorType {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "ACCELERATOR_TYPE_UNSPECIFIED",
            Self::NvidiaTeslaK80 => "NVIDIA_TESLA_K80",
            Self::NvidiaTeslaP100 => "NVIDIA_TESLA_P100",
            Self::NvidiaTeslaV100 => "NVIDIA_TESLA_V100",
            Self::NvidiaTeslaP4 => "NVIDIA_TESLA_P4",
            Self::NvidiaTeslaT4 => "NVIDIA_TESLA_T4",
            Self::NvidiaTeslaA100 => "NVIDIA_TESLA_A100",
            Self::NvidiaA10080gb => "NVIDIA_A100_80GB",
            Self::NvidiaL4 => "NVIDIA_L4",
            Self::NvidiaH10080gb => "NVIDIA_H100_80GB",
            Self::NvidiaH100Mega80gb => "NVIDIA_H100_MEGA_80GB",
            Self::TpuV2 => "TPU_V2",
            Self::TpuV3 => "TPU_V3",
            Self::TpuV4Pod => "TPU_V4_POD",
            Self::TpuV5Litepod => "TPU_V5_LITEPOD",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "ACCELERATOR_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
            "NVIDIA_TESLA_K80" => Some(Self::NvidiaTeslaK80),
            "NVIDIA_TESLA_P100" => Some(Self::NvidiaTeslaP100),
            "NVIDIA_TESLA_V100" => Some(Self::NvidiaTeslaV100),
            "NVIDIA_TESLA_P4" => Some(Self::NvidiaTeslaP4),
            "NVIDIA_TESLA_T4" => Some(Self::NvidiaTeslaT4),
            "NVIDIA_TESLA_A100" => Some(Self::NvidiaTeslaA100),
            "NVIDIA_A100_80GB" => Some(Self::NvidiaA10080gb),
            "NVIDIA_L4" => Some(Self::NvidiaL4),
            "NVIDIA_H100_80GB" => Some(Self::NvidiaH10080gb),
            "NVIDIA_H100_MEGA_80GB" => Some(Self::NvidiaH100Mega80gb),
            "TPU_V2" => Some(Self::TpuV2),
            "TPU_V3" => Some(Self::TpuV3),
            "TPU_V4_POD" => Some(Self::TpuV4Pod),
            "TPU_V5_LITEPOD" => Some(Self::TpuV5Litepod),
            _ => None,
        }
    }
}
/// References an API call. It contains more information about long running
/// operation and Jobs that are triggered by the API call.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UserActionReference {
    /// The method name of the API RPC call. For example,
    /// "/google.cloud.aiplatform.{apiVersion}.DatasetService.CreateDataset"
    #[prost(string, tag = "3")]
    pub method: ::prost::alloc::string::String,
    #[prost(oneof = "user_action_reference::Reference", tags = "1, 2")]
    pub reference: ::core::option::Option<user_action_reference::Reference>,
}
/// Nested message and enum types in `UserActionReference`.
pub mod user_action_reference {
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Reference {
        /// For API calls that return a long running operation.
        /// Resource name of the long running operation.
        /// Format:
        /// `projects/{project}/locations/{location}/operations/{operation}`
        #[prost(string, tag = "1")]
        Operation(::prost::alloc::string::String),
        /// For API calls that start a LabelingJob.
        /// Resource name of the LabelingJob.
        /// Format:
        /// `projects/{project}/locations/{location}/dataLabelingJobs/{data_labeling_job}`
        #[prost(string, tag = "2")]
        DataLabelingJob(::prost::alloc::string::String),
    }
}
/// Used to assign specific AnnotationSpec to a particular area of a DataItem or
/// the whole part of the DataItem.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Annotation {
    /// Output only. Resource name of the Annotation.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. Google Cloud Storage URI points to a YAML file describing
    /// [payload][google.cloud.aiplatform.v1.Annotation.payload]. The schema is
    /// defined as an [OpenAPI 3.0.2 Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    /// The schema files that can be used here are found in
    /// gs://google-cloud-aiplatform/schema/dataset/annotation/, note that the
    /// chosen schema must be consistent with the parent Dataset's
    /// [metadata][google.cloud.aiplatform.v1.Dataset.metadata_schema_uri].
    #[prost(string, tag = "2")]
    pub payload_schema_uri: ::prost::alloc::string::String,
    /// Required. The schema of the payload can be found in
    /// [payload_schema][google.cloud.aiplatform.v1.Annotation.payload_schema_uri].
    #[prost(message, optional, tag = "3")]
    pub payload: ::core::option::Option<::prost_types::Value>,
    /// Output only. Timestamp when this Annotation was created.
    #[prost(message, optional, tag = "4")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Annotation was last updated.
    #[prost(message, optional, tag = "7")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. Used to perform consistent read-modify-write updates. If not set,
    /// a blind "overwrite" update happens.
    #[prost(string, tag = "8")]
    pub etag: ::prost::alloc::string::String,
    /// Output only. The source of the Annotation.
    #[prost(message, optional, tag = "5")]
    pub annotation_source: ::core::option::Option<UserActionReference>,
    /// Optional. The labels with user-defined metadata to organize your
    /// Annotations.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// No more than 64 user labels can be associated with one Annotation(System
    /// labels are excluded).
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable. Following system labels exist for each Annotation:
    ///
    /// * "aiplatform.googleapis.com/annotation_set_name":
    ///    optional, name of the UI's annotation set this Annotation belongs to.
    ///    If not set, the Annotation is not visible in the UI.
    ///
    /// * "aiplatform.googleapis.com/payload_schema":
    ///    output only, its value is the
    ///    [payload_schema's][google.cloud.aiplatform.v1.Annotation.payload_schema_uri]
    ///    title.
    #[prost(map = "string, string", tag = "6")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
/// Identifies a concept with which DataItems may be annotated with.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AnnotationSpec {
    /// Output only. Resource name of the AnnotationSpec.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The user-defined name of the AnnotationSpec.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Output only. Timestamp when this AnnotationSpec was created.
    #[prost(message, optional, tag = "3")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when AnnotationSpec was last updated.
    #[prost(message, optional, tag = "4")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. Used to perform consistent read-modify-write updates. If not set,
    /// a blind "overwrite" update happens.
    #[prost(string, tag = "5")]
    pub etag: ::prost::alloc::string::String,
}
/// The generic reusable api auth config.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ApiAuth {
    /// The auth config.
    #[prost(oneof = "api_auth::AuthConfig", tags = "1")]
    pub auth_config: ::core::option::Option<api_auth::AuthConfig>,
}
/// Nested message and enum types in `ApiAuth`.
pub mod api_auth {
    /// The API secret.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ApiKeyConfig {
        /// Required. The SecretManager secret version resource name storing API key.
        /// e.g. projects/{project}/secrets/{secret}/versions/{version}
        #[prost(string, tag = "1")]
        pub api_key_secret_version: ::prost::alloc::string::String,
    }
    /// The auth config.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum AuthConfig {
        /// The API secret.
        #[prost(message, tag = "1")]
        ApiKeyConfig(ApiKeyConfig),
    }
}
/// Instance of a general artifact.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Artifact {
    /// Output only. The resource name of the Artifact.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// User provided display name of the Artifact.
    /// May be up to 128 Unicode characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// The uniform resource identifier of the artifact file.
    /// May be empty if there is no actual artifact file.
    #[prost(string, tag = "6")]
    pub uri: ::prost::alloc::string::String,
    /// An eTag used to perform consistent read-modify-write updates. If not set, a
    /// blind "overwrite" update happens.
    #[prost(string, tag = "9")]
    pub etag: ::prost::alloc::string::String,
    /// The labels with user-defined metadata to organize your Artifacts.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// No more than 64 user labels can be associated with one Artifact (System
    /// labels are excluded).
    #[prost(map = "string, string", tag = "10")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Output only. Timestamp when this Artifact was created.
    #[prost(message, optional, tag = "11")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Artifact was last updated.
    #[prost(message, optional, tag = "12")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// The state of this Artifact. This is a property of the Artifact, and does
    /// not imply or capture any ongoing process. This property is managed by
    /// clients (such as Vertex AI Pipelines), and the system does not prescribe
    /// or check the validity of state transitions.
    #[prost(enumeration = "artifact::State", tag = "13")]
    pub state: i32,
    /// The title of the schema describing the metadata.
    ///
    /// Schema title and version is expected to be registered in earlier Create
    /// Schema calls. And both are used together as unique identifiers to identify
    /// schemas within the local metadata store.
    #[prost(string, tag = "14")]
    pub schema_title: ::prost::alloc::string::String,
    /// The version of the schema in schema_name to use.
    ///
    /// Schema title and version is expected to be registered in earlier Create
    /// Schema calls. And both are used together as unique identifiers to identify
    /// schemas within the local metadata store.
    #[prost(string, tag = "15")]
    pub schema_version: ::prost::alloc::string::String,
    /// Properties of the Artifact.
    /// Top level metadata keys' heading and trailing spaces will be trimmed.
    /// The size of this field should not exceed 200KB.
    #[prost(message, optional, tag = "16")]
    pub metadata: ::core::option::Option<::prost_types::Struct>,
    /// Description of the Artifact
    #[prost(string, tag = "17")]
    pub description: ::prost::alloc::string::String,
}
/// Nested message and enum types in `Artifact`.
pub mod artifact {
    /// Describes the state of the Artifact.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// Unspecified state for the Artifact.
        Unspecified = 0,
        /// A state used by systems like Vertex AI Pipelines to indicate that the
        /// underlying data item represented by this Artifact is being created.
        Pending = 1,
        /// A state indicating that the Artifact should exist, unless something
        /// external to the system deletes it.
        Live = 2,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STATE_UNSPECIFIED",
                Self::Pending => "PENDING",
                Self::Live => "LIVE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "PENDING" => Some(Self::Pending),
                "LIVE" => Some(Self::Live),
                _ => None,
            }
        }
    }
}
/// Success and error statistics of processing multiple entities
/// (for example, DataItems or structured data rows) in batch.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct CompletionStats {
    /// Output only. The number of entities that had been processed successfully.
    #[prost(int64, tag = "1")]
    pub successful_count: i64,
    /// Output only. The number of entities for which any error was encountered.
    #[prost(int64, tag = "2")]
    pub failed_count: i64,
    /// Output only. In cases when enough errors are encountered a job, pipeline,
    /// or operation may be failed as a whole. Below is the number of entities for
    /// which the processing had not been finished (either in successful or failed
    /// state). Set to -1 if the number is unknown (for example, the operation
    /// failed before the total entity number could be collected).
    #[prost(int64, tag = "3")]
    pub incomplete_count: i64,
    /// Output only. The number of the successful forecast points that are
    /// generated by the forecasting model. This is ONLY used by the forecasting
    /// batch prediction.
    #[prost(int64, tag = "5")]
    pub successful_forecast_point_count: i64,
}
/// Represents a customer-managed encryption key spec that can be applied to
/// a top-level resource.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EncryptionSpec {
    /// Required. The Cloud KMS resource identifier of the customer managed
    /// encryption key used to protect a resource. Has the form:
    /// `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`.
    /// The key needs to be in the same region as where the compute resource is
    /// created.
    #[prost(string, tag = "1")]
    pub kms_key_name: ::prost::alloc::string::String,
}
/// Metadata describing the Model's input and output for explanation.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExplanationMetadata {
    /// Required. Map from feature names to feature input metadata. Keys are the
    /// name of the features. Values are the specification of the feature.
    ///
    /// An empty InputMetadata is valid. It describes a text feature which has the
    /// name specified as the key in
    /// [ExplanationMetadata.inputs][google.cloud.aiplatform.v1.ExplanationMetadata.inputs].
    /// The baseline of the empty feature is chosen by Vertex AI.
    ///
    /// For Vertex AI-provided Tensorflow images, the key can be any friendly
    /// name of the feature. Once specified,
    /// [featureAttributions][google.cloud.aiplatform.v1.Attribution.feature_attributions]
    /// are keyed by this key (if not grouped with another feature).
    ///
    /// For custom images, the key must match with the key in
    /// [instance][google.cloud.aiplatform.v1.ExplainRequest.instances].
    #[prost(map = "string, message", tag = "1")]
    pub inputs: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        explanation_metadata::InputMetadata,
    >,
    /// Required. Map from output names to output metadata.
    ///
    /// For Vertex AI-provided Tensorflow images, keys can be any user defined
    /// string that consists of any UTF-8 characters.
    ///
    /// For custom images, keys are the name of the output field in the prediction
    /// to be explained.
    ///
    /// Currently only one key is allowed.
    #[prost(map = "string, message", tag = "2")]
    pub outputs: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        explanation_metadata::OutputMetadata,
    >,
    /// Points to a YAML file stored on Google Cloud Storage describing the format
    /// of the [feature
    /// attributions][google.cloud.aiplatform.v1.Attribution.feature_attributions].
    /// The schema is defined as an OpenAPI 3.0.2 [Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    /// AutoML tabular Models always have this field populated by Vertex AI.
    /// Note: The URI given on output may be different, including the URI scheme,
    /// than the one given on input. The output URI will point to a location where
    /// the user only has a read access.
    #[prost(string, tag = "3")]
    pub feature_attributions_schema_uri: ::prost::alloc::string::String,
    /// Name of the source to generate embeddings for example based explanations.
    #[prost(string, tag = "5")]
    pub latent_space_source: ::prost::alloc::string::String,
}
/// Nested message and enum types in `ExplanationMetadata`.
pub mod explanation_metadata {
    /// Metadata of the input of a feature.
    ///
    /// Fields other than
    /// [InputMetadata.input_baselines][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.input_baselines]
    /// are applicable only for Models that are using Vertex AI-provided images for
    /// Tensorflow.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct InputMetadata {
        /// Baseline inputs for this feature.
        ///
        /// If no baseline is specified, Vertex AI chooses the baseline for this
        /// feature. If multiple baselines are specified, Vertex AI returns the
        /// average attributions across them in
        /// [Attribution.feature_attributions][google.cloud.aiplatform.v1.Attribution.feature_attributions].
        ///
        /// For Vertex AI-provided Tensorflow images (both 1.x and 2.x), the shape
        /// of each baseline must match the shape of the input tensor. If a scalar is
        /// provided, we broadcast to the same shape as the input tensor.
        ///
        /// For custom images, the element of the baselines must be in the same
        /// format as the feature's input in the
        /// [instance][google.cloud.aiplatform.v1.ExplainRequest.instances][]. The
        /// schema of any single instance may be specified via Endpoint's
        /// DeployedModels' [Model's][google.cloud.aiplatform.v1.DeployedModel.model]
        /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
        /// [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri].
        #[prost(message, repeated, tag = "1")]
        pub input_baselines: ::prost::alloc::vec::Vec<::prost_types::Value>,
        /// Name of the input tensor for this feature. Required and is only
        /// applicable to Vertex AI-provided images for Tensorflow.
        #[prost(string, tag = "2")]
        pub input_tensor_name: ::prost::alloc::string::String,
        /// Defines how the feature is encoded into the input tensor. Defaults to
        /// IDENTITY.
        #[prost(enumeration = "input_metadata::Encoding", tag = "3")]
        pub encoding: i32,
        /// Modality of the feature. Valid values are: numeric, image. Defaults to
        /// numeric.
        #[prost(string, tag = "4")]
        pub modality: ::prost::alloc::string::String,
        /// The domain details of the input feature value. Like min/max, original
        /// mean or standard deviation if normalized.
        #[prost(message, optional, tag = "5")]
        pub feature_value_domain: ::core::option::Option<
            input_metadata::FeatureValueDomain,
        >,
        /// Specifies the index of the values of the input tensor.
        /// Required when the input tensor is a sparse representation. Refer to
        /// Tensorflow documentation for more details:
        /// <https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.>
        #[prost(string, tag = "6")]
        pub indices_tensor_name: ::prost::alloc::string::String,
        /// Specifies the shape of the values of the input if the input is a sparse
        /// representation. Refer to Tensorflow documentation for more details:
        /// <https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.>
        #[prost(string, tag = "7")]
        pub dense_shape_tensor_name: ::prost::alloc::string::String,
        /// A list of feature names for each index in the input tensor.
        /// Required when the input
        /// [InputMetadata.encoding][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.encoding]
        /// is BAG_OF_FEATURES, BAG_OF_FEATURES_SPARSE, INDICATOR.
        #[prost(string, repeated, tag = "8")]
        pub index_feature_mapping: ::prost::alloc::vec::Vec<
            ::prost::alloc::string::String,
        >,
        /// Encoded tensor is a transformation of the input tensor. Must be provided
        /// if choosing
        /// [Integrated Gradients
        /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution]
        /// or [XRAI
        /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.xrai_attribution]
        /// and the input tensor is not differentiable.
        ///
        /// An encoded tensor is generated if the input tensor is encoded by a lookup
        /// table.
        #[prost(string, tag = "9")]
        pub encoded_tensor_name: ::prost::alloc::string::String,
        /// A list of baselines for the encoded tensor.
        ///
        /// The shape of each baseline should match the shape of the encoded tensor.
        /// If a scalar is provided, Vertex AI broadcasts to the same shape as the
        /// encoded tensor.
        #[prost(message, repeated, tag = "10")]
        pub encoded_baselines: ::prost::alloc::vec::Vec<::prost_types::Value>,
        /// Visualization configurations for image explanation.
        #[prost(message, optional, tag = "11")]
        pub visualization: ::core::option::Option<input_metadata::Visualization>,
        /// Name of the group that the input belongs to. Features with the same group
        /// name will be treated as one feature when computing attributions. Features
        /// grouped together can have different shapes in value. If provided, there
        /// will be one single attribution generated in
        /// [Attribution.feature_attributions][google.cloud.aiplatform.v1.Attribution.feature_attributions],
        /// keyed by the group name.
        #[prost(string, tag = "12")]
        pub group_name: ::prost::alloc::string::String,
    }
    /// Nested message and enum types in `InputMetadata`.
    pub mod input_metadata {
        /// Domain details of the input feature value. Provides numeric information
        /// about the feature, such as its range (min, max). If the feature has been
        /// pre-processed, for example with z-scoring, then it provides information
        /// about how to recover the original feature. For example, if the input
        /// feature is an image and it has been pre-processed to obtain 0-mean and
        /// stddev = 1 values, then original_mean, and original_stddev refer to the
        /// mean and stddev of the original feature (e.g. image tensor) from which
        /// input feature (with mean = 0 and stddev = 1) was obtained.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct FeatureValueDomain {
            /// The minimum permissible value for this feature.
            #[prost(float, tag = "1")]
            pub min_value: f32,
            /// The maximum permissible value for this feature.
            #[prost(float, tag = "2")]
            pub max_value: f32,
            /// If this input feature has been normalized to a mean value of 0,
            /// the original_mean specifies the mean value of the domain prior to
            /// normalization.
            #[prost(float, tag = "3")]
            pub original_mean: f32,
            /// If this input feature has been normalized to a standard deviation of
            /// 1.0, the original_stddev specifies the standard deviation of the domain
            /// prior to normalization.
            #[prost(float, tag = "4")]
            pub original_stddev: f32,
        }
        /// Visualization configurations for image explanation.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct Visualization {
            /// Type of the image visualization. Only applicable to
            /// [Integrated Gradients
            /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution].
            /// OUTLINES shows regions of attribution, while PIXELS shows per-pixel
            /// attribution. Defaults to OUTLINES.
            #[prost(enumeration = "visualization::Type", tag = "1")]
            pub r#type: i32,
            /// Whether to only highlight pixels with positive contributions, negative
            /// or both. Defaults to POSITIVE.
            #[prost(enumeration = "visualization::Polarity", tag = "2")]
            pub polarity: i32,
            /// The color scheme used for the highlighted areas.
            ///
            /// Defaults to PINK_GREEN for
            /// [Integrated Gradients
            /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution],
            /// which shows positive attributions in green and negative in pink.
            ///
            /// Defaults to VIRIDIS for
            /// [XRAI
            /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.xrai_attribution],
            /// which highlights the most influential regions in yellow and the least
            /// influential in blue.
            #[prost(enumeration = "visualization::ColorMap", tag = "3")]
            pub color_map: i32,
            /// Excludes attributions above the specified percentile from the
            /// highlighted areas. Using the clip_percent_upperbound and
            /// clip_percent_lowerbound together can be useful for filtering out noise
            /// and making it easier to see areas of strong attribution. Defaults to
            /// 99.9.
            #[prost(float, tag = "4")]
            pub clip_percent_upperbound: f32,
            /// Excludes attributions below the specified percentile, from the
            /// highlighted areas. Defaults to 62.
            #[prost(float, tag = "5")]
            pub clip_percent_lowerbound: f32,
            /// How the original image is displayed in the visualization.
            /// Adjusting the overlay can help increase visual clarity if the original
            /// image makes it difficult to view the visualization. Defaults to NONE.
            #[prost(enumeration = "visualization::OverlayType", tag = "6")]
            pub overlay_type: i32,
        }
        /// Nested message and enum types in `Visualization`.
        pub mod visualization {
            /// Type of the image visualization. Only applicable to
            /// [Integrated Gradients
            /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution].
            #[derive(
                Clone,
                Copy,
                Debug,
                PartialEq,
                Eq,
                Hash,
                PartialOrd,
                Ord,
                ::prost::Enumeration
            )]
            #[repr(i32)]
            pub enum Type {
                /// Should not be used.
                Unspecified = 0,
                /// Shows which pixel contributed to the image prediction.
                Pixels = 1,
                /// Shows which region contributed to the image prediction by outlining
                /// the region.
                Outlines = 2,
            }
            impl Type {
                /// String value of the enum field names used in the ProtoBuf definition.
                ///
                /// The values are not transformed in any way and thus are considered stable
                /// (if the ProtoBuf definition does not change) and safe for programmatic use.
                pub fn as_str_name(&self) -> &'static str {
                    match self {
                        Self::Unspecified => "TYPE_UNSPECIFIED",
                        Self::Pixels => "PIXELS",
                        Self::Outlines => "OUTLINES",
                    }
                }
                /// Creates an enum from field names used in the ProtoBuf definition.
                pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                    match value {
                        "TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                        "PIXELS" => Some(Self::Pixels),
                        "OUTLINES" => Some(Self::Outlines),
                        _ => None,
                    }
                }
            }
            /// Whether to only highlight pixels with positive contributions, negative
            /// or both. Defaults to POSITIVE.
            #[derive(
                Clone,
                Copy,
                Debug,
                PartialEq,
                Eq,
                Hash,
                PartialOrd,
                Ord,
                ::prost::Enumeration
            )]
            #[repr(i32)]
            pub enum Polarity {
                /// Default value. This is the same as POSITIVE.
                Unspecified = 0,
                /// Highlights the pixels/outlines that were most influential to the
                /// model's prediction.
                Positive = 1,
                /// Setting polarity to negative highlights areas that does not lead to
                /// the models's current prediction.
                Negative = 2,
                /// Shows both positive and negative attributions.
                Both = 3,
            }
            impl Polarity {
                /// String value of the enum field names used in the ProtoBuf definition.
                ///
                /// The values are not transformed in any way and thus are considered stable
                /// (if the ProtoBuf definition does not change) and safe for programmatic use.
                pub fn as_str_name(&self) -> &'static str {
                    match self {
                        Self::Unspecified => "POLARITY_UNSPECIFIED",
                        Self::Positive => "POSITIVE",
                        Self::Negative => "NEGATIVE",
                        Self::Both => "BOTH",
                    }
                }
                /// Creates an enum from field names used in the ProtoBuf definition.
                pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                    match value {
                        "POLARITY_UNSPECIFIED" => Some(Self::Unspecified),
                        "POSITIVE" => Some(Self::Positive),
                        "NEGATIVE" => Some(Self::Negative),
                        "BOTH" => Some(Self::Both),
                        _ => None,
                    }
                }
            }
            /// The color scheme used for highlighting areas.
            #[derive(
                Clone,
                Copy,
                Debug,
                PartialEq,
                Eq,
                Hash,
                PartialOrd,
                Ord,
                ::prost::Enumeration
            )]
            #[repr(i32)]
            pub enum ColorMap {
                /// Should not be used.
                Unspecified = 0,
                /// Positive: green. Negative: pink.
                PinkGreen = 1,
                /// Viridis color map: A perceptually uniform color mapping which is
                /// easier to see by those with colorblindness and progresses from yellow
                /// to green to blue. Positive: yellow. Negative: blue.
                Viridis = 2,
                /// Positive: red. Negative: red.
                Red = 3,
                /// Positive: green. Negative: green.
                Green = 4,
                /// Positive: green. Negative: red.
                RedGreen = 6,
                /// PiYG palette.
                PinkWhiteGreen = 5,
            }
            impl ColorMap {
                /// String value of the enum field names used in the ProtoBuf definition.
                ///
                /// The values are not transformed in any way and thus are considered stable
                /// (if the ProtoBuf definition does not change) and safe for programmatic use.
                pub fn as_str_name(&self) -> &'static str {
                    match self {
                        Self::Unspecified => "COLOR_MAP_UNSPECIFIED",
                        Self::PinkGreen => "PINK_GREEN",
                        Self::Viridis => "VIRIDIS",
                        Self::Red => "RED",
                        Self::Green => "GREEN",
                        Self::RedGreen => "RED_GREEN",
                        Self::PinkWhiteGreen => "PINK_WHITE_GREEN",
                    }
                }
                /// Creates an enum from field names used in the ProtoBuf definition.
                pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                    match value {
                        "COLOR_MAP_UNSPECIFIED" => Some(Self::Unspecified),
                        "PINK_GREEN" => Some(Self::PinkGreen),
                        "VIRIDIS" => Some(Self::Viridis),
                        "RED" => Some(Self::Red),
                        "GREEN" => Some(Self::Green),
                        "RED_GREEN" => Some(Self::RedGreen),
                        "PINK_WHITE_GREEN" => Some(Self::PinkWhiteGreen),
                        _ => None,
                    }
                }
            }
            /// How the original image is displayed in the visualization.
            #[derive(
                Clone,
                Copy,
                Debug,
                PartialEq,
                Eq,
                Hash,
                PartialOrd,
                Ord,
                ::prost::Enumeration
            )]
            #[repr(i32)]
            pub enum OverlayType {
                /// Default value. This is the same as NONE.
                Unspecified = 0,
                /// No overlay.
                None = 1,
                /// The attributions are shown on top of the original image.
                Original = 2,
                /// The attributions are shown on top of grayscaled version of the
                /// original image.
                Grayscale = 3,
                /// The attributions are used as a mask to reveal predictive parts of
                /// the image and hide the un-predictive parts.
                MaskBlack = 4,
            }
            impl OverlayType {
                /// String value of the enum field names used in the ProtoBuf definition.
                ///
                /// The values are not transformed in any way and thus are considered stable
                /// (if the ProtoBuf definition does not change) and safe for programmatic use.
                pub fn as_str_name(&self) -> &'static str {
                    match self {
                        Self::Unspecified => "OVERLAY_TYPE_UNSPECIFIED",
                        Self::None => "NONE",
                        Self::Original => "ORIGINAL",
                        Self::Grayscale => "GRAYSCALE",
                        Self::MaskBlack => "MASK_BLACK",
                    }
                }
                /// Creates an enum from field names used in the ProtoBuf definition.
                pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                    match value {
                        "OVERLAY_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                        "NONE" => Some(Self::None),
                        "ORIGINAL" => Some(Self::Original),
                        "GRAYSCALE" => Some(Self::Grayscale),
                        "MASK_BLACK" => Some(Self::MaskBlack),
                        _ => None,
                    }
                }
            }
        }
        /// Defines how a feature is encoded. Defaults to IDENTITY.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum Encoding {
            /// Default value. This is the same as IDENTITY.
            Unspecified = 0,
            /// The tensor represents one feature.
            Identity = 1,
            /// The tensor represents a bag of features where each index maps to
            /// a feature.
            /// [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.index_feature_mapping]
            /// must be provided for this encoding. For example:
            /// ```
            /// input = \[27, 6.0, 150\]
            /// index_feature_mapping = \["age", "height", "weight"\]
            /// ```
            BagOfFeatures = 2,
            /// The tensor represents a bag of features where each index maps to a
            /// feature. Zero values in the tensor indicates feature being
            /// non-existent.
            /// [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.index_feature_mapping]
            /// must be provided for this encoding. For example:
            /// ```
            /// input = \[2, 0, 5, 0, 1\]
            /// index_feature_mapping = \["a", "b", "c", "d", "e"\]
            /// ```
            BagOfFeaturesSparse = 3,
            /// The tensor is a list of binaries representing whether a feature exists
            /// or not (1 indicates existence).
            /// [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.index_feature_mapping]
            /// must be provided for this encoding. For example:
            /// ```
            /// input = \[1, 0, 1, 0, 1\]
            /// index_feature_mapping = \["a", "b", "c", "d", "e"\]
            /// ```
            Indicator = 4,
            /// The tensor is encoded into a 1-dimensional array represented by an
            /// encoded tensor.
            /// [InputMetadata.encoded_tensor_name][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.encoded_tensor_name]
            /// must be provided for this encoding. For example:
            /// ```
            /// input = \["This", "is", "a", "test", "."\]
            /// encoded = \[0.1, 0.2, 0.3, 0.4, 0.5\]
            /// ```
            CombinedEmbedding = 5,
            /// Select this encoding when the input tensor is encoded into a
            /// 2-dimensional array represented by an encoded tensor.
            /// [InputMetadata.encoded_tensor_name][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata.encoded_tensor_name]
            /// must be provided for this encoding. The first dimension of the encoded
            /// tensor's shape is the same as the input tensor's shape. For example:
            /// ```
            /// input = \["This", "is", "a", "test", "."\]
            /// encoded = \[[0.1, 0.2, 0.3, 0.4, 0.5\],
            ///             \[0.2, 0.1, 0.4, 0.3, 0.5\],
            ///             \[0.5, 0.1, 0.3, 0.5, 0.4\],
            ///             \[0.5, 0.3, 0.1, 0.2, 0.4\],
            ///             \[0.4, 0.3, 0.2, 0.5, 0.1]\]
            /// ```
            ConcatEmbedding = 6,
        }
        impl Encoding {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "ENCODING_UNSPECIFIED",
                    Self::Identity => "IDENTITY",
                    Self::BagOfFeatures => "BAG_OF_FEATURES",
                    Self::BagOfFeaturesSparse => "BAG_OF_FEATURES_SPARSE",
                    Self::Indicator => "INDICATOR",
                    Self::CombinedEmbedding => "COMBINED_EMBEDDING",
                    Self::ConcatEmbedding => "CONCAT_EMBEDDING",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "ENCODING_UNSPECIFIED" => Some(Self::Unspecified),
                    "IDENTITY" => Some(Self::Identity),
                    "BAG_OF_FEATURES" => Some(Self::BagOfFeatures),
                    "BAG_OF_FEATURES_SPARSE" => Some(Self::BagOfFeaturesSparse),
                    "INDICATOR" => Some(Self::Indicator),
                    "COMBINED_EMBEDDING" => Some(Self::CombinedEmbedding),
                    "CONCAT_EMBEDDING" => Some(Self::ConcatEmbedding),
                    _ => None,
                }
            }
        }
    }
    /// Metadata of the prediction output to be explained.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct OutputMetadata {
        /// Name of the output tensor. Required and is only applicable to Vertex
        /// AI provided images for Tensorflow.
        #[prost(string, tag = "3")]
        pub output_tensor_name: ::prost::alloc::string::String,
        /// Defines how to map
        /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index]
        /// to
        /// [Attribution.output_display_name][google.cloud.aiplatform.v1.Attribution.output_display_name].
        ///
        /// If neither of the fields are specified,
        /// [Attribution.output_display_name][google.cloud.aiplatform.v1.Attribution.output_display_name]
        /// will not be populated.
        #[prost(oneof = "output_metadata::DisplayNameMapping", tags = "1, 2")]
        pub display_name_mapping: ::core::option::Option<
            output_metadata::DisplayNameMapping,
        >,
    }
    /// Nested message and enum types in `OutputMetadata`.
    pub mod output_metadata {
        /// Defines how to map
        /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index]
        /// to
        /// [Attribution.output_display_name][google.cloud.aiplatform.v1.Attribution.output_display_name].
        ///
        /// If neither of the fields are specified,
        /// [Attribution.output_display_name][google.cloud.aiplatform.v1.Attribution.output_display_name]
        /// will not be populated.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum DisplayNameMapping {
            /// Static mapping between the index and display name.
            ///
            /// Use this if the outputs are a deterministic n-dimensional array, e.g. a
            /// list of scores of all the classes in a pre-defined order for a
            /// multi-classification Model. It's not feasible if the outputs are
            /// non-deterministic, e.g. the Model produces top-k classes or sort the
            /// outputs by their values.
            ///
            /// The shape of the value must be an n-dimensional array of strings. The
            /// number of dimensions must match that of the outputs to be explained.
            /// The
            /// [Attribution.output_display_name][google.cloud.aiplatform.v1.Attribution.output_display_name]
            /// is populated by locating in the mapping with
            /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index].
            #[prost(message, tag = "1")]
            IndexDisplayNameMapping(::prost_types::Value),
            /// Specify a field name in the prediction to look for the display name.
            ///
            /// Use this if the prediction contains the display names for the outputs.
            ///
            /// The display names in the prediction must have the same shape of the
            /// outputs, so that it can be located by
            /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index]
            /// for a specific output.
            #[prost(string, tag = "2")]
            DisplayNameMappingKey(::prost::alloc::string::String),
        }
    }
}
/// The storage details for Avro input content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AvroSource {
    /// Required. Google Cloud Storage location.
    #[prost(message, optional, tag = "1")]
    pub gcs_source: ::core::option::Option<GcsSource>,
}
/// The storage details for CSV input content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CsvSource {
    /// Required. Google Cloud Storage location.
    #[prost(message, optional, tag = "1")]
    pub gcs_source: ::core::option::Option<GcsSource>,
}
/// The Google Cloud Storage location for the input content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GcsSource {
    /// Required. Google Cloud Storage URI(-s) to the input file(s). May contain
    /// wildcards. For more information on wildcards, see
    /// <https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames.>
    #[prost(string, repeated, tag = "1")]
    pub uris: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// The Google Cloud Storage location where the output is to be written to.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GcsDestination {
    /// Required. Google Cloud Storage URI to output directory. If the uri doesn't
    /// end with
    /// '/', a '/' will be automatically appended. The directory is created if it
    /// doesn't exist.
    #[prost(string, tag = "1")]
    pub output_uri_prefix: ::prost::alloc::string::String,
}
/// The BigQuery location for the input content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BigQuerySource {
    /// Required. BigQuery URI to a table, up to 2000 characters long.
    /// Accepted forms:
    ///
    /// *  BigQuery path. For example: `bq://projectId.bqDatasetId.bqTableId`.
    #[prost(string, tag = "1")]
    pub input_uri: ::prost::alloc::string::String,
}
/// The BigQuery location for the output content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BigQueryDestination {
    /// Required. BigQuery URI to a project or table, up to 2000 characters long.
    ///
    /// When only the project is specified, the Dataset and Table is created.
    /// When the full table reference is specified, the Dataset must exist and
    /// table must not exist.
    ///
    /// Accepted forms:
    ///
    /// *  BigQuery path. For example:
    /// `bq://projectId` or `bq://projectId.bqDatasetId` or
    /// `bq://projectId.bqDatasetId.bqTableId`.
    #[prost(string, tag = "1")]
    pub output_uri: ::prost::alloc::string::String,
}
/// The storage details for CSV output content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CsvDestination {
    /// Required. Google Cloud Storage location.
    #[prost(message, optional, tag = "1")]
    pub gcs_destination: ::core::option::Option<GcsDestination>,
}
/// The storage details for TFRecord output content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TfRecordDestination {
    /// Required. Google Cloud Storage location.
    #[prost(message, optional, tag = "1")]
    pub gcs_destination: ::core::option::Option<GcsDestination>,
}
/// The Container Registry location for the container image.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ContainerRegistryDestination {
    /// Required. Container Registry URI of a container image.
    /// Only Google Container Registry and Artifact Registry are supported now.
    /// Accepted forms:
    ///
    /// *  Google Container Registry path. For example:
    ///     `gcr.io/projectId/imageName:tag`.
    ///
    /// *  Artifact Registry path. For example:
    ///     `us-central1-docker.pkg.dev/projectId/repoName/imageName:tag`.
    ///
    /// If a tag is not specified, "latest" will be used as the default tag.
    #[prost(string, tag = "1")]
    pub output_uri: ::prost::alloc::string::String,
}
/// The Google Drive location for the input content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GoogleDriveSource {
    /// Required. Google Drive resource IDs.
    #[prost(message, repeated, tag = "1")]
    pub resource_ids: ::prost::alloc::vec::Vec<google_drive_source::ResourceId>,
}
/// Nested message and enum types in `GoogleDriveSource`.
pub mod google_drive_source {
    /// The type and ID of the Google Drive resource.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ResourceId {
        /// Required. The type of the Google Drive resource.
        #[prost(enumeration = "resource_id::ResourceType", tag = "1")]
        pub resource_type: i32,
        /// Required. The ID of the Google Drive resource.
        #[prost(string, tag = "2")]
        pub resource_id: ::prost::alloc::string::String,
    }
    /// Nested message and enum types in `ResourceId`.
    pub mod resource_id {
        /// The type of the Google Drive resource.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum ResourceType {
            /// Unspecified resource type.
            Unspecified = 0,
            /// File resource type.
            File = 1,
            /// Folder resource type.
            Folder = 2,
        }
        impl ResourceType {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "RESOURCE_TYPE_UNSPECIFIED",
                    Self::File => "RESOURCE_TYPE_FILE",
                    Self::Folder => "RESOURCE_TYPE_FOLDER",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "RESOURCE_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                    "RESOURCE_TYPE_FILE" => Some(Self::File),
                    "RESOURCE_TYPE_FOLDER" => Some(Self::Folder),
                    _ => None,
                }
            }
        }
    }
}
/// The input content is encapsulated and uploaded in the request.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct DirectUploadSource {}
/// The Slack source for the ImportRagFilesRequest.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SlackSource {
    /// Required. The Slack channels.
    #[prost(message, repeated, tag = "1")]
    pub channels: ::prost::alloc::vec::Vec<slack_source::SlackChannels>,
}
/// Nested message and enum types in `SlackSource`.
pub mod slack_source {
    /// SlackChannels contains the Slack channels and corresponding access token.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct SlackChannels {
        /// Required. The Slack channel IDs.
        #[prost(message, repeated, tag = "1")]
        pub channels: ::prost::alloc::vec::Vec<slack_channels::SlackChannel>,
        /// Required. The SecretManager secret version resource name (e.g.
        /// projects/{project}/secrets/{secret}/versions/{version}) storing the
        /// Slack channel access token that has access to the slack channel IDs.
        /// See: <https://api.slack.com/tutorials/tracks/getting-a-token.>
        #[prost(message, optional, tag = "3")]
        pub api_key_config: ::core::option::Option<super::api_auth::ApiKeyConfig>,
    }
    /// Nested message and enum types in `SlackChannels`.
    pub mod slack_channels {
        /// SlackChannel contains the Slack channel ID and the time range to import.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct SlackChannel {
            /// Required. The Slack channel ID.
            #[prost(string, tag = "1")]
            pub channel_id: ::prost::alloc::string::String,
            /// Optional. The starting timestamp for messages to import.
            #[prost(message, optional, tag = "2")]
            pub start_time: ::core::option::Option<::prost_types::Timestamp>,
            /// Optional. The ending timestamp for messages to import.
            #[prost(message, optional, tag = "3")]
            pub end_time: ::core::option::Option<::prost_types::Timestamp>,
        }
    }
}
/// The Jira source for the ImportRagFilesRequest.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JiraSource {
    /// Required. The Jira queries.
    #[prost(message, repeated, tag = "1")]
    pub jira_queries: ::prost::alloc::vec::Vec<jira_source::JiraQueries>,
}
/// Nested message and enum types in `JiraSource`.
pub mod jira_source {
    /// JiraQueries contains the Jira queries and corresponding authentication.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct JiraQueries {
        /// A list of Jira projects to import in their entirety.
        #[prost(string, repeated, tag = "3")]
        pub projects: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        /// A list of custom Jira queries to import. For information about JQL (Jira
        /// Query Language), see
        /// <https://support.atlassian.com/jira-service-management-cloud/docs/use-advanced-search-with-jira-query-language-jql/>
        #[prost(string, repeated, tag = "4")]
        pub custom_queries: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        /// Required. The Jira email address.
        #[prost(string, tag = "5")]
        pub email: ::prost::alloc::string::String,
        /// Required. The Jira server URI.
        #[prost(string, tag = "6")]
        pub server_uri: ::prost::alloc::string::String,
        /// Required. The SecretManager secret version resource name (e.g.
        /// projects/{project}/secrets/{secret}/versions/{version}) storing the
        /// Jira API key. See [Manage API tokens for your Atlassian
        /// account](<https://support.atlassian.com/atlassian-account/docs/manage-api-tokens-for-your-atlassian-account/>).
        #[prost(message, optional, tag = "7")]
        pub api_key_config: ::core::option::Option<super::api_auth::ApiKeyConfig>,
    }
}
/// The SharePointSources to pass to ImportRagFiles.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SharePointSources {
    /// The SharePoint sources.
    #[prost(message, repeated, tag = "1")]
    pub share_point_sources: ::prost::alloc::vec::Vec<
        share_point_sources::SharePointSource,
    >,
}
/// Nested message and enum types in `SharePointSources`.
pub mod share_point_sources {
    /// An individual SharePointSource.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct SharePointSource {
        /// The Application ID for the app registered in Microsoft Azure Portal.
        /// The application must also be configured with MS Graph permissions
        /// "Files.ReadAll", "Sites.ReadAll" and BrowserSiteLists.Read.All.
        #[prost(string, tag = "1")]
        pub client_id: ::prost::alloc::string::String,
        /// The application secret for the app registered in Azure.
        #[prost(message, optional, tag = "2")]
        pub client_secret: ::core::option::Option<super::api_auth::ApiKeyConfig>,
        /// Unique identifier of the Azure Active Directory Instance.
        #[prost(string, tag = "3")]
        pub tenant_id: ::prost::alloc::string::String,
        /// The name of the SharePoint site to download from. This can be the site
        /// name or the site id.
        #[prost(string, tag = "4")]
        pub sharepoint_site_name: ::prost::alloc::string::String,
        /// Output only. The SharePoint file id. Output only.
        #[prost(string, tag = "9")]
        pub file_id: ::prost::alloc::string::String,
        /// The SharePoint folder source. If not provided, uses "root".
        #[prost(oneof = "share_point_source::FolderSource", tags = "5, 6")]
        pub folder_source: ::core::option::Option<share_point_source::FolderSource>,
        /// The SharePoint drive source.
        #[prost(oneof = "share_point_source::DriveSource", tags = "7, 8")]
        pub drive_source: ::core::option::Option<share_point_source::DriveSource>,
    }
    /// Nested message and enum types in `SharePointSource`.
    pub mod share_point_source {
        /// The SharePoint folder source. If not provided, uses "root".
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum FolderSource {
            /// The path of the SharePoint folder to download from.
            #[prost(string, tag = "5")]
            SharepointFolderPath(::prost::alloc::string::String),
            /// The ID of the SharePoint folder to download from.
            #[prost(string, tag = "6")]
            SharepointFolderId(::prost::alloc::string::String),
        }
        /// The SharePoint drive source.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum DriveSource {
            /// The name of the drive to download from.
            #[prost(string, tag = "7")]
            DriveName(::prost::alloc::string::String),
            /// The ID of the drive to download from.
            #[prost(string, tag = "8")]
            DriveId(::prost::alloc::string::String),
        }
    }
}
/// Explanation of a prediction (provided in
/// [PredictResponse.predictions][google.cloud.aiplatform.v1.PredictResponse.predictions])
/// produced by the Model on a given
/// [instance][google.cloud.aiplatform.v1.ExplainRequest.instances].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Explanation {
    /// Output only. Feature attributions grouped by predicted outputs.
    ///
    /// For Models that predict only one output, such as regression Models that
    /// predict only one score, there is only one attibution that explains the
    /// predicted output. For Models that predict multiple outputs, such as
    /// multiclass Models that predict multiple classes, each element explains one
    /// specific item.
    /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index]
    /// can be used to identify which output this attribution is explaining.
    ///
    /// By default, we provide Shapley values for the predicted class. However,
    /// you can configure the explanation request to generate Shapley values for
    /// any other classes too. For example, if a model predicts a probability of
    /// `0.4` for approving a loan application, the model's decision is to reject
    /// the application since `p(reject) = 0.6 > p(approve) = 0.4`, and the default
    /// Shapley values would be computed for rejection decision and not approval,
    /// even though the latter might be the positive class.
    ///
    /// If users set
    /// [ExplanationParameters.top_k][google.cloud.aiplatform.v1.ExplanationParameters.top_k],
    /// the attributions are sorted by
    /// [instance_output_value][google.cloud.aiplatform.v1.Attribution.instance_output_value]
    /// in descending order. If
    /// [ExplanationParameters.output_indices][google.cloud.aiplatform.v1.ExplanationParameters.output_indices]
    /// is specified, the attributions are stored by
    /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index]
    /// in the same order as they appear in the output_indices.
    #[prost(message, repeated, tag = "1")]
    pub attributions: ::prost::alloc::vec::Vec<Attribution>,
    /// Output only. List of the nearest neighbors for example-based explanations.
    ///
    /// For models deployed with the examples explanations feature enabled, the
    /// attributions field is empty and instead the neighbors field is populated.
    #[prost(message, repeated, tag = "2")]
    pub neighbors: ::prost::alloc::vec::Vec<Neighbor>,
}
/// Aggregated explanation metrics for a Model over a set of instances.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelExplanation {
    /// Output only. Aggregated attributions explaining the Model's prediction
    /// outputs over the set of instances. The attributions are grouped by outputs.
    ///
    /// For Models that predict only one output, such as regression Models that
    /// predict only one score, there is only one attibution that explains the
    /// predicted output. For Models that predict multiple outputs, such as
    /// multiclass Models that predict multiple classes, each element explains one
    /// specific item.
    /// [Attribution.output_index][google.cloud.aiplatform.v1.Attribution.output_index]
    /// can be used to identify which output this attribution is explaining.
    ///
    /// The
    /// [baselineOutputValue][google.cloud.aiplatform.v1.Attribution.baseline_output_value],
    /// [instanceOutputValue][google.cloud.aiplatform.v1.Attribution.instance_output_value]
    /// and
    /// [featureAttributions][google.cloud.aiplatform.v1.Attribution.feature_attributions]
    /// fields are averaged over the test data.
    ///
    /// NOTE: Currently AutoML tabular classification Models produce only one
    /// attribution, which averages attributions over all the classes it predicts.
    /// [Attribution.approximation_error][google.cloud.aiplatform.v1.Attribution.approximation_error]
    /// is not populated.
    #[prost(message, repeated, tag = "1")]
    pub mean_attributions: ::prost::alloc::vec::Vec<Attribution>,
}
/// Attribution that explains a particular prediction output.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Attribution {
    /// Output only. Model predicted output if the input instance is constructed
    /// from the baselines of all the features defined in
    /// [ExplanationMetadata.inputs][google.cloud.aiplatform.v1.ExplanationMetadata.inputs].
    /// The field name of the output is determined by the key in
    /// [ExplanationMetadata.outputs][google.cloud.aiplatform.v1.ExplanationMetadata.outputs].
    ///
    /// If the Model's predicted output has multiple dimensions (rank > 1), this is
    /// the value in the output located by
    /// [output_index][google.cloud.aiplatform.v1.Attribution.output_index].
    ///
    /// If there are multiple baselines, their output values are averaged.
    #[prost(double, tag = "1")]
    pub baseline_output_value: f64,
    /// Output only. Model predicted output on the corresponding [explanation
    /// instance][ExplainRequest.instances]. The field name of the output is
    /// determined by the key in
    /// [ExplanationMetadata.outputs][google.cloud.aiplatform.v1.ExplanationMetadata.outputs].
    ///
    /// If the Model predicted output has multiple dimensions, this is the value in
    /// the output located by
    /// [output_index][google.cloud.aiplatform.v1.Attribution.output_index].
    #[prost(double, tag = "2")]
    pub instance_output_value: f64,
    /// Output only. Attributions of each explained feature. Features are extracted
    /// from the [prediction
    /// instances][google.cloud.aiplatform.v1.ExplainRequest.instances] according
    /// to [explanation metadata for
    /// inputs][google.cloud.aiplatform.v1.ExplanationMetadata.inputs].
    ///
    /// The value is a struct, whose keys are the name of the feature. The values
    /// are how much the feature in the
    /// [instance][google.cloud.aiplatform.v1.ExplainRequest.instances] contributed
    /// to the predicted result.
    ///
    /// The format of the value is determined by the feature's input format:
    ///
    ///    * If the feature is a scalar value, the attribution value is a
    ///      [floating number][google.protobuf.Value.number_value].
    ///
    ///    * If the feature is an array of scalar values, the attribution value is
    ///      an [array][google.protobuf.Value.list_value].
    ///
    ///    * If the feature is a struct, the attribution value is a
    ///      [struct][google.protobuf.Value.struct_value]. The keys in the
    ///      attribution value struct are the same as the keys in the feature
    ///      struct. The formats of the values in the attribution struct are
    ///      determined by the formats of the values in the feature struct.
    ///
    /// The
    /// [ExplanationMetadata.feature_attributions_schema_uri][google.cloud.aiplatform.v1.ExplanationMetadata.feature_attributions_schema_uri]
    /// field, pointed to by the
    /// [ExplanationSpec][google.cloud.aiplatform.v1.ExplanationSpec] field of the
    /// [Endpoint.deployed_models][google.cloud.aiplatform.v1.Endpoint.deployed_models]
    /// object, points to the schema file that describes the features and their
    /// attribution values (if it is populated).
    #[prost(message, optional, tag = "3")]
    pub feature_attributions: ::core::option::Option<::prost_types::Value>,
    /// Output only. The index that locates the explained prediction output.
    ///
    /// If the prediction output is a scalar value, output_index is not populated.
    /// If the prediction output has multiple dimensions, the length of the
    /// output_index list is the same as the number of dimensions of the output.
    /// The i-th element in output_index is the element index of the i-th dimension
    /// of the output vector. Indices start from 0.
    #[prost(int32, repeated, packed = "false", tag = "4")]
    pub output_index: ::prost::alloc::vec::Vec<i32>,
    /// Output only. The display name of the output identified by
    /// [output_index][google.cloud.aiplatform.v1.Attribution.output_index]. For
    /// example, the predicted class name by a multi-classification Model.
    ///
    /// This field is only populated iff the Model predicts display names as a
    /// separate field along with the explained output. The predicted display name
    /// must has the same shape of the explained output, and can be located using
    /// output_index.
    #[prost(string, tag = "5")]
    pub output_display_name: ::prost::alloc::string::String,
    /// Output only. Error of
    /// [feature_attributions][google.cloud.aiplatform.v1.Attribution.feature_attributions]
    /// caused by approximation used in the explanation method. Lower value means
    /// more precise attributions.
    ///
    /// * For Sampled Shapley
    /// [attribution][google.cloud.aiplatform.v1.ExplanationParameters.sampled_shapley_attribution],
    /// increasing
    /// [path_count][google.cloud.aiplatform.v1.SampledShapleyAttribution.path_count]
    /// might reduce the error.
    /// * For Integrated Gradients
    /// [attribution][google.cloud.aiplatform.v1.ExplanationParameters.integrated_gradients_attribution],
    /// increasing
    /// [step_count][google.cloud.aiplatform.v1.IntegratedGradientsAttribution.step_count]
    /// might reduce the error.
    /// * For [XRAI
    /// attribution][google.cloud.aiplatform.v1.ExplanationParameters.xrai_attribution],
    /// increasing
    /// [step_count][google.cloud.aiplatform.v1.XraiAttribution.step_count] might
    /// reduce the error.
    ///
    /// See [this introduction](/vertex-ai/docs/explainable-ai/overview)
    /// for more information.
    #[prost(double, tag = "6")]
    pub approximation_error: f64,
    /// Output only. Name of the explain output. Specified as the key in
    /// [ExplanationMetadata.outputs][google.cloud.aiplatform.v1.ExplanationMetadata.outputs].
    #[prost(string, tag = "7")]
    pub output_name: ::prost::alloc::string::String,
}
/// Neighbors for example-based explanations.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Neighbor {
    /// Output only. The neighbor id.
    #[prost(string, tag = "1")]
    pub neighbor_id: ::prost::alloc::string::String,
    /// Output only. The neighbor distance.
    #[prost(double, tag = "2")]
    pub neighbor_distance: f64,
}
/// Specification of Model explanation.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExplanationSpec {
    /// Required. Parameters that configure explaining of the Model's predictions.
    #[prost(message, optional, tag = "1")]
    pub parameters: ::core::option::Option<ExplanationParameters>,
    /// Optional. Metadata describing the Model's input and output for explanation.
    #[prost(message, optional, tag = "2")]
    pub metadata: ::core::option::Option<ExplanationMetadata>,
}
/// Parameters to configure explaining for Model's predictions.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExplanationParameters {
    /// If populated, returns attributions for top K indices of outputs
    /// (defaults to 1). Only applies to Models that predicts more than one outputs
    /// (e,g, multi-class Models). When set to -1, returns explanations for all
    /// outputs.
    #[prost(int32, tag = "4")]
    pub top_k: i32,
    /// If populated, only returns attributions that have
    /// [output_index][google.cloud.aiplatform.v1.Attribution.output_index]
    /// contained in output_indices. It must be an ndarray of integers, with the
    /// same shape of the output it's explaining.
    ///
    /// If not populated, returns attributions for
    /// [top_k][google.cloud.aiplatform.v1.ExplanationParameters.top_k] indices of
    /// outputs. If neither top_k nor output_indices is populated, returns the
    /// argmax index of the outputs.
    ///
    /// Only applicable to Models that predict multiple outputs (e,g, multi-class
    /// Models that predict multiple classes).
    #[prost(message, optional, tag = "5")]
    pub output_indices: ::core::option::Option<::prost_types::ListValue>,
    #[prost(oneof = "explanation_parameters::Method", tags = "1, 2, 3, 7")]
    pub method: ::core::option::Option<explanation_parameters::Method>,
}
/// Nested message and enum types in `ExplanationParameters`.
pub mod explanation_parameters {
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Method {
        /// An attribution method that approximates Shapley values for features that
        /// contribute to the label being predicted. A sampling strategy is used to
        /// approximate the value rather than considering all subsets of features.
        /// Refer to this paper for model details: <https://arxiv.org/abs/1306.4265.>
        #[prost(message, tag = "1")]
        SampledShapleyAttribution(super::SampledShapleyAttribution),
        /// An attribution method that computes Aumann-Shapley values taking
        /// advantage of the model's fully differentiable structure. Refer to this
        /// paper for more details: <https://arxiv.org/abs/1703.01365>
        #[prost(message, tag = "2")]
        IntegratedGradientsAttribution(super::IntegratedGradientsAttribution),
        /// An attribution method that redistributes Integrated Gradients
        /// attribution to segmented regions, taking advantage of the model's fully
        /// differentiable structure. Refer to this paper for
        /// more details: <https://arxiv.org/abs/1906.02825>
        ///
        /// XRAI currently performs better on natural images, like a picture of a
        /// house or an animal. If the images are taken in artificial environments,
        /// like a lab or manufacturing line, or from diagnostic equipment, like
        /// x-rays or quality-control cameras, use Integrated Gradients instead.
        #[prost(message, tag = "3")]
        XraiAttribution(super::XraiAttribution),
        /// Example-based explanations that returns the nearest neighbors from the
        /// provided dataset.
        #[prost(message, tag = "7")]
        Examples(super::Examples),
    }
}
/// An attribution method that approximates Shapley values for features that
/// contribute to the label being predicted. A sampling strategy is used to
/// approximate the value rather than considering all subsets of features.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SampledShapleyAttribution {
    /// Required. The number of feature permutations to consider when approximating
    /// the Shapley values.
    ///
    /// Valid range of its value is \[1, 50\], inclusively.
    #[prost(int32, tag = "1")]
    pub path_count: i32,
}
/// An attribution method that computes the Aumann-Shapley value taking advantage
/// of the model's fully differentiable structure. Refer to this paper for
/// more details: <https://arxiv.org/abs/1703.01365>
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct IntegratedGradientsAttribution {
    /// Required. The number of steps for approximating the path integral.
    /// A good value to start is 50 and gradually increase until the
    /// sum to diff property is within the desired error range.
    ///
    /// Valid range of its value is \[1, 100\], inclusively.
    #[prost(int32, tag = "1")]
    pub step_count: i32,
    /// Config for SmoothGrad approximation of gradients.
    ///
    /// When enabled, the gradients are approximated by averaging the gradients
    /// from noisy samples in the vicinity of the inputs. Adding
    /// noise can help improve the computed gradients. Refer to this paper for more
    /// details: <https://arxiv.org/pdf/1706.03825.pdf>
    #[prost(message, optional, tag = "2")]
    pub smooth_grad_config: ::core::option::Option<SmoothGradConfig>,
    /// Config for IG with blur baseline.
    ///
    /// When enabled, a linear path from the maximally blurred image to the input
    /// image is created. Using a blurred baseline instead of zero (black image) is
    /// motivated by the BlurIG approach explained here:
    /// <https://arxiv.org/abs/2004.03383>
    #[prost(message, optional, tag = "3")]
    pub blur_baseline_config: ::core::option::Option<BlurBaselineConfig>,
}
/// An explanation method that redistributes Integrated Gradients
/// attributions to segmented regions, taking advantage of the model's fully
/// differentiable structure. Refer to this paper for more details:
/// <https://arxiv.org/abs/1906.02825>
///
/// Supported only by image Models.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct XraiAttribution {
    /// Required. The number of steps for approximating the path integral.
    /// A good value to start is 50 and gradually increase until the
    /// sum to diff property is met within the desired error range.
    ///
    /// Valid range of its value is \[1, 100\], inclusively.
    #[prost(int32, tag = "1")]
    pub step_count: i32,
    /// Config for SmoothGrad approximation of gradients.
    ///
    /// When enabled, the gradients are approximated by averaging the gradients
    /// from noisy samples in the vicinity of the inputs. Adding
    /// noise can help improve the computed gradients. Refer to this paper for more
    /// details: <https://arxiv.org/pdf/1706.03825.pdf>
    #[prost(message, optional, tag = "2")]
    pub smooth_grad_config: ::core::option::Option<SmoothGradConfig>,
    /// Config for XRAI with blur baseline.
    ///
    /// When enabled, a linear path from the maximally blurred image to the input
    /// image is created. Using a blurred baseline instead of zero (black image) is
    /// motivated by the BlurIG approach explained here:
    /// <https://arxiv.org/abs/2004.03383>
    #[prost(message, optional, tag = "3")]
    pub blur_baseline_config: ::core::option::Option<BlurBaselineConfig>,
}
/// Config for SmoothGrad approximation of gradients.
///
/// When enabled, the gradients are approximated by averaging the gradients from
/// noisy samples in the vicinity of the inputs. Adding noise can help improve
/// the computed gradients. Refer to this paper for more details:
/// <https://arxiv.org/pdf/1706.03825.pdf>
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SmoothGradConfig {
    /// The number of gradient samples to use for
    /// approximation. The higher this number, the more accurate the gradient
    /// is, but the runtime complexity increases by this factor as well.
    /// Valid range of its value is \[1, 50\]. Defaults to 3.
    #[prost(int32, tag = "3")]
    pub noisy_sample_count: i32,
    /// Represents the standard deviation of the gaussian kernel
    /// that will be used to add noise to the interpolated inputs
    /// prior to computing gradients.
    #[prost(oneof = "smooth_grad_config::GradientNoiseSigma", tags = "1, 2")]
    pub gradient_noise_sigma: ::core::option::Option<
        smooth_grad_config::GradientNoiseSigma,
    >,
}
/// Nested message and enum types in `SmoothGradConfig`.
pub mod smooth_grad_config {
    /// Represents the standard deviation of the gaussian kernel
    /// that will be used to add noise to the interpolated inputs
    /// prior to computing gradients.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum GradientNoiseSigma {
        /// This is a single float value and will be used to add noise to all the
        /// features. Use this field when all features are normalized to have the
        /// same distribution: scale to range \[0, 1\], \[-1, 1\] or z-scoring, where
        /// features are normalized to have 0-mean and 1-variance. Learn more about
        /// [normalization](<https://developers.google.com/machine-learning/data-prep/transform/normalization>).
        ///
        /// For best results the recommended value is about 10% - 20% of the standard
        /// deviation of the input feature. Refer to section 3.2 of the SmoothGrad
        /// paper: <https://arxiv.org/pdf/1706.03825.pdf.> Defaults to 0.1.
        ///
        /// If the distribution is different per feature, set
        /// [feature_noise_sigma][google.cloud.aiplatform.v1.SmoothGradConfig.feature_noise_sigma]
        /// instead for each feature.
        #[prost(float, tag = "1")]
        NoiseSigma(f32),
        /// This is similar to
        /// [noise_sigma][google.cloud.aiplatform.v1.SmoothGradConfig.noise_sigma],
        /// but provides additional flexibility. A separate noise sigma can be
        /// provided for each feature, which is useful if their distributions are
        /// different. No noise is added to features that are not set. If this field
        /// is unset,
        /// [noise_sigma][google.cloud.aiplatform.v1.SmoothGradConfig.noise_sigma]
        /// will be used for all features.
        #[prost(message, tag = "2")]
        FeatureNoiseSigma(super::FeatureNoiseSigma),
    }
}
/// Noise sigma by features. Noise sigma represents the standard deviation of the
/// gaussian kernel that will be used to add noise to interpolated inputs prior
/// to computing gradients.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FeatureNoiseSigma {
    /// Noise sigma per feature. No noise is added to features that are not set.
    #[prost(message, repeated, tag = "1")]
    pub noise_sigma: ::prost::alloc::vec::Vec<feature_noise_sigma::NoiseSigmaForFeature>,
}
/// Nested message and enum types in `FeatureNoiseSigma`.
pub mod feature_noise_sigma {
    /// Noise sigma for a single feature.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct NoiseSigmaForFeature {
        /// The name of the input feature for which noise sigma is provided. The
        /// features are defined in
        /// [explanation metadata
        /// inputs][google.cloud.aiplatform.v1.ExplanationMetadata.inputs].
        #[prost(string, tag = "1")]
        pub name: ::prost::alloc::string::String,
        /// This represents the standard deviation of the Gaussian kernel that will
        /// be used to add noise to the feature prior to computing gradients. Similar
        /// to [noise_sigma][google.cloud.aiplatform.v1.SmoothGradConfig.noise_sigma]
        /// but represents the noise added to the current feature. Defaults to 0.1.
        #[prost(float, tag = "2")]
        pub sigma: f32,
    }
}
/// Config for blur baseline.
///
/// When enabled, a linear path from the maximally blurred image to the input
/// image is created. Using a blurred baseline instead of zero (black image) is
/// motivated by the BlurIG approach explained here:
/// <https://arxiv.org/abs/2004.03383>
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct BlurBaselineConfig {
    /// The standard deviation of the blur kernel for the blurred baseline. The
    /// same blurring parameter is used for both the height and the width
    /// dimension. If not set, the method defaults to the zero (i.e. black for
    /// images) baseline.
    #[prost(float, tag = "1")]
    pub max_blur_sigma: f32,
}
/// Example-based explainability that returns the nearest neighbors from the
/// provided dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Examples {
    /// The number of neighbors to return when querying for examples.
    #[prost(int32, tag = "3")]
    pub neighbor_count: i32,
    #[prost(oneof = "examples::Source", tags = "5")]
    pub source: ::core::option::Option<examples::Source>,
    #[prost(oneof = "examples::Config", tags = "2, 4")]
    pub config: ::core::option::Option<examples::Config>,
}
/// Nested message and enum types in `Examples`.
pub mod examples {
    /// The Cloud Storage input instances.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ExampleGcsSource {
        /// The format in which instances are given, if not specified, assume it's
        /// JSONL format. Currently only JSONL format is supported.
        #[prost(enumeration = "example_gcs_source::DataFormat", tag = "1")]
        pub data_format: i32,
        /// The Cloud Storage location for the input instances.
        #[prost(message, optional, tag = "2")]
        pub gcs_source: ::core::option::Option<super::GcsSource>,
    }
    /// Nested message and enum types in `ExampleGcsSource`.
    pub mod example_gcs_source {
        /// The format of the input example instances.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum DataFormat {
            /// Format unspecified, used when unset.
            Unspecified = 0,
            /// Examples are stored in JSONL files.
            Jsonl = 1,
        }
        impl DataFormat {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "DATA_FORMAT_UNSPECIFIED",
                    Self::Jsonl => "JSONL",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "DATA_FORMAT_UNSPECIFIED" => Some(Self::Unspecified),
                    "JSONL" => Some(Self::Jsonl),
                    _ => None,
                }
            }
        }
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Source {
        /// The Cloud Storage input instances.
        #[prost(message, tag = "5")]
        ExampleGcsSource(ExampleGcsSource),
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Config {
        /// The full configuration for the generated index, the semantics are the
        /// same as [metadata][google.cloud.aiplatform.v1.Index.metadata] and should
        /// match
        /// [NearestNeighborSearchConfig](<https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations-example-based#nearest-neighbor-search-config>).
        #[prost(message, tag = "2")]
        NearestNeighborSearchConfig(::prost_types::Value),
        /// Simplified preset configuration, which automatically sets configuration
        /// values based on the desired query speed-precision trade-off and modality.
        #[prost(message, tag = "4")]
        Presets(super::Presets),
    }
}
/// Preset configuration for example-based explanations
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct Presets {
    /// Preset option controlling parameters for speed-precision trade-off when
    /// querying for examples. If omitted, defaults to `PRECISE`.
    #[prost(enumeration = "presets::Query", optional, tag = "1")]
    pub query: ::core::option::Option<i32>,
    /// The modality of the uploaded model, which automatically configures the
    /// distance measurement and feature normalization for the underlying example
    /// index and queries. If your model does not precisely fit one of these types,
    /// it is okay to choose the closest type.
    #[prost(enumeration = "presets::Modality", tag = "2")]
    pub modality: i32,
}
/// Nested message and enum types in `Presets`.
pub mod presets {
    /// Preset option controlling parameters for query speed-precision trade-off
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Query {
        /// More precise neighbors as a trade-off against slower response.
        Precise = 0,
        /// Faster response as a trade-off against less precise neighbors.
        Fast = 1,
    }
    impl Query {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Precise => "PRECISE",
                Self::Fast => "FAST",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "PRECISE" => Some(Self::Precise),
                "FAST" => Some(Self::Fast),
                _ => None,
            }
        }
    }
    /// Preset option controlling parameters for different modalities
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Modality {
        /// Should not be set. Added as a recommended best practice for enums
        Unspecified = 0,
        /// IMAGE modality
        Image = 1,
        /// TEXT modality
        Text = 2,
        /// TABULAR modality
        Tabular = 3,
    }
    impl Modality {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "MODALITY_UNSPECIFIED",
                Self::Image => "IMAGE",
                Self::Text => "TEXT",
                Self::Tabular => "TABULAR",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "MODALITY_UNSPECIFIED" => Some(Self::Unspecified),
                "IMAGE" => Some(Self::Image),
                "TEXT" => Some(Self::Text),
                "TABULAR" => Some(Self::Tabular),
                _ => None,
            }
        }
    }
}
/// The [ExplanationSpec][google.cloud.aiplatform.v1.ExplanationSpec] entries
/// that can be overridden at [online
/// explanation][google.cloud.aiplatform.v1.PredictionService.Explain] time.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExplanationSpecOverride {
    /// The parameters to be overridden. Note that the
    /// attribution method cannot be changed. If not specified,
    /// no parameter is overridden.
    #[prost(message, optional, tag = "1")]
    pub parameters: ::core::option::Option<ExplanationParameters>,
    /// The metadata to be overridden. If not specified, no metadata is overridden.
    #[prost(message, optional, tag = "2")]
    pub metadata: ::core::option::Option<ExplanationMetadataOverride>,
    /// The example-based explanations parameter overrides.
    #[prost(message, optional, tag = "3")]
    pub examples_override: ::core::option::Option<ExamplesOverride>,
}
/// The [ExplanationMetadata][google.cloud.aiplatform.v1.ExplanationMetadata]
/// entries that can be overridden at [online
/// explanation][google.cloud.aiplatform.v1.PredictionService.Explain] time.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExplanationMetadataOverride {
    /// Required. Overrides the [input
    /// metadata][google.cloud.aiplatform.v1.ExplanationMetadata.inputs] of the
    /// features. The key is the name of the feature to be overridden. The keys
    /// specified here must exist in the input metadata to be overridden. If a
    /// feature is not specified here, the corresponding feature's input metadata
    /// is not overridden.
    #[prost(map = "string, message", tag = "1")]
    pub inputs: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        explanation_metadata_override::InputMetadataOverride,
    >,
}
/// Nested message and enum types in `ExplanationMetadataOverride`.
pub mod explanation_metadata_override {
    /// The [input
    /// metadata][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata]
    /// entries to be overridden.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct InputMetadataOverride {
        /// Baseline inputs for this feature.
        ///
        /// This overrides the `input_baseline` field of the
        /// [ExplanationMetadata.InputMetadata][google.cloud.aiplatform.v1.ExplanationMetadata.InputMetadata]
        /// object of the corresponding feature's input metadata. If it's not
        /// specified, the original baselines are not overridden.
        #[prost(message, repeated, tag = "1")]
        pub input_baselines: ::prost::alloc::vec::Vec<::prost_types::Value>,
    }
}
/// Overrides for example-based explanations.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExamplesOverride {
    /// The number of neighbors to return.
    #[prost(int32, tag = "1")]
    pub neighbor_count: i32,
    /// The number of neighbors to return that have the same crowding tag.
    #[prost(int32, tag = "2")]
    pub crowding_count: i32,
    /// Restrict the resulting nearest neighbors to respect these constraints.
    #[prost(message, repeated, tag = "3")]
    pub restrictions: ::prost::alloc::vec::Vec<ExamplesRestrictionsNamespace>,
    /// If true, return the embeddings instead of neighbors.
    #[prost(bool, tag = "4")]
    pub return_embeddings: bool,
    /// The format of the data being provided with each call.
    #[prost(enumeration = "examples_override::DataFormat", tag = "5")]
    pub data_format: i32,
}
/// Nested message and enum types in `ExamplesOverride`.
pub mod examples_override {
    /// Data format enum.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum DataFormat {
        /// Unspecified format. Must not be used.
        Unspecified = 0,
        /// Provided data is a set of model inputs.
        Instances = 1,
        /// Provided data is a set of embeddings.
        Embeddings = 2,
    }
    impl DataFormat {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "DATA_FORMAT_UNSPECIFIED",
                Self::Instances => "INSTANCES",
                Self::Embeddings => "EMBEDDINGS",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "DATA_FORMAT_UNSPECIFIED" => Some(Self::Unspecified),
                "INSTANCES" => Some(Self::Instances),
                "EMBEDDINGS" => Some(Self::Embeddings),
                _ => None,
            }
        }
    }
}
/// Restrictions namespace for example-based explanations overrides.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExamplesRestrictionsNamespace {
    /// The namespace name.
    #[prost(string, tag = "1")]
    pub namespace_name: ::prost::alloc::string::String,
    /// The list of allowed tags.
    #[prost(string, repeated, tag = "2")]
    pub allow: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// The list of deny tags.
    #[prost(string, repeated, tag = "3")]
    pub deny: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Describes the state of a job.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum JobState {
    /// The job state is unspecified.
    Unspecified = 0,
    /// The job has been just created or resumed and processing has not yet begun.
    Queued = 1,
    /// The service is preparing to run the job.
    Pending = 2,
    /// The job is in progress.
    Running = 3,
    /// The job completed successfully.
    Succeeded = 4,
    /// The job failed.
    Failed = 5,
    /// The job is being cancelled. From this state the job may only go to
    /// either `JOB_STATE_SUCCEEDED`, `JOB_STATE_FAILED` or `JOB_STATE_CANCELLED`.
    Cancelling = 6,
    /// The job has been cancelled.
    Cancelled = 7,
    /// The job has been stopped, and can be resumed.
    Paused = 8,
    /// The job has expired.
    Expired = 9,
    /// The job is being updated. Only jobs in the `RUNNING` state can be updated.
    /// After updating, the job goes back to the `RUNNING` state.
    Updating = 10,
    /// The job is partially succeeded, some results may be missing due to errors.
    PartiallySucceeded = 11,
}
impl JobState {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "JOB_STATE_UNSPECIFIED",
            Self::Queued => "JOB_STATE_QUEUED",
            Self::Pending => "JOB_STATE_PENDING",
            Self::Running => "JOB_STATE_RUNNING",
            Self::Succeeded => "JOB_STATE_SUCCEEDED",
            Self::Failed => "JOB_STATE_FAILED",
            Self::Cancelling => "JOB_STATE_CANCELLING",
            Self::Cancelled => "JOB_STATE_CANCELLED",
            Self::Paused => "JOB_STATE_PAUSED",
            Self::Expired => "JOB_STATE_EXPIRED",
            Self::Updating => "JOB_STATE_UPDATING",
            Self::PartiallySucceeded => "JOB_STATE_PARTIALLY_SUCCEEDED",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "JOB_STATE_UNSPECIFIED" => Some(Self::Unspecified),
            "JOB_STATE_QUEUED" => Some(Self::Queued),
            "JOB_STATE_PENDING" => Some(Self::Pending),
            "JOB_STATE_RUNNING" => Some(Self::Running),
            "JOB_STATE_SUCCEEDED" => Some(Self::Succeeded),
            "JOB_STATE_FAILED" => Some(Self::Failed),
            "JOB_STATE_CANCELLING" => Some(Self::Cancelling),
            "JOB_STATE_CANCELLED" => Some(Self::Cancelled),
            "JOB_STATE_PAUSED" => Some(Self::Paused),
            "JOB_STATE_EXPIRED" => Some(Self::Expired),
            "JOB_STATE_UPDATING" => Some(Self::Updating),
            "JOB_STATE_PARTIALLY_SUCCEEDED" => Some(Self::PartiallySucceeded),
            _ => None,
        }
    }
}
/// A ReservationAffinity can be used to configure a Vertex AI resource (e.g., a
/// DeployedModel) to draw its Compute Engine resources from a Shared
/// Reservation, or exclusively from on-demand capacity.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReservationAffinity {
    /// Required. Specifies the reservation affinity type.
    #[prost(enumeration = "reservation_affinity::Type", tag = "1")]
    pub reservation_affinity_type: i32,
    /// Optional. Corresponds to the label key of a reservation resource. To target
    /// a SPECIFIC_RESERVATION by name, use
    /// `compute.googleapis.com/reservation-name` as the key and specify the name
    /// of your reservation as its value.
    #[prost(string, tag = "2")]
    pub key: ::prost::alloc::string::String,
    /// Optional. Corresponds to the label values of a reservation resource. This
    /// must be the full resource name of the reservation.
    #[prost(string, repeated, tag = "3")]
    pub values: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Nested message and enum types in `ReservationAffinity`.
pub mod reservation_affinity {
    /// Identifies a type of reservation affinity.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Type {
        /// Default value. This should not be used.
        Unspecified = 0,
        /// Do not consume from any reserved capacity, only use on-demand.
        NoReservation = 1,
        /// Consume any reservation available, falling back to on-demand.
        AnyReservation = 2,
        /// Consume from a specific reservation. When chosen, the reservation
        /// must be identified via the `key` and `values` fields.
        SpecificReservation = 3,
    }
    impl Type {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "TYPE_UNSPECIFIED",
                Self::NoReservation => "NO_RESERVATION",
                Self::AnyReservation => "ANY_RESERVATION",
                Self::SpecificReservation => "SPECIFIC_RESERVATION",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "NO_RESERVATION" => Some(Self::NoReservation),
                "ANY_RESERVATION" => Some(Self::AnyReservation),
                "SPECIFIC_RESERVATION" => Some(Self::SpecificReservation),
                _ => None,
            }
        }
    }
}
/// Specification of a single machine.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MachineSpec {
    /// Immutable. The type of the machine.
    ///
    /// See the [list of machine types supported for
    /// prediction](<https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types>)
    ///
    /// See the [list of machine types supported for custom
    /// training](<https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types>).
    ///
    /// For [DeployedModel][google.cloud.aiplatform.v1.DeployedModel] this field is
    /// optional, and the default value is `n1-standard-2`. For
    /// [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob] or as
    /// part of [WorkerPoolSpec][google.cloud.aiplatform.v1.WorkerPoolSpec] this
    /// field is required.
    #[prost(string, tag = "1")]
    pub machine_type: ::prost::alloc::string::String,
    /// Immutable. The type of accelerator(s) that may be attached to the machine
    /// as per
    /// [accelerator_count][google.cloud.aiplatform.v1.MachineSpec.accelerator_count].
    #[prost(enumeration = "AcceleratorType", tag = "2")]
    pub accelerator_type: i32,
    /// The number of accelerators to attach to the machine.
    #[prost(int32, tag = "3")]
    pub accelerator_count: i32,
    /// Immutable. The topology of the TPUs. Corresponds to the TPU topologies
    /// available from GKE. (Example: tpu_topology: "2x2x1").
    #[prost(string, tag = "4")]
    pub tpu_topology: ::prost::alloc::string::String,
    /// Optional. Immutable. Configuration controlling how this resource pool
    /// consumes reservation.
    #[prost(message, optional, tag = "5")]
    pub reservation_affinity: ::core::option::Option<ReservationAffinity>,
}
/// A description of resources that are dedicated to a DeployedModel, and
/// that need a higher degree of manual configuration.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DedicatedResources {
    /// Required. Immutable. The specification of a single machine used by the
    /// prediction.
    #[prost(message, optional, tag = "1")]
    pub machine_spec: ::core::option::Option<MachineSpec>,
    /// Required. Immutable. The minimum number of machine replicas this
    /// DeployedModel will be always deployed on. This value must be greater than
    /// or equal to 1.
    ///
    /// If traffic against the DeployedModel increases, it may dynamically be
    /// deployed onto more replicas, and as traffic decreases, some of these extra
    /// replicas may be freed.
    #[prost(int32, tag = "2")]
    pub min_replica_count: i32,
    /// Immutable. The maximum number of replicas this DeployedModel may be
    /// deployed on when the traffic against it increases. If the requested value
    /// is too large, the deployment will error, but if deployment succeeds then
    /// the ability to scale the model to that many replicas is guaranteed (barring
    /// service outages). If traffic against the DeployedModel increases beyond
    /// what its replicas at maximum may handle, a portion of the traffic will be
    /// dropped. If this value is not provided, will use
    /// [min_replica_count][google.cloud.aiplatform.v1.DedicatedResources.min_replica_count]
    /// as the default value.
    ///
    /// The value of this field impacts the charge against Vertex CPU and GPU
    /// quotas. Specifically, you will be charged for (max_replica_count *
    /// number of cores in the selected machine type) and (max_replica_count *
    /// number of GPUs per replica in the selected machine type).
    #[prost(int32, tag = "3")]
    pub max_replica_count: i32,
    /// Optional. Number of required available replicas for the deployment to
    /// succeed. This field is only needed when partial model deployment/mutation
    /// is desired. If set, the model deploy/mutate operation will succeed once
    /// available_replica_count reaches required_replica_count, and the rest of
    /// the replicas will be retried. If not set, the default
    /// required_replica_count will be min_replica_count.
    #[prost(int32, tag = "9")]
    pub required_replica_count: i32,
    /// Immutable. The metric specifications that overrides a resource
    /// utilization metric (CPU utilization, accelerator's duty cycle, and so on)
    /// target value (default to 60 if not set). At most one entry is allowed per
    /// metric.
    ///
    /// If
    /// [machine_spec.accelerator_count][google.cloud.aiplatform.v1.MachineSpec.accelerator_count]
    /// is above 0, the autoscaling will be based on both CPU utilization and
    /// accelerator's duty cycle metrics and scale up when either metrics exceeds
    /// its target value while scale down if both metrics are under their target
    /// value. The default target value is 60 for both metrics.
    ///
    /// If
    /// [machine_spec.accelerator_count][google.cloud.aiplatform.v1.MachineSpec.accelerator_count]
    /// is 0, the autoscaling will be based on CPU utilization metric only with
    /// default target value 60 if not explicitly set.
    ///
    /// For example, in the case of Online Prediction, if you want to override
    /// target CPU utilization to 80, you should set
    /// [autoscaling_metric_specs.metric_name][google.cloud.aiplatform.v1.AutoscalingMetricSpec.metric_name]
    /// to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and
    /// [autoscaling_metric_specs.target][google.cloud.aiplatform.v1.AutoscalingMetricSpec.target]
    /// to `80`.
    #[prost(message, repeated, tag = "4")]
    pub autoscaling_metric_specs: ::prost::alloc::vec::Vec<AutoscalingMetricSpec>,
    /// Optional. If true, schedule the deployment workload on [spot
    /// VMs](<https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms>).
    #[prost(bool, tag = "5")]
    pub spot: bool,
}
/// A description of resources that to large degree are decided by Vertex AI,
/// and require only a modest additional configuration.
/// Each Model supporting these resources documents its specific guidelines.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct AutomaticResources {
    /// Immutable. The minimum number of replicas this DeployedModel will be always
    /// deployed on. If traffic against it increases, it may dynamically be
    /// deployed onto more replicas up to
    /// [max_replica_count][google.cloud.aiplatform.v1.AutomaticResources.max_replica_count],
    /// and as traffic decreases, some of these extra replicas may be freed. If the
    /// requested value is too large, the deployment will error.
    #[prost(int32, tag = "1")]
    pub min_replica_count: i32,
    /// Immutable. The maximum number of replicas this DeployedModel may be
    /// deployed on when the traffic against it increases. If the requested value
    /// is too large, the deployment will error, but if deployment succeeds then
    /// the ability to scale the model to that many replicas is guaranteed (barring
    /// service outages). If traffic against the DeployedModel increases beyond
    /// what its replicas at maximum may handle, a portion of the traffic will be
    /// dropped. If this value is not provided, a no upper bound for scaling under
    /// heavy traffic will be assume, though Vertex AI may be unable to scale
    /// beyond certain replica number.
    #[prost(int32, tag = "2")]
    pub max_replica_count: i32,
}
/// A description of resources that are used for performing batch operations, are
/// dedicated to a Model, and need manual configuration.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchDedicatedResources {
    /// Required. Immutable. The specification of a single machine.
    #[prost(message, optional, tag = "1")]
    pub machine_spec: ::core::option::Option<MachineSpec>,
    /// Immutable. The number of machine replicas used at the start of the batch
    /// operation. If not set, Vertex AI decides starting number, not greater than
    /// [max_replica_count][google.cloud.aiplatform.v1.BatchDedicatedResources.max_replica_count]
    #[prost(int32, tag = "2")]
    pub starting_replica_count: i32,
    /// Immutable. The maximum number of machine replicas the batch operation may
    /// be scaled to. The default value is 10.
    #[prost(int32, tag = "3")]
    pub max_replica_count: i32,
}
/// Statistics information about resource consumption.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ResourcesConsumed {
    /// Output only. The number of replica hours used. Note that many replicas may
    /// run in parallel, and additionally any given work may be queued for some
    /// time. Therefore this value is not strictly related to wall time.
    #[prost(double, tag = "1")]
    pub replica_hours: f64,
}
/// Represents the spec of disk options.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DiskSpec {
    /// Type of the boot disk (default is "pd-ssd").
    /// Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or
    /// "pd-standard" (Persistent Disk Hard Disk Drive).
    #[prost(string, tag = "1")]
    pub boot_disk_type: ::prost::alloc::string::String,
    /// Size in GB of the boot disk (default is 100GB).
    #[prost(int32, tag = "2")]
    pub boot_disk_size_gb: i32,
}
/// Represents the spec of [persistent
/// disk][<https://cloud.google.com/compute/docs/disks/persistent-disks]> options.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PersistentDiskSpec {
    /// Type of the disk (default is "pd-standard").
    /// Valid values: "pd-ssd" (Persistent Disk Solid State Drive)
    /// "pd-standard" (Persistent Disk Hard Disk Drive)
    /// "pd-balanced" (Balanced Persistent Disk)
    /// "pd-extreme" (Extreme Persistent Disk)
    #[prost(string, tag = "1")]
    pub disk_type: ::prost::alloc::string::String,
    /// Size in GB of the disk (default is 100GB).
    #[prost(int64, tag = "2")]
    pub disk_size_gb: i64,
}
/// Represents a mount configuration for Network File System (NFS) to mount.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NfsMount {
    /// Required. IP address of the NFS server.
    #[prost(string, tag = "1")]
    pub server: ::prost::alloc::string::String,
    /// Required. Source path exported from NFS server.
    /// Has to start with '/', and combined with the ip address, it indicates
    /// the source mount path in the form of `server:path`
    #[prost(string, tag = "2")]
    pub path: ::prost::alloc::string::String,
    /// Required. Destination mount path. The NFS will be mounted for the user
    /// under /mnt/nfs/<mount_point>
    #[prost(string, tag = "3")]
    pub mount_point: ::prost::alloc::string::String,
}
/// The metric specification that defines the target resource utilization
/// (CPU utilization, accelerator's duty cycle, and so on) for calculating the
/// desired replica count.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AutoscalingMetricSpec {
    /// Required. The resource metric name.
    /// Supported metrics:
    ///
    /// * For Online Prediction:
    /// * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle`
    /// * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
    #[prost(string, tag = "1")]
    pub metric_name: ::prost::alloc::string::String,
    /// The target resource utilization in percentage (1% - 100%) for the given
    /// metric; once the real usage deviates from the target by a certain
    /// percentage, the machine replicas change. The default value is 60
    /// (representing 60%) if not provided.
    #[prost(int32, tag = "2")]
    pub target: i32,
}
/// A set of Shielded Instance options.
/// See [Images using supported Shielded VM
/// features](<https://cloud.google.com/compute/docs/instances/modifying-shielded-vm>).
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ShieldedVmConfig {
    /// Defines whether the instance has [Secure
    /// Boot](<https://cloud.google.com/compute/shielded-vm/docs/shielded-vm#secure-boot>)
    /// enabled.
    ///
    /// Secure Boot helps ensure that the system only runs authentic software by
    /// verifying the digital signature of all boot components, and halting the
    /// boot process if signature verification fails.
    #[prost(bool, tag = "1")]
    pub enable_secure_boot: bool,
}
/// Manual batch tuning parameters.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ManualBatchTuningParameters {
    /// Immutable. The number of the records (e.g. instances) of the operation
    /// given in each batch to a machine replica. Machine type, and size of a
    /// single record should be considered when setting this parameter, higher
    /// value speeds up the batch operation's execution, but too high value will
    /// result in a whole batch not fitting in a machine's memory, and the whole
    /// operation will fail.
    /// The default value is 64.
    #[prost(int32, tag = "1")]
    pub batch_size: i32,
}
/// Points to a DeployedModel.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeployedModelRef {
    /// Immutable. A resource name of an Endpoint.
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// Immutable. An ID of a DeployedModel in the above Endpoint.
    #[prost(string, tag = "2")]
    pub deployed_model_id: ::prost::alloc::string::String,
}
/// Represents an environment variable present in a Container or Python Module.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EnvVar {
    /// Required. Name of the environment variable. Must be a valid C identifier.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. Variables that reference a $(VAR_NAME) are expanded
    /// using the previous defined environment variables in the container and
    /// any service environment variables. If a variable cannot be resolved,
    /// the reference in the input string will be unchanged. The $(VAR_NAME)
    /// syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped
    /// references will never be expanded, regardless of whether the variable
    /// exists or not.
    #[prost(string, tag = "2")]
    pub value: ::prost::alloc::string::String,
}
/// A trained machine learning Model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Model {
    /// The resource name of the Model.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. Immutable. The version ID of the model.
    /// A new version is committed when a new model version is uploaded or
    /// trained under an existing model id. It is an auto-incrementing decimal
    /// number in string representation.
    #[prost(string, tag = "28")]
    pub version_id: ::prost::alloc::string::String,
    /// User provided version aliases so that a model version can be referenced via
    /// alias (i.e.
    /// `projects/{project}/locations/{location}/models/{model_id}@{version_alias}`
    /// instead of auto-generated version id (i.e.
    /// `projects/{project}/locations/{location}/models/{model_id}@{version_id})`.
    /// The format is [a-z][a-zA-Z0-9-]{0,126}\[a-z0-9\] to distinguish from
    /// version_id. A default version alias will be created for the first version
    /// of the model, and there must be exactly one default version alias for a
    /// model.
    #[prost(string, repeated, tag = "29")]
    pub version_aliases: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Output only. Timestamp when this version was created.
    #[prost(message, optional, tag = "31")]
    pub version_create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this version was most recently updated.
    #[prost(message, optional, tag = "32")]
    pub version_update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Required. The display name of the Model.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// The description of the Model.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// The description of this version.
    #[prost(string, tag = "30")]
    pub version_description: ::prost::alloc::string::String,
    /// The default checkpoint id of a model version.
    #[prost(string, tag = "53")]
    pub default_checkpoint_id: ::prost::alloc::string::String,
    /// The schemata that describe formats of the Model's predictions and
    /// explanations as given and returned via
    /// [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict]
    /// and
    /// [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
    #[prost(message, optional, tag = "4")]
    pub predict_schemata: ::core::option::Option<PredictSchemata>,
    /// Immutable. Points to a YAML file stored on Google Cloud Storage describing
    /// additional information about the Model, that is specific to it. Unset if
    /// the Model does not have any additional information. The schema is defined
    /// as an OpenAPI 3.0.2 [Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    /// AutoML Models always have this field populated by Vertex AI, if no
    /// additional metadata is needed, this field is set to an empty string.
    /// Note: The URI given on output will be immutable and probably different,
    /// including the URI scheme, than the one given on input. The output URI will
    /// point to a location where the user only has a read access.
    #[prost(string, tag = "5")]
    pub metadata_schema_uri: ::prost::alloc::string::String,
    /// Immutable. An additional information about the Model; the schema of the
    /// metadata can be found in
    /// [metadata_schema][google.cloud.aiplatform.v1.Model.metadata_schema_uri].
    /// Unset if the Model does not have any additional information.
    #[prost(message, optional, tag = "6")]
    pub metadata: ::core::option::Option<::prost_types::Value>,
    /// Output only. The formats in which this Model may be exported. If empty,
    /// this Model is not available for export.
    #[prost(message, repeated, tag = "20")]
    pub supported_export_formats: ::prost::alloc::vec::Vec<model::ExportFormat>,
    /// Output only. The resource name of the TrainingPipeline that uploaded this
    /// Model, if any.
    #[prost(string, tag = "7")]
    pub training_pipeline: ::prost::alloc::string::String,
    /// Optional. This field is populated if the model is produced by a pipeline
    /// job.
    #[prost(string, tag = "47")]
    pub pipeline_job: ::prost::alloc::string::String,
    /// Input only. The specification of the container that is to be used when
    /// deploying this Model. The specification is ingested upon
    /// [ModelService.UploadModel][google.cloud.aiplatform.v1.ModelService.UploadModel],
    /// and all binaries it contains are copied and stored internally by Vertex AI.
    /// Not required for AutoML Models.
    #[prost(message, optional, tag = "9")]
    pub container_spec: ::core::option::Option<ModelContainerSpec>,
    /// Immutable. The path to the directory containing the Model artifact and any
    /// of its supporting files. Not required for AutoML Models.
    #[prost(string, tag = "26")]
    pub artifact_uri: ::prost::alloc::string::String,
    /// Output only. When this Model is deployed, its prediction resources are
    /// described by the `prediction_resources` field of the
    /// [Endpoint.deployed_models][google.cloud.aiplatform.v1.Endpoint.deployed_models]
    /// object. Because not all Models support all resource configuration types,
    /// the configuration types this Model supports are listed here. If no
    /// configuration types are listed, the Model cannot be deployed to an
    /// [Endpoint][google.cloud.aiplatform.v1.Endpoint] and does not support
    /// online predictions
    /// ([PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict]
    /// or
    /// [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain]).
    /// Such a Model can serve predictions by using a
    /// [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob], if it
    /// has at least one entry each in
    /// [supported_input_storage_formats][google.cloud.aiplatform.v1.Model.supported_input_storage_formats]
    /// and
    /// [supported_output_storage_formats][google.cloud.aiplatform.v1.Model.supported_output_storage_formats].
    #[prost(
        enumeration = "model::DeploymentResourcesType",
        repeated,
        packed = "false",
        tag = "10"
    )]
    pub supported_deployment_resources_types: ::prost::alloc::vec::Vec<i32>,
    /// Output only. The formats this Model supports in
    /// [BatchPredictionJob.input_config][google.cloud.aiplatform.v1.BatchPredictionJob.input_config].
    /// If
    /// [PredictSchemata.instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
    /// exists, the instances should be given as per that schema.
    ///
    /// The possible formats are:
    ///
    /// * `jsonl`
    /// The JSON Lines format, where each instance is a single line. Uses
    /// [GcsSource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.gcs_source].
    ///
    /// * `csv`
    /// The CSV format, where each instance is a single comma-separated line.
    /// The first line in the file is the header, containing comma-separated field
    /// names. Uses
    /// [GcsSource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.gcs_source].
    ///
    /// * `tf-record`
    /// The TFRecord format, where each instance is a single record in tfrecord
    /// syntax. Uses
    /// [GcsSource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.gcs_source].
    ///
    /// * `tf-record-gzip`
    /// Similar to `tf-record`, but the file is gzipped. Uses
    /// [GcsSource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.gcs_source].
    ///
    /// * `bigquery`
    /// Each instance is a single row in BigQuery. Uses
    /// [BigQuerySource][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.bigquery_source].
    ///
    /// * `file-list`
    /// Each line of the file is the location of an instance to process, uses
    /// `gcs_source` field of the
    /// [InputConfig][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig]
    /// object.
    ///
    ///
    /// If this Model doesn't support any of these formats it means it cannot be
    /// used with a
    /// [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
    /// However, if it has
    /// [supported_deployment_resources_types][google.cloud.aiplatform.v1.Model.supported_deployment_resources_types],
    /// it could serve online predictions by using
    /// [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict]
    /// or
    /// [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
    #[prost(string, repeated, tag = "11")]
    pub supported_input_storage_formats: ::prost::alloc::vec::Vec<
        ::prost::alloc::string::String,
    >,
    /// Output only. The formats this Model supports in
    /// [BatchPredictionJob.output_config][google.cloud.aiplatform.v1.BatchPredictionJob.output_config].
    /// If both
    /// [PredictSchemata.instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
    /// and
    /// [PredictSchemata.prediction_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.prediction_schema_uri]
    /// exist, the predictions are returned together with their instances. In other
    /// words, the prediction has the original instance data first, followed by the
    /// actual prediction content (as per the schema).
    ///
    /// The possible formats are:
    ///
    /// * `jsonl`
    /// The JSON Lines format, where each prediction is a single line. Uses
    /// [GcsDestination][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.gcs_destination].
    ///
    /// * `csv`
    /// The CSV format, where each prediction is a single comma-separated line.
    /// The first line in the file is the header, containing comma-separated field
    /// names. Uses
    /// [GcsDestination][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.gcs_destination].
    ///
    /// * `bigquery`
    /// Each prediction is a single row in a BigQuery table, uses
    /// [BigQueryDestination][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.bigquery_destination]
    /// .
    ///
    ///
    /// If this Model doesn't support any of these formats it means it cannot be
    /// used with a
    /// [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
    /// However, if it has
    /// [supported_deployment_resources_types][google.cloud.aiplatform.v1.Model.supported_deployment_resources_types],
    /// it could serve online predictions by using
    /// [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict]
    /// or
    /// [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
    #[prost(string, repeated, tag = "12")]
    pub supported_output_storage_formats: ::prost::alloc::vec::Vec<
        ::prost::alloc::string::String,
    >,
    /// Output only. Timestamp when this Model was uploaded into Vertex AI.
    #[prost(message, optional, tag = "13")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Model was most recently updated.
    #[prost(message, optional, tag = "14")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The pointers to DeployedModels created from this Model. Note
    /// that Model could have been deployed to Endpoints in different Locations.
    #[prost(message, repeated, tag = "15")]
    pub deployed_models: ::prost::alloc::vec::Vec<DeployedModelRef>,
    /// The default explanation specification for this Model.
    ///
    /// The Model can be used for
    /// [requesting
    /// explanation][google.cloud.aiplatform.v1.PredictionService.Explain] after
    /// being [deployed][google.cloud.aiplatform.v1.EndpointService.DeployModel] if
    /// it is populated. The Model can be used for [batch
    /// explanation][google.cloud.aiplatform.v1.BatchPredictionJob.generate_explanation]
    /// if it is populated.
    ///
    /// All fields of the explanation_spec can be overridden by
    /// [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
    /// of
    /// [DeployModelRequest.deployed_model][google.cloud.aiplatform.v1.DeployModelRequest.deployed_model],
    /// or
    /// [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
    /// of [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
    ///
    /// If the default explanation specification is not set for this Model, this
    /// Model can still be used for
    /// [requesting
    /// explanation][google.cloud.aiplatform.v1.PredictionService.Explain] by
    /// setting
    /// [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
    /// of
    /// [DeployModelRequest.deployed_model][google.cloud.aiplatform.v1.DeployModelRequest.deployed_model]
    /// and for [batch
    /// explanation][google.cloud.aiplatform.v1.BatchPredictionJob.generate_explanation]
    /// by setting
    /// [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
    /// of [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
    #[prost(message, optional, tag = "23")]
    pub explanation_spec: ::core::option::Option<ExplanationSpec>,
    /// Used to perform consistent read-modify-write updates. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "16")]
    pub etag: ::prost::alloc::string::String,
    /// The labels with user-defined metadata to organize your Models.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "17")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Stats of data used for training or evaluating the Model.
    ///
    /// Only populated when the Model is trained by a TrainingPipeline with
    /// [data_input_config][google.cloud.aiplatform.v1.TrainingPipeline.input_data_config].
    #[prost(message, optional, tag = "21")]
    pub data_stats: ::core::option::Option<model::DataStats>,
    /// Customer-managed encryption key spec for a Model. If set, this
    /// Model and all sub-resources of this Model will be secured by this key.
    #[prost(message, optional, tag = "24")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Output only. Source of a model. It can either be automl training pipeline,
    /// custom training pipeline, BigQuery ML, or saved and tuned from Genie or
    /// Model Garden.
    #[prost(message, optional, tag = "38")]
    pub model_source_info: ::core::option::Option<ModelSourceInfo>,
    /// Output only. If this Model is a copy of another Model, this contains info
    /// about the original.
    #[prost(message, optional, tag = "34")]
    pub original_model_info: ::core::option::Option<model::OriginalModelInfo>,
    /// Output only. The resource name of the Artifact that was created in
    /// MetadataStore when creating the Model. The Artifact resource name pattern
    /// is
    /// `projects/{project}/locations/{location}/metadataStores/{metadata_store}/artifacts/{artifact}`.
    #[prost(string, tag = "44")]
    pub metadata_artifact: ::prost::alloc::string::String,
    /// Optional. User input field to specify the base model source. Currently it
    /// only supports specifing the Model Garden models and Genie models.
    #[prost(message, optional, tag = "50")]
    pub base_model_source: ::core::option::Option<model::BaseModelSource>,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "51")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "52")]
    pub satisfies_pzi: bool,
}
/// Nested message and enum types in `Model`.
pub mod model {
    /// Represents export format supported by the Model.
    /// All formats export to Google Cloud Storage.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ExportFormat {
        /// Output only. The ID of the export format.
        /// The possible format IDs are:
        ///
        /// * `tflite`
        /// Used for Android mobile devices.
        ///
        /// * `edgetpu-tflite`
        /// Used for [Edge TPU](<https://cloud.google.com/edge-tpu/>) devices.
        ///
        /// * `tf-saved-model`
        /// A tensorflow model in SavedModel format.
        ///
        /// * `tf-js`
        /// A [TensorFlow.js](<https://www.tensorflow.org/js>) model that can be used
        /// in the browser and in Node.js using JavaScript.
        ///
        /// * `core-ml`
        /// Used for iOS mobile devices.
        ///
        /// * `custom-trained`
        /// A Model that was uploaded or trained by custom code.
        #[prost(string, tag = "1")]
        pub id: ::prost::alloc::string::String,
        /// Output only. The content of this Model that may be exported.
        #[prost(
            enumeration = "export_format::ExportableContent",
            repeated,
            packed = "false",
            tag = "2"
        )]
        pub exportable_contents: ::prost::alloc::vec::Vec<i32>,
    }
    /// Nested message and enum types in `ExportFormat`.
    pub mod export_format {
        /// The Model content that can be exported.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum ExportableContent {
            /// Should not be used.
            Unspecified = 0,
            /// Model artifact and any of its supported files. Will be exported to the
            /// location specified by the `artifactDestination` field of the
            /// [ExportModelRequest.output_config][google.cloud.aiplatform.v1.ExportModelRequest.output_config]
            /// object.
            Artifact = 1,
            /// The container image that is to be used when deploying this Model. Will
            /// be exported to the location specified by the `imageDestination` field
            /// of the
            /// [ExportModelRequest.output_config][google.cloud.aiplatform.v1.ExportModelRequest.output_config]
            /// object.
            Image = 2,
        }
        impl ExportableContent {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "EXPORTABLE_CONTENT_UNSPECIFIED",
                    Self::Artifact => "ARTIFACT",
                    Self::Image => "IMAGE",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "EXPORTABLE_CONTENT_UNSPECIFIED" => Some(Self::Unspecified),
                    "ARTIFACT" => Some(Self::Artifact),
                    "IMAGE" => Some(Self::Image),
                    _ => None,
                }
            }
        }
    }
    /// Stats of data used for train or evaluate the Model.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct DataStats {
        /// Number of DataItems that were used for training this Model.
        #[prost(int64, tag = "1")]
        pub training_data_items_count: i64,
        /// Number of DataItems that were used for validating this Model during
        /// training.
        #[prost(int64, tag = "2")]
        pub validation_data_items_count: i64,
        /// Number of DataItems that were used for evaluating this Model. If the
        /// Model is evaluated multiple times, this will be the number of test
        /// DataItems used by the first evaluation. If the Model is not evaluated,
        /// the number is 0.
        #[prost(int64, tag = "3")]
        pub test_data_items_count: i64,
        /// Number of Annotations that are used for training this Model.
        #[prost(int64, tag = "4")]
        pub training_annotations_count: i64,
        /// Number of Annotations that are used for validating this Model during
        /// training.
        #[prost(int64, tag = "5")]
        pub validation_annotations_count: i64,
        /// Number of Annotations that are used for evaluating this Model. If the
        /// Model is evaluated multiple times, this will be the number of test
        /// Annotations used by the first evaluation. If the Model is not evaluated,
        /// the number is 0.
        #[prost(int64, tag = "6")]
        pub test_annotations_count: i64,
    }
    /// Contains information about the original Model if this Model is a copy.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct OriginalModelInfo {
        /// Output only. The resource name of the Model this Model is a copy of,
        /// including the revision. Format:
        /// `projects/{project}/locations/{location}/models/{model_id}@{version_id}`
        #[prost(string, tag = "1")]
        pub model: ::prost::alloc::string::String,
    }
    /// User input field to specify the base model source. Currently it only
    /// supports specifing the Model Garden models and Genie models.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct BaseModelSource {
        #[prost(oneof = "base_model_source::Source", tags = "1, 2")]
        pub source: ::core::option::Option<base_model_source::Source>,
    }
    /// Nested message and enum types in `BaseModelSource`.
    pub mod base_model_source {
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum Source {
            /// Source information of Model Garden models.
            #[prost(message, tag = "1")]
            ModelGardenSource(super::super::ModelGardenSource),
            /// Information about the base model of Genie models.
            #[prost(message, tag = "2")]
            GenieSource(super::super::GenieSource),
        }
    }
    /// Identifies a type of Model's prediction resources.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum DeploymentResourcesType {
        /// Should not be used.
        Unspecified = 0,
        /// Resources that are dedicated to the
        /// [DeployedModel][google.cloud.aiplatform.v1.DeployedModel], and that need
        /// a higher degree of manual configuration.
        DedicatedResources = 1,
        /// Resources that to large degree are decided by Vertex AI, and require
        /// only a modest additional configuration.
        AutomaticResources = 2,
        /// Resources that can be shared by multiple
        /// [DeployedModels][google.cloud.aiplatform.v1.DeployedModel]. A
        /// pre-configured
        /// [DeploymentResourcePool][google.cloud.aiplatform.v1.DeploymentResourcePool]
        /// is required.
        SharedResources = 3,
    }
    impl DeploymentResourcesType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "DEPLOYMENT_RESOURCES_TYPE_UNSPECIFIED",
                Self::DedicatedResources => "DEDICATED_RESOURCES",
                Self::AutomaticResources => "AUTOMATIC_RESOURCES",
                Self::SharedResources => "SHARED_RESOURCES",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "DEPLOYMENT_RESOURCES_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "DEDICATED_RESOURCES" => Some(Self::DedicatedResources),
                "AUTOMATIC_RESOURCES" => Some(Self::AutomaticResources),
                "SHARED_RESOURCES" => Some(Self::SharedResources),
                _ => None,
            }
        }
    }
}
/// Contains information about the Large Model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct LargeModelReference {
    /// Required. The unique name of the large Foundation or pre-built model. Like
    /// "chat-bison", "text-bison". Or model name with version ID, like
    /// "chat-bison@001", "text-bison@005", etc.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Contains information about the source of the models generated from Model
/// Garden.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelGardenSource {
    /// Required. The model garden source model resource name.
    #[prost(string, tag = "1")]
    pub public_model_name: ::prost::alloc::string::String,
}
/// Contains information about the source of the models generated from Generative
/// AI Studio.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenieSource {
    /// Required. The public base model URI.
    #[prost(string, tag = "1")]
    pub base_model_uri: ::prost::alloc::string::String,
}
/// Contains the schemata used in Model's predictions and explanations via
/// [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict],
/// [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain]
/// and [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PredictSchemata {
    /// Immutable. Points to a YAML file stored on Google Cloud Storage describing
    /// the format of a single instance, which are used in
    /// [PredictRequest.instances][google.cloud.aiplatform.v1.PredictRequest.instances],
    /// [ExplainRequest.instances][google.cloud.aiplatform.v1.ExplainRequest.instances]
    /// and
    /// [BatchPredictionJob.input_config][google.cloud.aiplatform.v1.BatchPredictionJob.input_config].
    /// The schema is defined as an OpenAPI 3.0.2 [Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    /// AutoML Models always have this field populated by Vertex AI.
    /// Note: The URI given on output will be immutable and probably different,
    /// including the URI scheme, than the one given on input. The output URI will
    /// point to a location where the user only has a read access.
    #[prost(string, tag = "1")]
    pub instance_schema_uri: ::prost::alloc::string::String,
    /// Immutable. Points to a YAML file stored on Google Cloud Storage describing
    /// the parameters of prediction and explanation via
    /// [PredictRequest.parameters][google.cloud.aiplatform.v1.PredictRequest.parameters],
    /// [ExplainRequest.parameters][google.cloud.aiplatform.v1.ExplainRequest.parameters]
    /// and
    /// [BatchPredictionJob.model_parameters][google.cloud.aiplatform.v1.BatchPredictionJob.model_parameters].
    /// The schema is defined as an OpenAPI 3.0.2 [Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    /// AutoML Models always have this field populated by Vertex AI, if no
    /// parameters are supported, then it is set to an empty string.
    /// Note: The URI given on output will be immutable and probably different,
    /// including the URI scheme, than the one given on input. The output URI will
    /// point to a location where the user only has a read access.
    #[prost(string, tag = "2")]
    pub parameters_schema_uri: ::prost::alloc::string::String,
    /// Immutable. Points to a YAML file stored on Google Cloud Storage describing
    /// the format of a single prediction produced by this Model, which are
    /// returned via
    /// [PredictResponse.predictions][google.cloud.aiplatform.v1.PredictResponse.predictions],
    /// [ExplainResponse.explanations][google.cloud.aiplatform.v1.ExplainResponse.explanations],
    /// and
    /// [BatchPredictionJob.output_config][google.cloud.aiplatform.v1.BatchPredictionJob.output_config].
    /// The schema is defined as an OpenAPI 3.0.2 [Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    /// AutoML Models always have this field populated by Vertex AI.
    /// Note: The URI given on output will be immutable and probably different,
    /// including the URI scheme, than the one given on input. The output URI will
    /// point to a location where the user only has a read access.
    #[prost(string, tag = "3")]
    pub prediction_schema_uri: ::prost::alloc::string::String,
}
/// Specification of a container for serving predictions. Some fields in this
/// message correspond to fields in the [Kubernetes Container v1 core
/// specification](<https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core>).
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelContainerSpec {
    /// Required. Immutable. URI of the Docker image to be used as the custom
    /// container for serving predictions. This URI must identify an image in
    /// Artifact Registry or Container Registry. Learn more about the [container
    /// publishing
    /// requirements](<https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#publishing>),
    /// including permissions requirements for the Vertex AI Service Agent.
    ///
    /// The container image is ingested upon
    /// [ModelService.UploadModel][google.cloud.aiplatform.v1.ModelService.UploadModel],
    /// stored internally, and this original path is afterwards not used.
    ///
    /// To learn about the requirements for the Docker image itself, see
    /// [Custom container
    /// requirements](<https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#>).
    ///
    /// You can use the URI to one of Vertex AI's [pre-built container images for
    /// prediction](<https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers>)
    /// in this field.
    #[prost(string, tag = "1")]
    pub image_uri: ::prost::alloc::string::String,
    /// Immutable. Specifies the command that runs when the container starts. This
    /// overrides the container's
    /// [ENTRYPOINT](<https://docs.docker.com/engine/reference/builder/#entrypoint>).
    /// Specify this field as an array of executable and arguments, similar to a
    /// Docker `ENTRYPOINT`'s "exec" form, not its "shell" form.
    ///
    /// If you do not specify this field, then the container's `ENTRYPOINT` runs,
    /// in conjunction with the
    /// [args][google.cloud.aiplatform.v1.ModelContainerSpec.args] field or the
    /// container's [`CMD`](<https://docs.docker.com/engine/reference/builder/#cmd>),
    /// if either exists. If this field is not specified and the container does not
    /// have an `ENTRYPOINT`, then refer to the Docker documentation about [how
    /// `CMD` and `ENTRYPOINT`
    /// interact](<https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact>).
    ///
    /// If you specify this field, then you can also specify the `args` field to
    /// provide additional arguments for this command. However, if you specify this
    /// field, then the container's `CMD` is ignored. See the
    /// [Kubernetes documentation about how the
    /// `command` and `args` fields interact with a container's `ENTRYPOINT` and
    /// `CMD`](<https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes>).
    ///
    /// In this field, you can reference [environment variables set by Vertex
    /// AI](<https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables>)
    /// and environment variables set in the
    /// [env][google.cloud.aiplatform.v1.ModelContainerSpec.env] field. You cannot
    /// reference environment variables set in the Docker image. In order for
    /// environment variables to be expanded, reference them by using the following
    /// syntax: <code>$(<var>VARIABLE_NAME</var>)</code> Note that this differs
    /// from Bash variable expansion, which does not use parentheses. If a variable
    /// cannot be resolved, the reference in the input string is used unchanged. To
    /// avoid variable expansion, you can escape this syntax with `$$`; for
    /// example: <code>$$(<var>VARIABLE_NAME</var>)</code> This field corresponds
    /// to the `command` field of the Kubernetes Containers [v1 core
    /// API](<https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core>).
    #[prost(string, repeated, tag = "2")]
    pub command: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Immutable. Specifies arguments for the command that runs when the container
    /// starts. This overrides the container's
    /// [`CMD`](<https://docs.docker.com/engine/reference/builder/#cmd>). Specify
    /// this field as an array of executable and arguments, similar to a Docker
    /// `CMD`'s "default parameters" form.
    ///
    /// If you don't specify this field but do specify the
    /// [command][google.cloud.aiplatform.v1.ModelContainerSpec.command] field,
    /// then the command from the `command` field runs without any additional
    /// arguments. See the [Kubernetes documentation about how the `command` and
    /// `args` fields interact with a container's `ENTRYPOINT` and
    /// `CMD`](<https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes>).
    ///
    /// If you don't specify this field and don't specify the `command` field,
    /// then the container's
    /// [`ENTRYPOINT`](<https://docs.docker.com/engine/reference/builder/#cmd>) and
    /// `CMD` determine what runs based on their default behavior. See the Docker
    /// documentation about [how `CMD` and `ENTRYPOINT`
    /// interact](<https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact>).
    ///
    /// In this field, you can reference [environment variables
    /// set by Vertex
    /// AI](<https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables>)
    /// and environment variables set in the
    /// [env][google.cloud.aiplatform.v1.ModelContainerSpec.env] field. You cannot
    /// reference environment variables set in the Docker image. In order for
    /// environment variables to be expanded, reference them by using the following
    /// syntax: <code>$(<var>VARIABLE_NAME</var>)</code> Note that this differs
    /// from Bash variable expansion, which does not use parentheses. If a variable
    /// cannot be resolved, the reference in the input string is used unchanged. To
    /// avoid variable expansion, you can escape this syntax with `$$`; for
    /// example: <code>$$(<var>VARIABLE_NAME</var>)</code> This field corresponds
    /// to the `args` field of the Kubernetes Containers [v1 core
    /// API](<https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core>).
    #[prost(string, repeated, tag = "3")]
    pub args: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Immutable. List of environment variables to set in the container. After the
    /// container starts running, code running in the container can read these
    /// environment variables.
    ///
    /// Additionally, the
    /// [command][google.cloud.aiplatform.v1.ModelContainerSpec.command] and
    /// [args][google.cloud.aiplatform.v1.ModelContainerSpec.args] fields can
    /// reference these variables. Later entries in this list can also reference
    /// earlier entries. For example, the following example sets the variable
    /// `VAR_2` to have the value `foo bar`:
    ///
    /// ```json
    /// [
    ///    {
    ///      "name": "VAR_1",
    ///      "value": "foo"
    ///    },
    ///    {
    ///      "name": "VAR_2",
    ///      "value": "$(VAR_1) bar"
    ///    }
    /// ]
    /// ```
    ///
    /// If you switch the order of the variables in the example, then the expansion
    /// does not occur.
    ///
    /// This field corresponds to the `env` field of the Kubernetes Containers
    /// [v1 core
    /// API](<https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core>).
    #[prost(message, repeated, tag = "4")]
    pub env: ::prost::alloc::vec::Vec<EnvVar>,
    /// Immutable. List of ports to expose from the container. Vertex AI sends any
    /// prediction requests that it receives to the first port on this list. Vertex
    /// AI also sends
    /// [liveness and health
    /// checks](<https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#liveness>)
    /// to this port.
    ///
    /// If you do not specify this field, it defaults to following value:
    ///
    /// ```json
    /// [
    ///    {
    ///      "containerPort": 8080
    ///    }
    /// ]
    /// ```
    ///
    /// Vertex AI does not use ports other than the first one listed. This field
    /// corresponds to the `ports` field of the Kubernetes Containers
    /// [v1 core
    /// API](<https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core>).
    #[prost(message, repeated, tag = "5")]
    pub ports: ::prost::alloc::vec::Vec<Port>,
    /// Immutable. HTTP path on the container to send prediction requests to.
    /// Vertex AI forwards requests sent using
    /// [projects.locations.endpoints.predict][google.cloud.aiplatform.v1.PredictionService.Predict]
    /// to this path on the container's IP address and port. Vertex AI then returns
    /// the container's response in the API response.
    ///
    /// For example, if you set this field to `/foo`, then when Vertex AI
    /// receives a prediction request, it forwards the request body in a POST
    /// request to the `/foo` path on the port of your container specified by the
    /// first value of this `ModelContainerSpec`'s
    /// [ports][google.cloud.aiplatform.v1.ModelContainerSpec.ports] field.
    ///
    /// If you don't specify this field, it defaults to the following value when
    /// you [deploy this Model to an
    /// Endpoint][google.cloud.aiplatform.v1.EndpointService.DeployModel]:
    /// <code>/v1/endpoints/<var>ENDPOINT</var>/deployedModels/<var>DEPLOYED_MODEL</var>:predict</code>
    /// The placeholders in this value are replaced as follows:
    ///
    /// * <var>ENDPOINT</var>: The last segment (following `endpoints/`)of the
    ///    Endpoint.name][] field of the Endpoint where this Model has been
    ///    deployed. (Vertex AI makes this value available to your container code
    ///    as the [`AIP_ENDPOINT_ID` environment
    ///   variable](<https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables>).)
    ///
    /// * <var>DEPLOYED_MODEL</var>:
    /// [DeployedModel.id][google.cloud.aiplatform.v1.DeployedModel.id] of the
    /// `DeployedModel`.
    ///    (Vertex AI makes this value available to your container code
    ///    as the [`AIP_DEPLOYED_MODEL_ID` environment
    ///    variable](<https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables>).)
    #[prost(string, tag = "6")]
    pub predict_route: ::prost::alloc::string::String,
    /// Immutable. HTTP path on the container to send health checks to. Vertex AI
    /// intermittently sends GET requests to this path on the container's IP
    /// address and port to check that the container is healthy. Read more about
    /// [health
    /// checks](<https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health>).
    ///
    /// For example, if you set this field to `/bar`, then Vertex AI
    /// intermittently sends a GET request to the `/bar` path on the port of your
    /// container specified by the first value of this `ModelContainerSpec`'s
    /// [ports][google.cloud.aiplatform.v1.ModelContainerSpec.ports] field.
    ///
    /// If you don't specify this field, it defaults to the following value when
    /// you [deploy this Model to an
    /// Endpoint][google.cloud.aiplatform.v1.EndpointService.DeployModel]:
    /// <code>/v1/endpoints/<var>ENDPOINT</var>/deployedModels/<var>DEPLOYED_MODEL</var>:predict</code>
    /// The placeholders in this value are replaced as follows:
    ///
    /// * <var>ENDPOINT</var>: The last segment (following `endpoints/`)of the
    ///    Endpoint.name][] field of the Endpoint where this Model has been
    ///    deployed. (Vertex AI makes this value available to your container code
    ///    as the [`AIP_ENDPOINT_ID` environment
    ///    variable](<https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables>).)
    ///
    /// * <var>DEPLOYED_MODEL</var>:
    /// [DeployedModel.id][google.cloud.aiplatform.v1.DeployedModel.id] of the
    /// `DeployedModel`.
    ///    (Vertex AI makes this value available to your container code as the
    ///    [`AIP_DEPLOYED_MODEL_ID` environment
    ///    variable](<https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables>).)
    #[prost(string, tag = "7")]
    pub health_route: ::prost::alloc::string::String,
    /// Immutable. List of ports to expose from the container. Vertex AI sends gRPC
    /// prediction requests that it receives to the first port on this list. Vertex
    /// AI also sends liveness and health checks to this port.
    ///
    /// If you do not specify this field, gRPC requests to the container will be
    /// disabled.
    ///
    /// Vertex AI does not use ports other than the first one listed. This field
    /// corresponds to the `ports` field of the Kubernetes Containers v1 core API.
    #[prost(message, repeated, tag = "9")]
    pub grpc_ports: ::prost::alloc::vec::Vec<Port>,
    /// Immutable. Deployment timeout.
    /// Limit for deployment timeout is 2 hours.
    #[prost(message, optional, tag = "10")]
    pub deployment_timeout: ::core::option::Option<::prost_types::Duration>,
    /// Immutable. The amount of the VM memory to reserve as the shared memory for
    /// the model in megabytes.
    #[prost(int64, tag = "11")]
    pub shared_memory_size_mb: i64,
    /// Immutable. Specification for Kubernetes startup probe.
    #[prost(message, optional, tag = "12")]
    pub startup_probe: ::core::option::Option<Probe>,
    /// Immutable. Specification for Kubernetes readiness probe.
    #[prost(message, optional, tag = "13")]
    pub health_probe: ::core::option::Option<Probe>,
}
/// Represents a network port in a container.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct Port {
    /// The number of the port to expose on the pod's IP address.
    /// Must be a valid port number, between 1 and 65535 inclusive.
    #[prost(int32, tag = "3")]
    pub container_port: i32,
}
/// Detail description of the source information of the model.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ModelSourceInfo {
    /// Type of the model source.
    #[prost(enumeration = "model_source_info::ModelSourceType", tag = "1")]
    pub source_type: i32,
    /// If this Model is copy of another Model. If true then
    /// [source_type][google.cloud.aiplatform.v1.ModelSourceInfo.source_type]
    /// pertains to the original.
    #[prost(bool, tag = "2")]
    pub copy: bool,
}
/// Nested message and enum types in `ModelSourceInfo`.
pub mod model_source_info {
    /// Source of the model.
    /// Different from `objective` field, this `ModelSourceType` enum
    /// indicates the source from which the model was accessed or obtained,
    /// whereas the `objective` indicates the overall aim or function of this
    /// model.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum ModelSourceType {
        /// Should not be used.
        Unspecified = 0,
        /// The Model is uploaded by automl training pipeline.
        Automl = 1,
        /// The Model is uploaded by user or custom training pipeline.
        Custom = 2,
        /// The Model is registered and sync'ed from BigQuery ML.
        Bqml = 3,
        /// The Model is saved or tuned from Model Garden.
        ModelGarden = 4,
        /// The Model is saved or tuned from Genie.
        Genie = 5,
        /// The Model is uploaded by text embedding finetuning pipeline.
        CustomTextEmbedding = 6,
        /// The Model is saved or tuned from Marketplace.
        Marketplace = 7,
    }
    impl ModelSourceType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "MODEL_SOURCE_TYPE_UNSPECIFIED",
                Self::Automl => "AUTOML",
                Self::Custom => "CUSTOM",
                Self::Bqml => "BQML",
                Self::ModelGarden => "MODEL_GARDEN",
                Self::Genie => "GENIE",
                Self::CustomTextEmbedding => "CUSTOM_TEXT_EMBEDDING",
                Self::Marketplace => "MARKETPLACE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "MODEL_SOURCE_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "AUTOML" => Some(Self::Automl),
                "CUSTOM" => Some(Self::Custom),
                "BQML" => Some(Self::Bqml),
                "MODEL_GARDEN" => Some(Self::ModelGarden),
                "GENIE" => Some(Self::Genie),
                "CUSTOM_TEXT_EMBEDDING" => Some(Self::CustomTextEmbedding),
                "MARKETPLACE" => Some(Self::Marketplace),
                _ => None,
            }
        }
    }
}
/// Probe describes a health check to be performed against a container to
/// determine whether it is alive or ready to receive traffic.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Probe {
    /// How often (in seconds) to perform the probe. Default to 10 seconds.
    /// Minimum value is 1. Must be less than timeout_seconds.
    ///
    /// Maps to Kubernetes probe argument 'periodSeconds'.
    #[prost(int32, tag = "2")]
    pub period_seconds: i32,
    /// Number of seconds after which the probe times out. Defaults to 1 second.
    /// Minimum value is 1. Must be greater or equal to period_seconds.
    ///
    /// Maps to Kubernetes probe argument 'timeoutSeconds'.
    #[prost(int32, tag = "3")]
    pub timeout_seconds: i32,
    #[prost(oneof = "probe::ProbeType", tags = "1")]
    pub probe_type: ::core::option::Option<probe::ProbeType>,
}
/// Nested message and enum types in `Probe`.
pub mod probe {
    /// ExecAction specifies a command to execute.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ExecAction {
        /// Command is the command line to execute inside the container, the working
        /// directory for the command is root ('/') in the container's filesystem.
        /// The command is simply exec'd, it is not run inside a shell, so
        /// traditional shell instructions ('|', etc) won't work. To use a shell, you
        /// need to explicitly call out to that shell. Exit status of 0 is treated as
        /// live/healthy and non-zero is unhealthy.
        #[prost(string, repeated, tag = "1")]
        pub command: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum ProbeType {
        /// ExecAction probes the health of a container by executing a command.
        #[prost(message, tag = "1")]
        Exec(ExecAction),
    }
}
/// Contains model information necessary to perform batch prediction without
/// requiring a full model import.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UnmanagedContainerModel {
    /// The path to the directory containing the Model artifact and any of its
    /// supporting files.
    #[prost(string, tag = "1")]
    pub artifact_uri: ::prost::alloc::string::String,
    /// Contains the schemata used in Model's predictions and explanations
    #[prost(message, optional, tag = "2")]
    pub predict_schemata: ::core::option::Option<PredictSchemata>,
    /// Input only. The specification of the container that is to be used when
    /// deploying this Model.
    #[prost(message, optional, tag = "3")]
    pub container_spec: ::core::option::Option<ModelContainerSpec>,
}
/// A job that uses a
/// [Model][google.cloud.aiplatform.v1.BatchPredictionJob.model] to produce
/// predictions on multiple [input
/// instances][google.cloud.aiplatform.v1.BatchPredictionJob.input_config]. If
/// predictions for significant portion of the instances fail, the job may finish
/// without attempting predictions for all remaining instances.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchPredictionJob {
    /// Output only. Resource name of the BatchPredictionJob.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The user-defined name of this BatchPredictionJob.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// The name of the Model resource that produces the predictions via this job,
    /// must share the same ancestor Location.
    /// Starting this job has no impact on any existing deployments of the Model
    /// and their resources.
    /// Exactly one of model and unmanaged_container_model must be set.
    ///
    /// The model resource name may contain version id or version alias to specify
    /// the version.
    ///   Example: `projects/{project}/locations/{location}/models/{model}@2`
    ///               or
    ///             `projects/{project}/locations/{location}/models/{model}@golden`
    /// if no version is specified, the default version will be deployed.
    ///
    /// The model resource could also be a publisher model.
    ///   Example: `publishers/{publisher}/models/{model}`
    ///               or
    ///            `projects/{project}/locations/{location}/publishers/{publisher}/models/{model}`
    #[prost(string, tag = "3")]
    pub model: ::prost::alloc::string::String,
    /// Output only. The version ID of the Model that produces the predictions via
    /// this job.
    #[prost(string, tag = "30")]
    pub model_version_id: ::prost::alloc::string::String,
    /// Contains model information necessary to perform batch prediction without
    /// requiring uploading to model registry.
    /// Exactly one of model and unmanaged_container_model must be set.
    #[prost(message, optional, tag = "28")]
    pub unmanaged_container_model: ::core::option::Option<UnmanagedContainerModel>,
    /// Required. Input configuration of the instances on which predictions are
    /// performed. The schema of any single instance may be specified via the
    /// [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
    /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
    /// [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri].
    #[prost(message, optional, tag = "4")]
    pub input_config: ::core::option::Option<batch_prediction_job::InputConfig>,
    /// Configuration for how to convert batch prediction input instances to the
    /// prediction instances that are sent to the Model.
    #[prost(message, optional, tag = "27")]
    pub instance_config: ::core::option::Option<batch_prediction_job::InstanceConfig>,
    /// The parameters that govern the predictions. The schema of the parameters
    /// may be specified via the
    /// [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
    /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
    /// [parameters_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri].
    #[prost(message, optional, tag = "5")]
    pub model_parameters: ::core::option::Option<::prost_types::Value>,
    /// Required. The Configuration specifying where output predictions should
    /// be written.
    /// The schema of any single prediction may be specified as a concatenation
    /// of [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
    /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
    /// [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
    /// and
    /// [prediction_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.prediction_schema_uri].
    #[prost(message, optional, tag = "6")]
    pub output_config: ::core::option::Option<batch_prediction_job::OutputConfig>,
    /// The config of resources used by the Model during the batch prediction. If
    /// the Model
    /// [supports][google.cloud.aiplatform.v1.Model.supported_deployment_resources_types]
    /// DEDICATED_RESOURCES this config may be provided (and the job will use these
    /// resources), if the Model doesn't support AUTOMATIC_RESOURCES, this config
    /// must be provided.
    #[prost(message, optional, tag = "7")]
    pub dedicated_resources: ::core::option::Option<BatchDedicatedResources>,
    /// The service account that the DeployedModel's container runs as. If not
    /// specified, a system generated one will be used, which
    /// has minimal permissions and the custom container, if used, may not have
    /// enough permission to access other Google Cloud resources.
    ///
    /// Users deploying the Model must have the `iam.serviceAccounts.actAs`
    /// permission on this service account.
    #[prost(string, tag = "29")]
    pub service_account: ::prost::alloc::string::String,
    /// Immutable. Parameters configuring the batch behavior. Currently only
    /// applicable when
    /// [dedicated_resources][google.cloud.aiplatform.v1.BatchPredictionJob.dedicated_resources]
    /// are used (in other cases Vertex AI does the tuning itself).
    #[prost(message, optional, tag = "8")]
    pub manual_batch_tuning_parameters: ::core::option::Option<
        ManualBatchTuningParameters,
    >,
    /// Generate explanation with the batch prediction results.
    ///
    /// When set to `true`, the batch prediction output changes based on the
    /// `predictions_format` field of the
    /// [BatchPredictionJob.output_config][google.cloud.aiplatform.v1.BatchPredictionJob.output_config]
    /// object:
    ///
    ///   * `bigquery`: output includes a column named `explanation`. The value
    ///     is a struct that conforms to the
    ///     [Explanation][google.cloud.aiplatform.v1.Explanation] object.
    ///   * `jsonl`: The JSON objects on each line include an additional entry
    ///     keyed `explanation`. The value of the entry is a JSON object that
    ///     conforms to the [Explanation][google.cloud.aiplatform.v1.Explanation]
    ///     object.
    ///   * `csv`: Generating explanations for CSV format is not supported.
    ///
    /// If this field is set to true, either the
    /// [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec]
    /// or
    /// [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
    /// must be populated.
    #[prost(bool, tag = "23")]
    pub generate_explanation: bool,
    /// Explanation configuration for this BatchPredictionJob. Can be
    /// specified only if
    /// [generate_explanation][google.cloud.aiplatform.v1.BatchPredictionJob.generate_explanation]
    /// is set to `true`.
    ///
    /// This value overrides the value of
    /// [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec].
    /// All fields of
    /// [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
    /// are optional in the request. If a field of the
    /// [explanation_spec][google.cloud.aiplatform.v1.BatchPredictionJob.explanation_spec]
    /// object is not populated, the corresponding field of the
    /// [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec]
    /// object is inherited.
    #[prost(message, optional, tag = "25")]
    pub explanation_spec: ::core::option::Option<ExplanationSpec>,
    /// Output only. Information further describing the output of this job.
    #[prost(message, optional, tag = "9")]
    pub output_info: ::core::option::Option<batch_prediction_job::OutputInfo>,
    /// Output only. The detailed state of the job.
    #[prost(enumeration = "JobState", tag = "10")]
    pub state: i32,
    /// Output only. Only populated when the job's state is JOB_STATE_FAILED or
    /// JOB_STATE_CANCELLED.
    #[prost(message, optional, tag = "11")]
    pub error: ::core::option::Option<super::super::super::rpc::Status>,
    /// Output only. Partial failures encountered.
    /// For example, single files that can't be read.
    /// This field never exceeds 20 entries.
    /// Status details fields contain standard Google Cloud error details.
    #[prost(message, repeated, tag = "12")]
    pub partial_failures: ::prost::alloc::vec::Vec<super::super::super::rpc::Status>,
    /// Output only. Information about resources that had been consumed by this
    /// job. Provided in real time at best effort basis, as well as a final value
    /// once the job completes.
    ///
    /// Note: This field currently may be not populated for batch predictions that
    /// use AutoML Models.
    #[prost(message, optional, tag = "13")]
    pub resources_consumed: ::core::option::Option<ResourcesConsumed>,
    /// Output only. Statistics on completed and failed prediction instances.
    #[prost(message, optional, tag = "14")]
    pub completion_stats: ::core::option::Option<CompletionStats>,
    /// Output only. Time when the BatchPredictionJob was created.
    #[prost(message, optional, tag = "15")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the BatchPredictionJob for the first time entered
    /// the `JOB_STATE_RUNNING` state.
    #[prost(message, optional, tag = "16")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the BatchPredictionJob entered any of the following
    /// states: `JOB_STATE_SUCCEEDED`, `JOB_STATE_FAILED`, `JOB_STATE_CANCELLED`.
    #[prost(message, optional, tag = "17")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the BatchPredictionJob was most recently updated.
    #[prost(message, optional, tag = "18")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// The labels with user-defined metadata to organize BatchPredictionJobs.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "19")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Customer-managed encryption key options for a BatchPredictionJob. If this
    /// is set, then all resources created by the BatchPredictionJob will be
    /// encrypted with the provided encryption key.
    #[prost(message, optional, tag = "24")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// For custom-trained Models and AutoML Tabular Models, the container of the
    /// DeployedModel instances will send `stderr` and `stdout` streams to
    /// Cloud Logging by default. Please note that the logs incur cost,
    /// which are subject to [Cloud Logging
    /// pricing](<https://cloud.google.com/logging/pricing>).
    ///
    /// User can disable container logging by setting this flag to true.
    #[prost(bool, tag = "34")]
    pub disable_container_logging: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "36")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "37")]
    pub satisfies_pzi: bool,
}
/// Nested message and enum types in `BatchPredictionJob`.
pub mod batch_prediction_job {
    /// Configures the input to
    /// [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob]. See
    /// [Model.supported_input_storage_formats][google.cloud.aiplatform.v1.Model.supported_input_storage_formats]
    /// for Model's supported input formats, and how instances should be expressed
    /// via any of them.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct InputConfig {
        /// Required. The format in which instances are given, must be one of the
        /// [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
        /// [supported_input_storage_formats][google.cloud.aiplatform.v1.Model.supported_input_storage_formats].
        #[prost(string, tag = "1")]
        pub instances_format: ::prost::alloc::string::String,
        /// Required. The source of the input.
        #[prost(oneof = "input_config::Source", tags = "2, 3")]
        pub source: ::core::option::Option<input_config::Source>,
    }
    /// Nested message and enum types in `InputConfig`.
    pub mod input_config {
        /// Required. The source of the input.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum Source {
            /// The Cloud Storage location for the input instances.
            #[prost(message, tag = "2")]
            GcsSource(super::super::GcsSource),
            /// The BigQuery location of the input table.
            /// The schema of the table should be in the format described by the given
            /// context OpenAPI Schema, if one is provided. The table may contain
            /// additional columns that are not described by the schema, and they will
            /// be ignored.
            #[prost(message, tag = "3")]
            BigquerySource(super::super::BigQuerySource),
        }
    }
    /// Configuration defining how to transform batch prediction input instances to
    /// the instances that the Model accepts.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct InstanceConfig {
        /// The format of the instance that the Model accepts. Vertex AI will
        /// convert compatible
        /// [batch prediction input instance
        /// formats][google.cloud.aiplatform.v1.BatchPredictionJob.InputConfig.instances_format]
        /// to the specified format.
        ///
        /// Supported values are:
        ///
        /// * `object`: Each input is converted to JSON object format.
        ///      * For `bigquery`, each row is converted to an object.
        ///      * For `jsonl`, each line of the JSONL input must be an object.
        ///      * Does not apply to `csv`, `file-list`, `tf-record`, or
        ///        `tf-record-gzip`.
        ///
        /// * `array`: Each input is converted to JSON array format.
        ///      * For `bigquery`, each row is converted to an array. The order
        ///        of columns is determined by the BigQuery column order, unless
        ///        [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
        ///        is populated.
        ///        [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
        ///        must be populated for specifying field orders.
        ///      * For `jsonl`, if each line of the JSONL input is an object,
        ///        [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
        ///        must be populated for specifying field orders.
        ///      * Does not apply to `csv`, `file-list`, `tf-record`, or
        ///        `tf-record-gzip`.
        ///
        /// If not specified, Vertex AI converts the batch prediction input as
        /// follows:
        ///
        ///   * For `bigquery` and `csv`, the behavior is the same as `array`. The
        ///     order of columns is the same as defined in the file or table, unless
        ///     [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
        ///     is populated.
        ///   * For `jsonl`, the prediction instance format is determined by
        ///     each line of the input.
        ///   * For `tf-record`/`tf-record-gzip`, each record will be converted to
        ///     an object in the format of `{"b64": <value>}`, where `<value>` is
        ///     the Base64-encoded string of the content of the record.
        ///   * For `file-list`, each file in the list will be converted to an
        ///     object in the format of `{"b64": <value>}`, where `<value>` is
        ///     the Base64-encoded string of the content of the file.
        #[prost(string, tag = "1")]
        pub instance_type: ::prost::alloc::string::String,
        /// The name of the field that is considered as a key.
        ///
        /// The values identified by the key field is not included in the transformed
        /// instances that is sent to the Model. This is similar to
        /// specifying this name of the field in
        /// [excluded_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.excluded_fields].
        /// In addition, the batch prediction output will not include the instances.
        /// Instead the output will only include the value of the key field, in a
        /// field named `key` in the output:
        ///
        ///   * For `jsonl` output format, the output will have a `key` field
        ///     instead of the `instance` field.
        ///   * For `csv`/`bigquery` output format, the output will have have a `key`
        ///     column instead of the instance feature columns.
        ///
        /// The input must be JSONL with objects at each line, CSV, BigQuery
        /// or TfRecord.
        #[prost(string, tag = "2")]
        pub key_field: ::prost::alloc::string::String,
        /// Fields that will be included in the prediction instance that is
        /// sent to the Model.
        ///
        /// If
        /// [instance_type][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.instance_type]
        /// is `array`, the order of field names in included_fields also determines
        /// the order of the values in the array.
        ///
        /// When included_fields is populated,
        /// [excluded_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.excluded_fields]
        /// must be empty.
        ///
        /// The input must be JSONL with objects at each line, BigQuery
        /// or TfRecord.
        #[prost(string, repeated, tag = "3")]
        pub included_fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        /// Fields that will be excluded in the prediction instance that is
        /// sent to the Model.
        ///
        /// Excluded will be attached to the batch prediction output if
        /// [key_field][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.key_field]
        /// is not specified.
        ///
        /// When excluded_fields is populated,
        /// [included_fields][google.cloud.aiplatform.v1.BatchPredictionJob.InstanceConfig.included_fields]
        /// must be empty.
        ///
        /// The input must be JSONL with objects at each line, BigQuery
        /// or TfRecord.
        #[prost(string, repeated, tag = "4")]
        pub excluded_fields: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    }
    /// Configures the output of
    /// [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob]. See
    /// [Model.supported_output_storage_formats][google.cloud.aiplatform.v1.Model.supported_output_storage_formats]
    /// for supported output formats, and how predictions are expressed via any of
    /// them.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct OutputConfig {
        /// Required. The format in which Vertex AI gives the predictions, must be
        /// one of the [Model's][google.cloud.aiplatform.v1.BatchPredictionJob.model]
        /// [supported_output_storage_formats][google.cloud.aiplatform.v1.Model.supported_output_storage_formats].
        #[prost(string, tag = "1")]
        pub predictions_format: ::prost::alloc::string::String,
        /// Required. The destination of the output.
        #[prost(oneof = "output_config::Destination", tags = "2, 3")]
        pub destination: ::core::option::Option<output_config::Destination>,
    }
    /// Nested message and enum types in `OutputConfig`.
    pub mod output_config {
        /// Required. The destination of the output.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum Destination {
            /// The Cloud Storage location of the directory where the output is
            /// to be written to. In the given directory a new directory is created.
            /// Its name is `prediction-<model-display-name>-<job-create-time>`,
            /// where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
            /// Inside of it files `predictions_0001.<extension>`,
            /// `predictions_0002.<extension>`, ..., `predictions_N.<extension>`
            /// are created where `<extension>` depends on chosen
            /// [predictions_format][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.predictions_format],
            /// and N may equal 0001 and depends on the total number of successfully
            /// predicted instances. If the Model has both
            /// [instance][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
            /// and
            /// [prediction][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri]
            /// schemata defined then each such file contains predictions as per the
            /// [predictions_format][google.cloud.aiplatform.v1.BatchPredictionJob.OutputConfig.predictions_format].
            /// If prediction for any instance failed (partially or completely), then
            /// an additional `errors_0001.<extension>`, `errors_0002.<extension>`,...,
            /// `errors_N.<extension>` files are created (N depends on total number
            /// of failed predictions). These files contain the failed instances,
            /// as per their schema, followed by an additional `error` field which as
            /// value has [google.rpc.Status][google.rpc.Status]
            /// containing only `code` and `message` fields.
            #[prost(message, tag = "2")]
            GcsDestination(super::super::GcsDestination),
            /// The BigQuery project or dataset location where the output is to be
            /// written to. If project is provided, a new dataset is created with name
            /// `prediction_<model-display-name>_<job-create-time>`
            /// where <model-display-name> is made
            /// BigQuery-dataset-name compatible (for example, most special characters
            /// become underscores), and timestamp is in
            /// YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset
            /// two tables will be created, `predictions`, and `errors`.
            /// If the Model has both
            /// [instance][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
            /// and
            /// [prediction][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri]
            /// schemata defined then the tables have columns as follows: The
            /// `predictions` table contains instances for which the prediction
            /// succeeded, it has columns as per a concatenation of the Model's
            /// instance and prediction schemata. The `errors` table contains rows for
            /// which the prediction has failed, it has instance columns, as per the
            /// instance schema, followed by a single "errors" column, which as values
            /// has [google.rpc.Status][google.rpc.Status]
            /// represented as a STRUCT, and containing only `code` and `message`.
            #[prost(message, tag = "3")]
            BigqueryDestination(super::super::BigQueryDestination),
        }
    }
    /// Further describes this job's output.
    /// Supplements
    /// [output_config][google.cloud.aiplatform.v1.BatchPredictionJob.output_config].
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct OutputInfo {
        /// Output only. The name of the BigQuery table created, in
        /// `predictions_<timestamp>`
        /// format, into which the prediction output is written.
        /// Can be used by UI to generate the BigQuery output path, for example.
        #[prost(string, tag = "4")]
        pub bigquery_output_table: ::prost::alloc::string::String,
        /// The output location into which prediction output is written.
        #[prost(oneof = "output_info::OutputLocation", tags = "1, 2")]
        pub output_location: ::core::option::Option<output_info::OutputLocation>,
    }
    /// Nested message and enum types in `OutputInfo`.
    pub mod output_info {
        /// The output location into which prediction output is written.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum OutputLocation {
            /// Output only. The full path of the Cloud Storage directory created, into
            /// which the prediction output is written.
            #[prost(string, tag = "1")]
            GcsOutputDirectory(::prost::alloc::string::String),
            /// Output only. The path of the BigQuery dataset created, in
            /// `bq://projectId.bqDatasetId`
            /// format, into which the prediction output is written.
            #[prost(string, tag = "2")]
            BigqueryOutputDataset(::prost::alloc::string::String),
        }
    }
}
/// Schema is used to define the format of input/output data. Represents a select
/// subset of an [OpenAPI 3.0 schema
/// object](<https://spec.openapis.org/oas/v3.0.3#schema-object>). More fields may
/// be added in the future as needed.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Schema {
    /// Optional. The type of the data.
    #[prost(enumeration = "Type", tag = "1")]
    pub r#type: i32,
    /// Optional. The format of the data.
    /// Supported formats:
    ///   for NUMBER type: "float", "double"
    ///   for INTEGER type: "int32", "int64"
    ///   for STRING type: "email", "byte", etc
    #[prost(string, tag = "7")]
    pub format: ::prost::alloc::string::String,
    /// Optional. The title of the Schema.
    #[prost(string, tag = "24")]
    pub title: ::prost::alloc::string::String,
    /// Optional. The description of the data.
    #[prost(string, tag = "8")]
    pub description: ::prost::alloc::string::String,
    /// Optional. Indicates if the value may be null.
    #[prost(bool, tag = "6")]
    pub nullable: bool,
    /// Optional. Default value of the data.
    #[prost(message, optional, tag = "23")]
    pub default: ::core::option::Option<::prost_types::Value>,
    /// Optional. SCHEMA FIELDS FOR TYPE ARRAY
    /// Schema of the elements of Type.ARRAY.
    #[prost(message, optional, boxed, tag = "2")]
    pub items: ::core::option::Option<::prost::alloc::boxed::Box<Schema>>,
    /// Optional. Minimum number of the elements for Type.ARRAY.
    #[prost(int64, tag = "21")]
    pub min_items: i64,
    /// Optional. Maximum number of the elements for Type.ARRAY.
    #[prost(int64, tag = "22")]
    pub max_items: i64,
    /// Optional. Possible values of the element of primitive type with enum
    /// format. Examples:
    /// 1. We can define direction as :
    /// {type:STRING, format:enum, enum:\["EAST", NORTH", "SOUTH", "WEST"\]}
    /// 2. We can define apartment number as :
    /// {type:INTEGER, format:enum, enum:\["101", "201", "301"\]}
    #[prost(string, repeated, tag = "9")]
    pub r#enum: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. SCHEMA FIELDS FOR TYPE OBJECT
    /// Properties of Type.OBJECT.
    #[prost(map = "string, message", tag = "3")]
    pub properties: ::std::collections::HashMap<::prost::alloc::string::String, Schema>,
    /// Optional. The order of the properties.
    /// Not a standard field in open api spec. Only used to support the order of
    /// the properties.
    #[prost(string, repeated, tag = "25")]
    pub property_ordering: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. Required properties of Type.OBJECT.
    #[prost(string, repeated, tag = "5")]
    pub required: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. Minimum number of the properties for Type.OBJECT.
    #[prost(int64, tag = "14")]
    pub min_properties: i64,
    /// Optional. Maximum number of the properties for Type.OBJECT.
    #[prost(int64, tag = "15")]
    pub max_properties: i64,
    /// Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER
    /// Minimum value of the Type.INTEGER and Type.NUMBER
    #[prost(double, tag = "16")]
    pub minimum: f64,
    /// Optional. Maximum value of the Type.INTEGER and Type.NUMBER
    #[prost(double, tag = "17")]
    pub maximum: f64,
    /// Optional. SCHEMA FIELDS FOR TYPE STRING
    /// Minimum length of the Type.STRING
    #[prost(int64, tag = "18")]
    pub min_length: i64,
    /// Optional. Maximum length of the Type.STRING
    #[prost(int64, tag = "19")]
    pub max_length: i64,
    /// Optional. Pattern of the Type.STRING to restrict a string to a regular
    /// expression.
    #[prost(string, tag = "20")]
    pub pattern: ::prost::alloc::string::String,
    /// Optional. Example of the object. Will only populated when the object is the
    /// root.
    #[prost(message, optional, tag = "4")]
    pub example: ::core::option::Option<::prost_types::Value>,
    /// Optional. The value should be validated against any (one or more) of the
    /// subschemas in the list.
    #[prost(message, repeated, tag = "11")]
    pub any_of: ::prost::alloc::vec::Vec<Schema>,
}
/// Type contains the list of OpenAPI data types as defined by
/// <https://swagger.io/docs/specification/data-models/data-types/>
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum Type {
    /// Not specified, should not be used.
    Unspecified = 0,
    /// OpenAPI string type
    String = 1,
    /// OpenAPI number type
    Number = 2,
    /// OpenAPI integer type
    Integer = 3,
    /// OpenAPI boolean type
    Boolean = 4,
    /// OpenAPI array type
    Array = 5,
    /// OpenAPI object type
    Object = 6,
}
impl Type {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "TYPE_UNSPECIFIED",
            Self::String => "STRING",
            Self::Number => "NUMBER",
            Self::Integer => "INTEGER",
            Self::Boolean => "BOOLEAN",
            Self::Array => "ARRAY",
            Self::Object => "OBJECT",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "TYPE_UNSPECIFIED" => Some(Self::Unspecified),
            "STRING" => Some(Self::String),
            "NUMBER" => Some(Self::Number),
            "INTEGER" => Some(Self::Integer),
            "BOOLEAN" => Some(Self::Boolean),
            "ARRAY" => Some(Self::Array),
            "OBJECT" => Some(Self::Object),
            _ => None,
        }
    }
}
/// Tool details that the model may use to generate response.
///
/// A `Tool` is a piece of code that enables the system to interact with
/// external systems to perform an action, or set of actions, outside of
/// knowledge and scope of the model. A Tool object should contain exactly
/// one type of Tool (e.g FunctionDeclaration, Retrieval or
/// GoogleSearchRetrieval).
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Tool {
    /// Optional. Function tool type.
    /// One or more function declarations to be passed to the model along with the
    /// current user query. Model may decide to call a subset of these functions
    /// by populating [FunctionCall][content.part.function_call] in the response.
    /// User should provide a [FunctionResponse][content.part.function_response]
    /// for each function call in the next turn. Based on the function responses,
    /// Model will generate the final response back to the user.
    /// Maximum 128 function declarations can be provided.
    #[prost(message, repeated, tag = "1")]
    pub function_declarations: ::prost::alloc::vec::Vec<FunctionDeclaration>,
    /// Optional. Retrieval tool type.
    /// System will always execute the provided retrieval tool(s) to get external
    /// knowledge to answer the prompt. Retrieval results are presented to the
    /// model for generation.
    #[prost(message, optional, tag = "2")]
    pub retrieval: ::core::option::Option<Retrieval>,
    /// Optional. GoogleSearchRetrieval tool type.
    /// Specialized retrieval tool that is powered by Google search.
    #[prost(message, optional, tag = "3")]
    pub google_search_retrieval: ::core::option::Option<GoogleSearchRetrieval>,
    /// Optional. CodeExecution tool type.
    /// Enables the model to execute code as part of generation.
    /// This field is only used by the Gemini Developer API services.
    #[prost(message, optional, tag = "4")]
    pub code_execution: ::core::option::Option<tool::CodeExecution>,
}
/// Nested message and enum types in `Tool`.
pub mod tool {
    /// Tool that executes code generated by the model, and automatically returns
    /// the result to the model.
    ///
    /// See also \[ExecutableCode\]and \[CodeExecutionResult\] which are input and
    /// output to this tool.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct CodeExecution {}
}
/// Structured representation of a function declaration as defined by the
/// [OpenAPI 3.0 specification](<https://spec.openapis.org/oas/v3.0.3>). Included
/// in this declaration are the function name, description, parameters and
/// response type. This FunctionDeclaration is a representation of a block of
/// code that can be used as a `Tool` by the model and executed by the client.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FunctionDeclaration {
    /// Required. The name of the function to call.
    /// Must start with a letter or an underscore.
    /// Must be a-z, A-Z, 0-9, or contain underscores, dots and dashes, with a
    /// maximum length of 64.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. Description and purpose of the function.
    /// Model uses it to decide how and whether to call the function.
    #[prost(string, tag = "2")]
    pub description: ::prost::alloc::string::String,
    /// Optional. Describes the parameters to this function in JSON Schema Object
    /// format. Reflects the Open API 3.03 Parameter Object. string Key: the name
    /// of the parameter. Parameter names are case sensitive. Schema Value: the
    /// Schema defining the type used for the parameter. For function with no
    /// parameters, this can be left unset. Parameter names must start with a
    /// letter or an underscore and must only contain chars a-z, A-Z, 0-9, or
    /// underscores with a maximum length of 64. Example with 1 required and 1
    /// optional parameter: type: OBJECT properties:
    ///   param1:
    ///     type: STRING
    ///   param2:
    ///     type: INTEGER
    /// required:
    ///   - param1
    #[prost(message, optional, tag = "3")]
    pub parameters: ::core::option::Option<Schema>,
    /// Optional. Describes the output from this function in JSON Schema format.
    /// Reflects the Open API 3.03 Response Object. The Schema defines the type
    /// used for the response value of the function.
    #[prost(message, optional, tag = "4")]
    pub response: ::core::option::Option<Schema>,
}
/// A predicted \[FunctionCall\] returned from the model that contains a string
/// representing the \[FunctionDeclaration.name\] and a structured JSON object
/// containing the parameters and their values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FunctionCall {
    /// Required. The name of the function to call.
    /// Matches \[FunctionDeclaration.name\].
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. Required. The function parameters and values in JSON object
    /// format. See \[FunctionDeclaration.parameters\] for parameter details.
    #[prost(message, optional, tag = "2")]
    pub args: ::core::option::Option<::prost_types::Struct>,
}
/// The result output from a \[FunctionCall\] that contains a string representing
/// the \[FunctionDeclaration.name\] and a structured JSON object containing any
/// output from the function is used as context to the model. This should contain
/// the result of a \[FunctionCall\] made based on model prediction.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FunctionResponse {
    /// Required. The name of the function to call.
    /// Matches \[FunctionDeclaration.name\] and \[FunctionCall.name\].
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The function response in JSON object format.
    /// Use "output" key to specify function output and "error" key to specify
    /// error details (if any). If "output" and "error" keys are not specified,
    /// then whole "response" is treated as function output.
    #[prost(message, optional, tag = "2")]
    pub response: ::core::option::Option<::prost_types::Struct>,
}
/// Code generated by the model that is meant to be executed, and the result
/// returned to the model.
///
/// Generated when using the \[FunctionDeclaration\] tool and
/// \[FunctionCallingConfig\] mode is set to \[Mode.CODE\].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExecutableCode {
    /// Required. Programming language of the `code`.
    #[prost(enumeration = "executable_code::Language", tag = "1")]
    pub language: i32,
    /// Required. The code to be executed.
    #[prost(string, tag = "2")]
    pub code: ::prost::alloc::string::String,
}
/// Nested message and enum types in `ExecutableCode`.
pub mod executable_code {
    /// Supported programming languages for the generated code.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Language {
        /// Unspecified language. This value should not be used.
        Unspecified = 0,
        /// Python >= 3.10, with numpy and simpy available.
        Python = 1,
    }
    impl Language {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "LANGUAGE_UNSPECIFIED",
                Self::Python => "PYTHON",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "LANGUAGE_UNSPECIFIED" => Some(Self::Unspecified),
                "PYTHON" => Some(Self::Python),
                _ => None,
            }
        }
    }
}
/// Result of executing the \[ExecutableCode\].
///
/// Always follows a `part` containing the \[ExecutableCode\].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CodeExecutionResult {
    /// Required. Outcome of the code execution.
    #[prost(enumeration = "code_execution_result::Outcome", tag = "1")]
    pub outcome: i32,
    /// Optional. Contains stdout when code execution is successful, stderr or
    /// other description otherwise.
    #[prost(string, tag = "2")]
    pub output: ::prost::alloc::string::String,
}
/// Nested message and enum types in `CodeExecutionResult`.
pub mod code_execution_result {
    /// Enumeration of possible outcomes of the code execution.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Outcome {
        /// Unspecified status. This value should not be used.
        Unspecified = 0,
        /// Code execution completed successfully.
        Ok = 1,
        /// Code execution finished but with a failure. `stderr` should contain the
        /// reason.
        Failed = 2,
        /// Code execution ran for too long, and was cancelled. There may or may not
        /// be a partial output present.
        DeadlineExceeded = 3,
    }
    impl Outcome {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "OUTCOME_UNSPECIFIED",
                Self::Ok => "OUTCOME_OK",
                Self::Failed => "OUTCOME_FAILED",
                Self::DeadlineExceeded => "OUTCOME_DEADLINE_EXCEEDED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "OUTCOME_UNSPECIFIED" => Some(Self::Unspecified),
                "OUTCOME_OK" => Some(Self::Ok),
                "OUTCOME_FAILED" => Some(Self::Failed),
                "OUTCOME_DEADLINE_EXCEEDED" => Some(Self::DeadlineExceeded),
                _ => None,
            }
        }
    }
}
/// Defines a retrieval tool that model can call to access external knowledge.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Retrieval {
    /// Optional. Deprecated. This option is no longer supported.
    #[deprecated]
    #[prost(bool, tag = "3")]
    pub disable_attribution: bool,
    /// The source of the retrieval.
    #[prost(oneof = "retrieval::Source", tags = "2, 4")]
    pub source: ::core::option::Option<retrieval::Source>,
}
/// Nested message and enum types in `Retrieval`.
pub mod retrieval {
    /// The source of the retrieval.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Source {
        /// Set to use data source powered by Vertex AI Search.
        #[prost(message, tag = "2")]
        VertexAiSearch(super::VertexAiSearch),
        /// Set to use data source powered by Vertex RAG store.
        /// User data is uploaded via the VertexRagDataService.
        #[prost(message, tag = "4")]
        VertexRagStore(super::VertexRagStore),
    }
}
/// Retrieve from Vertex RAG Store for grounding.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct VertexRagStore {
    /// Optional. The representation of the rag source. It can be used to specify
    /// corpus only or ragfiles. Currently only support one corpus or multiple
    /// files from one corpus. In the future we may open up multiple corpora
    /// support.
    #[prost(message, repeated, tag = "4")]
    pub rag_resources: ::prost::alloc::vec::Vec<vertex_rag_store::RagResource>,
    /// Optional. Number of top k results to return from the selected corpora.
    #[deprecated]
    #[prost(int32, optional, tag = "2")]
    pub similarity_top_k: ::core::option::Option<i32>,
    /// Optional. Only return results with vector distance smaller than the
    /// threshold.
    #[deprecated]
    #[prost(double, optional, tag = "3")]
    pub vector_distance_threshold: ::core::option::Option<f64>,
    /// Optional. The retrieval config for the Rag query.
    #[prost(message, optional, tag = "6")]
    pub rag_retrieval_config: ::core::option::Option<RagRetrievalConfig>,
}
/// Nested message and enum types in `VertexRagStore`.
pub mod vertex_rag_store {
    /// The definition of the Rag resource.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct RagResource {
        /// Optional. RagCorpora resource name.
        /// Format:
        /// `projects/{project}/locations/{location}/ragCorpora/{rag_corpus}`
        #[prost(string, tag = "1")]
        pub rag_corpus: ::prost::alloc::string::String,
        /// Optional. rag_file_id. The files should be in the same rag_corpus set in
        /// rag_corpus field.
        #[prost(string, repeated, tag = "2")]
        pub rag_file_ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    }
}
/// Retrieve from Vertex AI Search datastore for grounding.
/// See <https://cloud.google.com/products/agent-builder>
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct VertexAiSearch {
    /// Required. Fully-qualified Vertex AI Search data store resource ID.
    /// Format:
    /// `projects/{project}/locations/{location}/collections/{collection}/dataStores/{dataStore}`
    #[prost(string, tag = "1")]
    pub datastore: ::prost::alloc::string::String,
}
/// Tool to retrieve public web data for grounding, powered by Google.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct GoogleSearchRetrieval {
    /// Specifies the dynamic retrieval configuration for the given source.
    #[prost(message, optional, tag = "2")]
    pub dynamic_retrieval_config: ::core::option::Option<DynamicRetrievalConfig>,
}
/// Describes the options to customize dynamic retrieval.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct DynamicRetrievalConfig {
    /// The mode of the predictor to be used in dynamic retrieval.
    #[prost(enumeration = "dynamic_retrieval_config::Mode", tag = "1")]
    pub mode: i32,
    /// Optional. The threshold to be used in dynamic retrieval.
    /// If not set, a system default value is used.
    #[prost(float, optional, tag = "2")]
    pub dynamic_threshold: ::core::option::Option<f32>,
}
/// Nested message and enum types in `DynamicRetrievalConfig`.
pub mod dynamic_retrieval_config {
    /// The mode of the predictor to be used in dynamic retrieval.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Mode {
        /// Always trigger retrieval.
        Unspecified = 0,
        /// Run retrieval only when system decides it is necessary.
        Dynamic = 1,
    }
    impl Mode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "MODE_UNSPECIFIED",
                Self::Dynamic => "MODE_DYNAMIC",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "MODE_DYNAMIC" => Some(Self::Dynamic),
                _ => None,
            }
        }
    }
}
/// Tool config. This config is shared for all tools provided in the request.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolConfig {
    /// Optional. Function calling config.
    #[prost(message, optional, tag = "1")]
    pub function_calling_config: ::core::option::Option<FunctionCallingConfig>,
    /// Optional. Retrieval config.
    #[prost(message, optional, tag = "2")]
    pub retrieval_config: ::core::option::Option<RetrievalConfig>,
}
/// Function calling config.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FunctionCallingConfig {
    /// Optional. Function calling mode.
    #[prost(enumeration = "function_calling_config::Mode", tag = "1")]
    pub mode: i32,
    /// Optional. Function names to call. Only set when the Mode is ANY. Function
    /// names should match \[FunctionDeclaration.name\]. With mode set to ANY, model
    /// will predict a function call from the set of function names provided.
    #[prost(string, repeated, tag = "2")]
    pub allowed_function_names: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Nested message and enum types in `FunctionCallingConfig`.
pub mod function_calling_config {
    /// Function calling mode.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Mode {
        /// Unspecified function calling mode. This value should not be used.
        Unspecified = 0,
        /// Default model behavior, model decides to predict either a function call
        /// or a natural language response.
        Auto = 1,
        /// Model is constrained to always predicting a function call only.
        /// If "allowed_function_names" are set, the predicted function call will be
        /// limited to any one of "allowed_function_names", else the predicted
        /// function call will be any one of the provided "function_declarations".
        Any = 2,
        /// Model will not predict any function call. Model behavior is same as when
        /// not passing any function declarations.
        None = 3,
    }
    impl Mode {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "MODE_UNSPECIFIED",
                Self::Auto => "AUTO",
                Self::Any => "ANY",
                Self::None => "NONE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "MODE_UNSPECIFIED" => Some(Self::Unspecified),
                "AUTO" => Some(Self::Auto),
                "ANY" => Some(Self::Any),
                "NONE" => Some(Self::None),
                _ => None,
            }
        }
    }
}
/// Retrieval config.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RetrievalConfig {
    /// The location of the user.
    #[prost(message, optional, tag = "1")]
    pub lat_lng: ::core::option::Option<super::super::super::r#type::LatLng>,
    /// The language code of the user.
    #[prost(string, optional, tag = "2")]
    pub language_code: ::core::option::Option<::prost::alloc::string::String>,
}
/// Specifies the context retrieval config.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RagRetrievalConfig {
    /// Optional. The number of contexts to retrieve.
    #[prost(int32, tag = "1")]
    pub top_k: i32,
    /// Optional. Config for filters.
    #[prost(message, optional, tag = "3")]
    pub filter: ::core::option::Option<rag_retrieval_config::Filter>,
}
/// Nested message and enum types in `RagRetrievalConfig`.
pub mod rag_retrieval_config {
    /// Config for filters.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Filter {
        /// Optional. String for metadata filtering.
        #[prost(string, tag = "2")]
        pub metadata_filter: ::prost::alloc::string::String,
        /// Filter contexts retrieved from the vector DB based on either vector
        /// distance or vector similarity.
        #[prost(oneof = "filter::VectorDbThreshold", tags = "3, 4")]
        pub vector_db_threshold: ::core::option::Option<filter::VectorDbThreshold>,
    }
    /// Nested message and enum types in `Filter`.
    pub mod filter {
        /// Filter contexts retrieved from the vector DB based on either vector
        /// distance or vector similarity.
        #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
        pub enum VectorDbThreshold {
            /// Optional. Only returns contexts with vector distance smaller than the
            /// threshold.
            #[prost(double, tag = "3")]
            VectorDistanceThreshold(f64),
            /// Optional. Only returns contexts with vector similarity larger than the
            /// threshold.
            #[prost(double, tag = "4")]
            VectorSimilarityThreshold(f64),
        }
    }
}
/// The base structured datatype containing multi-part content of a message.
///
/// A `Content` includes a `role` field designating the producer of the `Content`
/// and a `parts` field containing multi-part data that contains the content of
/// the message turn.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Content {
    /// Optional. The producer of the content. Must be either 'user' or 'model'.
    ///
    /// Useful to set for multi-turn conversations, otherwise can be left blank
    /// or unset.
    #[prost(string, tag = "1")]
    pub role: ::prost::alloc::string::String,
    /// Required. Ordered `Parts` that constitute a single message. Parts may have
    /// different IANA MIME types.
    #[prost(message, repeated, tag = "2")]
    pub parts: ::prost::alloc::vec::Vec<Part>,
}
/// A datatype containing media that is part of a multi-part `Content` message.
///
/// A `Part` consists of data which has an associated datatype. A `Part` can only
/// contain one of the accepted types in `Part.data`.
///
/// A `Part` must have a fixed IANA MIME type identifying the type and subtype
/// of the media if `inline_data` or `file_data` field is filled with raw bytes.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Part {
    #[prost(oneof = "part::Data", tags = "1, 2, 3, 5, 6, 8, 9")]
    pub data: ::core::option::Option<part::Data>,
    #[prost(oneof = "part::Metadata", tags = "4")]
    pub metadata: ::core::option::Option<part::Metadata>,
}
/// Nested message and enum types in `Part`.
pub mod part {
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Data {
        /// Optional. Text part (can be code).
        #[prost(string, tag = "1")]
        Text(::prost::alloc::string::String),
        /// Optional. Inlined bytes data.
        #[prost(message, tag = "2")]
        InlineData(super::Blob),
        /// Optional. URI based data.
        #[prost(message, tag = "3")]
        FileData(super::FileData),
        /// Optional. A predicted \[FunctionCall\] returned from the model that
        /// contains a string representing the \[FunctionDeclaration.name\] with the
        /// parameters and their values.
        #[prost(message, tag = "5")]
        FunctionCall(super::FunctionCall),
        /// Optional. The result output of a \[FunctionCall\] that contains a string
        /// representing the \[FunctionDeclaration.name\] and a structured JSON object
        /// containing any output from the function call. It is used as context to
        /// the model.
        #[prost(message, tag = "6")]
        FunctionResponse(super::FunctionResponse),
        /// Optional. Code generated by the model that is meant to be executed.
        #[prost(message, tag = "8")]
        ExecutableCode(super::ExecutableCode),
        /// Optional. Result of executing the \[ExecutableCode\].
        #[prost(message, tag = "9")]
        CodeExecutionResult(super::CodeExecutionResult),
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum Metadata {
        /// Optional. Video metadata. The metadata should only be specified while the
        /// video data is presented in inline_data or file_data.
        #[prost(message, tag = "4")]
        VideoMetadata(super::VideoMetadata),
    }
}
/// Content blob.
///
/// It's preferred to send as [text][google.cloud.aiplatform.v1.Part.text]
/// directly rather than raw bytes.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Blob {
    /// Required. The IANA standard MIME type of the source data.
    #[prost(string, tag = "1")]
    pub mime_type: ::prost::alloc::string::String,
    /// Required. Raw bytes.
    #[prost(bytes = "vec", tag = "2")]
    pub data: ::prost::alloc::vec::Vec<u8>,
}
/// URI based data.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FileData {
    /// Required. The IANA standard MIME type of the source data.
    #[prost(string, tag = "1")]
    pub mime_type: ::prost::alloc::string::String,
    /// Required. URI.
    #[prost(string, tag = "2")]
    pub file_uri: ::prost::alloc::string::String,
}
/// Metadata describes the input video content.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct VideoMetadata {
    /// Optional. The start offset of the video.
    #[prost(message, optional, tag = "1")]
    pub start_offset: ::core::option::Option<::prost_types::Duration>,
    /// Optional. The end offset of the video.
    #[prost(message, optional, tag = "2")]
    pub end_offset: ::core::option::Option<::prost_types::Duration>,
}
/// Generation config.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerationConfig {
    /// Optional. Controls the randomness of predictions.
    #[prost(float, optional, tag = "1")]
    pub temperature: ::core::option::Option<f32>,
    /// Optional. If specified, nucleus sampling will be used.
    #[prost(float, optional, tag = "2")]
    pub top_p: ::core::option::Option<f32>,
    /// Optional. If specified, top-k sampling will be used.
    #[prost(float, optional, tag = "3")]
    pub top_k: ::core::option::Option<f32>,
    /// Optional. Number of candidates to generate.
    #[prost(int32, optional, tag = "4")]
    pub candidate_count: ::core::option::Option<i32>,
    /// Optional. The maximum number of output tokens to generate per message.
    #[prost(int32, optional, tag = "5")]
    pub max_output_tokens: ::core::option::Option<i32>,
    /// Optional. Stop sequences.
    #[prost(string, repeated, tag = "6")]
    pub stop_sequences: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. If true, export the logprobs results in response.
    #[prost(bool, optional, tag = "18")]
    pub response_logprobs: ::core::option::Option<bool>,
    /// Optional. Logit probabilities.
    #[prost(int32, optional, tag = "7")]
    pub logprobs: ::core::option::Option<i32>,
    /// Optional. Positive penalties.
    #[prost(float, optional, tag = "8")]
    pub presence_penalty: ::core::option::Option<f32>,
    /// Optional. Frequency penalties.
    #[prost(float, optional, tag = "9")]
    pub frequency_penalty: ::core::option::Option<f32>,
    /// Optional. Seed.
    #[prost(int32, optional, tag = "12")]
    pub seed: ::core::option::Option<i32>,
    /// Optional. Output response mimetype of the generated candidate text.
    /// Supported mimetype:
    /// - `text/plain`: (default) Text output.
    /// - `application/json`: JSON response in the candidates.
    /// The model needs to be prompted to output the appropriate response type,
    /// otherwise the behavior is undefined.
    /// This is a preview feature.
    #[prost(string, tag = "13")]
    pub response_mime_type: ::prost::alloc::string::String,
    /// Optional. The `Schema` object allows the definition of input and output
    /// data types. These types can be objects, but also primitives and arrays.
    /// Represents a select subset of an [OpenAPI 3.0 schema
    /// object](<https://spec.openapis.org/oas/v3.0.3#schema>).
    /// If set, a compatible response_mime_type must also be set.
    /// Compatible mimetypes:
    /// `application/json`: Schema for JSON response.
    #[prost(message, optional, tag = "16")]
    pub response_schema: ::core::option::Option<Schema>,
    /// Optional. Routing configuration.
    #[prost(message, optional, tag = "17")]
    pub routing_config: ::core::option::Option<generation_config::RoutingConfig>,
}
/// Nested message and enum types in `GenerationConfig`.
pub mod generation_config {
    /// The configuration for routing the request to a specific model.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct RoutingConfig {
        /// Routing mode.
        #[prost(oneof = "routing_config::RoutingConfig", tags = "1, 2")]
        pub routing_config: ::core::option::Option<routing_config::RoutingConfig>,
    }
    /// Nested message and enum types in `RoutingConfig`.
    pub mod routing_config {
        /// When automated routing is specified, the routing will be determined by
        /// the pretrained routing model and customer provided model routing
        /// preference.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct AutoRoutingMode {
            /// The model routing preference.
            #[prost(
                enumeration = "auto_routing_mode::ModelRoutingPreference",
                optional,
                tag = "1"
            )]
            pub model_routing_preference: ::core::option::Option<i32>,
        }
        /// Nested message and enum types in `AutoRoutingMode`.
        pub mod auto_routing_mode {
            /// The model routing preference.
            #[derive(
                Clone,
                Copy,
                Debug,
                PartialEq,
                Eq,
                Hash,
                PartialOrd,
                Ord,
                ::prost::Enumeration
            )]
            #[repr(i32)]
            pub enum ModelRoutingPreference {
                /// Unspecified model routing preference.
                Unknown = 0,
                /// Prefer higher quality over low cost.
                PrioritizeQuality = 1,
                /// Balanced model routing preference.
                Balanced = 2,
                /// Prefer lower cost over higher quality.
                PrioritizeCost = 3,
            }
            impl ModelRoutingPreference {
                /// String value of the enum field names used in the ProtoBuf definition.
                ///
                /// The values are not transformed in any way and thus are considered stable
                /// (if the ProtoBuf definition does not change) and safe for programmatic use.
                pub fn as_str_name(&self) -> &'static str {
                    match self {
                        Self::Unknown => "UNKNOWN",
                        Self::PrioritizeQuality => "PRIORITIZE_QUALITY",
                        Self::Balanced => "BALANCED",
                        Self::PrioritizeCost => "PRIORITIZE_COST",
                    }
                }
                /// Creates an enum from field names used in the ProtoBuf definition.
                pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                    match value {
                        "UNKNOWN" => Some(Self::Unknown),
                        "PRIORITIZE_QUALITY" => Some(Self::PrioritizeQuality),
                        "BALANCED" => Some(Self::Balanced),
                        "PRIORITIZE_COST" => Some(Self::PrioritizeCost),
                        _ => None,
                    }
                }
            }
        }
        /// When manual routing is set, the specified model will be used directly.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct ManualRoutingMode {
            /// The model name to use. Only the public LLM models are accepted. e.g.
            /// 'gemini-1.5-pro-001'.
            #[prost(string, optional, tag = "1")]
            pub model_name: ::core::option::Option<::prost::alloc::string::String>,
        }
        /// Routing mode.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum RoutingConfig {
            /// Automated routing.
            #[prost(message, tag = "1")]
            AutoMode(AutoRoutingMode),
            /// Manual routing.
            #[prost(message, tag = "2")]
            ManualMode(ManualRoutingMode),
        }
    }
}
/// Safety settings.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetySetting {
    /// Required. Harm category.
    #[prost(enumeration = "HarmCategory", tag = "1")]
    pub category: i32,
    /// Required. The harm block threshold.
    #[prost(enumeration = "safety_setting::HarmBlockThreshold", tag = "2")]
    pub threshold: i32,
    /// Optional. Specify if the threshold is used for probability or severity
    /// score. If not specified, the threshold is used for probability score.
    #[prost(enumeration = "safety_setting::HarmBlockMethod", tag = "4")]
    pub method: i32,
}
/// Nested message and enum types in `SafetySetting`.
pub mod safety_setting {
    /// Probability based thresholds levels for blocking.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum HarmBlockThreshold {
        /// Unspecified harm block threshold.
        Unspecified = 0,
        /// Block low threshold and above (i.e. block more).
        BlockLowAndAbove = 1,
        /// Block medium threshold and above.
        BlockMediumAndAbove = 2,
        /// Block only high threshold (i.e. block less).
        BlockOnlyHigh = 3,
        /// Block none.
        BlockNone = 4,
        /// Turn off the safety filter.
        Off = 5,
    }
    impl HarmBlockThreshold {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "HARM_BLOCK_THRESHOLD_UNSPECIFIED",
                Self::BlockLowAndAbove => "BLOCK_LOW_AND_ABOVE",
                Self::BlockMediumAndAbove => "BLOCK_MEDIUM_AND_ABOVE",
                Self::BlockOnlyHigh => "BLOCK_ONLY_HIGH",
                Self::BlockNone => "BLOCK_NONE",
                Self::Off => "OFF",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "HARM_BLOCK_THRESHOLD_UNSPECIFIED" => Some(Self::Unspecified),
                "BLOCK_LOW_AND_ABOVE" => Some(Self::BlockLowAndAbove),
                "BLOCK_MEDIUM_AND_ABOVE" => Some(Self::BlockMediumAndAbove),
                "BLOCK_ONLY_HIGH" => Some(Self::BlockOnlyHigh),
                "BLOCK_NONE" => Some(Self::BlockNone),
                "OFF" => Some(Self::Off),
                _ => None,
            }
        }
    }
    /// Probability vs severity.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum HarmBlockMethod {
        /// The harm block method is unspecified.
        Unspecified = 0,
        /// The harm block method uses both probability and severity scores.
        Severity = 1,
        /// The harm block method uses the probability score.
        Probability = 2,
    }
    impl HarmBlockMethod {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "HARM_BLOCK_METHOD_UNSPECIFIED",
                Self::Severity => "SEVERITY",
                Self::Probability => "PROBABILITY",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "HARM_BLOCK_METHOD_UNSPECIFIED" => Some(Self::Unspecified),
                "SEVERITY" => Some(Self::Severity),
                "PROBABILITY" => Some(Self::Probability),
                _ => None,
            }
        }
    }
}
/// Safety rating corresponding to the generated content.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetyRating {
    /// Output only. Harm category.
    #[prost(enumeration = "HarmCategory", tag = "1")]
    pub category: i32,
    /// Output only. Harm probability levels in the content.
    #[prost(enumeration = "safety_rating::HarmProbability", tag = "2")]
    pub probability: i32,
    /// Output only. Harm probability score.
    #[prost(float, tag = "5")]
    pub probability_score: f32,
    /// Output only. Harm severity levels in the content.
    #[prost(enumeration = "safety_rating::HarmSeverity", tag = "6")]
    pub severity: i32,
    /// Output only. Harm severity score.
    #[prost(float, tag = "7")]
    pub severity_score: f32,
    /// Output only. Indicates whether the content was filtered out because of this
    /// rating.
    #[prost(bool, tag = "3")]
    pub blocked: bool,
}
/// Nested message and enum types in `SafetyRating`.
pub mod safety_rating {
    /// Harm probability levels in the content.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum HarmProbability {
        /// Harm probability unspecified.
        Unspecified = 0,
        /// Negligible level of harm.
        Negligible = 1,
        /// Low level of harm.
        Low = 2,
        /// Medium level of harm.
        Medium = 3,
        /// High level of harm.
        High = 4,
    }
    impl HarmProbability {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "HARM_PROBABILITY_UNSPECIFIED",
                Self::Negligible => "NEGLIGIBLE",
                Self::Low => "LOW",
                Self::Medium => "MEDIUM",
                Self::High => "HIGH",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "HARM_PROBABILITY_UNSPECIFIED" => Some(Self::Unspecified),
                "NEGLIGIBLE" => Some(Self::Negligible),
                "LOW" => Some(Self::Low),
                "MEDIUM" => Some(Self::Medium),
                "HIGH" => Some(Self::High),
                _ => None,
            }
        }
    }
    /// Harm severity levels.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum HarmSeverity {
        /// Harm severity unspecified.
        Unspecified = 0,
        /// Negligible level of harm severity.
        Negligible = 1,
        /// Low level of harm severity.
        Low = 2,
        /// Medium level of harm severity.
        Medium = 3,
        /// High level of harm severity.
        High = 4,
    }
    impl HarmSeverity {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "HARM_SEVERITY_UNSPECIFIED",
                Self::Negligible => "HARM_SEVERITY_NEGLIGIBLE",
                Self::Low => "HARM_SEVERITY_LOW",
                Self::Medium => "HARM_SEVERITY_MEDIUM",
                Self::High => "HARM_SEVERITY_HIGH",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "HARM_SEVERITY_UNSPECIFIED" => Some(Self::Unspecified),
                "HARM_SEVERITY_NEGLIGIBLE" => Some(Self::Negligible),
                "HARM_SEVERITY_LOW" => Some(Self::Low),
                "HARM_SEVERITY_MEDIUM" => Some(Self::Medium),
                "HARM_SEVERITY_HIGH" => Some(Self::High),
                _ => None,
            }
        }
    }
}
/// A collection of source attributions for a piece of content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CitationMetadata {
    /// Output only. List of citations.
    #[prost(message, repeated, tag = "1")]
    pub citations: ::prost::alloc::vec::Vec<Citation>,
}
/// Source attributions for content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Citation {
    /// Output only. Start index into the content.
    #[prost(int32, tag = "1")]
    pub start_index: i32,
    /// Output only. End index into the content.
    #[prost(int32, tag = "2")]
    pub end_index: i32,
    /// Output only. Url reference of the attribution.
    #[prost(string, tag = "3")]
    pub uri: ::prost::alloc::string::String,
    /// Output only. Title of the attribution.
    #[prost(string, tag = "4")]
    pub title: ::prost::alloc::string::String,
    /// Output only. License of the attribution.
    #[prost(string, tag = "5")]
    pub license: ::prost::alloc::string::String,
    /// Output only. Publication date of the attribution.
    #[prost(message, optional, tag = "6")]
    pub publication_date: ::core::option::Option<super::super::super::r#type::Date>,
}
/// A response candidate generated from the model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Candidate {
    /// Output only. Index of the candidate.
    #[prost(int32, tag = "1")]
    pub index: i32,
    /// Output only. Content parts of the candidate.
    #[prost(message, optional, tag = "2")]
    pub content: ::core::option::Option<Content>,
    /// Output only. Confidence score of the candidate.
    #[prost(double, tag = "8")]
    pub score: f64,
    /// Output only. Average log probability score of the candidate.
    #[prost(double, tag = "9")]
    pub avg_logprobs: f64,
    /// Output only. Log-likelihood scores for the response tokens and top tokens
    #[prost(message, optional, tag = "10")]
    pub logprobs_result: ::core::option::Option<LogprobsResult>,
    /// Output only. The reason why the model stopped generating tokens.
    /// If empty, the model has not stopped generating the tokens.
    #[prost(enumeration = "candidate::FinishReason", tag = "3")]
    pub finish_reason: i32,
    /// Output only. List of ratings for the safety of a response candidate.
    ///
    /// There is at most one rating per category.
    #[prost(message, repeated, tag = "4")]
    pub safety_ratings: ::prost::alloc::vec::Vec<SafetyRating>,
    /// Output only. Describes the reason the mode stopped generating tokens in
    /// more detail. This is only filled when `finish_reason` is set.
    #[prost(string, optional, tag = "5")]
    pub finish_message: ::core::option::Option<::prost::alloc::string::String>,
    /// Output only. Source attribution of the generated content.
    #[prost(message, optional, tag = "6")]
    pub citation_metadata: ::core::option::Option<CitationMetadata>,
    /// Output only. Metadata specifies sources used to ground generated content.
    #[prost(message, optional, tag = "7")]
    pub grounding_metadata: ::core::option::Option<GroundingMetadata>,
}
/// Nested message and enum types in `Candidate`.
pub mod candidate {
    /// The reason why the model stopped generating tokens.
    /// If empty, the model has not stopped generating the tokens.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum FinishReason {
        /// The finish reason is unspecified.
        Unspecified = 0,
        /// Token generation reached a natural stopping point or a configured stop
        /// sequence.
        Stop = 1,
        /// Token generation reached the configured maximum output tokens.
        MaxTokens = 2,
        /// Token generation stopped because the content potentially contains safety
        /// violations. NOTE: When streaming,
        /// [content][google.cloud.aiplatform.v1.Candidate.content] is empty if
        /// content filters blocks the output.
        Safety = 3,
        /// Token generation stopped because the content potentially contains
        /// copyright violations.
        Recitation = 4,
        /// All other reasons that stopped the token generation.
        Other = 5,
        /// Token generation stopped because the content contains forbidden terms.
        Blocklist = 6,
        /// Token generation stopped for potentially containing prohibited content.
        ProhibitedContent = 7,
        /// Token generation stopped because the content potentially contains
        /// Sensitive Personally Identifiable Information (SPII).
        Spii = 8,
        /// The function call generated by the model is invalid.
        MalformedFunctionCall = 9,
    }
    impl FinishReason {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "FINISH_REASON_UNSPECIFIED",
                Self::Stop => "STOP",
                Self::MaxTokens => "MAX_TOKENS",
                Self::Safety => "SAFETY",
                Self::Recitation => "RECITATION",
                Self::Other => "OTHER",
                Self::Blocklist => "BLOCKLIST",
                Self::ProhibitedContent => "PROHIBITED_CONTENT",
                Self::Spii => "SPII",
                Self::MalformedFunctionCall => "MALFORMED_FUNCTION_CALL",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "FINISH_REASON_UNSPECIFIED" => Some(Self::Unspecified),
                "STOP" => Some(Self::Stop),
                "MAX_TOKENS" => Some(Self::MaxTokens),
                "SAFETY" => Some(Self::Safety),
                "RECITATION" => Some(Self::Recitation),
                "OTHER" => Some(Self::Other),
                "BLOCKLIST" => Some(Self::Blocklist),
                "PROHIBITED_CONTENT" => Some(Self::ProhibitedContent),
                "SPII" => Some(Self::Spii),
                "MALFORMED_FUNCTION_CALL" => Some(Self::MalformedFunctionCall),
                _ => None,
            }
        }
    }
}
/// Logprobs Result
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct LogprobsResult {
    /// Length = total number of decoding steps.
    #[prost(message, repeated, tag = "1")]
    pub top_candidates: ::prost::alloc::vec::Vec<logprobs_result::TopCandidates>,
    /// Length = total number of decoding steps.
    /// The chosen candidates may or may not be in top_candidates.
    #[prost(message, repeated, tag = "2")]
    pub chosen_candidates: ::prost::alloc::vec::Vec<logprobs_result::Candidate>,
}
/// Nested message and enum types in `LogprobsResult`.
pub mod logprobs_result {
    /// Candidate for the logprobs token and score.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Candidate {
        /// The candidate’s token string value.
        #[prost(string, optional, tag = "1")]
        pub token: ::core::option::Option<::prost::alloc::string::String>,
        /// The candidate’s token id value.
        #[prost(int32, optional, tag = "3")]
        pub token_id: ::core::option::Option<i32>,
        /// The candidate's log probability.
        #[prost(float, optional, tag = "2")]
        pub log_probability: ::core::option::Option<f32>,
    }
    /// Candidates with top log probabilities at each decoding step.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct TopCandidates {
        /// Sorted by log probability in descending order.
        #[prost(message, repeated, tag = "1")]
        pub candidates: ::prost::alloc::vec::Vec<Candidate>,
    }
}
/// Segment of the content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Segment {
    /// Output only. The index of a Part object within its parent Content object.
    #[prost(int32, tag = "1")]
    pub part_index: i32,
    /// Output only. Start index in the given Part, measured in bytes. Offset from
    /// the start of the Part, inclusive, starting at zero.
    #[prost(int32, tag = "2")]
    pub start_index: i32,
    /// Output only. End index in the given Part, measured in bytes. Offset from
    /// the start of the Part, exclusive, starting at zero.
    #[prost(int32, tag = "3")]
    pub end_index: i32,
    /// Output only. The text corresponding to the segment from the response.
    #[prost(string, tag = "4")]
    pub text: ::prost::alloc::string::String,
}
/// Grounding chunk.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundingChunk {
    /// Chunk type.
    #[prost(oneof = "grounding_chunk::ChunkType", tags = "1, 2")]
    pub chunk_type: ::core::option::Option<grounding_chunk::ChunkType>,
}
/// Nested message and enum types in `GroundingChunk`.
pub mod grounding_chunk {
    /// Chunk from the web.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Web {
        /// URI reference of the chunk.
        #[prost(string, optional, tag = "1")]
        pub uri: ::core::option::Option<::prost::alloc::string::String>,
        /// Title of the chunk.
        #[prost(string, optional, tag = "2")]
        pub title: ::core::option::Option<::prost::alloc::string::String>,
    }
    /// Chunk from context retrieved by the retrieval tools.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct RetrievedContext {
        /// URI reference of the attribution.
        #[prost(string, optional, tag = "1")]
        pub uri: ::core::option::Option<::prost::alloc::string::String>,
        /// Title of the attribution.
        #[prost(string, optional, tag = "2")]
        pub title: ::core::option::Option<::prost::alloc::string::String>,
        /// Text of the attribution.
        #[prost(string, optional, tag = "3")]
        pub text: ::core::option::Option<::prost::alloc::string::String>,
    }
    /// Chunk type.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum ChunkType {
        /// Grounding chunk from the web.
        #[prost(message, tag = "1")]
        Web(Web),
        /// Grounding chunk from context retrieved by the retrieval tools.
        #[prost(message, tag = "2")]
        RetrievedContext(RetrievedContext),
    }
}
/// Grounding support.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundingSupport {
    /// Segment of the content this support belongs to.
    #[prost(message, optional, tag = "1")]
    pub segment: ::core::option::Option<Segment>,
    /// A list of indices (into 'grounding_chunk') specifying the
    /// citations associated with the claim. For instance \[1,3,4\] means
    /// that grounding_chunk\[1\], grounding_chunk\[3\],
    /// grounding_chunk\[4\] are the retrieved content attributed to the claim.
    #[prost(int32, repeated, tag = "2")]
    pub grounding_chunk_indices: ::prost::alloc::vec::Vec<i32>,
    /// Confidence score of the support references. Ranges from 0 to 1. 1 is the
    /// most confident. This list must have the same size as the
    /// grounding_chunk_indices.
    #[prost(float, repeated, tag = "3")]
    pub confidence_scores: ::prost::alloc::vec::Vec<f32>,
}
/// Metadata returned to client when grounding is enabled.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundingMetadata {
    /// Optional. Web search queries for the following-up web search.
    #[prost(string, repeated, tag = "1")]
    pub web_search_queries: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. Google search entry for the following-up web searches.
    #[prost(message, optional, tag = "4")]
    pub search_entry_point: ::core::option::Option<SearchEntryPoint>,
    /// List of supporting references retrieved from specified grounding source.
    #[prost(message, repeated, tag = "5")]
    pub grounding_chunks: ::prost::alloc::vec::Vec<GroundingChunk>,
    /// Optional. List of grounding support.
    #[prost(message, repeated, tag = "6")]
    pub grounding_supports: ::prost::alloc::vec::Vec<GroundingSupport>,
    /// Optional. Output only. Retrieval metadata.
    #[prost(message, optional, tag = "7")]
    pub retrieval_metadata: ::core::option::Option<RetrievalMetadata>,
}
/// Google search entry point.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchEntryPoint {
    /// Optional. Web content snippet that can be embedded in a web page or an app
    /// webview.
    #[prost(string, tag = "1")]
    pub rendered_content: ::prost::alloc::string::String,
    /// Optional. Base64 encoded JSON representing array of <search term, search
    /// url> tuple.
    #[prost(bytes = "vec", tag = "2")]
    pub sdk_blob: ::prost::alloc::vec::Vec<u8>,
}
/// Metadata related to retrieval in the grounding flow.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RetrievalMetadata {
    /// Optional. Score indicating how likely information from Google Search could
    /// help answer the prompt. The score is in the range `\[0, 1\]`, where 0 is the
    /// least likely and 1 is the most likely. This score is only populated when
    /// Google Search grounding and dynamic retrieval is enabled. It will be
    /// compared to the threshold to determine whether to trigger Google Search.
    #[prost(float, tag = "2")]
    pub google_search_dynamic_retrieval_score: f32,
}
/// Represents token counting info for a single modality.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ModalityTokenCount {
    /// The modality associated with this token count.
    #[prost(enumeration = "Modality", tag = "1")]
    pub modality: i32,
    /// Number of tokens.
    #[prost(int32, tag = "2")]
    pub token_count: i32,
}
/// Harm categories that will block the content.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum HarmCategory {
    /// The harm category is unspecified.
    Unspecified = 0,
    /// The harm category is hate speech.
    HateSpeech = 1,
    /// The harm category is dangerous content.
    DangerousContent = 2,
    /// The harm category is harassment.
    Harassment = 3,
    /// The harm category is sexually explicit content.
    SexuallyExplicit = 4,
    /// The harm category is civic integrity.
    CivicIntegrity = 5,
}
impl HarmCategory {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "HARM_CATEGORY_UNSPECIFIED",
            Self::HateSpeech => "HARM_CATEGORY_HATE_SPEECH",
            Self::DangerousContent => "HARM_CATEGORY_DANGEROUS_CONTENT",
            Self::Harassment => "HARM_CATEGORY_HARASSMENT",
            Self::SexuallyExplicit => "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            Self::CivicIntegrity => "HARM_CATEGORY_CIVIC_INTEGRITY",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "HARM_CATEGORY_UNSPECIFIED" => Some(Self::Unspecified),
            "HARM_CATEGORY_HATE_SPEECH" => Some(Self::HateSpeech),
            "HARM_CATEGORY_DANGEROUS_CONTENT" => Some(Self::DangerousContent),
            "HARM_CATEGORY_HARASSMENT" => Some(Self::Harassment),
            "HARM_CATEGORY_SEXUALLY_EXPLICIT" => Some(Self::SexuallyExplicit),
            "HARM_CATEGORY_CIVIC_INTEGRITY" => Some(Self::CivicIntegrity),
            _ => None,
        }
    }
}
/// Content Part modality
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum Modality {
    /// Unspecified modality.
    Unspecified = 0,
    /// Plain text.
    Text = 1,
    /// Image.
    Image = 2,
    /// Video.
    Video = 3,
    /// Audio.
    Audio = 4,
    /// Document, e.g. PDF.
    Document = 5,
}
impl Modality {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "MODALITY_UNSPECIFIED",
            Self::Text => "TEXT",
            Self::Image => "IMAGE",
            Self::Video => "VIDEO",
            Self::Audio => "AUDIO",
            Self::Document => "DOCUMENT",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "MODALITY_UNSPECIFIED" => Some(Self::Unspecified),
            "TEXT" => Some(Self::Text),
            "IMAGE" => Some(Self::Image),
            "VIDEO" => Some(Self::Video),
            "AUDIO" => Some(Self::Audio),
            "DOCUMENT" => Some(Self::Document),
            _ => None,
        }
    }
}
/// A resource used in LLM queries for users to explicitly specify what to cache
/// and how to cache.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CachedContent {
    /// Immutable. Identifier. The server-generated resource name of the cached
    /// content Format:
    /// projects/{project}/locations/{location}/cachedContents/{cached_content}
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. Immutable. The user-generated meaningful display name of the
    /// cached content.
    #[prost(string, tag = "11")]
    pub display_name: ::prost::alloc::string::String,
    /// Immutable. The name of the publisher model to use for cached content.
    /// Format:
    /// projects/{project}/locations/{location}/publishers/{publisher}/models/{model}
    #[prost(string, tag = "2")]
    pub model: ::prost::alloc::string::String,
    /// Optional. Input only. Immutable. Developer set system instruction.
    /// Currently, text only
    #[prost(message, optional, tag = "3")]
    pub system_instruction: ::core::option::Option<Content>,
    /// Optional. Input only. Immutable. The content to cache
    #[prost(message, repeated, tag = "4")]
    pub contents: ::prost::alloc::vec::Vec<Content>,
    /// Optional. Input only. Immutable. A list of `Tools` the model may use to
    /// generate the next response
    #[prost(message, repeated, tag = "5")]
    pub tools: ::prost::alloc::vec::Vec<Tool>,
    /// Optional. Input only. Immutable. Tool config. This config is shared for all
    /// tools
    #[prost(message, optional, tag = "6")]
    pub tool_config: ::core::option::Option<ToolConfig>,
    /// Output only. Creatation time of the cache entry.
    #[prost(message, optional, tag = "7")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. When the cache entry was last updated in UTC time.
    #[prost(message, optional, tag = "8")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Metadata on the usage of the cached content.
    #[prost(message, optional, tag = "12")]
    pub usage_metadata: ::core::option::Option<cached_content::UsageMetadata>,
    /// Expiration time of the cached content.
    #[prost(oneof = "cached_content::Expiration", tags = "9, 10")]
    pub expiration: ::core::option::Option<cached_content::Expiration>,
}
/// Nested message and enum types in `CachedContent`.
pub mod cached_content {
    /// Metadata on the usage of the cached content.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct UsageMetadata {
        /// Total number of tokens that the cached content consumes.
        #[prost(int32, tag = "1")]
        pub total_token_count: i32,
        /// Number of text characters.
        #[prost(int32, tag = "2")]
        pub text_count: i32,
        /// Number of images.
        #[prost(int32, tag = "3")]
        pub image_count: i32,
        /// Duration of video in seconds.
        #[prost(int32, tag = "4")]
        pub video_duration_seconds: i32,
        /// Duration of audio in seconds.
        #[prost(int32, tag = "5")]
        pub audio_duration_seconds: i32,
    }
    /// Expiration time of the cached content.
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum Expiration {
        /// Timestamp of when this resource is considered expired.
        /// This is *always* provided on output, regardless of what was sent
        /// on input.
        #[prost(message, tag = "9")]
        ExpireTime(::prost_types::Timestamp),
        /// Input only. The TTL for this resource. The expiration time is computed:
        /// now + TTL.
        #[prost(message, tag = "10")]
        Ttl(::prost_types::Duration),
    }
}
/// Instance of a general context.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Context {
    /// Immutable. The resource name of the Context.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// User provided display name of the Context.
    /// May be up to 128 Unicode characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// An eTag used to perform consistent read-modify-write updates. If not set, a
    /// blind "overwrite" update happens.
    #[prost(string, tag = "8")]
    pub etag: ::prost::alloc::string::String,
    /// The labels with user-defined metadata to organize your Contexts.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// No more than 64 user labels can be associated with one Context (System
    /// labels are excluded).
    #[prost(map = "string, string", tag = "9")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Output only. Timestamp when this Context was created.
    #[prost(message, optional, tag = "10")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Context was last updated.
    #[prost(message, optional, tag = "11")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. A list of resource names of Contexts that are parents of this
    /// Context. A Context may have at most 10 parent_contexts.
    #[prost(string, repeated, tag = "12")]
    pub parent_contexts: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// The title of the schema describing the metadata.
    ///
    /// Schema title and version is expected to be registered in earlier Create
    /// Schema calls. And both are used together as unique identifiers to identify
    /// schemas within the local metadata store.
    #[prost(string, tag = "13")]
    pub schema_title: ::prost::alloc::string::String,
    /// The version of the schema in schema_name to use.
    ///
    /// Schema title and version is expected to be registered in earlier Create
    /// Schema calls. And both are used together as unique identifiers to identify
    /// schemas within the local metadata store.
    #[prost(string, tag = "14")]
    pub schema_version: ::prost::alloc::string::String,
    /// Properties of the Context.
    /// Top level metadata keys' heading and trailing spaces will be trimmed.
    /// The size of this field should not exceed 200KB.
    #[prost(message, optional, tag = "15")]
    pub metadata: ::core::option::Option<::prost_types::Struct>,
    /// Description of the Context
    #[prost(string, tag = "16")]
    pub description: ::prost::alloc::string::String,
}
/// Represents a job that runs custom workloads such as a Docker container or a
/// Python package. A CustomJob can have multiple worker pools and each worker
/// pool can have its own machine and input spec. A CustomJob will be cleaned up
/// once the job enters terminal state (failed or succeeded).
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CustomJob {
    /// Output only. Resource name of a CustomJob.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The display name of the CustomJob.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Required. Job spec.
    #[prost(message, optional, tag = "4")]
    pub job_spec: ::core::option::Option<CustomJobSpec>,
    /// Output only. The detailed state of the job.
    #[prost(enumeration = "JobState", tag = "5")]
    pub state: i32,
    /// Output only. Time when the CustomJob was created.
    #[prost(message, optional, tag = "6")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the CustomJob for the first time entered the
    /// `JOB_STATE_RUNNING` state.
    #[prost(message, optional, tag = "7")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the CustomJob entered any of the following states:
    /// `JOB_STATE_SUCCEEDED`, `JOB_STATE_FAILED`, `JOB_STATE_CANCELLED`.
    #[prost(message, optional, tag = "8")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the CustomJob was most recently updated.
    #[prost(message, optional, tag = "9")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Only populated when job's state is `JOB_STATE_FAILED` or
    /// `JOB_STATE_CANCELLED`.
    #[prost(message, optional, tag = "10")]
    pub error: ::core::option::Option<super::super::super::rpc::Status>,
    /// The labels with user-defined metadata to organize CustomJobs.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "11")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Customer-managed encryption key options for a CustomJob. If this is set,
    /// then all resources created by the CustomJob will be encrypted with the
    /// provided encryption key.
    #[prost(message, optional, tag = "12")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Output only. URIs for accessing [interactive
    /// shells](<https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell>)
    /// (one URI for each training node). Only available if
    /// [job_spec.enable_web_access][google.cloud.aiplatform.v1.CustomJobSpec.enable_web_access]
    /// is `true`.
    ///
    /// The keys are names of each node in the training job; for example,
    /// `workerpool0-0` for the primary node, `workerpool1-0` for the first node in
    /// the second worker pool, and `workerpool1-1` for the second node in the
    /// second worker pool.
    ///
    /// The values are the URIs for each node's interactive shell.
    #[prost(map = "string, string", tag = "16")]
    pub web_access_uris: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "18")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "19")]
    pub satisfies_pzi: bool,
}
/// Represents the spec of a CustomJob.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CustomJobSpec {
    /// Optional. The ID of the PersistentResource in the same Project and Location
    /// which to run
    ///
    /// If this is specified, the job will be run on existing machines held by the
    /// PersistentResource instead of on-demand short-live machines.
    /// The network and CMEK configs on the job should be consistent with those on
    /// the PersistentResource, otherwise, the job will be rejected.
    #[prost(string, tag = "14")]
    pub persistent_resource_id: ::prost::alloc::string::String,
    /// Required. The spec of the worker pools including machine type and Docker
    /// image. All worker pools except the first one are optional and can be
    /// skipped by providing an empty value.
    #[prost(message, repeated, tag = "1")]
    pub worker_pool_specs: ::prost::alloc::vec::Vec<WorkerPoolSpec>,
    /// Scheduling options for a CustomJob.
    #[prost(message, optional, tag = "3")]
    pub scheduling: ::core::option::Option<Scheduling>,
    /// Specifies the service account for workload run-as account.
    /// Users submitting jobs must have act-as permission on this run-as account.
    /// If unspecified, the [Vertex AI Custom Code Service
    /// Agent](<https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents>)
    /// for the CustomJob's project is used.
    #[prost(string, tag = "4")]
    pub service_account: ::prost::alloc::string::String,
    /// Optional. The full name of the Compute Engine
    /// [network](/compute/docs/networks-and-firewalls#networks) to which the Job
    /// should be peered. For example, `projects/12345/global/networks/myVPC`.
    /// [Format](/compute/docs/reference/rest/v1/networks/insert)
    /// is of the form `projects/{project}/global/networks/{network}`.
    /// Where {project} is a project number, as in `12345`, and {network} is a
    /// network name.
    ///
    /// To specify this field, you must have already [configured VPC Network
    /// Peering for Vertex
    /// AI](<https://cloud.google.com/vertex-ai/docs/general/vpc-peering>).
    ///
    /// If this field is left unspecified, the job is not peered with any network.
    #[prost(string, tag = "5")]
    pub network: ::prost::alloc::string::String,
    /// Optional. A list of names for the reserved ip ranges under the VPC network
    /// that can be used for this job.
    ///
    /// If set, we will deploy the job within the provided ip ranges. Otherwise,
    /// the job will be deployed to any ip ranges under the provided VPC
    /// network.
    ///
    /// Example: \['vertex-ai-ip-range'\].
    #[prost(string, repeated, tag = "13")]
    pub reserved_ip_ranges: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// The Cloud Storage location to store the output of this CustomJob or
    /// HyperparameterTuningJob. For HyperparameterTuningJob,
    /// the baseOutputDirectory of
    /// each child CustomJob backing a Trial is set to a subdirectory of name
    /// [id][google.cloud.aiplatform.v1.Trial.id] under its parent
    /// HyperparameterTuningJob's baseOutputDirectory.
    ///
    /// The following Vertex AI environment variables will be passed to
    /// containers or python modules when this field is set:
    ///
    ///    For CustomJob:
    ///
    ///    * AIP_MODEL_DIR = `<base_output_directory>/model/`
    ///    * AIP_CHECKPOINT_DIR = `<base_output_directory>/checkpoints/`
    ///    * AIP_TENSORBOARD_LOG_DIR = `<base_output_directory>/logs/`
    ///
    ///    For CustomJob backing a Trial of HyperparameterTuningJob:
    ///
    ///    * AIP_MODEL_DIR = `<base_output_directory>/<trial_id>/model/`
    ///    * AIP_CHECKPOINT_DIR = `<base_output_directory>/<trial_id>/checkpoints/`
    ///    * AIP_TENSORBOARD_LOG_DIR = `<base_output_directory>/<trial_id>/logs/`
    #[prost(message, optional, tag = "6")]
    pub base_output_directory: ::core::option::Option<GcsDestination>,
    /// The ID of the location to store protected artifacts. e.g. us-central1.
    /// Populate only when the location is different than CustomJob location.
    /// List of supported locations:
    /// <https://cloud.google.com/vertex-ai/docs/general/locations>
    #[prost(string, tag = "19")]
    pub protected_artifact_location_id: ::prost::alloc::string::String,
    /// Optional. The name of a Vertex AI
    /// [Tensorboard][google.cloud.aiplatform.v1.Tensorboard] resource to which
    /// this CustomJob will upload Tensorboard logs. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
    #[prost(string, tag = "7")]
    pub tensorboard: ::prost::alloc::string::String,
    /// Optional. Whether you want Vertex AI to enable [interactive shell
    /// access](<https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell>)
    /// to training containers.
    ///
    /// If set to `true`, you can access interactive shells at the URIs given
    /// by
    /// [CustomJob.web_access_uris][google.cloud.aiplatform.v1.CustomJob.web_access_uris]
    /// or
    /// [Trial.web_access_uris][google.cloud.aiplatform.v1.Trial.web_access_uris]
    /// (within
    /// [HyperparameterTuningJob.trials][google.cloud.aiplatform.v1.HyperparameterTuningJob.trials]).
    #[prost(bool, tag = "10")]
    pub enable_web_access: bool,
    /// Optional. Whether you want Vertex AI to enable access to the customized
    /// dashboard in training chief container.
    ///
    /// If set to `true`, you can access the dashboard at the URIs given
    /// by
    /// [CustomJob.web_access_uris][google.cloud.aiplatform.v1.CustomJob.web_access_uris]
    /// or
    /// [Trial.web_access_uris][google.cloud.aiplatform.v1.Trial.web_access_uris]
    /// (within
    /// [HyperparameterTuningJob.trials][google.cloud.aiplatform.v1.HyperparameterTuningJob.trials]).
    #[prost(bool, tag = "16")]
    pub enable_dashboard_access: bool,
    /// Optional. The Experiment associated with this job.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadataStores}/contexts/{experiment-name}`
    #[prost(string, tag = "17")]
    pub experiment: ::prost::alloc::string::String,
    /// Optional. The Experiment Run associated with this job.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadataStores}/contexts/{experiment-name}-{experiment-run-name}`
    #[prost(string, tag = "18")]
    pub experiment_run: ::prost::alloc::string::String,
    /// Optional. The name of the Model resources for which to generate a mapping
    /// to artifact URIs. Applicable only to some of the Google-provided custom
    /// jobs. Format: `projects/{project}/locations/{location}/models/{model}`
    ///
    /// In order to retrieve a specific version of the model, also provide
    /// the version ID or version alias.
    ///    Example: `projects/{project}/locations/{location}/models/{model}@2`
    ///               or
    ///             `projects/{project}/locations/{location}/models/{model}@golden`
    /// If no version ID or alias is specified, the "default" version will be
    /// returned. The "default" version alias is created for the first version of
    /// the model, and can be moved to other versions later on. There will be
    /// exactly one default version.
    #[prost(string, repeated, tag = "20")]
    pub models: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Represents the spec of a worker pool in a job.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct WorkerPoolSpec {
    /// Optional. Immutable. The specification of a single machine.
    #[prost(message, optional, tag = "1")]
    pub machine_spec: ::core::option::Option<MachineSpec>,
    /// Optional. The number of worker replicas to use for this worker pool.
    #[prost(int64, tag = "2")]
    pub replica_count: i64,
    /// Optional. List of NFS mount spec.
    #[prost(message, repeated, tag = "4")]
    pub nfs_mounts: ::prost::alloc::vec::Vec<NfsMount>,
    /// Disk spec.
    #[prost(message, optional, tag = "5")]
    pub disk_spec: ::core::option::Option<DiskSpec>,
    /// The custom task to be executed in this worker pool.
    #[prost(oneof = "worker_pool_spec::Task", tags = "6, 7")]
    pub task: ::core::option::Option<worker_pool_spec::Task>,
}
/// Nested message and enum types in `WorkerPoolSpec`.
pub mod worker_pool_spec {
    /// The custom task to be executed in this worker pool.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Task {
        /// The custom container task.
        #[prost(message, tag = "6")]
        ContainerSpec(super::ContainerSpec),
        /// The Python packaged task.
        #[prost(message, tag = "7")]
        PythonPackageSpec(super::PythonPackageSpec),
    }
}
/// The spec of a Container.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ContainerSpec {
    /// Required. The URI of a container image in the Container Registry that is to
    /// be run on each worker replica.
    #[prost(string, tag = "1")]
    pub image_uri: ::prost::alloc::string::String,
    /// The command to be invoked when the container is started.
    /// It overrides the entrypoint instruction in Dockerfile when provided.
    #[prost(string, repeated, tag = "2")]
    pub command: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// The arguments to be passed when starting the container.
    #[prost(string, repeated, tag = "3")]
    pub args: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Environment variables to be passed to the container.
    /// Maximum limit is 100.
    #[prost(message, repeated, tag = "4")]
    pub env: ::prost::alloc::vec::Vec<EnvVar>,
}
/// The spec of a Python packaged code.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PythonPackageSpec {
    /// Required. The URI of a container image in Artifact Registry that will run
    /// the provided Python package. Vertex AI provides a wide range of executor
    /// images with pre-installed packages to meet users' various use cases. See
    /// the list of [pre-built containers for
    /// training](<https://cloud.google.com/vertex-ai/docs/training/pre-built-containers>).
    /// You must use an image from this list.
    #[prost(string, tag = "1")]
    pub executor_image_uri: ::prost::alloc::string::String,
    /// Required. The Google Cloud Storage location of the Python package files
    /// which are the training program and its dependent packages. The maximum
    /// number of package URIs is 100.
    #[prost(string, repeated, tag = "2")]
    pub package_uris: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Required. The Python module name to run after installing the packages.
    #[prost(string, tag = "3")]
    pub python_module: ::prost::alloc::string::String,
    /// Command line arguments to be passed to the Python task.
    #[prost(string, repeated, tag = "4")]
    pub args: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Environment variables to be passed to the python module.
    /// Maximum limit is 100.
    #[prost(message, repeated, tag = "5")]
    pub env: ::prost::alloc::vec::Vec<EnvVar>,
}
/// All parameters related to queuing and scheduling of custom jobs.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct Scheduling {
    /// Optional. The maximum job running time. The default is 7 days.
    #[prost(message, optional, tag = "1")]
    pub timeout: ::core::option::Option<::prost_types::Duration>,
    /// Optional. Restarts the entire CustomJob if a worker gets restarted.
    /// This feature can be used by distributed training jobs that are not
    /// resilient to workers leaving and joining a job.
    #[prost(bool, tag = "3")]
    pub restart_job_on_worker_restart: bool,
    /// Optional. This determines which type of scheduling strategy to use.
    #[prost(enumeration = "scheduling::Strategy", tag = "4")]
    pub strategy: i32,
    /// Optional. Indicates if the job should retry for internal errors after the
    /// job starts running. If true, overrides
    /// `Scheduling.restart_job_on_worker_restart` to false.
    #[prost(bool, tag = "5")]
    pub disable_retries: bool,
    /// Optional. This is the maximum duration that a job will wait for the
    /// requested resources to be provisioned if the scheduling strategy is set to
    /// \[Strategy.DWS_FLEX_START\].
    /// If set to 0, the job will wait indefinitely. The default is 24 hours.
    #[prost(message, optional, tag = "6")]
    pub max_wait_duration: ::core::option::Option<::prost_types::Duration>,
}
/// Nested message and enum types in `Scheduling`.
pub mod scheduling {
    /// Optional. This determines which type of scheduling strategy to use. Right
    /// now users have two options such as STANDARD which will use regular on
    /// demand resources to schedule the job, the other is SPOT which would
    /// leverage spot resources alongwith regular resources to schedule
    /// the job.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Strategy {
        /// Strategy will default to STANDARD.
        Unspecified = 0,
        /// Deprecated. Regular on-demand provisioning strategy.
        OnDemand = 1,
        /// Deprecated. Low cost by making potential use of spot resources.
        LowCost = 2,
        /// Standard provisioning strategy uses regular on-demand resources.
        Standard = 3,
        /// Spot provisioning strategy uses spot resources.
        Spot = 4,
        /// Flex Start strategy uses DWS to queue for resources.
        FlexStart = 6,
    }
    impl Strategy {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STRATEGY_UNSPECIFIED",
                Self::OnDemand => "ON_DEMAND",
                Self::LowCost => "LOW_COST",
                Self::Standard => "STANDARD",
                Self::Spot => "SPOT",
                Self::FlexStart => "FLEX_START",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STRATEGY_UNSPECIFIED" => Some(Self::Unspecified),
                "ON_DEMAND" => Some(Self::OnDemand),
                "LOW_COST" => Some(Self::LowCost),
                "STANDARD" => Some(Self::Standard),
                "SPOT" => Some(Self::Spot),
                "FLEX_START" => Some(Self::FlexStart),
                _ => None,
            }
        }
    }
}
/// A piece of data in a Dataset. Could be an image, a video, a document or plain
/// text.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DataItem {
    /// Output only. The resource name of the DataItem.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. Timestamp when this DataItem was created.
    #[prost(message, optional, tag = "2")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this DataItem was last updated.
    #[prost(message, optional, tag = "6")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. The labels with user-defined metadata to organize your DataItems.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// No more than 64 user labels can be associated with one DataItem(System
    /// labels are excluded).
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable.
    #[prost(map = "string, string", tag = "3")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Required. The data that the DataItem represents (for example, an image or a
    /// text snippet). The schema of the payload is stored in the parent Dataset's
    /// [metadata schema's][google.cloud.aiplatform.v1.Dataset.metadata_schema_uri]
    /// dataItemSchemaUri field.
    #[prost(message, optional, tag = "4")]
    pub payload: ::core::option::Option<::prost_types::Value>,
    /// Optional. Used to perform consistent read-modify-write updates. If not set,
    /// a blind "overwrite" update happens.
    #[prost(string, tag = "7")]
    pub etag: ::prost::alloc::string::String,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "10")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "11")]
    pub satisfies_pzi: bool,
}
/// DataLabelingJob is used to trigger a human labeling job on unlabeled data
/// from the following Dataset:
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DataLabelingJob {
    /// Output only. Resource name of the DataLabelingJob.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The user-defined name of the DataLabelingJob.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    /// Display name of a DataLabelingJob.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Required. Dataset resource names. Right now we only support labeling from a
    /// single Dataset. Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}`
    #[prost(string, repeated, tag = "3")]
    pub datasets: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Labels to assign to annotations generated by this DataLabelingJob.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable.
    #[prost(map = "string, string", tag = "12")]
    pub annotation_labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Required. Number of labelers to work on each DataItem.
    #[prost(int32, tag = "4")]
    pub labeler_count: i32,
    /// Required. The Google Cloud Storage location of the instruction pdf. This
    /// pdf is shared with labelers, and provides detailed description on how to
    /// label DataItems in Datasets.
    #[prost(string, tag = "5")]
    pub instruction_uri: ::prost::alloc::string::String,
    /// Required. Points to a YAML file stored on Google Cloud Storage describing
    /// the config for a specific type of DataLabelingJob. The schema files that
    /// can be used here are found in the
    /// <https://storage.googleapis.com/google-cloud-aiplatform> bucket in the
    /// /schema/datalabelingjob/inputs/ folder.
    #[prost(string, tag = "6")]
    pub inputs_schema_uri: ::prost::alloc::string::String,
    /// Required. Input config parameters for the DataLabelingJob.
    #[prost(message, optional, tag = "7")]
    pub inputs: ::core::option::Option<::prost_types::Value>,
    /// Output only. The detailed state of the job.
    #[prost(enumeration = "JobState", tag = "8")]
    pub state: i32,
    /// Output only. Current labeling job progress percentage scaled in interval
    /// \[0, 100\], indicating the percentage of DataItems that has been finished.
    #[prost(int32, tag = "13")]
    pub labeling_progress: i32,
    /// Output only. Estimated cost(in US dollars) that the DataLabelingJob has
    /// incurred to date.
    #[prost(message, optional, tag = "14")]
    pub current_spend: ::core::option::Option<super::super::super::r#type::Money>,
    /// Output only. Timestamp when this DataLabelingJob was created.
    #[prost(message, optional, tag = "9")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this DataLabelingJob was updated most recently.
    #[prost(message, optional, tag = "10")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. DataLabelingJob errors. It is only populated when job's state
    /// is `JOB_STATE_FAILED` or `JOB_STATE_CANCELLED`.
    #[prost(message, optional, tag = "22")]
    pub error: ::core::option::Option<super::super::super::rpc::Status>,
    /// The labels with user-defined metadata to organize your DataLabelingJobs.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable. Following system labels exist for each DataLabelingJob:
    ///
    /// * "aiplatform.googleapis.com/schema": output only, its value is the
    ///    [inputs_schema][google.cloud.aiplatform.v1.DataLabelingJob.inputs_schema_uri]'s
    ///    title.
    #[prost(map = "string, string", tag = "11")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// The SpecialistPools' resource names associated with this job.
    #[prost(string, repeated, tag = "16")]
    pub specialist_pools: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Customer-managed encryption key spec for a DataLabelingJob. If set, this
    /// DataLabelingJob will be secured by this key.
    ///
    /// Note: Annotations created in the DataLabelingJob are associated with
    /// the EncryptionSpec of the Dataset they are exported to.
    #[prost(message, optional, tag = "20")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Parameters that configure the active learning pipeline. Active learning
    /// will label the data incrementally via several iterations. For every
    /// iteration, it will select a batch of data based on the sampling strategy.
    #[prost(message, optional, tag = "21")]
    pub active_learning_config: ::core::option::Option<ActiveLearningConfig>,
}
/// Parameters that configure the active learning pipeline. Active learning will
///   label the data incrementally by several iterations. For every iteration, it
///   will select a batch of data based on the sampling strategy.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ActiveLearningConfig {
    /// Active learning data sampling config. For every active learning labeling
    /// iteration, it will select a batch of data based on the sampling strategy.
    #[prost(message, optional, tag = "3")]
    pub sample_config: ::core::option::Option<SampleConfig>,
    /// CMLE training config. For every active learning labeling iteration, system
    /// will train a machine learning model on CMLE. The trained model will be used
    /// by data sampling algorithm to select DataItems.
    #[prost(message, optional, tag = "4")]
    pub training_config: ::core::option::Option<TrainingConfig>,
    /// Required. Max human labeling DataItems. The rest part will be labeled by
    /// machine.
    #[prost(oneof = "active_learning_config::HumanLabelingBudget", tags = "1, 2")]
    pub human_labeling_budget: ::core::option::Option<
        active_learning_config::HumanLabelingBudget,
    >,
}
/// Nested message and enum types in `ActiveLearningConfig`.
pub mod active_learning_config {
    /// Required. Max human labeling DataItems. The rest part will be labeled by
    /// machine.
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum HumanLabelingBudget {
        /// Max number of human labeled DataItems.
        #[prost(int64, tag = "1")]
        MaxDataItemCount(i64),
        /// Max percent of total DataItems for human labeling.
        #[prost(int32, tag = "2")]
        MaxDataItemPercentage(i32),
    }
}
/// Active learning data sampling config. For every active learning labeling
/// iteration, it will select a batch of data based on the sampling strategy.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SampleConfig {
    /// Field to choose sampling strategy. Sampling strategy will decide which data
    /// should be selected for human labeling in every batch.
    #[prost(enumeration = "sample_config::SampleStrategy", tag = "5")]
    pub sample_strategy: i32,
    /// Decides sample size for the initial batch. initial_batch_sample_percentage
    /// is used by default.
    #[prost(oneof = "sample_config::InitialBatchSampleSize", tags = "1")]
    pub initial_batch_sample_size: ::core::option::Option<
        sample_config::InitialBatchSampleSize,
    >,
    /// Decides sample size for the following batches.
    /// following_batch_sample_percentage is used by default.
    #[prost(oneof = "sample_config::FollowingBatchSampleSize", tags = "3")]
    pub following_batch_sample_size: ::core::option::Option<
        sample_config::FollowingBatchSampleSize,
    >,
}
/// Nested message and enum types in `SampleConfig`.
pub mod sample_config {
    /// Sample strategy decides which subset of DataItems should be selected for
    /// human labeling in every batch.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum SampleStrategy {
        /// Default will be treated as UNCERTAINTY.
        Unspecified = 0,
        /// Sample the most uncertain data to label.
        Uncertainty = 1,
    }
    impl SampleStrategy {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "SAMPLE_STRATEGY_UNSPECIFIED",
                Self::Uncertainty => "UNCERTAINTY",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "SAMPLE_STRATEGY_UNSPECIFIED" => Some(Self::Unspecified),
                "UNCERTAINTY" => Some(Self::Uncertainty),
                _ => None,
            }
        }
    }
    /// Decides sample size for the initial batch. initial_batch_sample_percentage
    /// is used by default.
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum InitialBatchSampleSize {
        /// The percentage of data needed to be labeled in the first batch.
        #[prost(int32, tag = "1")]
        InitialBatchSamplePercentage(i32),
    }
    /// Decides sample size for the following batches.
    /// following_batch_sample_percentage is used by default.
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum FollowingBatchSampleSize {
        /// The percentage of data needed to be labeled in each following batch
        /// (except the first batch).
        #[prost(int32, tag = "3")]
        FollowingBatchSamplePercentage(i32),
    }
}
/// CMLE training config. For every active learning labeling iteration, system
/// will train a machine learning model on CMLE. The trained model will be used
/// by data sampling algorithm to select DataItems.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct TrainingConfig {
    /// The timeout hours for the CMLE training job, expressed in milli hours
    /// i.e. 1,000 value in this field means 1 hour.
    #[prost(int64, tag = "1")]
    pub timeout_training_milli_hours: i64,
}
/// A SavedQuery is a view of the dataset. It references a subset of annotations
/// by problem type and filters.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SavedQuery {
    /// Output only. Resource name of the SavedQuery.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The user-defined name of the SavedQuery.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Some additional information about the SavedQuery.
    #[prost(message, optional, tag = "12")]
    pub metadata: ::core::option::Option<::prost_types::Value>,
    /// Output only. Timestamp when this SavedQuery was created.
    #[prost(message, optional, tag = "3")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when SavedQuery was last updated.
    #[prost(message, optional, tag = "4")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Filters on the Annotations in the dataset.
    #[prost(string, tag = "5")]
    pub annotation_filter: ::prost::alloc::string::String,
    /// Required. Problem type of the SavedQuery.
    /// Allowed values:
    ///
    /// * IMAGE_CLASSIFICATION_SINGLE_LABEL
    /// * IMAGE_CLASSIFICATION_MULTI_LABEL
    /// * IMAGE_BOUNDING_POLY
    /// * IMAGE_BOUNDING_BOX
    /// * TEXT_CLASSIFICATION_SINGLE_LABEL
    /// * TEXT_CLASSIFICATION_MULTI_LABEL
    /// * TEXT_EXTRACTION
    /// * TEXT_SENTIMENT
    /// * VIDEO_CLASSIFICATION
    /// * VIDEO_OBJECT_TRACKING
    #[prost(string, tag = "6")]
    pub problem_type: ::prost::alloc::string::String,
    /// Output only. Number of AnnotationSpecs in the context of the SavedQuery.
    #[prost(int32, tag = "10")]
    pub annotation_spec_count: i32,
    /// Used to perform a consistent read-modify-write update. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "8")]
    pub etag: ::prost::alloc::string::String,
    /// Output only. If the Annotations belonging to the SavedQuery can be used for
    /// AutoML training.
    #[prost(bool, tag = "9")]
    pub support_automl_training: bool,
}
/// A collection of DataItems and Annotations on them.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Dataset {
    /// Output only. Identifier. The resource name of the Dataset.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The user-defined name of the Dataset.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// The description of the Dataset.
    #[prost(string, tag = "16")]
    pub description: ::prost::alloc::string::String,
    /// Required. Points to a YAML file stored on Google Cloud Storage describing
    /// additional information about the Dataset. The schema is defined as an
    /// OpenAPI 3.0.2 Schema Object. The schema files that can be used here are
    /// found in gs://google-cloud-aiplatform/schema/dataset/metadata/.
    #[prost(string, tag = "3")]
    pub metadata_schema_uri: ::prost::alloc::string::String,
    /// Required. Additional information about the Dataset.
    #[prost(message, optional, tag = "8")]
    pub metadata: ::core::option::Option<::prost_types::Value>,
    /// Output only. The number of DataItems in this Dataset. Only apply for
    /// non-structured Dataset.
    #[prost(int64, tag = "10")]
    pub data_item_count: i64,
    /// Output only. Timestamp when this Dataset was created.
    #[prost(message, optional, tag = "4")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Dataset was last updated.
    #[prost(message, optional, tag = "5")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Used to perform consistent read-modify-write updates. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "6")]
    pub etag: ::prost::alloc::string::String,
    /// The labels with user-defined metadata to organize your Datasets.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// No more than 64 user labels can be associated with one Dataset (System
    /// labels are excluded).
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable. Following system labels exist for each Dataset:
    ///
    /// * "aiplatform.googleapis.com/dataset_metadata_schema": output only, its
    ///    value is the
    ///    [metadata_schema's][google.cloud.aiplatform.v1.Dataset.metadata_schema_uri]
    ///    title.
    #[prost(map = "string, string", tag = "7")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// All SavedQueries belong to the Dataset will be returned in List/Get
    /// Dataset response. The annotation_specs field
    /// will not be populated except for UI cases which will only use
    /// [annotation_spec_count][google.cloud.aiplatform.v1.SavedQuery.annotation_spec_count].
    /// In CreateDataset request, a SavedQuery is created together if
    /// this field is set, up to one SavedQuery can be set in CreateDatasetRequest.
    /// The SavedQuery should not contain any AnnotationSpec.
    #[prost(message, repeated, tag = "9")]
    pub saved_queries: ::prost::alloc::vec::Vec<SavedQuery>,
    /// Customer-managed encryption key spec for a Dataset. If set, this Dataset
    /// and all sub-resources of this Dataset will be secured by this key.
    #[prost(message, optional, tag = "11")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Output only. The resource name of the Artifact that was created in
    /// MetadataStore when creating the Dataset. The Artifact resource name pattern
    /// is
    /// `projects/{project}/locations/{location}/metadataStores/{metadata_store}/artifacts/{artifact}`.
    #[prost(string, tag = "17")]
    pub metadata_artifact: ::prost::alloc::string::String,
    /// Optional. Reference to the public base model last used by the dataset. Only
    /// set for prompt datasets.
    #[prost(string, tag = "18")]
    pub model_reference: ::prost::alloc::string::String,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "19")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "20")]
    pub satisfies_pzi: bool,
}
/// Describes the location from where we import data into a Dataset, together
/// with the labels that will be applied to the DataItems and the Annotations.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ImportDataConfig {
    /// Labels that will be applied to newly imported DataItems. If an identical
    /// DataItem as one being imported already exists in the Dataset, then these
    /// labels will be appended to these of the already existing one, and if labels
    /// with identical key is imported before, the old label value will be
    /// overwritten. If two DataItems are identical in the same import data
    /// operation, the labels will be combined and if key collision happens in this
    /// case, one of the values will be picked randomly. Two DataItems are
    /// considered identical if their content bytes are identical (e.g. image bytes
    /// or pdf bytes).
    /// These labels will be overridden by Annotation labels specified inside index
    /// file referenced by
    /// [import_schema_uri][google.cloud.aiplatform.v1.ImportDataConfig.import_schema_uri],
    /// e.g. jsonl file.
    #[prost(map = "string, string", tag = "2")]
    pub data_item_labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Labels that will be applied to newly imported Annotations. If two
    /// Annotations are identical, one of them will be deduped. Two Annotations are
    /// considered identical if their
    /// [payload][google.cloud.aiplatform.v1.Annotation.payload],
    /// [payload_schema_uri][google.cloud.aiplatform.v1.Annotation.payload_schema_uri]
    /// and all of their [labels][google.cloud.aiplatform.v1.Annotation.labels] are
    /// the same. These labels will be overridden by Annotation labels specified
    /// inside index file referenced by
    /// [import_schema_uri][google.cloud.aiplatform.v1.ImportDataConfig.import_schema_uri],
    /// e.g. jsonl file.
    #[prost(map = "string, string", tag = "3")]
    pub annotation_labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Required. Points to a YAML file stored on Google Cloud Storage describing
    /// the import format. Validation will be done against the schema. The schema
    /// is defined as an [OpenAPI 3.0.2 Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    #[prost(string, tag = "4")]
    pub import_schema_uri: ::prost::alloc::string::String,
    /// The source of the input.
    #[prost(oneof = "import_data_config::Source", tags = "1")]
    pub source: ::core::option::Option<import_data_config::Source>,
}
/// Nested message and enum types in `ImportDataConfig`.
pub mod import_data_config {
    /// The source of the input.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Source {
        /// The Google Cloud Storage location for the input content.
        #[prost(message, tag = "1")]
        GcsSource(super::GcsSource),
    }
}
/// Describes what part of the Dataset is to be exported, the destination of
/// the export and how to export.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExportDataConfig {
    /// An expression for filtering what part of the Dataset is to be exported.
    /// Only Annotations that match this filter will be exported. The filter syntax
    /// is the same as in
    /// [ListAnnotations][google.cloud.aiplatform.v1.DatasetService.ListAnnotations].
    #[prost(string, tag = "2")]
    pub annotations_filter: ::prost::alloc::string::String,
    /// The ID of a SavedQuery (annotation set) under the Dataset specified by
    /// [ExportDataRequest.name][google.cloud.aiplatform.v1.ExportDataRequest.name]
    /// used for filtering Annotations for training.
    ///
    /// Only used for custom training data export use cases.
    /// Only applicable to Datasets that have SavedQueries.
    ///
    /// Only Annotations that are associated with this SavedQuery are used in
    /// respectively training. When used in conjunction with
    /// [annotations_filter][google.cloud.aiplatform.v1.ExportDataConfig.annotations_filter],
    /// the Annotations used for training are filtered by both
    /// [saved_query_id][google.cloud.aiplatform.v1.ExportDataConfig.saved_query_id]
    /// and
    /// [annotations_filter][google.cloud.aiplatform.v1.ExportDataConfig.annotations_filter].
    ///
    /// Only one of
    /// [saved_query_id][google.cloud.aiplatform.v1.ExportDataConfig.saved_query_id]
    /// and
    /// [annotation_schema_uri][google.cloud.aiplatform.v1.ExportDataConfig.annotation_schema_uri]
    /// should be specified as both of them represent the same thing: problem type.
    #[prost(string, tag = "11")]
    pub saved_query_id: ::prost::alloc::string::String,
    /// The Cloud Storage URI that points to a YAML file describing the annotation
    /// schema. The schema is defined as an OpenAPI 3.0.2 [Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    /// The schema files that can be used here are found in
    /// gs://google-cloud-aiplatform/schema/dataset/annotation/, note that the
    /// chosen schema must be consistent with
    /// [metadata][google.cloud.aiplatform.v1.Dataset.metadata_schema_uri] of the
    /// Dataset specified by
    /// [ExportDataRequest.name][google.cloud.aiplatform.v1.ExportDataRequest.name].
    ///
    /// Only used for custom training data export use cases.
    /// Only applicable to Datasets that have DataItems and Annotations.
    ///
    /// Only Annotations that both match this schema and belong to DataItems not
    /// ignored by the split method are used in respectively training, validation
    /// or test role, depending on the role of the DataItem they are on.
    ///
    /// When used in conjunction with
    /// [annotations_filter][google.cloud.aiplatform.v1.ExportDataConfig.annotations_filter],
    /// the Annotations used for training are filtered by both
    /// [annotations_filter][google.cloud.aiplatform.v1.ExportDataConfig.annotations_filter]
    /// and
    /// [annotation_schema_uri][google.cloud.aiplatform.v1.ExportDataConfig.annotation_schema_uri].
    #[prost(string, tag = "12")]
    pub annotation_schema_uri: ::prost::alloc::string::String,
    /// Indicates the usage of the exported files.
    #[prost(enumeration = "export_data_config::ExportUse", tag = "4")]
    pub export_use: i32,
    /// The destination of the output.
    #[prost(oneof = "export_data_config::Destination", tags = "1")]
    pub destination: ::core::option::Option<export_data_config::Destination>,
    /// The instructions how the export data should be split between the
    /// training, validation and test sets.
    #[prost(oneof = "export_data_config::Split", tags = "5, 7")]
    pub split: ::core::option::Option<export_data_config::Split>,
}
/// Nested message and enum types in `ExportDataConfig`.
pub mod export_data_config {
    /// ExportUse indicates the usage of the exported files. It restricts file
    /// destination, format, annotations to be exported, whether to allow
    /// unannotated data to be exported and whether to clone files to temp Cloud
    /// Storage bucket.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum ExportUse {
        /// Regular user export.
        Unspecified = 0,
        /// Export for custom code training.
        CustomCodeTraining = 6,
    }
    impl ExportUse {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "EXPORT_USE_UNSPECIFIED",
                Self::CustomCodeTraining => "CUSTOM_CODE_TRAINING",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "EXPORT_USE_UNSPECIFIED" => Some(Self::Unspecified),
                "CUSTOM_CODE_TRAINING" => Some(Self::CustomCodeTraining),
                _ => None,
            }
        }
    }
    /// The destination of the output.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Destination {
        /// The Google Cloud Storage location where the output is to be written to.
        /// In the given directory a new directory will be created with name:
        /// `export-data-<dataset-display-name>-<timestamp-of-export-call>` where
        /// timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. All export
        /// output will be written into that directory. Inside that directory,
        /// annotations with the same schema will be grouped into sub directories
        /// which are named with the corresponding annotations' schema title. Inside
        /// these sub directories, a schema.yaml will be created to describe the
        /// output format.
        #[prost(message, tag = "1")]
        GcsDestination(super::GcsDestination),
    }
    /// The instructions how the export data should be split between the
    /// training, validation and test sets.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Split {
        /// Split based on fractions defining the size of each set.
        #[prost(message, tag = "5")]
        FractionSplit(super::ExportFractionSplit),
        /// Split based on the provided filters for each set.
        #[prost(message, tag = "7")]
        FilterSplit(super::ExportFilterSplit),
    }
}
/// Assigns the input data to training, validation, and test sets as per the
/// given fractions. Any of `training_fraction`, `validation_fraction` and
/// `test_fraction` may optionally be provided, they must sum to up to 1. If the
/// provided ones sum to less than 1, the remainder is assigned to sets as
/// decided by Vertex AI. If none of the fractions are set, by default roughly
/// 80% of data is used for training, 10% for validation, and 10% for test.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ExportFractionSplit {
    /// The fraction of the input data that is to be used to train the Model.
    #[prost(double, tag = "1")]
    pub training_fraction: f64,
    /// The fraction of the input data that is to be used to validate the Model.
    #[prost(double, tag = "2")]
    pub validation_fraction: f64,
    /// The fraction of the input data that is to be used to evaluate the Model.
    #[prost(double, tag = "3")]
    pub test_fraction: f64,
}
/// Assigns input data to training, validation, and test sets based on the given
/// filters, data pieces not matched by any filter are ignored. Currently only
/// supported for Datasets containing DataItems.
/// If any of the filters in this message are to match nothing, then they can be
/// set as '-' (the minus sign).
///
/// Supported only for unstructured Datasets.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExportFilterSplit {
    /// Required. A filter on DataItems of the Dataset. DataItems that match
    /// this filter are used to train the Model. A filter with same syntax
    /// as the one used in
    /// [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems]
    /// may be used. If a single DataItem is matched by more than one of the
    /// FilterSplit filters, then it is assigned to the first set that applies to
    /// it in the training, validation, test order.
    #[prost(string, tag = "1")]
    pub training_filter: ::prost::alloc::string::String,
    /// Required. A filter on DataItems of the Dataset. DataItems that match
    /// this filter are used to validate the Model. A filter with same syntax
    /// as the one used in
    /// [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems]
    /// may be used. If a single DataItem is matched by more than one of the
    /// FilterSplit filters, then it is assigned to the first set that applies to
    /// it in the training, validation, test order.
    #[prost(string, tag = "2")]
    pub validation_filter: ::prost::alloc::string::String,
    /// Required. A filter on DataItems of the Dataset. DataItems that match
    /// this filter are used to test the Model. A filter with same syntax
    /// as the one used in
    /// [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems]
    /// may be used. If a single DataItem is matched by more than one of the
    /// FilterSplit filters, then it is assigned to the first set that applies to
    /// it in the training, validation, test order.
    #[prost(string, tag = "3")]
    pub test_filter: ::prost::alloc::string::String,
}
/// Describes the dataset version.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DatasetVersion {
    /// Output only. Identifier. The resource name of the DatasetVersion.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. Timestamp when this DatasetVersion was created.
    #[prost(message, optional, tag = "2")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this DatasetVersion was last updated.
    #[prost(message, optional, tag = "6")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Used to perform consistent read-modify-write updates. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "3")]
    pub etag: ::prost::alloc::string::String,
    /// Output only. Name of the associated BigQuery dataset.
    #[prost(string, tag = "4")]
    pub big_query_dataset_name: ::prost::alloc::string::String,
    /// The user-defined name of the DatasetVersion.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "7")]
    pub display_name: ::prost::alloc::string::String,
    /// Required. Output only. Additional information about the DatasetVersion.
    #[prost(message, optional, tag = "8")]
    pub metadata: ::core::option::Option<::prost_types::Value>,
    /// Output only. Reference to the public base model last used by the dataset
    /// version. Only set for prompt dataset versions.
    #[prost(string, tag = "9")]
    pub model_reference: ::prost::alloc::string::String,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "10")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "11")]
    pub satisfies_pzi: bool,
}
/// Generic Metadata shared by all operations.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenericOperationMetadata {
    /// Output only. Partial failures encountered.
    /// E.g. single files that couldn't be read.
    /// This field should never exceed 20 entries.
    /// Status details field will contain standard Google Cloud error details.
    #[prost(message, repeated, tag = "1")]
    pub partial_failures: ::prost::alloc::vec::Vec<super::super::super::rpc::Status>,
    /// Output only. Time when the operation was created.
    #[prost(message, optional, tag = "2")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the operation was updated for the last time.
    /// If the operation has finished (successfully or not), this is the finish
    /// time.
    #[prost(message, optional, tag = "3")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
}
/// Details of operations that perform deletes of any entities.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [DatasetService.CreateDataset][google.cloud.aiplatform.v1.DatasetService.CreateDataset].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateDatasetRequest {
    /// Required. The resource name of the Location to create the Dataset in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The Dataset to create.
    #[prost(message, optional, tag = "2")]
    pub dataset: ::core::option::Option<Dataset>,
}
/// Runtime operation information for
/// [DatasetService.CreateDataset][google.cloud.aiplatform.v1.DatasetService.CreateDataset].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateDatasetOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [DatasetService.GetDataset][google.cloud.aiplatform.v1.DatasetService.GetDataset].
/// Next ID: 4
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetDatasetRequest {
    /// Required. The name of the Dataset resource.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "2")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [DatasetService.UpdateDataset][google.cloud.aiplatform.v1.DatasetService.UpdateDataset].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateDatasetRequest {
    /// Required. The Dataset which replaces the resource on the server.
    #[prost(message, optional, tag = "1")]
    pub dataset: ::core::option::Option<Dataset>,
    /// Required. The update mask applies to the resource.
    /// For the `FieldMask` definition, see
    /// [google.protobuf.FieldMask][google.protobuf.FieldMask]. Updatable fields:
    ///
    ///    * `display_name`
    ///    * `description`
    ///    * `labels`
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [DatasetService.UpdateDatasetVersion][google.cloud.aiplatform.v1.DatasetService.UpdateDatasetVersion].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateDatasetVersionRequest {
    /// Required. The DatasetVersion which replaces the resource on the server.
    #[prost(message, optional, tag = "1")]
    pub dataset_version: ::core::option::Option<DatasetVersion>,
    /// Required. The update mask applies to the resource.
    /// For the `FieldMask` definition, see
    /// [google.protobuf.FieldMask][google.protobuf.FieldMask]. Updatable fields:
    ///
    ///    * `display_name`
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [DatasetService.ListDatasets][google.cloud.aiplatform.v1.DatasetService.ListDatasets].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListDatasetsRequest {
    /// Required. The name of the Dataset's parent resource.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// An expression for filtering the results of the request. For field names
    /// both snake_case and camelCase are supported.
    ///
    ///    * `display_name`: supports = and !=
    ///    * `metadata_schema_uri`: supports = and !=
    ///    * `labels` supports general map functions that is:
    ///      * `labels.key=value` - key:value equality
    ///      * `labels.key:* or labels:key - key existence
    ///      * A key including a space must be quoted. `labels."a key"`.
    ///
    /// Some examples:
    ///
    ///    * `displayName="myDisplayName"`
    ///    * `labels.myKey="myValue"`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    /// Supported fields:
    ///
    ///    * `display_name`
    ///    * `create_time`
    ///    * `update_time`
    #[prost(string, tag = "6")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [DatasetService.ListDatasets][google.cloud.aiplatform.v1.DatasetService.ListDatasets].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListDatasetsResponse {
    /// A list of Datasets that matches the specified filter in the request.
    #[prost(message, repeated, tag = "1")]
    pub datasets: ::prost::alloc::vec::Vec<Dataset>,
    /// The standard List next-page token.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [DatasetService.DeleteDataset][google.cloud.aiplatform.v1.DatasetService.DeleteDataset].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteDatasetRequest {
    /// Required. The resource name of the Dataset to delete.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [DatasetService.ImportData][google.cloud.aiplatform.v1.DatasetService.ImportData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ImportDataRequest {
    /// Required. The name of the Dataset resource.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The desired input locations. The contents of all input locations
    /// will be imported in one batch.
    #[prost(message, repeated, tag = "2")]
    pub import_configs: ::prost::alloc::vec::Vec<ImportDataConfig>,
}
/// Response message for
/// [DatasetService.ImportData][google.cloud.aiplatform.v1.DatasetService.ImportData].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ImportDataResponse {}
/// Runtime operation information for
/// [DatasetService.ImportData][google.cloud.aiplatform.v1.DatasetService.ImportData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ImportDataOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [DatasetService.ExportData][google.cloud.aiplatform.v1.DatasetService.ExportData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExportDataRequest {
    /// Required. The name of the Dataset resource.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The desired output location.
    #[prost(message, optional, tag = "2")]
    pub export_config: ::core::option::Option<ExportDataConfig>,
}
/// Response message for
/// [DatasetService.ExportData][google.cloud.aiplatform.v1.DatasetService.ExportData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExportDataResponse {
    /// All of the files that are exported in this export operation. For custom
    /// code training export, only three (training, validation and test)
    /// Cloud Storage paths in wildcard format are populated
    /// (for example, gs://.../training-*).
    #[prost(string, repeated, tag = "1")]
    pub exported_files: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Only present for custom code training export use case. Records data stats,
    /// i.e., train/validation/test item/annotation counts calculated during
    /// the export operation.
    #[prost(message, optional, tag = "2")]
    pub data_stats: ::core::option::Option<model::DataStats>,
}
/// Runtime operation information for
/// [DatasetService.ExportData][google.cloud.aiplatform.v1.DatasetService.ExportData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExportDataOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// A Google Cloud Storage directory which path ends with '/'. The exported
    /// data is stored in the directory.
    #[prost(string, tag = "2")]
    pub gcs_output_directory: ::prost::alloc::string::String,
}
/// Request message for
/// [DatasetService.CreateDatasetVersion][google.cloud.aiplatform.v1.DatasetService.CreateDatasetVersion].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateDatasetVersionRequest {
    /// Required. The name of the Dataset resource.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The version to be created. The same CMEK policies with the
    /// original Dataset will be applied the dataset version. So here we don't need
    /// to specify the EncryptionSpecType here.
    #[prost(message, optional, tag = "2")]
    pub dataset_version: ::core::option::Option<DatasetVersion>,
}
/// Runtime operation information for
/// [DatasetService.CreateDatasetVersion][google.cloud.aiplatform.v1.DatasetService.CreateDatasetVersion].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateDatasetVersionOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [DatasetService.DeleteDatasetVersion][google.cloud.aiplatform.v1.DatasetService.DeleteDatasetVersion].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteDatasetVersionRequest {
    /// Required. The resource name of the Dataset version to delete.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}/datasetVersions/{dataset_version}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [DatasetService.GetDatasetVersion][google.cloud.aiplatform.v1.DatasetService.GetDatasetVersion].
/// Next ID: 4
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetDatasetVersionRequest {
    /// Required. The resource name of the Dataset version to delete.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}/datasetVersions/{dataset_version}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "2")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [DatasetService.ListDatasetVersions][google.cloud.aiplatform.v1.DatasetService.ListDatasetVersions].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListDatasetVersionsRequest {
    /// Required. The resource name of the Dataset to list DatasetVersions from.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. The standard list filter.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// Optional. The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// Optional. The standard list page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Optional. Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// Optional. A comma-separated list of fields to order by, sorted in ascending
    /// order. Use "desc" after a field name for descending.
    #[prost(string, tag = "6")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [DatasetService.ListDatasetVersions][google.cloud.aiplatform.v1.DatasetService.ListDatasetVersions].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListDatasetVersionsResponse {
    /// A list of DatasetVersions that matches the specified filter in the request.
    #[prost(message, repeated, tag = "1")]
    pub dataset_versions: ::prost::alloc::vec::Vec<DatasetVersion>,
    /// The standard List next-page token.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [DatasetService.RestoreDatasetVersion][google.cloud.aiplatform.v1.DatasetService.RestoreDatasetVersion].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RestoreDatasetVersionRequest {
    /// Required. The name of the DatasetVersion resource.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}/datasetVersions/{dataset_version}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Runtime operation information for
/// [DatasetService.RestoreDatasetVersion][google.cloud.aiplatform.v1.DatasetService.RestoreDatasetVersion].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RestoreDatasetVersionOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListDataItemsRequest {
    /// Required. The resource name of the Dataset to list DataItems from.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    #[prost(string, tag = "6")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListDataItemsResponse {
    /// A list of DataItems that matches the specified filter in the request.
    #[prost(message, repeated, tag = "1")]
    pub data_items: ::prost::alloc::vec::Vec<DataItem>,
    /// The standard List next-page token.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [DatasetService.SearchDataItems][google.cloud.aiplatform.v1.DatasetService.SearchDataItems].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchDataItemsRequest {
    /// Required. The resource name of the Dataset from which to search DataItems.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}`
    #[prost(string, tag = "1")]
    pub dataset: ::prost::alloc::string::String,
    /// The resource name of a SavedQuery(annotation set in UI).
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}/savedQueries/{saved_query}`
    /// All of the search will be done in the context of this SavedQuery.
    #[deprecated]
    #[prost(string, tag = "2")]
    pub saved_query: ::prost::alloc::string::String,
    /// The resource name of a DataLabelingJob.
    /// Format:
    /// `projects/{project}/locations/{location}/dataLabelingJobs/{data_labeling_job}`
    /// If this field is set, all of the search will be done in the context of
    /// this DataLabelingJob.
    #[prost(string, tag = "3")]
    pub data_labeling_job: ::prost::alloc::string::String,
    /// An expression for filtering the DataItem that will be returned.
    ///
    ///    * `data_item_id` - for = or !=.
    ///    * `labeled` - for = or !=.
    ///    * `has_annotation(ANNOTATION_SPEC_ID)` - true only for DataItem that
    ///      have at least one annotation with annotation_spec_id =
    ///      `ANNOTATION_SPEC_ID` in the context of SavedQuery or DataLabelingJob.
    ///
    /// For example:
    ///
    /// * `data_item=1`
    /// * `has_annotation(5)`
    #[prost(string, tag = "4")]
    pub data_item_filter: ::prost::alloc::string::String,
    /// An expression for filtering the Annotations that will be returned per
    /// DataItem.
    ///    * `annotation_spec_id` - for = or !=.
    #[deprecated]
    #[prost(string, tag = "5")]
    pub annotations_filter: ::prost::alloc::string::String,
    /// An expression that specifies what Annotations will be returned per
    /// DataItem. Annotations satisfied either of the conditions will be returned.
    ///    * `annotation_spec_id` - for = or !=.
    /// Must specify `saved_query_id=` - saved query id that annotations should
    /// belong to.
    #[prost(string, repeated, tag = "11")]
    pub annotation_filters: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Mask specifying which fields of
    /// [DataItemView][google.cloud.aiplatform.v1.DataItemView] to read.
    #[prost(message, optional, tag = "6")]
    pub field_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// If set, only up to this many of Annotations will be returned per
    /// DataItemView. The maximum value is 1000. If not set, the maximum value will
    /// be used.
    #[prost(int32, tag = "7")]
    pub annotations_limit: i32,
    /// Requested page size. Server may return fewer results than requested.
    /// Default and maximum page size is 100.
    #[prost(int32, tag = "8")]
    pub page_size: i32,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    #[deprecated]
    #[prost(string, tag = "9")]
    pub order_by: ::prost::alloc::string::String,
    /// A token identifying a page of results for the server to return
    /// Typically obtained via
    /// [SearchDataItemsResponse.next_page_token][google.cloud.aiplatform.v1.SearchDataItemsResponse.next_page_token]
    /// of the previous
    /// [DatasetService.SearchDataItems][google.cloud.aiplatform.v1.DatasetService.SearchDataItems]
    /// call.
    #[prost(string, tag = "10")]
    pub page_token: ::prost::alloc::string::String,
    #[prost(oneof = "search_data_items_request::Order", tags = "12, 13")]
    pub order: ::core::option::Option<search_data_items_request::Order>,
}
/// Nested message and enum types in `SearchDataItemsRequest`.
pub mod search_data_items_request {
    /// Expression that allows ranking results based on annotation's property.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct OrderByAnnotation {
        /// Required. Saved query of the Annotation. Only Annotations belong to this
        /// saved query will be considered for ordering.
        #[prost(string, tag = "1")]
        pub saved_query: ::prost::alloc::string::String,
        /// A comma-separated list of annotation fields to order by, sorted in
        /// ascending order. Use "desc" after a field name for descending. Must also
        /// specify saved_query.
        #[prost(string, tag = "2")]
        pub order_by: ::prost::alloc::string::String,
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Order {
        /// A comma-separated list of data item fields to order by, sorted in
        /// ascending order. Use "desc" after a field name for descending.
        #[prost(string, tag = "12")]
        OrderByDataItem(::prost::alloc::string::String),
        /// Expression that allows ranking results based on annotation's property.
        #[prost(message, tag = "13")]
        OrderByAnnotation(OrderByAnnotation),
    }
}
/// Response message for
/// [DatasetService.SearchDataItems][google.cloud.aiplatform.v1.DatasetService.SearchDataItems].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchDataItemsResponse {
    /// The DataItemViews read.
    #[prost(message, repeated, tag = "1")]
    pub data_item_views: ::prost::alloc::vec::Vec<DataItemView>,
    /// A token to retrieve next page of results.
    /// Pass to
    /// [SearchDataItemsRequest.page_token][google.cloud.aiplatform.v1.SearchDataItemsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// A container for a single DataItem and Annotations on it.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DataItemView {
    /// The DataItem.
    #[prost(message, optional, tag = "1")]
    pub data_item: ::core::option::Option<DataItem>,
    /// The Annotations on the DataItem. If too many Annotations should be returned
    /// for the DataItem, this field will be truncated per annotations_limit in
    /// request. If it was, then the has_truncated_annotations will be set to true.
    #[prost(message, repeated, tag = "2")]
    pub annotations: ::prost::alloc::vec::Vec<Annotation>,
    /// True if and only if the Annotations field has been truncated. It happens if
    /// more Annotations for this DataItem met the request's annotation_filter than
    /// are allowed to be returned by annotations_limit.
    /// Note that if Annotations field is not being returned due to field mask,
    /// then this field will not be set to true no matter how many Annotations are
    /// there.
    #[prost(bool, tag = "3")]
    pub has_truncated_annotations: bool,
}
/// Request message for
/// [DatasetService.ListSavedQueries][google.cloud.aiplatform.v1.DatasetService.ListSavedQueries].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListSavedQueriesRequest {
    /// Required. The resource name of the Dataset to list SavedQueries from.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    #[prost(string, tag = "6")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [DatasetService.ListSavedQueries][google.cloud.aiplatform.v1.DatasetService.ListSavedQueries].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListSavedQueriesResponse {
    /// A list of SavedQueries that match the specified filter in the request.
    #[prost(message, repeated, tag = "1")]
    pub saved_queries: ::prost::alloc::vec::Vec<SavedQuery>,
    /// The standard List next-page token.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [DatasetService.DeleteSavedQuery][google.cloud.aiplatform.v1.DatasetService.DeleteSavedQuery].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteSavedQueryRequest {
    /// Required. The resource name of the SavedQuery to delete.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}/savedQueries/{saved_query}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [DatasetService.GetAnnotationSpec][google.cloud.aiplatform.v1.DatasetService.GetAnnotationSpec].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetAnnotationSpecRequest {
    /// Required. The name of the AnnotationSpec resource.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}/annotationSpecs/{annotation_spec}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "2")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [DatasetService.ListAnnotations][google.cloud.aiplatform.v1.DatasetService.ListAnnotations].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListAnnotationsRequest {
    /// Required. The resource name of the DataItem to list Annotations from.
    /// Format:
    /// `projects/{project}/locations/{location}/datasets/{dataset}/dataItems/{data_item}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    #[prost(string, tag = "6")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [DatasetService.ListAnnotations][google.cloud.aiplatform.v1.DatasetService.ListAnnotations].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListAnnotationsResponse {
    /// A list of Annotations that matches the specified filter in the request.
    #[prost(message, repeated, tag = "1")]
    pub annotations: ::prost::alloc::vec::Vec<Annotation>,
    /// The standard List next-page token.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod dataset_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// The service that manages Vertex AI Dataset and its child resources.
    #[derive(Debug, Clone)]
    pub struct DatasetServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl DatasetServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> DatasetServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> DatasetServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            DatasetServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a Dataset.
        pub async fn create_dataset(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateDatasetRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/CreateDataset",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "CreateDataset",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a Dataset.
        pub async fn get_dataset(
            &mut self,
            request: impl tonic::IntoRequest<super::GetDatasetRequest>,
        ) -> std::result::Result<tonic::Response<super::Dataset>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/GetDataset",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "GetDataset",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a Dataset.
        pub async fn update_dataset(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateDatasetRequest>,
        ) -> std::result::Result<tonic::Response<super::Dataset>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/UpdateDataset",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "UpdateDataset",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Datasets in a Location.
        pub async fn list_datasets(
            &mut self,
            request: impl tonic::IntoRequest<super::ListDatasetsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListDatasetsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/ListDatasets",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "ListDatasets",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a Dataset.
        pub async fn delete_dataset(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteDatasetRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/DeleteDataset",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "DeleteDataset",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Imports data into a Dataset.
        pub async fn import_data(
            &mut self,
            request: impl tonic::IntoRequest<super::ImportDataRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/ImportData",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "ImportData",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Exports data from a Dataset.
        pub async fn export_data(
            &mut self,
            request: impl tonic::IntoRequest<super::ExportDataRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/ExportData",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "ExportData",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Create a version from a Dataset.
        pub async fn create_dataset_version(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateDatasetVersionRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/CreateDatasetVersion",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "CreateDatasetVersion",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a DatasetVersion.
        pub async fn update_dataset_version(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateDatasetVersionRequest>,
        ) -> std::result::Result<tonic::Response<super::DatasetVersion>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/UpdateDatasetVersion",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "UpdateDatasetVersion",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a Dataset version.
        pub async fn delete_dataset_version(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteDatasetVersionRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/DeleteDatasetVersion",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "DeleteDatasetVersion",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a Dataset version.
        pub async fn get_dataset_version(
            &mut self,
            request: impl tonic::IntoRequest<super::GetDatasetVersionRequest>,
        ) -> std::result::Result<tonic::Response<super::DatasetVersion>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/GetDatasetVersion",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "GetDatasetVersion",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists DatasetVersions in a Dataset.
        pub async fn list_dataset_versions(
            &mut self,
            request: impl tonic::IntoRequest<super::ListDatasetVersionsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListDatasetVersionsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/ListDatasetVersions",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "ListDatasetVersions",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Restores a dataset version.
        pub async fn restore_dataset_version(
            &mut self,
            request: impl tonic::IntoRequest<super::RestoreDatasetVersionRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/RestoreDatasetVersion",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "RestoreDatasetVersion",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists DataItems in a Dataset.
        pub async fn list_data_items(
            &mut self,
            request: impl tonic::IntoRequest<super::ListDataItemsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListDataItemsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/ListDataItems",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "ListDataItems",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Searches DataItems in a Dataset.
        pub async fn search_data_items(
            &mut self,
            request: impl tonic::IntoRequest<super::SearchDataItemsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::SearchDataItemsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/SearchDataItems",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "SearchDataItems",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists SavedQueries in a Dataset.
        pub async fn list_saved_queries(
            &mut self,
            request: impl tonic::IntoRequest<super::ListSavedQueriesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListSavedQueriesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/ListSavedQueries",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "ListSavedQueries",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a SavedQuery.
        pub async fn delete_saved_query(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteSavedQueryRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/DeleteSavedQuery",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "DeleteSavedQuery",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets an AnnotationSpec.
        pub async fn get_annotation_spec(
            &mut self,
            request: impl tonic::IntoRequest<super::GetAnnotationSpecRequest>,
        ) -> std::result::Result<tonic::Response<super::AnnotationSpec>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/GetAnnotationSpec",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "GetAnnotationSpec",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Annotations belongs to a dataitem
        /// This RPC is only available in InternalDatasetService. It is only used for
        /// exporting conversation data to CCAI Insights.
        pub async fn list_annotations(
            &mut self,
            request: impl tonic::IntoRequest<super::ListAnnotationsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListAnnotationsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DatasetService/ListAnnotations",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DatasetService",
                        "ListAnnotations",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Points to a DeployedIndex.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeployedIndexRef {
    /// Immutable. A resource name of the IndexEndpoint.
    #[prost(string, tag = "1")]
    pub index_endpoint: ::prost::alloc::string::String,
    /// Immutable. The ID of the DeployedIndex in the above IndexEndpoint.
    #[prost(string, tag = "2")]
    pub deployed_index_id: ::prost::alloc::string::String,
    /// Output only. The display name of the DeployedIndex.
    #[prost(string, tag = "3")]
    pub display_name: ::prost::alloc::string::String,
}
/// A description of resources that can be shared by multiple DeployedModels,
/// whose underlying specification consists of a DedicatedResources.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeploymentResourcePool {
    /// Immutable. The resource name of the DeploymentResourcePool.
    /// Format:
    /// `projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The underlying DedicatedResources that the DeploymentResourcePool
    /// uses.
    #[prost(message, optional, tag = "2")]
    pub dedicated_resources: ::core::option::Option<DedicatedResources>,
    /// Customer-managed encryption key spec for a DeploymentResourcePool. If set,
    /// this DeploymentResourcePool will be secured by this key. Endpoints and the
    /// DeploymentResourcePool they deploy in need to have the same EncryptionSpec.
    #[prost(message, optional, tag = "5")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// The service account that the DeploymentResourcePool's container(s) run as.
    /// Specify the email address of the service account. If this service account
    /// is not specified, the container(s) run as a service account that doesn't
    /// have access to the resource project.
    ///
    /// Users deploying the Models to this DeploymentResourcePool must have the
    /// `iam.serviceAccounts.actAs` permission on this service account.
    #[prost(string, tag = "6")]
    pub service_account: ::prost::alloc::string::String,
    /// If the DeploymentResourcePool is deployed with custom-trained Models or
    /// AutoML Tabular Models, the container(s) of the DeploymentResourcePool will
    /// send `stderr` and `stdout` streams to Cloud Logging by default.
    /// Please note that the logs incur cost, which are subject to [Cloud Logging
    /// pricing](<https://cloud.google.com/logging/pricing>).
    ///
    /// User can disable container logging by setting this flag to true.
    #[prost(bool, tag = "7")]
    pub disable_container_logging: bool,
    /// Output only. Timestamp when this DeploymentResourcePool was created.
    #[prost(message, optional, tag = "4")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "8")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "9")]
    pub satisfies_pzi: bool,
}
/// PSC config that is used to automatically create forwarding rule via
/// ServiceConnectionMap.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PscAutomationConfig {
    /// Required. Project id used to create forwarding rule.
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Required. The full name of the Google Compute Engine
    /// [network](<https://cloud.google.com/compute/docs/networks-and-firewalls#networks>).
    /// [Format](<https://cloud.google.com/compute/docs/reference/rest/v1/networks/insert>):
    /// `projects/{project}/global/networks/{network}`.
    /// Where {project} is a project number, as in '12345', and {network} is
    /// network name.
    #[prost(string, tag = "2")]
    pub network: ::prost::alloc::string::String,
}
/// Represents configuration for private service connect.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PrivateServiceConnectConfig {
    /// Required. If true, expose the IndexEndpoint via private service connect.
    #[prost(bool, tag = "1")]
    pub enable_private_service_connect: bool,
    /// A list of Projects from which the forwarding rule will target the service
    /// attachment.
    #[prost(string, repeated, tag = "2")]
    pub project_allowlist: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Output only. The name of the generated service attachment resource.
    /// This is only populated if the endpoint is deployed with
    /// PrivateServiceConnect.
    #[prost(string, tag = "5")]
    pub service_attachment: ::prost::alloc::string::String,
}
/// PscAutomatedEndpoints defines the output of the forwarding rule
/// automatically created by each PscAutomationConfig.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PscAutomatedEndpoints {
    /// Corresponding project_id in pscAutomationConfigs
    #[prost(string, tag = "1")]
    pub project_id: ::prost::alloc::string::String,
    /// Corresponding network in pscAutomationConfigs.
    #[prost(string, tag = "2")]
    pub network: ::prost::alloc::string::String,
    /// Ip Address created by the automated forwarding rule.
    #[prost(string, tag = "3")]
    pub match_address: ::prost::alloc::string::String,
}
/// Models are deployed into it, and afterwards Endpoint is called to obtain
/// predictions and explanations.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Endpoint {
    /// Output only. The resource name of the Endpoint.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The display name of the Endpoint.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// The description of the Endpoint.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// Output only. The models deployed in this Endpoint.
    /// To add or remove DeployedModels use
    /// [EndpointService.DeployModel][google.cloud.aiplatform.v1.EndpointService.DeployModel]
    /// and
    /// [EndpointService.UndeployModel][google.cloud.aiplatform.v1.EndpointService.UndeployModel]
    /// respectively.
    #[prost(message, repeated, tag = "4")]
    pub deployed_models: ::prost::alloc::vec::Vec<DeployedModel>,
    /// A map from a DeployedModel's ID to the percentage of this Endpoint's
    /// traffic that should be forwarded to that DeployedModel.
    ///
    /// If a DeployedModel's ID is not listed in this map, then it receives no
    /// traffic.
    ///
    /// The traffic percentage values must add up to 100, or map must be empty if
    /// the Endpoint is to not accept any traffic at a moment.
    #[prost(map = "string, int32", tag = "5")]
    pub traffic_split: ::std::collections::HashMap<::prost::alloc::string::String, i32>,
    /// Used to perform consistent read-modify-write updates. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "6")]
    pub etag: ::prost::alloc::string::String,
    /// The labels with user-defined metadata to organize your Endpoints.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "7")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Output only. Timestamp when this Endpoint was created.
    #[prost(message, optional, tag = "8")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Endpoint was last updated.
    #[prost(message, optional, tag = "9")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Customer-managed encryption key spec for an Endpoint. If set, this
    /// Endpoint and all sub-resources of this Endpoint will be secured by
    /// this key.
    #[prost(message, optional, tag = "10")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Optional. The full name of the Google Compute Engine
    /// [network](<https://cloud.google.com//compute/docs/networks-and-firewalls#networks>)
    /// to which the Endpoint should be peered.
    ///
    /// Private services access must already be configured for the network. If left
    /// unspecified, the Endpoint is not peered with any network.
    ///
    /// Only one of the fields,
    /// [network][google.cloud.aiplatform.v1.Endpoint.network] or
    /// [enable_private_service_connect][google.cloud.aiplatform.v1.Endpoint.enable_private_service_connect],
    /// can be set.
    ///
    /// [Format](<https://cloud.google.com/compute/docs/reference/rest/v1/networks/insert>):
    /// `projects/{project}/global/networks/{network}`.
    /// Where `{project}` is a project number, as in `12345`, and `{network}` is
    /// network name.
    #[prost(string, tag = "13")]
    pub network: ::prost::alloc::string::String,
    /// Deprecated: If true, expose the Endpoint via private service connect.
    ///
    /// Only one of the fields,
    /// [network][google.cloud.aiplatform.v1.Endpoint.network] or
    /// [enable_private_service_connect][google.cloud.aiplatform.v1.Endpoint.enable_private_service_connect],
    /// can be set.
    #[deprecated]
    #[prost(bool, tag = "17")]
    pub enable_private_service_connect: bool,
    /// Optional. Configuration for private service connect.
    ///
    /// [network][google.cloud.aiplatform.v1.Endpoint.network] and
    /// [private_service_connect_config][google.cloud.aiplatform.v1.Endpoint.private_service_connect_config]
    /// are mutually exclusive.
    #[prost(message, optional, tag = "21")]
    pub private_service_connect_config: ::core::option::Option<
        PrivateServiceConnectConfig,
    >,
    /// Output only. Resource name of the Model Monitoring job associated with this
    /// Endpoint if monitoring is enabled by
    /// [JobService.CreateModelDeploymentMonitoringJob][google.cloud.aiplatform.v1.JobService.CreateModelDeploymentMonitoringJob].
    /// Format:
    /// `projects/{project}/locations/{location}/modelDeploymentMonitoringJobs/{model_deployment_monitoring_job}`
    #[prost(string, tag = "14")]
    pub model_deployment_monitoring_job: ::prost::alloc::string::String,
    /// Configures the request-response logging for online prediction.
    #[prost(message, optional, tag = "18")]
    pub predict_request_response_logging_config: ::core::option::Option<
        PredictRequestResponseLoggingConfig,
    >,
    /// If true, the endpoint will be exposed through a dedicated
    /// DNS \[Endpoint.dedicated_endpoint_dns\]. Your request to the dedicated DNS
    /// will be isolated from other users' traffic and will have better performance
    /// and reliability.
    /// Note: Once you enabled dedicated endpoint, you won't be able to send
    /// request to the shared DNS {region}-aiplatform.googleapis.com. The
    /// limitation will be removed soon.
    #[prost(bool, tag = "24")]
    pub dedicated_endpoint_enabled: bool,
    /// Output only. DNS of the dedicated endpoint. Will only be populated if
    /// dedicated_endpoint_enabled is true.
    /// Format:
    /// `<https://{endpoint_id}.{region}-{project_number}.prediction.vertexai.goog`.>
    #[prost(string, tag = "25")]
    pub dedicated_endpoint_dns: ::prost::alloc::string::String,
    /// Configurations that are applied to the endpoint for online prediction.
    #[prost(message, optional, tag = "23")]
    pub client_connection_config: ::core::option::Option<ClientConnectionConfig>,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "27")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "28")]
    pub satisfies_pzi: bool,
}
/// A deployment of a Model. Endpoints contain one or more DeployedModels.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeployedModel {
    /// Immutable. The ID of the DeployedModel. If not provided upon deployment,
    /// Vertex AI will generate a value for this ID.
    ///
    /// This value should be 1-10 characters, and valid characters are `/\[0-9\]/`.
    #[prost(string, tag = "1")]
    pub id: ::prost::alloc::string::String,
    /// Required. The resource name of the Model that this is the deployment of.
    /// Note that the Model may be in a different location than the DeployedModel's
    /// Endpoint.
    ///
    /// The resource name may contain version id or version alias to specify the
    /// version.
    ///   Example: `projects/{project}/locations/{location}/models/{model}@2`
    ///               or
    ///             `projects/{project}/locations/{location}/models/{model}@golden`
    /// if no version is specified, the default version will be deployed.
    #[prost(string, tag = "2")]
    pub model: ::prost::alloc::string::String,
    /// Output only. The version ID of the model that is deployed.
    #[prost(string, tag = "18")]
    pub model_version_id: ::prost::alloc::string::String,
    /// The display name of the DeployedModel. If not provided upon creation,
    /// the Model's display_name is used.
    #[prost(string, tag = "3")]
    pub display_name: ::prost::alloc::string::String,
    /// Output only. Timestamp when the DeployedModel was created.
    #[prost(message, optional, tag = "6")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Explanation configuration for this DeployedModel.
    ///
    /// When deploying a Model using
    /// [EndpointService.DeployModel][google.cloud.aiplatform.v1.EndpointService.DeployModel],
    /// this value overrides the value of
    /// [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec].
    /// All fields of
    /// [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
    /// are optional in the request. If a field of
    /// [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
    /// is not populated, the value of the same field of
    /// [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec]
    /// is inherited. If the corresponding
    /// [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec]
    /// is not populated, all fields of the
    /// [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
    /// will be used for the explanation configuration.
    #[prost(message, optional, tag = "9")]
    pub explanation_spec: ::core::option::Option<ExplanationSpec>,
    /// If true, deploy the model without explainable feature, regardless the
    /// existence of
    /// [Model.explanation_spec][google.cloud.aiplatform.v1.Model.explanation_spec]
    /// or
    /// [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec].
    #[prost(bool, tag = "19")]
    pub disable_explanations: bool,
    /// The service account that the DeployedModel's container runs as. Specify the
    /// email address of the service account. If this service account is not
    /// specified, the container runs as a service account that doesn't have access
    /// to the resource project.
    ///
    /// Users deploying the Model must have the `iam.serviceAccounts.actAs`
    /// permission on this service account.
    #[prost(string, tag = "11")]
    pub service_account: ::prost::alloc::string::String,
    /// For custom-trained Models and AutoML Tabular Models, the container of the
    /// DeployedModel instances will send `stderr` and `stdout` streams to
    /// Cloud Logging by default. Please note that the logs incur cost,
    /// which are subject to [Cloud Logging
    /// pricing](<https://cloud.google.com/logging/pricing>).
    ///
    /// User can disable container logging by setting this flag to true.
    #[prost(bool, tag = "15")]
    pub disable_container_logging: bool,
    /// If true, online prediction access logs are sent to Cloud
    /// Logging.
    /// These logs are like standard server access logs, containing
    /// information like timestamp and latency for each prediction request.
    ///
    /// Note that logs may incur a cost, especially if your project
    /// receives prediction requests at a high queries per second rate (QPS).
    /// Estimate your costs before enabling this option.
    #[prost(bool, tag = "13")]
    pub enable_access_logging: bool,
    /// Output only. Provide paths for users to send predict/explain/health
    /// requests directly to the deployed model services running on Cloud via
    /// private services access. This field is populated if
    /// [network][google.cloud.aiplatform.v1.Endpoint.network] is configured.
    #[prost(message, optional, tag = "14")]
    pub private_endpoints: ::core::option::Option<PrivateEndpoints>,
    /// Configuration for faster model deployment.
    #[prost(message, optional, tag = "23")]
    pub faster_deployment_config: ::core::option::Option<FasterDeploymentConfig>,
    /// Output only. Runtime status of the deployed model.
    #[prost(message, optional, tag = "26")]
    pub status: ::core::option::Option<deployed_model::Status>,
    /// System labels to apply to Model Garden deployments.
    /// System labels are managed by Google for internal use only.
    #[prost(map = "string, string", tag = "28")]
    pub system_labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. Spec for configuring speculative decoding.
    #[prost(message, optional, tag = "30")]
    pub speculative_decoding_spec: ::core::option::Option<SpeculativeDecodingSpec>,
    /// The prediction (for example, the machine) resources that the DeployedModel
    /// uses. The user is billed for the resources (at least their minimal amount)
    /// even if the DeployedModel receives no traffic.
    /// Not all Models support all resources types. See
    /// [Model.supported_deployment_resources_types][google.cloud.aiplatform.v1.Model.supported_deployment_resources_types].
    /// Required except for Large Model Deploy use cases.
    #[prost(oneof = "deployed_model::PredictionResources", tags = "7, 8, 17")]
    pub prediction_resources: ::core::option::Option<
        deployed_model::PredictionResources,
    >,
}
/// Nested message and enum types in `DeployedModel`.
pub mod deployed_model {
    /// Runtime status of the deployed model.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Status {
        /// Output only. The latest deployed model's status message (if any).
        #[prost(string, tag = "1")]
        pub message: ::prost::alloc::string::String,
        /// Output only. The time at which the status was last updated.
        #[prost(message, optional, tag = "2")]
        pub last_update_time: ::core::option::Option<::prost_types::Timestamp>,
        /// Output only. The number of available replicas of the deployed model.
        #[prost(int32, tag = "3")]
        pub available_replica_count: i32,
    }
    /// The prediction (for example, the machine) resources that the DeployedModel
    /// uses. The user is billed for the resources (at least their minimal amount)
    /// even if the DeployedModel receives no traffic.
    /// Not all Models support all resources types. See
    /// [Model.supported_deployment_resources_types][google.cloud.aiplatform.v1.Model.supported_deployment_resources_types].
    /// Required except for Large Model Deploy use cases.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum PredictionResources {
        /// A description of resources that are dedicated to the DeployedModel, and
        /// that need a higher degree of manual configuration.
        #[prost(message, tag = "7")]
        DedicatedResources(super::DedicatedResources),
        /// A description of resources that to large degree are decided by Vertex
        /// AI, and require only a modest additional configuration.
        #[prost(message, tag = "8")]
        AutomaticResources(super::AutomaticResources),
        /// The resource name of the shared DeploymentResourcePool to deploy on.
        /// Format:
        /// `projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}`
        #[prost(string, tag = "17")]
        SharedResources(::prost::alloc::string::String),
    }
}
/// PrivateEndpoints proto is used to provide paths for users to send
/// requests privately.
/// To send request via private service access, use predict_http_uri,
/// explain_http_uri or health_http_uri. To send request via private service
/// connect, use service_attachment.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PrivateEndpoints {
    /// Output only. Http(s) path to send prediction requests.
    #[prost(string, tag = "1")]
    pub predict_http_uri: ::prost::alloc::string::String,
    /// Output only. Http(s) path to send explain requests.
    #[prost(string, tag = "2")]
    pub explain_http_uri: ::prost::alloc::string::String,
    /// Output only. Http(s) path to send health check requests.
    #[prost(string, tag = "3")]
    pub health_http_uri: ::prost::alloc::string::String,
    /// Output only. The name of the service attachment resource. Populated if
    /// private service connect is enabled.
    #[prost(string, tag = "4")]
    pub service_attachment: ::prost::alloc::string::String,
}
/// Configuration for logging request-response to a BigQuery table.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PredictRequestResponseLoggingConfig {
    /// If logging is enabled or not.
    #[prost(bool, tag = "1")]
    pub enabled: bool,
    /// Percentage of requests to be logged, expressed as a fraction in
    /// range(0,1].
    #[prost(double, tag = "2")]
    pub sampling_rate: f64,
    /// BigQuery table for logging.
    /// If only given a project, a new dataset will be created with name
    /// `logging_<endpoint-display-name>_<endpoint-id>` where
    /// <endpoint-display-name> will be made BigQuery-dataset-name compatible (e.g.
    /// most special characters will become underscores). If no table name is
    /// given, a new table will be created with name `request_response_logging`
    #[prost(message, optional, tag = "3")]
    pub bigquery_destination: ::core::option::Option<BigQueryDestination>,
}
/// Configurations (e.g. inference timeout) that are applied on your endpoints.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ClientConnectionConfig {
    /// Customizable online prediction request timeout.
    #[prost(message, optional, tag = "1")]
    pub inference_timeout: ::core::option::Option<::prost_types::Duration>,
}
/// Configuration for faster model deployment.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct FasterDeploymentConfig {
    /// If true, enable fast tryout feature for this deployed model.
    #[prost(bool, tag = "2")]
    pub fast_tryout_enabled: bool,
}
/// Configuration for Speculative Decoding.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SpeculativeDecodingSpec {
    /// The number of speculative tokens to generate at each step.
    #[prost(int32, tag = "1")]
    pub speculative_token_count: i32,
    /// The type of speculation method to use.
    #[prost(oneof = "speculative_decoding_spec::Speculation", tags = "2, 3")]
    pub speculation: ::core::option::Option<speculative_decoding_spec::Speculation>,
}
/// Nested message and enum types in `SpeculativeDecodingSpec`.
pub mod speculative_decoding_spec {
    /// Draft model speculation works by using the smaller model to generate
    /// candidate tokens for speculative decoding.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct DraftModelSpeculation {
        /// Required. The resource name of the draft model.
        #[prost(string, tag = "1")]
        pub draft_model: ::prost::alloc::string::String,
    }
    /// N-Gram speculation works by trying to find matching tokens in the
    /// previous prompt sequence and use those as speculation for generating
    /// new tokens.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct NgramSpeculation {
        /// The number of last N input tokens used as ngram to search/match
        /// against the previous prompt sequence.
        /// This is equal to the N in N-Gram.
        /// The default value is 3 if not specified.
        #[prost(int32, tag = "1")]
        pub ngram_size: i32,
    }
    /// The type of speculation method to use.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Speculation {
        /// draft model speculation.
        #[prost(message, tag = "2")]
        DraftModelSpeculation(DraftModelSpeculation),
        /// N-Gram speculation.
        #[prost(message, tag = "3")]
        NgramSpeculation(NgramSpeculation),
    }
}
/// Request message for CreateDeploymentResourcePool method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateDeploymentResourcePoolRequest {
    /// Required. The parent location resource where this DeploymentResourcePool
    /// will be created. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The DeploymentResourcePool to create.
    #[prost(message, optional, tag = "2")]
    pub deployment_resource_pool: ::core::option::Option<DeploymentResourcePool>,
    /// Required. The ID to use for the DeploymentResourcePool, which
    /// will become the final component of the DeploymentResourcePool's resource
    /// name.
    ///
    /// The maximum length is 63 characters, and valid characters
    /// are `/^[a-z](\[a-z0-9-\]{0,61}\[a-z0-9\])?$/`.
    #[prost(string, tag = "3")]
    pub deployment_resource_pool_id: ::prost::alloc::string::String,
}
/// Runtime operation information for CreateDeploymentResourcePool method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateDeploymentResourcePoolOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for GetDeploymentResourcePool method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetDeploymentResourcePoolRequest {
    /// Required. The name of the DeploymentResourcePool to retrieve.
    /// Format:
    /// `projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for ListDeploymentResourcePools method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListDeploymentResourcePoolsRequest {
    /// Required. The parent Location which owns this collection of
    /// DeploymentResourcePools. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The maximum number of DeploymentResourcePools to return. The service may
    /// return fewer than this value.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// A page token, received from a previous `ListDeploymentResourcePools` call.
    /// Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// `ListDeploymentResourcePools` must match the call that provided the page
    /// token.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response message for ListDeploymentResourcePools method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListDeploymentResourcePoolsResponse {
    /// The DeploymentResourcePools from the specified location.
    #[prost(message, repeated, tag = "1")]
    pub deployment_resource_pools: ::prost::alloc::vec::Vec<DeploymentResourcePool>,
    /// A token, which can be sent as `page_token` to retrieve the next page.
    /// If this field is omitted, there are no subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for UpdateDeploymentResourcePool method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateDeploymentResourcePoolRequest {
    /// Required. The DeploymentResourcePool to update.
    ///
    /// The DeploymentResourcePool's `name` field is used to identify the
    /// DeploymentResourcePool to update.
    /// Format:
    /// `projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}`
    #[prost(message, optional, tag = "1")]
    pub deployment_resource_pool: ::core::option::Option<DeploymentResourcePool>,
    /// Required. The list of fields to update.
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Runtime operation information for UpdateDeploymentResourcePool method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateDeploymentResourcePoolOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for DeleteDeploymentResourcePool method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteDeploymentResourcePoolRequest {
    /// Required. The name of the DeploymentResourcePool to delete.
    /// Format:
    /// `projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for QueryDeployedModels method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryDeployedModelsRequest {
    /// Required. The name of the target DeploymentResourcePool to query.
    /// Format:
    /// `projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}`
    #[prost(string, tag = "1")]
    pub deployment_resource_pool: ::prost::alloc::string::String,
    /// The maximum number of DeployedModels to return. The service may return
    /// fewer than this value.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// A page token, received from a previous `QueryDeployedModels` call.
    /// Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// `QueryDeployedModels` must match the call that provided the page
    /// token.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response message for QueryDeployedModels method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryDeployedModelsResponse {
    /// DEPRECATED Use deployed_model_refs instead.
    #[deprecated]
    #[prost(message, repeated, tag = "1")]
    pub deployed_models: ::prost::alloc::vec::Vec<DeployedModel>,
    /// A token, which can be sent as `page_token` to retrieve the next page.
    /// If this field is omitted, there are no subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
    /// References to the DeployedModels that share the specified
    /// deploymentResourcePool.
    #[prost(message, repeated, tag = "3")]
    pub deployed_model_refs: ::prost::alloc::vec::Vec<DeployedModelRef>,
    /// The total number of DeployedModels on this DeploymentResourcePool.
    #[prost(int32, tag = "4")]
    pub total_deployed_model_count: i32,
    /// The total number of Endpoints that have DeployedModels on this
    /// DeploymentResourcePool.
    #[prost(int32, tag = "5")]
    pub total_endpoint_count: i32,
}
/// Generated client implementations.
pub mod deployment_resource_pool_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service that manages the DeploymentResourcePool resource.
    #[derive(Debug, Clone)]
    pub struct DeploymentResourcePoolServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl DeploymentResourcePoolServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> DeploymentResourcePoolServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> DeploymentResourcePoolServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            DeploymentResourcePoolServiceClient::new(
                InterceptedService::new(inner, interceptor),
            )
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Create a DeploymentResourcePool.
        pub async fn create_deployment_resource_pool(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateDeploymentResourcePoolRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DeploymentResourcePoolService/CreateDeploymentResourcePool",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DeploymentResourcePoolService",
                        "CreateDeploymentResourcePool",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Get a DeploymentResourcePool.
        pub async fn get_deployment_resource_pool(
            &mut self,
            request: impl tonic::IntoRequest<super::GetDeploymentResourcePoolRequest>,
        ) -> std::result::Result<
            tonic::Response<super::DeploymentResourcePool>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DeploymentResourcePoolService/GetDeploymentResourcePool",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DeploymentResourcePoolService",
                        "GetDeploymentResourcePool",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// List DeploymentResourcePools in a location.
        pub async fn list_deployment_resource_pools(
            &mut self,
            request: impl tonic::IntoRequest<super::ListDeploymentResourcePoolsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListDeploymentResourcePoolsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DeploymentResourcePoolService/ListDeploymentResourcePools",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DeploymentResourcePoolService",
                        "ListDeploymentResourcePools",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Update a DeploymentResourcePool.
        pub async fn update_deployment_resource_pool(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateDeploymentResourcePoolRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DeploymentResourcePoolService/UpdateDeploymentResourcePool",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DeploymentResourcePoolService",
                        "UpdateDeploymentResourcePool",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Delete a DeploymentResourcePool.
        pub async fn delete_deployment_resource_pool(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteDeploymentResourcePoolRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DeploymentResourcePoolService/DeleteDeploymentResourcePool",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DeploymentResourcePoolService",
                        "DeleteDeploymentResourcePool",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// List DeployedModels that have been deployed on this DeploymentResourcePool.
        pub async fn query_deployed_models(
            &mut self,
            request: impl tonic::IntoRequest<super::QueryDeployedModelsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::QueryDeployedModelsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.DeploymentResourcePoolService/QueryDeployedModels",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.DeploymentResourcePoolService",
                        "QueryDeployedModels",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Request message for
/// [EndpointService.CreateEndpoint][google.cloud.aiplatform.v1.EndpointService.CreateEndpoint].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateEndpointRequest {
    /// Required. The resource name of the Location to create the Endpoint in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The Endpoint to create.
    #[prost(message, optional, tag = "2")]
    pub endpoint: ::core::option::Option<Endpoint>,
    /// Immutable. The ID to use for endpoint, which will become the final
    /// component of the endpoint resource name.
    /// If not provided, Vertex AI will generate a value for this ID.
    ///
    /// If the first character is a letter, this value may be up to 63 characters,
    /// and valid characters are `\[a-z0-9-\]`. The last character must be a letter
    /// or number.
    ///
    /// If the first character is a number, this value may be up to 9 characters,
    /// and valid characters are `\[0-9\]` with no leading zeros.
    ///
    /// When using HTTP/JSON, this field is populated
    /// based on a query string argument, such as `?endpoint_id=12345`. This is the
    /// fallback for fields that are not included in either the URI or the body.
    #[prost(string, tag = "4")]
    pub endpoint_id: ::prost::alloc::string::String,
}
/// Runtime operation information for
/// [EndpointService.CreateEndpoint][google.cloud.aiplatform.v1.EndpointService.CreateEndpoint].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateEndpointOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [EndpointService.GetEndpoint][google.cloud.aiplatform.v1.EndpointService.GetEndpoint]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetEndpointRequest {
    /// Required. The name of the Endpoint resource.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [EndpointService.ListEndpoints][google.cloud.aiplatform.v1.EndpointService.ListEndpoints].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListEndpointsRequest {
    /// Required. The resource name of the Location from which to list the
    /// Endpoints. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. An expression for filtering the results of the request. For field
    /// names both snake_case and camelCase are supported.
    ///
    ///    * `endpoint` supports `=` and `!=`. `endpoint` represents the Endpoint
    ///      ID, i.e. the last segment of the Endpoint's
    ///      [resource name][google.cloud.aiplatform.v1.Endpoint.name].
    ///    * `display_name` supports `=` and `!=`.
    ///    * `labels` supports general map functions that is:
    ///      * `labels.key=value` - key:value equality
    ///      * `labels.key:*` or `labels:key` - key existence
    ///      * A key including a space must be quoted. `labels."a key"`.
    ///    * `base_model_name` only supports `=`.
    ///
    /// Some examples:
    ///
    ///    * `endpoint=1`
    ///    * `displayName="myDisplayName"`
    ///    * `labels.myKey="myValue"`
    ///    * `baseModelName="text-bison"`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// Optional. The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// Optional. The standard list page token.
    /// Typically obtained via
    /// [ListEndpointsResponse.next_page_token][google.cloud.aiplatform.v1.ListEndpointsResponse.next_page_token]
    /// of the previous
    /// [EndpointService.ListEndpoints][google.cloud.aiplatform.v1.EndpointService.ListEndpoints]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Optional. Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    /// Supported fields:
    ///
    ///    * `display_name`
    ///    * `create_time`
    ///    * `update_time`
    ///
    /// Example: `display_name, create_time desc`.
    #[prost(string, tag = "6")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [EndpointService.ListEndpoints][google.cloud.aiplatform.v1.EndpointService.ListEndpoints].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListEndpointsResponse {
    /// List of Endpoints in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub endpoints: ::prost::alloc::vec::Vec<Endpoint>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListEndpointsRequest.page_token][google.cloud.aiplatform.v1.ListEndpointsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [EndpointService.UpdateEndpoint][google.cloud.aiplatform.v1.EndpointService.UpdateEndpoint].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateEndpointRequest {
    /// Required. The Endpoint which replaces the resource on the server.
    #[prost(message, optional, tag = "1")]
    pub endpoint: ::core::option::Option<Endpoint>,
    /// Required. The update mask applies to the resource. See
    /// [google.protobuf.FieldMask][google.protobuf.FieldMask].
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [EndpointService.UpdateEndpointLongRunning][google.cloud.aiplatform.v1.EndpointService.UpdateEndpointLongRunning].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateEndpointLongRunningRequest {
    /// Required. The Endpoint which replaces the resource on the server. Currently
    /// we only support updating the `client_connection_config` field, all the
    /// other fields' update will be blocked.
    #[prost(message, optional, tag = "1")]
    pub endpoint: ::core::option::Option<Endpoint>,
}
/// Runtime operation information for
/// [EndpointService.UpdateEndpointLongRunning][google.cloud.aiplatform.v1.EndpointService.UpdateEndpointLongRunning].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateEndpointOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [EndpointService.DeleteEndpoint][google.cloud.aiplatform.v1.EndpointService.DeleteEndpoint].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteEndpointRequest {
    /// Required. The name of the Endpoint resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [EndpointService.DeployModel][google.cloud.aiplatform.v1.EndpointService.DeployModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeployModelRequest {
    /// Required. The name of the Endpoint resource into which to deploy a Model.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// Required. The DeployedModel to be created within the Endpoint. Note that
    /// [Endpoint.traffic_split][google.cloud.aiplatform.v1.Endpoint.traffic_split]
    /// must be updated for the DeployedModel to start receiving traffic, either as
    /// part of this call, or via
    /// [EndpointService.UpdateEndpoint][google.cloud.aiplatform.v1.EndpointService.UpdateEndpoint].
    #[prost(message, optional, tag = "2")]
    pub deployed_model: ::core::option::Option<DeployedModel>,
    /// A map from a DeployedModel's ID to the percentage of this Endpoint's
    /// traffic that should be forwarded to that DeployedModel.
    ///
    /// If this field is non-empty, then the Endpoint's
    /// [traffic_split][google.cloud.aiplatform.v1.Endpoint.traffic_split] will be
    /// overwritten with it. To refer to the ID of the just being deployed Model, a
    /// "0" should be used, and the actual ID of the new DeployedModel will be
    /// filled in its place by this method. The traffic percentage values must add
    /// up to 100.
    ///
    /// If this field is empty, then the Endpoint's
    /// [traffic_split][google.cloud.aiplatform.v1.Endpoint.traffic_split] is not
    /// updated.
    #[prost(map = "string, int32", tag = "3")]
    pub traffic_split: ::std::collections::HashMap<::prost::alloc::string::String, i32>,
}
/// Response message for
/// [EndpointService.DeployModel][google.cloud.aiplatform.v1.EndpointService.DeployModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeployModelResponse {
    /// The DeployedModel that had been deployed in the Endpoint.
    #[prost(message, optional, tag = "1")]
    pub deployed_model: ::core::option::Option<DeployedModel>,
}
/// Runtime operation information for
/// [EndpointService.DeployModel][google.cloud.aiplatform.v1.EndpointService.DeployModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeployModelOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [EndpointService.UndeployModel][google.cloud.aiplatform.v1.EndpointService.UndeployModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UndeployModelRequest {
    /// Required. The name of the Endpoint resource from which to undeploy a Model.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// Required. The ID of the DeployedModel to be undeployed from the Endpoint.
    #[prost(string, tag = "2")]
    pub deployed_model_id: ::prost::alloc::string::String,
    /// If this field is provided, then the Endpoint's
    /// [traffic_split][google.cloud.aiplatform.v1.Endpoint.traffic_split] will be
    /// overwritten with it. If last DeployedModel is being undeployed from the
    /// Endpoint, the \[Endpoint.traffic_split\] will always end up empty when this
    /// call returns. A DeployedModel will be successfully undeployed only if it
    /// doesn't have any traffic assigned to it when this method executes, or if
    /// this field unassigns any traffic to it.
    #[prost(map = "string, int32", tag = "3")]
    pub traffic_split: ::std::collections::HashMap<::prost::alloc::string::String, i32>,
}
/// Response message for
/// [EndpointService.UndeployModel][google.cloud.aiplatform.v1.EndpointService.UndeployModel].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct UndeployModelResponse {}
/// Runtime operation information for
/// [EndpointService.UndeployModel][google.cloud.aiplatform.v1.EndpointService.UndeployModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UndeployModelOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [EndpointService.MutateDeployedModel][google.cloud.aiplatform.v1.EndpointService.MutateDeployedModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MutateDeployedModelRequest {
    /// Required. The name of the Endpoint resource into which to mutate a
    /// DeployedModel. Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// Required. The DeployedModel to be mutated within the Endpoint. Only the
    /// following fields can be mutated:
    ///
    /// * `min_replica_count` in either
    /// [DedicatedResources][google.cloud.aiplatform.v1.DedicatedResources] or
    /// [AutomaticResources][google.cloud.aiplatform.v1.AutomaticResources]
    /// * `max_replica_count` in either
    /// [DedicatedResources][google.cloud.aiplatform.v1.DedicatedResources] or
    /// [AutomaticResources][google.cloud.aiplatform.v1.AutomaticResources]
    /// * [autoscaling_metric_specs][google.cloud.aiplatform.v1.DedicatedResources.autoscaling_metric_specs]
    /// * `disable_container_logging` (v1 only)
    /// * `enable_container_logging` (v1beta1 only)
    #[prost(message, optional, tag = "2")]
    pub deployed_model: ::core::option::Option<DeployedModel>,
    /// Required. The update mask applies to the resource. See
    /// [google.protobuf.FieldMask][google.protobuf.FieldMask].
    #[prost(message, optional, tag = "4")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [EndpointService.MutateDeployedModel][google.cloud.aiplatform.v1.EndpointService.MutateDeployedModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MutateDeployedModelResponse {
    /// The DeployedModel that's being mutated.
    #[prost(message, optional, tag = "1")]
    pub deployed_model: ::core::option::Option<DeployedModel>,
}
/// Runtime operation information for
/// [EndpointService.MutateDeployedModel][google.cloud.aiplatform.v1.EndpointService.MutateDeployedModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MutateDeployedModelOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Generated client implementations.
pub mod endpoint_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for managing Vertex AI's Endpoints.
    #[derive(Debug, Clone)]
    pub struct EndpointServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl EndpointServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> EndpointServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> EndpointServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            EndpointServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates an Endpoint.
        pub async fn create_endpoint(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateEndpointRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.EndpointService/CreateEndpoint",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.EndpointService",
                        "CreateEndpoint",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets an Endpoint.
        pub async fn get_endpoint(
            &mut self,
            request: impl tonic::IntoRequest<super::GetEndpointRequest>,
        ) -> std::result::Result<tonic::Response<super::Endpoint>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.EndpointService/GetEndpoint",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.EndpointService",
                        "GetEndpoint",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Endpoints in a Location.
        pub async fn list_endpoints(
            &mut self,
            request: impl tonic::IntoRequest<super::ListEndpointsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListEndpointsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.EndpointService/ListEndpoints",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.EndpointService",
                        "ListEndpoints",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates an Endpoint.
        pub async fn update_endpoint(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateEndpointRequest>,
        ) -> std::result::Result<tonic::Response<super::Endpoint>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.EndpointService/UpdateEndpoint",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.EndpointService",
                        "UpdateEndpoint",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates an Endpoint with a long running operation.
        pub async fn update_endpoint_long_running(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateEndpointLongRunningRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.EndpointService/UpdateEndpointLongRunning",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.EndpointService",
                        "UpdateEndpointLongRunning",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes an Endpoint.
        pub async fn delete_endpoint(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteEndpointRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.EndpointService/DeleteEndpoint",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.EndpointService",
                        "DeleteEndpoint",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deploys a Model into this Endpoint, creating a DeployedModel within it.
        pub async fn deploy_model(
            &mut self,
            request: impl tonic::IntoRequest<super::DeployModelRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.EndpointService/DeployModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.EndpointService",
                        "DeployModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Undeploys a Model from an Endpoint, removing a DeployedModel from it, and
        /// freeing all resources it's using.
        pub async fn undeploy_model(
            &mut self,
            request: impl tonic::IntoRequest<super::UndeployModelRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.EndpointService/UndeployModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.EndpointService",
                        "UndeployModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates an existing deployed model. Updatable fields include
        /// `min_replica_count`, `max_replica_count`, `autoscaling_metric_specs`,
        /// `disable_container_logging` (v1 only), and `enable_container_logging`
        /// (v1beta1 only).
        pub async fn mutate_deployed_model(
            &mut self,
            request: impl tonic::IntoRequest<super::MutateDeployedModelRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.EndpointService/MutateDeployedModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.EndpointService",
                        "MutateDeployedModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Configuration of how features in Featurestore are monitored.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct FeaturestoreMonitoringConfig {
    /// The config for Snapshot Analysis Based Feature Monitoring.
    #[prost(message, optional, tag = "1")]
    pub snapshot_analysis: ::core::option::Option<
        featurestore_monitoring_config::SnapshotAnalysis,
    >,
    /// The config for ImportFeatures Analysis Based Feature Monitoring.
    #[prost(message, optional, tag = "2")]
    pub import_features_analysis: ::core::option::Option<
        featurestore_monitoring_config::ImportFeaturesAnalysis,
    >,
    /// Threshold for numerical features of anomaly detection.
    /// This is shared by all objectives of Featurestore Monitoring for numerical
    /// features (i.e. Features with type
    /// ([Feature.ValueType][google.cloud.aiplatform.v1.Feature.ValueType]) DOUBLE
    /// or INT64).
    #[prost(message, optional, tag = "3")]
    pub numerical_threshold_config: ::core::option::Option<
        featurestore_monitoring_config::ThresholdConfig,
    >,
    /// Threshold for categorical features of anomaly detection.
    /// This is shared by all types of Featurestore Monitoring for categorical
    /// features (i.e. Features with type
    /// ([Feature.ValueType][google.cloud.aiplatform.v1.Feature.ValueType]) BOOL or
    /// STRING).
    #[prost(message, optional, tag = "4")]
    pub categorical_threshold_config: ::core::option::Option<
        featurestore_monitoring_config::ThresholdConfig,
    >,
}
/// Nested message and enum types in `FeaturestoreMonitoringConfig`.
pub mod featurestore_monitoring_config {
    /// Configuration of the Featurestore's Snapshot Analysis Based Monitoring.
    /// This type of analysis generates statistics for each Feature based on a
    /// snapshot of the latest feature value of each entities every
    /// monitoring_interval.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct SnapshotAnalysis {
        /// The monitoring schedule for snapshot analysis.
        /// For EntityType-level config:
        ///    unset / disabled = true indicates disabled by
        ///    default for Features under it; otherwise by default enable snapshot
        ///    analysis monitoring with monitoring_interval for Features under it.
        /// Feature-level config:
        ///    disabled = true indicates disabled regardless of the EntityType-level
        ///    config; unset monitoring_interval indicates going with EntityType-level
        ///    config; otherwise run snapshot analysis monitoring with
        ///    monitoring_interval regardless of the EntityType-level config.
        /// Explicitly Disable the snapshot analysis based monitoring.
        #[prost(bool, tag = "1")]
        pub disabled: bool,
        /// Configuration of the snapshot analysis based monitoring pipeline
        /// running interval. The value indicates number of days.
        #[prost(int32, tag = "3")]
        pub monitoring_interval_days: i32,
        /// Customized export features time window for snapshot analysis. Unit is one
        /// day. Default value is 3 weeks. Minimum value is 1 day. Maximum value is
        /// 4000 days.
        #[prost(int32, tag = "4")]
        pub staleness_days: i32,
    }
    /// Configuration of the Featurestore's ImportFeature Analysis Based
    /// Monitoring. This type of analysis generates statistics for values of each
    /// Feature imported by every
    /// [ImportFeatureValues][google.cloud.aiplatform.v1.FeaturestoreService.ImportFeatureValues]
    /// operation.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct ImportFeaturesAnalysis {
        /// Whether to enable / disable / inherite default hebavior for import
        /// features analysis.
        #[prost(enumeration = "import_features_analysis::State", tag = "1")]
        pub state: i32,
        /// The baseline used to do anomaly detection for the statistics generated by
        /// import features analysis.
        #[prost(enumeration = "import_features_analysis::Baseline", tag = "2")]
        pub anomaly_detection_baseline: i32,
    }
    /// Nested message and enum types in `ImportFeaturesAnalysis`.
    pub mod import_features_analysis {
        /// The state defines whether to enable ImportFeature analysis.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum State {
            /// Should not be used.
            Unspecified = 0,
            /// The default behavior of whether to enable the monitoring.
            /// EntityType-level config: disabled.
            /// Feature-level config: inherited from the configuration of EntityType
            /// this Feature belongs to.
            Default = 1,
            /// Explicitly enables import features analysis.
            /// EntityType-level config: by default enables import features analysis
            /// for all Features under it. Feature-level config: enables import
            /// features analysis regardless of the EntityType-level config.
            Enabled = 2,
            /// Explicitly disables import features analysis.
            /// EntityType-level config: by default disables import features analysis
            /// for all Features under it. Feature-level config: disables import
            /// features analysis regardless of the EntityType-level config.
            Disabled = 3,
        }
        impl State {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "STATE_UNSPECIFIED",
                    Self::Default => "DEFAULT",
                    Self::Enabled => "ENABLED",
                    Self::Disabled => "DISABLED",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                    "DEFAULT" => Some(Self::Default),
                    "ENABLED" => Some(Self::Enabled),
                    "DISABLED" => Some(Self::Disabled),
                    _ => None,
                }
            }
        }
        /// Defines the baseline to do anomaly detection for feature values imported
        /// by each
        /// [ImportFeatureValues][google.cloud.aiplatform.v1.FeaturestoreService.ImportFeatureValues]
        /// operation.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum Baseline {
            /// Should not be used.
            Unspecified = 0,
            /// Choose the later one statistics generated by either most recent
            /// snapshot analysis or previous import features analysis. If non of them
            /// exists, skip anomaly detection and only generate a statistics.
            LatestStats = 1,
            /// Use the statistics generated by the most recent snapshot analysis if
            /// exists.
            MostRecentSnapshotStats = 2,
            /// Use the statistics generated by the previous import features analysis
            /// if exists.
            PreviousImportFeaturesStats = 3,
        }
        impl Baseline {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "BASELINE_UNSPECIFIED",
                    Self::LatestStats => "LATEST_STATS",
                    Self::MostRecentSnapshotStats => "MOST_RECENT_SNAPSHOT_STATS",
                    Self::PreviousImportFeaturesStats => "PREVIOUS_IMPORT_FEATURES_STATS",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "BASELINE_UNSPECIFIED" => Some(Self::Unspecified),
                    "LATEST_STATS" => Some(Self::LatestStats),
                    "MOST_RECENT_SNAPSHOT_STATS" => Some(Self::MostRecentSnapshotStats),
                    "PREVIOUS_IMPORT_FEATURES_STATS" => {
                        Some(Self::PreviousImportFeaturesStats)
                    }
                    _ => None,
                }
            }
        }
    }
    /// The config for Featurestore Monitoring threshold.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct ThresholdConfig {
        #[prost(oneof = "threshold_config::Threshold", tags = "1")]
        pub threshold: ::core::option::Option<threshold_config::Threshold>,
    }
    /// Nested message and enum types in `ThresholdConfig`.
    pub mod threshold_config {
        #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
        pub enum Threshold {
            /// Specify a threshold value that can trigger the alert.
            /// 1. For categorical feature, the distribution distance is calculated by
            /// L-inifinity norm.
            /// 2. For numerical feature, the distribution distance is calculated by
            /// Jensen–Shannon divergence. Each feature must have a non-zero threshold
            /// if they need to be monitored. Otherwise no alert will be triggered for
            /// that feature.
            #[prost(double, tag = "1")]
            Value(f64),
        }
    }
}
/// An entity type is a type of object in a system that needs to be modeled and
/// have stored information about. For example, driver is an entity type, and
/// driver0 is an instance of an entity type driver.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EntityType {
    /// Immutable. Name of the EntityType.
    /// Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity_type}`
    ///
    /// The last part entity_type is assigned by the client. The entity_type can be
    /// up to 64 characters long and can consist only of ASCII Latin letters A-Z
    /// and a-z and underscore(_), and ASCII digits 0-9 starting with a letter. The
    /// value will be unique given a featurestore.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. Description of the EntityType.
    #[prost(string, tag = "2")]
    pub description: ::prost::alloc::string::String,
    /// Output only. Timestamp when this EntityType was created.
    #[prost(message, optional, tag = "3")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this EntityType was most recently updated.
    #[prost(message, optional, tag = "4")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. The labels with user-defined metadata to organize your
    /// EntityTypes.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information on and examples of labels.
    /// No more than 64 user labels can be associated with one EntityType (System
    /// labels are excluded)."
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable.
    #[prost(map = "string, string", tag = "6")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. Used to perform a consistent read-modify-write updates. If not
    /// set, a blind "overwrite" update happens.
    #[prost(string, tag = "7")]
    pub etag: ::prost::alloc::string::String,
    /// Optional. The default monitoring configuration for all Features with value
    /// type
    /// ([Feature.ValueType][google.cloud.aiplatform.v1.Feature.ValueType]) BOOL,
    /// STRING, DOUBLE or INT64 under this EntityType.
    ///
    /// If this is populated with
    /// \[FeaturestoreMonitoringConfig.monitoring_interval\] specified, snapshot
    /// analysis monitoring is enabled. Otherwise, snapshot analysis monitoring is
    /// disabled.
    #[prost(message, optional, tag = "8")]
    pub monitoring_config: ::core::option::Option<FeaturestoreMonitoringConfig>,
    /// Optional. Config for data retention policy in offline storage.
    /// TTL in days for feature values that will be stored in offline storage.
    /// The Feature Store offline storage periodically removes obsolete feature
    /// values older than `offline_storage_ttl_days` since the feature generation
    /// time. If unset (or explicitly set to 0), default to 4000 days TTL.
    #[prost(int32, tag = "10")]
    pub offline_storage_ttl_days: i32,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "11")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "12")]
    pub satisfies_pzi: bool,
}
/// True positive, false positive, or false negative.
///
/// EvaluatedAnnotation is only available under ModelEvaluationSlice with slice
/// of `annotationSpec` dimension.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EvaluatedAnnotation {
    /// Output only. Type of the EvaluatedAnnotation.
    #[prost(enumeration = "evaluated_annotation::EvaluatedAnnotationType", tag = "1")]
    pub r#type: i32,
    /// Output only. The model predicted annotations.
    ///
    /// For true positive, there is one and only one prediction, which matches the
    /// only one ground truth annotation in
    /// [ground_truths][google.cloud.aiplatform.v1.EvaluatedAnnotation.ground_truths].
    ///
    /// For false positive, there is one and only one prediction, which doesn't
    /// match any ground truth annotation of the corresponding
    /// [data_item_view_id][google.cloud.aiplatform.v1.EvaluatedAnnotation.evaluated_data_item_view_id].
    ///
    /// For false negative, there are zero or more predictions which are similar to
    /// the only ground truth annotation in
    /// [ground_truths][google.cloud.aiplatform.v1.EvaluatedAnnotation.ground_truths]
    /// but not enough for a match.
    ///
    /// The schema of the prediction is stored in
    /// [ModelEvaluation.annotation_schema_uri][google.cloud.aiplatform.v1.ModelEvaluation.annotation_schema_uri]
    #[prost(message, repeated, tag = "2")]
    pub predictions: ::prost::alloc::vec::Vec<::prost_types::Value>,
    /// Output only. The ground truth Annotations, i.e. the Annotations that exist
    /// in the test data the Model is evaluated on.
    ///
    /// For true positive, there is one and only one ground truth annotation, which
    /// matches the only prediction in
    /// [predictions][google.cloud.aiplatform.v1.EvaluatedAnnotation.predictions].
    ///
    /// For false positive, there are zero or more ground truth annotations that
    /// are similar to the only prediction in
    /// [predictions][google.cloud.aiplatform.v1.EvaluatedAnnotation.predictions],
    /// but not enough for a match.
    ///
    /// For false negative, there is one and only one ground truth annotation,
    /// which doesn't match any predictions created by the model.
    ///
    /// The schema of the ground truth is stored in
    /// [ModelEvaluation.annotation_schema_uri][google.cloud.aiplatform.v1.ModelEvaluation.annotation_schema_uri]
    #[prost(message, repeated, tag = "3")]
    pub ground_truths: ::prost::alloc::vec::Vec<::prost_types::Value>,
    /// Output only. The data item payload that the Model predicted this
    /// EvaluatedAnnotation on.
    #[prost(message, optional, tag = "5")]
    pub data_item_payload: ::core::option::Option<::prost_types::Value>,
    /// Output only. ID of the EvaluatedDataItemView under the same ancestor
    /// ModelEvaluation. The EvaluatedDataItemView consists of all ground truths
    /// and predictions on
    /// [data_item_payload][google.cloud.aiplatform.v1.EvaluatedAnnotation.data_item_payload].
    #[prost(string, tag = "6")]
    pub evaluated_data_item_view_id: ::prost::alloc::string::String,
    /// Explanations of
    /// [predictions][google.cloud.aiplatform.v1.EvaluatedAnnotation.predictions].
    /// Each element of the explanations indicates the explanation for one
    /// explanation Method.
    ///
    /// The attributions list in the
    /// [EvaluatedAnnotationExplanation.explanation][google.cloud.aiplatform.v1.EvaluatedAnnotationExplanation.explanation]
    /// object corresponds to the
    /// [predictions][google.cloud.aiplatform.v1.EvaluatedAnnotation.predictions]
    /// list. For example, the second element in the attributions list explains the
    /// second element in the predictions list.
    #[prost(message, repeated, tag = "8")]
    pub explanations: ::prost::alloc::vec::Vec<EvaluatedAnnotationExplanation>,
    /// Annotations of model error analysis results.
    #[prost(message, repeated, tag = "9")]
    pub error_analysis_annotations: ::prost::alloc::vec::Vec<ErrorAnalysisAnnotation>,
}
/// Nested message and enum types in `EvaluatedAnnotation`.
pub mod evaluated_annotation {
    /// Describes the type of the EvaluatedAnnotation. The type is determined
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum EvaluatedAnnotationType {
        /// Invalid value.
        Unspecified = 0,
        /// The EvaluatedAnnotation is a true positive. It has a prediction created
        /// by the Model and a ground truth Annotation which the prediction matches.
        TruePositive = 1,
        /// The EvaluatedAnnotation is false positive. It has a prediction created by
        /// the Model which does not match any ground truth annotation.
        FalsePositive = 2,
        /// The EvaluatedAnnotation is false negative. It has a ground truth
        /// annotation which is not matched by any of the model created predictions.
        FalseNegative = 3,
    }
    impl EvaluatedAnnotationType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "EVALUATED_ANNOTATION_TYPE_UNSPECIFIED",
                Self::TruePositive => "TRUE_POSITIVE",
                Self::FalsePositive => "FALSE_POSITIVE",
                Self::FalseNegative => "FALSE_NEGATIVE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "EVALUATED_ANNOTATION_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "TRUE_POSITIVE" => Some(Self::TruePositive),
                "FALSE_POSITIVE" => Some(Self::FalsePositive),
                "FALSE_NEGATIVE" => Some(Self::FalseNegative),
                _ => None,
            }
        }
    }
}
/// Explanation result of the prediction produced by the Model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EvaluatedAnnotationExplanation {
    /// Explanation type.
    ///
    /// For AutoML Image Classification models, possible values are:
    ///
    ///    * `image-integrated-gradients`
    ///    * `image-xrai`
    #[prost(string, tag = "1")]
    pub explanation_type: ::prost::alloc::string::String,
    /// Explanation attribution response details.
    #[prost(message, optional, tag = "2")]
    pub explanation: ::core::option::Option<Explanation>,
}
/// Model error analysis for each annotation.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ErrorAnalysisAnnotation {
    /// Attributed items for a given annotation, typically representing neighbors
    /// from the training sets constrained by the query type.
    #[prost(message, repeated, tag = "1")]
    pub attributed_items: ::prost::alloc::vec::Vec<
        error_analysis_annotation::AttributedItem,
    >,
    /// The query type used for finding the attributed items.
    #[prost(enumeration = "error_analysis_annotation::QueryType", tag = "2")]
    pub query_type: i32,
    /// The outlier score of this annotated item. Usually defined as the min of all
    /// distances from attributed items.
    #[prost(double, tag = "3")]
    pub outlier_score: f64,
    /// The threshold used to determine if this annotation is an outlier or not.
    #[prost(double, tag = "4")]
    pub outlier_threshold: f64,
}
/// Nested message and enum types in `ErrorAnalysisAnnotation`.
pub mod error_analysis_annotation {
    /// Attributed items for a given annotation, typically representing neighbors
    /// from the training sets constrained by the query type.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct AttributedItem {
        /// The unique ID for each annotation. Used by FE to allocate the annotation
        /// in DB.
        #[prost(string, tag = "1")]
        pub annotation_resource_name: ::prost::alloc::string::String,
        /// The distance of this item to the annotation.
        #[prost(double, tag = "2")]
        pub distance: f64,
    }
    /// The query type used for finding the attributed items.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum QueryType {
        /// Unspecified query type for model error analysis.
        Unspecified = 0,
        /// Query similar samples across all classes in the dataset.
        AllSimilar = 1,
        /// Query similar samples from the same class of the input sample.
        SameClassSimilar = 2,
        /// Query dissimilar samples from the same class of the input sample.
        SameClassDissimilar = 3,
    }
    impl QueryType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "QUERY_TYPE_UNSPECIFIED",
                Self::AllSimilar => "ALL_SIMILAR",
                Self::SameClassSimilar => "SAME_CLASS_SIMILAR",
                Self::SameClassDissimilar => "SAME_CLASS_DISSIMILAR",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "QUERY_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "ALL_SIMILAR" => Some(Self::AllSimilar),
                "SAME_CLASS_SIMILAR" => Some(Self::SameClassSimilar),
                "SAME_CLASS_DISSIMILAR" => Some(Self::SameClassDissimilar),
                _ => None,
            }
        }
    }
}
/// Request message for EvaluationService.EvaluateInstances.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EvaluateInstancesRequest {
    /// Required. The resource name of the Location to evaluate the instances.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub location: ::prost::alloc::string::String,
    /// Instances and specs for evaluation
    #[prost(
        oneof = "evaluate_instances_request::MetricInputs",
        tags = "2, 3, 4, 5, 6, 8, 9, 12, 7, 23, 14, 15, 10, 24, 16, 17, 18, 28, 29, 19, 20, 21, 22, 31, 32"
    )]
    pub metric_inputs: ::core::option::Option<evaluate_instances_request::MetricInputs>,
}
/// Nested message and enum types in `EvaluateInstancesRequest`.
pub mod evaluate_instances_request {
    /// Instances and specs for evaluation
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum MetricInputs {
        /// Auto metric instances.
        /// Instances and metric spec for exact match metric.
        #[prost(message, tag = "2")]
        ExactMatchInput(super::ExactMatchInput),
        /// Instances and metric spec for bleu metric.
        #[prost(message, tag = "3")]
        BleuInput(super::BleuInput),
        /// Instances and metric spec for rouge metric.
        #[prost(message, tag = "4")]
        RougeInput(super::RougeInput),
        /// LLM-based metric instance.
        /// General text generation metrics, applicable to other categories.
        /// Input for fluency metric.
        #[prost(message, tag = "5")]
        FluencyInput(super::FluencyInput),
        /// Input for coherence metric.
        #[prost(message, tag = "6")]
        CoherenceInput(super::CoherenceInput),
        /// Input for safety metric.
        #[prost(message, tag = "8")]
        SafetyInput(super::SafetyInput),
        /// Input for groundedness metric.
        #[prost(message, tag = "9")]
        GroundednessInput(super::GroundednessInput),
        /// Input for fulfillment metric.
        #[prost(message, tag = "12")]
        FulfillmentInput(super::FulfillmentInput),
        /// Input for summarization quality metric.
        #[prost(message, tag = "7")]
        SummarizationQualityInput(super::SummarizationQualityInput),
        /// Input for pairwise summarization quality metric.
        #[prost(message, tag = "23")]
        PairwiseSummarizationQualityInput(super::PairwiseSummarizationQualityInput),
        /// Input for summarization helpfulness metric.
        #[prost(message, tag = "14")]
        SummarizationHelpfulnessInput(super::SummarizationHelpfulnessInput),
        /// Input for summarization verbosity metric.
        #[prost(message, tag = "15")]
        SummarizationVerbosityInput(super::SummarizationVerbosityInput),
        /// Input for question answering quality metric.
        #[prost(message, tag = "10")]
        QuestionAnsweringQualityInput(super::QuestionAnsweringQualityInput),
        /// Input for pairwise question answering quality metric.
        #[prost(message, tag = "24")]
        PairwiseQuestionAnsweringQualityInput(
            super::PairwiseQuestionAnsweringQualityInput,
        ),
        /// Input for question answering relevance metric.
        #[prost(message, tag = "16")]
        QuestionAnsweringRelevanceInput(super::QuestionAnsweringRelevanceInput),
        /// Input for question answering helpfulness
        /// metric.
        #[prost(message, tag = "17")]
        QuestionAnsweringHelpfulnessInput(super::QuestionAnsweringHelpfulnessInput),
        /// Input for question answering correctness
        /// metric.
        #[prost(message, tag = "18")]
        QuestionAnsweringCorrectnessInput(super::QuestionAnsweringCorrectnessInput),
        /// Input for pointwise metric.
        #[prost(message, tag = "28")]
        PointwiseMetricInput(super::PointwiseMetricInput),
        /// Input for pairwise metric.
        #[prost(message, tag = "29")]
        PairwiseMetricInput(super::PairwiseMetricInput),
        /// Tool call metric instances.
        /// Input for tool call valid metric.
        #[prost(message, tag = "19")]
        ToolCallValidInput(super::ToolCallValidInput),
        /// Input for tool name match metric.
        #[prost(message, tag = "20")]
        ToolNameMatchInput(super::ToolNameMatchInput),
        /// Input for tool parameter key match metric.
        #[prost(message, tag = "21")]
        ToolParameterKeyMatchInput(super::ToolParameterKeyMatchInput),
        /// Input for tool parameter key value match metric.
        #[prost(message, tag = "22")]
        ToolParameterKvMatchInput(super::ToolParameterKvMatchInput),
        /// Translation metrics.
        /// Input for Comet metric.
        #[prost(message, tag = "31")]
        CometInput(super::CometInput),
        /// Input for Metricx metric.
        #[prost(message, tag = "32")]
        MetricxInput(super::MetricxInput),
    }
}
/// Response message for EvaluationService.EvaluateInstances.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EvaluateInstancesResponse {
    /// Evaluation results will be served in the same order as presented in
    /// EvaluationRequest.instances.
    #[prost(
        oneof = "evaluate_instances_response::EvaluationResults",
        tags = "1, 2, 3, 4, 5, 7, 8, 11, 6, 22, 13, 14, 9, 23, 15, 16, 17, 27, 28, 18, 19, 20, 21, 29, 30"
    )]
    pub evaluation_results: ::core::option::Option<
        evaluate_instances_response::EvaluationResults,
    >,
}
/// Nested message and enum types in `EvaluateInstancesResponse`.
pub mod evaluate_instances_response {
    /// Evaluation results will be served in the same order as presented in
    /// EvaluationRequest.instances.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum EvaluationResults {
        /// Auto metric evaluation results.
        /// Results for exact match metric.
        #[prost(message, tag = "1")]
        ExactMatchResults(super::ExactMatchResults),
        /// Results for bleu metric.
        #[prost(message, tag = "2")]
        BleuResults(super::BleuResults),
        /// Results for rouge metric.
        #[prost(message, tag = "3")]
        RougeResults(super::RougeResults),
        /// LLM-based metric evaluation result.
        /// General text generation metrics, applicable to other categories.
        /// Result for fluency metric.
        #[prost(message, tag = "4")]
        FluencyResult(super::FluencyResult),
        /// Result for coherence metric.
        #[prost(message, tag = "5")]
        CoherenceResult(super::CoherenceResult),
        /// Result for safety metric.
        #[prost(message, tag = "7")]
        SafetyResult(super::SafetyResult),
        /// Result for groundedness metric.
        #[prost(message, tag = "8")]
        GroundednessResult(super::GroundednessResult),
        /// Result for fulfillment metric.
        #[prost(message, tag = "11")]
        FulfillmentResult(super::FulfillmentResult),
        /// Summarization only metrics.
        /// Result for summarization quality metric.
        #[prost(message, tag = "6")]
        SummarizationQualityResult(super::SummarizationQualityResult),
        /// Result for pairwise summarization quality metric.
        #[prost(message, tag = "22")]
        PairwiseSummarizationQualityResult(super::PairwiseSummarizationQualityResult),
        /// Result for summarization helpfulness metric.
        #[prost(message, tag = "13")]
        SummarizationHelpfulnessResult(super::SummarizationHelpfulnessResult),
        /// Result for summarization verbosity metric.
        #[prost(message, tag = "14")]
        SummarizationVerbosityResult(super::SummarizationVerbosityResult),
        /// Question answering only metrics.
        /// Result for question answering quality metric.
        #[prost(message, tag = "9")]
        QuestionAnsweringQualityResult(super::QuestionAnsweringQualityResult),
        /// Result for pairwise question answering quality metric.
        #[prost(message, tag = "23")]
        PairwiseQuestionAnsweringQualityResult(
            super::PairwiseQuestionAnsweringQualityResult,
        ),
        /// Result for question answering relevance metric.
        #[prost(message, tag = "15")]
        QuestionAnsweringRelevanceResult(super::QuestionAnsweringRelevanceResult),
        /// Result for question answering helpfulness metric.
        #[prost(message, tag = "16")]
        QuestionAnsweringHelpfulnessResult(super::QuestionAnsweringHelpfulnessResult),
        /// Result for question answering correctness metric.
        #[prost(message, tag = "17")]
        QuestionAnsweringCorrectnessResult(super::QuestionAnsweringCorrectnessResult),
        /// Generic metrics.
        /// Result for pointwise metric.
        #[prost(message, tag = "27")]
        PointwiseMetricResult(super::PointwiseMetricResult),
        /// Result for pairwise metric.
        #[prost(message, tag = "28")]
        PairwiseMetricResult(super::PairwiseMetricResult),
        /// Tool call metrics.
        ///   Results for tool call valid metric.
        #[prost(message, tag = "18")]
        ToolCallValidResults(super::ToolCallValidResults),
        /// Results for tool name match metric.
        #[prost(message, tag = "19")]
        ToolNameMatchResults(super::ToolNameMatchResults),
        /// Results for tool parameter key match  metric.
        #[prost(message, tag = "20")]
        ToolParameterKeyMatchResults(super::ToolParameterKeyMatchResults),
        /// Results for tool parameter key value match metric.
        #[prost(message, tag = "21")]
        ToolParameterKvMatchResults(super::ToolParameterKvMatchResults),
        /// Translation metrics.
        /// Result for Comet metric.
        #[prost(message, tag = "29")]
        CometResult(super::CometResult),
        /// Result for Metricx metric.
        #[prost(message, tag = "30")]
        MetricxResult(super::MetricxResult),
    }
}
/// Input for exact match metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExactMatchInput {
    /// Required. Spec for exact match metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<ExactMatchSpec>,
    /// Required. Repeated exact match instances.
    #[prost(message, repeated, tag = "2")]
    pub instances: ::prost::alloc::vec::Vec<ExactMatchInstance>,
}
/// Spec for exact match instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExactMatchInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for exact match metric - returns 1 if prediction and reference exactly
/// matches, otherwise 0.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ExactMatchSpec {}
/// Results for exact match metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExactMatchResults {
    /// Output only. Exact match metric values.
    #[prost(message, repeated, tag = "1")]
    pub exact_match_metric_values: ::prost::alloc::vec::Vec<ExactMatchMetricValue>,
}
/// Exact match metric value for an instance.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ExactMatchMetricValue {
    /// Output only. Exact match score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
}
/// Input for bleu metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BleuInput {
    /// Required. Spec for bleu score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<BleuSpec>,
    /// Required. Repeated bleu instances.
    #[prost(message, repeated, tag = "2")]
    pub instances: ::prost::alloc::vec::Vec<BleuInstance>,
}
/// Spec for bleu instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BleuInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for bleu score metric - calculates the precision of n-grams in the
/// prediction as compared to reference - returns a score ranging between 0 to 1.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct BleuSpec {
    /// Optional. Whether to use_effective_order to compute bleu score.
    #[prost(bool, tag = "1")]
    pub use_effective_order: bool,
}
/// Results for bleu metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BleuResults {
    /// Output only. Bleu metric values.
    #[prost(message, repeated, tag = "1")]
    pub bleu_metric_values: ::prost::alloc::vec::Vec<BleuMetricValue>,
}
/// Bleu metric value for an instance.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct BleuMetricValue {
    /// Output only. Bleu score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
}
/// Input for rouge metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RougeInput {
    /// Required. Spec for rouge score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<RougeSpec>,
    /// Required. Repeated rouge instances.
    #[prost(message, repeated, tag = "2")]
    pub instances: ::prost::alloc::vec::Vec<RougeInstance>,
}
/// Spec for rouge instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RougeInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for rouge score metric - calculates the recall of n-grams in prediction
/// as compared to reference - returns a score ranging between 0 and 1.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RougeSpec {
    /// Optional. Supported rouge types are rougen\[1-9\], rougeL, and rougeLsum.
    #[prost(string, tag = "1")]
    pub rouge_type: ::prost::alloc::string::String,
    /// Optional. Whether to use stemmer to compute rouge score.
    #[prost(bool, tag = "2")]
    pub use_stemmer: bool,
    /// Optional. Whether to split summaries while using rougeLsum.
    #[prost(bool, tag = "3")]
    pub split_summaries: bool,
}
/// Results for rouge metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RougeResults {
    /// Output only. Rouge metric values.
    #[prost(message, repeated, tag = "1")]
    pub rouge_metric_values: ::prost::alloc::vec::Vec<RougeMetricValue>,
}
/// Rouge metric value for an instance.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RougeMetricValue {
    /// Output only. Rouge score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
}
/// Input for coherence metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CoherenceInput {
    /// Required. Spec for coherence score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<CoherenceSpec>,
    /// Required. Coherence instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<CoherenceInstance>,
}
/// Spec for coherence instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CoherenceInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for coherence score metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct CoherenceSpec {
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "1")]
    pub version: i32,
}
/// Spec for coherence result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CoherenceResult {
    /// Output only. Coherence score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for coherence score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for coherence score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for fluency metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FluencyInput {
    /// Required. Spec for fluency score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<FluencySpec>,
    /// Required. Fluency instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<FluencyInstance>,
}
/// Spec for fluency instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FluencyInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for fluency score metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct FluencySpec {
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "1")]
    pub version: i32,
}
/// Spec for fluency result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FluencyResult {
    /// Output only. Fluency score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for fluency score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for fluency score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for safety metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SafetyInput {
    /// Required. Spec for safety metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<SafetySpec>,
    /// Required. Safety instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<SafetyInstance>,
}
/// Spec for safety instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SafetyInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for safety metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetySpec {
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "1")]
    pub version: i32,
}
/// Spec for safety result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SafetyResult {
    /// Output only. Safety score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for safety score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for safety score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for groundedness metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundednessInput {
    /// Required. Spec for groundedness metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<GroundednessSpec>,
    /// Required. Groundedness instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<GroundednessInstance>,
}
/// Spec for groundedness instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundednessInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Background information provided in context used to compare
    /// against the prediction.
    #[prost(string, optional, tag = "2")]
    pub context: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for groundedness metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct GroundednessSpec {
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "1")]
    pub version: i32,
}
/// Spec for groundedness result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundednessResult {
    /// Output only. Groundedness score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for groundedness score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for groundedness score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for fulfillment metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FulfillmentInput {
    /// Required. Spec for fulfillment score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<FulfillmentSpec>,
    /// Required. Fulfillment instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<FulfillmentInstance>,
}
/// Spec for fulfillment instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FulfillmentInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Inference instruction prompt to compare prediction with.
    #[prost(string, optional, tag = "2")]
    pub instruction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for fulfillment metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct FulfillmentSpec {
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "1")]
    pub version: i32,
}
/// Spec for fulfillment result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FulfillmentResult {
    /// Output only. Fulfillment score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for fulfillment score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for fulfillment score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for summarization quality metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SummarizationQualityInput {
    /// Required. Spec for summarization quality score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<SummarizationQualitySpec>,
    /// Required. Summarization quality instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<SummarizationQualityInstance>,
}
/// Spec for summarization quality instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SummarizationQualityInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Text to be summarized.
    #[prost(string, optional, tag = "3")]
    pub context: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Summarization prompt for LLM.
    #[prost(string, optional, tag = "4")]
    pub instruction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for summarization quality score metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SummarizationQualitySpec {
    /// Optional. Whether to use instance.reference to compute summarization
    /// quality.
    #[prost(bool, tag = "1")]
    pub use_reference: bool,
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "2")]
    pub version: i32,
}
/// Spec for summarization quality result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SummarizationQualityResult {
    /// Output only. Summarization Quality score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for summarization quality score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for summarization quality score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for pairwise summarization quality metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PairwiseSummarizationQualityInput {
    /// Required. Spec for pairwise summarization quality score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<PairwiseSummarizationQualitySpec>,
    /// Required. Pairwise summarization quality instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<PairwiseSummarizationQualityInstance>,
}
/// Spec for pairwise summarization quality instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PairwiseSummarizationQualityInstance {
    /// Required. Output of the candidate model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Output of the baseline model.
    #[prost(string, optional, tag = "2")]
    pub baseline_prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "3")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Text to be summarized.
    #[prost(string, optional, tag = "4")]
    pub context: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Summarization prompt for LLM.
    #[prost(string, optional, tag = "5")]
    pub instruction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for pairwise summarization quality score metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct PairwiseSummarizationQualitySpec {
    /// Optional. Whether to use instance.reference to compute pairwise
    /// summarization quality.
    #[prost(bool, tag = "1")]
    pub use_reference: bool,
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "2")]
    pub version: i32,
}
/// Spec for pairwise summarization quality result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PairwiseSummarizationQualityResult {
    /// Output only. Pairwise summarization prediction choice.
    #[prost(enumeration = "PairwiseChoice", tag = "1")]
    pub pairwise_choice: i32,
    /// Output only. Explanation for summarization quality score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for summarization quality score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for summarization helpfulness metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SummarizationHelpfulnessInput {
    /// Required. Spec for summarization helpfulness score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<SummarizationHelpfulnessSpec>,
    /// Required. Summarization helpfulness instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<SummarizationHelpfulnessInstance>,
}
/// Spec for summarization helpfulness instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SummarizationHelpfulnessInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Text to be summarized.
    #[prost(string, optional, tag = "3")]
    pub context: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Summarization prompt for LLM.
    #[prost(string, optional, tag = "4")]
    pub instruction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for summarization helpfulness score metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SummarizationHelpfulnessSpec {
    /// Optional. Whether to use instance.reference to compute summarization
    /// helpfulness.
    #[prost(bool, tag = "1")]
    pub use_reference: bool,
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "2")]
    pub version: i32,
}
/// Spec for summarization helpfulness result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SummarizationHelpfulnessResult {
    /// Output only. Summarization Helpfulness score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for summarization helpfulness score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for summarization helpfulness score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for summarization verbosity metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SummarizationVerbosityInput {
    /// Required. Spec for summarization verbosity score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<SummarizationVerbositySpec>,
    /// Required. Summarization verbosity instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<SummarizationVerbosityInstance>,
}
/// Spec for summarization verbosity instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SummarizationVerbosityInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Text to be summarized.
    #[prost(string, optional, tag = "3")]
    pub context: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Summarization prompt for LLM.
    #[prost(string, optional, tag = "4")]
    pub instruction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for summarization verbosity score metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SummarizationVerbositySpec {
    /// Optional. Whether to use instance.reference to compute summarization
    /// verbosity.
    #[prost(bool, tag = "1")]
    pub use_reference: bool,
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "2")]
    pub version: i32,
}
/// Spec for summarization verbosity result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SummarizationVerbosityResult {
    /// Output only. Summarization Verbosity score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for summarization verbosity score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for summarization verbosity score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for question answering quality metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringQualityInput {
    /// Required. Spec for question answering quality score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<QuestionAnsweringQualitySpec>,
    /// Required. Question answering quality instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<QuestionAnsweringQualityInstance>,
}
/// Spec for question answering quality instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringQualityInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Text to answer the question.
    #[prost(string, optional, tag = "3")]
    pub context: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Question Answering prompt for LLM.
    #[prost(string, optional, tag = "4")]
    pub instruction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for question answering quality score metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringQualitySpec {
    /// Optional. Whether to use instance.reference to compute question answering
    /// quality.
    #[prost(bool, tag = "1")]
    pub use_reference: bool,
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "2")]
    pub version: i32,
}
/// Spec for question answering quality result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringQualityResult {
    /// Output only. Question Answering Quality score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for question answering quality score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for question answering quality score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for pairwise question answering quality metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PairwiseQuestionAnsweringQualityInput {
    /// Required. Spec for pairwise question answering quality score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<PairwiseQuestionAnsweringQualitySpec>,
    /// Required. Pairwise question answering quality instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<PairwiseQuestionAnsweringQualityInstance>,
}
/// Spec for pairwise question answering quality instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PairwiseQuestionAnsweringQualityInstance {
    /// Required. Output of the candidate model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Output of the baseline model.
    #[prost(string, optional, tag = "2")]
    pub baseline_prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "3")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Text to answer the question.
    #[prost(string, optional, tag = "4")]
    pub context: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Question Answering prompt for LLM.
    #[prost(string, optional, tag = "5")]
    pub instruction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for pairwise question answering quality score metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct PairwiseQuestionAnsweringQualitySpec {
    /// Optional. Whether to use instance.reference to compute question answering
    /// quality.
    #[prost(bool, tag = "1")]
    pub use_reference: bool,
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "2")]
    pub version: i32,
}
/// Spec for pairwise question answering quality result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PairwiseQuestionAnsweringQualityResult {
    /// Output only. Pairwise question answering prediction choice.
    #[prost(enumeration = "PairwiseChoice", tag = "1")]
    pub pairwise_choice: i32,
    /// Output only. Explanation for question answering quality score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for question answering quality score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for question answering relevance metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringRelevanceInput {
    /// Required. Spec for question answering relevance score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<QuestionAnsweringRelevanceSpec>,
    /// Required. Question answering relevance instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<QuestionAnsweringRelevanceInstance>,
}
/// Spec for question answering relevance instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringRelevanceInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Text provided as context to answer the question.
    #[prost(string, optional, tag = "3")]
    pub context: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. The question asked and other instruction in the inference prompt.
    #[prost(string, optional, tag = "4")]
    pub instruction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for question answering relevance metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringRelevanceSpec {
    /// Optional. Whether to use instance.reference to compute question answering
    /// relevance.
    #[prost(bool, tag = "1")]
    pub use_reference: bool,
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "2")]
    pub version: i32,
}
/// Spec for question answering relevance result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringRelevanceResult {
    /// Output only. Question Answering Relevance score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for question answering relevance score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for question answering relevance score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for question answering helpfulness metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringHelpfulnessInput {
    /// Required. Spec for question answering helpfulness score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<QuestionAnsweringHelpfulnessSpec>,
    /// Required. Question answering helpfulness instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<QuestionAnsweringHelpfulnessInstance>,
}
/// Spec for question answering helpfulness instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringHelpfulnessInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Text provided as context to answer the question.
    #[prost(string, optional, tag = "3")]
    pub context: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. The question asked and other instruction in the inference prompt.
    #[prost(string, optional, tag = "4")]
    pub instruction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for question answering helpfulness metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringHelpfulnessSpec {
    /// Optional. Whether to use instance.reference to compute question answering
    /// helpfulness.
    #[prost(bool, tag = "1")]
    pub use_reference: bool,
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "2")]
    pub version: i32,
}
/// Spec for question answering helpfulness result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringHelpfulnessResult {
    /// Output only. Question Answering Helpfulness score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for question answering helpfulness score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for question answering helpfulness score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for question answering correctness metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringCorrectnessInput {
    /// Required. Spec for question answering correctness score metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<QuestionAnsweringCorrectnessSpec>,
    /// Required. Question answering correctness instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<QuestionAnsweringCorrectnessInstance>,
}
/// Spec for question answering correctness instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringCorrectnessInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Text provided as context to answer the question.
    #[prost(string, optional, tag = "3")]
    pub context: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. The question asked and other instruction in the inference prompt.
    #[prost(string, optional, tag = "4")]
    pub instruction: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for question answering correctness metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringCorrectnessSpec {
    /// Optional. Whether to use instance.reference to compute question answering
    /// correctness.
    #[prost(bool, tag = "1")]
    pub use_reference: bool,
    /// Optional. Which version to use for evaluation.
    #[prost(int32, tag = "2")]
    pub version: i32,
}
/// Spec for question answering correctness result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QuestionAnsweringCorrectnessResult {
    /// Output only. Question Answering Correctness score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for question answering correctness score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
    /// Output only. Confidence for question answering correctness score.
    #[prost(float, optional, tag = "3")]
    pub confidence: ::core::option::Option<f32>,
}
/// Input for pointwise metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PointwiseMetricInput {
    /// Required. Spec for pointwise metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<PointwiseMetricSpec>,
    /// Required. Pointwise metric instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<PointwiseMetricInstance>,
}
/// Pointwise metric instance. Usually one instance corresponds to one row in an
/// evaluation dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PointwiseMetricInstance {
    /// Instance for pointwise metric.
    #[prost(oneof = "pointwise_metric_instance::Instance", tags = "1")]
    pub instance: ::core::option::Option<pointwise_metric_instance::Instance>,
}
/// Nested message and enum types in `PointwiseMetricInstance`.
pub mod pointwise_metric_instance {
    /// Instance for pointwise metric.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Instance {
        /// Instance specified as a json string. String key-value pairs are expected
        /// in the json_instance to render
        /// PointwiseMetricSpec.instance_prompt_template.
        #[prost(string, tag = "1")]
        JsonInstance(::prost::alloc::string::String),
    }
}
/// Spec for pointwise metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PointwiseMetricSpec {
    /// Required. Metric prompt template for pointwise metric.
    #[prost(string, optional, tag = "1")]
    pub metric_prompt_template: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for pointwise metric result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PointwiseMetricResult {
    /// Output only. Pointwise metric score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
    /// Output only. Explanation for pointwise metric score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
}
/// Input for pairwise metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PairwiseMetricInput {
    /// Required. Spec for pairwise metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<PairwiseMetricSpec>,
    /// Required. Pairwise metric instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<PairwiseMetricInstance>,
}
/// Pairwise metric instance. Usually one instance corresponds to one row in an
/// evaluation dataset.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PairwiseMetricInstance {
    /// Instance for pairwise metric.
    #[prost(oneof = "pairwise_metric_instance::Instance", tags = "1")]
    pub instance: ::core::option::Option<pairwise_metric_instance::Instance>,
}
/// Nested message and enum types in `PairwiseMetricInstance`.
pub mod pairwise_metric_instance {
    /// Instance for pairwise metric.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Instance {
        /// Instance specified as a json string. String key-value pairs are expected
        /// in the json_instance to render
        /// PairwiseMetricSpec.instance_prompt_template.
        #[prost(string, tag = "1")]
        JsonInstance(::prost::alloc::string::String),
    }
}
/// Spec for pairwise metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PairwiseMetricSpec {
    /// Required. Metric prompt template for pairwise metric.
    #[prost(string, optional, tag = "1")]
    pub metric_prompt_template: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for pairwise metric result.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PairwiseMetricResult {
    /// Output only. Pairwise metric choice.
    #[prost(enumeration = "PairwiseChoice", tag = "1")]
    pub pairwise_choice: i32,
    /// Output only. Explanation for pairwise metric score.
    #[prost(string, tag = "2")]
    pub explanation: ::prost::alloc::string::String,
}
/// Input for tool call valid metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolCallValidInput {
    /// Required. Spec for tool call valid metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<ToolCallValidSpec>,
    /// Required. Repeated tool call valid instances.
    #[prost(message, repeated, tag = "2")]
    pub instances: ::prost::alloc::vec::Vec<ToolCallValidInstance>,
}
/// Spec for tool call valid metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ToolCallValidSpec {}
/// Spec for tool call valid instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolCallValidInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
}
/// Results for tool call valid metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolCallValidResults {
    /// Output only. Tool call valid metric values.
    #[prost(message, repeated, tag = "1")]
    pub tool_call_valid_metric_values: ::prost::alloc::vec::Vec<
        ToolCallValidMetricValue,
    >,
}
/// Tool call valid metric value for an instance.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ToolCallValidMetricValue {
    /// Output only. Tool call valid score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
}
/// Input for tool name match metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolNameMatchInput {
    /// Required. Spec for tool name match metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<ToolNameMatchSpec>,
    /// Required. Repeated tool name match instances.
    #[prost(message, repeated, tag = "2")]
    pub instances: ::prost::alloc::vec::Vec<ToolNameMatchInstance>,
}
/// Spec for tool name match metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ToolNameMatchSpec {}
/// Spec for tool name match instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolNameMatchInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
}
/// Results for tool name match metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolNameMatchResults {
    /// Output only. Tool name match metric values.
    #[prost(message, repeated, tag = "1")]
    pub tool_name_match_metric_values: ::prost::alloc::vec::Vec<
        ToolNameMatchMetricValue,
    >,
}
/// Tool name match metric value for an instance.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ToolNameMatchMetricValue {
    /// Output only. Tool name match score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
}
/// Input for tool parameter key match metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolParameterKeyMatchInput {
    /// Required. Spec for tool parameter key match metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<ToolParameterKeyMatchSpec>,
    /// Required. Repeated tool parameter key match instances.
    #[prost(message, repeated, tag = "2")]
    pub instances: ::prost::alloc::vec::Vec<ToolParameterKeyMatchInstance>,
}
/// Spec for tool parameter key match metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ToolParameterKeyMatchSpec {}
/// Spec for tool parameter key match instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolParameterKeyMatchInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
}
/// Results for tool parameter key match metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolParameterKeyMatchResults {
    /// Output only. Tool parameter key match metric values.
    #[prost(message, repeated, tag = "1")]
    pub tool_parameter_key_match_metric_values: ::prost::alloc::vec::Vec<
        ToolParameterKeyMatchMetricValue,
    >,
}
/// Tool parameter key match metric value for an instance.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ToolParameterKeyMatchMetricValue {
    /// Output only. Tool parameter key match score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
}
/// Input for tool parameter key value match metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolParameterKvMatchInput {
    /// Required. Spec for tool parameter key value match metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<ToolParameterKvMatchSpec>,
    /// Required. Repeated tool parameter key value match instances.
    #[prost(message, repeated, tag = "2")]
    pub instances: ::prost::alloc::vec::Vec<ToolParameterKvMatchInstance>,
}
/// Spec for tool parameter key value match metric.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ToolParameterKvMatchSpec {
    /// Optional. Whether to use STRICT string match on parameter values.
    #[prost(bool, tag = "1")]
    pub use_strict_string_match: bool,
}
/// Spec for tool parameter key value match instance.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolParameterKvMatchInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Required. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
}
/// Results for tool parameter key value match metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ToolParameterKvMatchResults {
    /// Output only. Tool parameter key value match metric values.
    #[prost(message, repeated, tag = "1")]
    pub tool_parameter_kv_match_metric_values: ::prost::alloc::vec::Vec<
        ToolParameterKvMatchMetricValue,
    >,
}
/// Tool parameter key value match metric value for an instance.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ToolParameterKvMatchMetricValue {
    /// Output only. Tool parameter key value match score.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
}
/// Input for Comet metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CometInput {
    /// Required. Spec for comet metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<CometSpec>,
    /// Required. Comet instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<CometInstance>,
}
/// Spec for Comet metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CometSpec {
    /// Required. Which version to use for evaluation.
    #[prost(enumeration = "comet_spec::CometVersion", optional, tag = "1")]
    pub version: ::core::option::Option<i32>,
    /// Optional. Source language in BCP-47 format.
    #[prost(string, tag = "2")]
    pub source_language: ::prost::alloc::string::String,
    /// Optional. Target language in BCP-47 format. Covers both prediction and
    /// reference.
    #[prost(string, tag = "3")]
    pub target_language: ::prost::alloc::string::String,
}
/// Nested message and enum types in `CometSpec`.
pub mod comet_spec {
    /// Comet version options.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum CometVersion {
        /// Comet version unspecified.
        Unspecified = 0,
        /// Comet 22 for translation + source + reference
        /// (source-reference-combined).
        Comet22SrcRef = 2,
    }
    impl CometVersion {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "COMET_VERSION_UNSPECIFIED",
                Self::Comet22SrcRef => "COMET_22_SRC_REF",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "COMET_VERSION_UNSPECIFIED" => Some(Self::Unspecified),
                "COMET_22_SRC_REF" => Some(Self::Comet22SrcRef),
                _ => None,
            }
        }
    }
}
/// Spec for Comet instance - The fields used for evaluation are dependent on the
/// comet version.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CometInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Source text in original language.
    #[prost(string, optional, tag = "3")]
    pub source: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for Comet result - calculates the comet score for the given instance
/// using the version specified in the spec.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct CometResult {
    /// Output only. Comet score. Range depends on version.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
}
/// Input for MetricX metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MetricxInput {
    /// Required. Spec for Metricx metric.
    #[prost(message, optional, tag = "1")]
    pub metric_spec: ::core::option::Option<MetricxSpec>,
    /// Required. Metricx instance.
    #[prost(message, optional, tag = "2")]
    pub instance: ::core::option::Option<MetricxInstance>,
}
/// Spec for MetricX metric.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MetricxSpec {
    /// Required. Which version to use for evaluation.
    #[prost(enumeration = "metricx_spec::MetricxVersion", optional, tag = "1")]
    pub version: ::core::option::Option<i32>,
    /// Optional. Source language in BCP-47 format.
    #[prost(string, tag = "2")]
    pub source_language: ::prost::alloc::string::String,
    /// Optional. Target language in BCP-47 format. Covers both prediction and
    /// reference.
    #[prost(string, tag = "3")]
    pub target_language: ::prost::alloc::string::String,
}
/// Nested message and enum types in `MetricxSpec`.
pub mod metricx_spec {
    /// MetricX Version options.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum MetricxVersion {
        /// MetricX version unspecified.
        Unspecified = 0,
        /// MetricX 2024 (2.6) for translation + reference (reference-based).
        Metricx24Ref = 1,
        /// MetricX 2024 (2.6) for translation + source (QE).
        Metricx24Src = 2,
        /// MetricX 2024 (2.6) for translation + source + reference
        /// (source-reference-combined).
        Metricx24SrcRef = 3,
    }
    impl MetricxVersion {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "METRICX_VERSION_UNSPECIFIED",
                Self::Metricx24Ref => "METRICX_24_REF",
                Self::Metricx24Src => "METRICX_24_SRC",
                Self::Metricx24SrcRef => "METRICX_24_SRC_REF",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "METRICX_VERSION_UNSPECIFIED" => Some(Self::Unspecified),
                "METRICX_24_REF" => Some(Self::Metricx24Ref),
                "METRICX_24_SRC" => Some(Self::Metricx24Src),
                "METRICX_24_SRC_REF" => Some(Self::Metricx24SrcRef),
                _ => None,
            }
        }
    }
}
/// Spec for MetricX instance - The fields used for evaluation are dependent on
/// the MetricX version.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MetricxInstance {
    /// Required. Output of the evaluated model.
    #[prost(string, optional, tag = "1")]
    pub prediction: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Ground truth used to compare against the prediction.
    #[prost(string, optional, tag = "2")]
    pub reference: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Source text in original language.
    #[prost(string, optional, tag = "3")]
    pub source: ::core::option::Option<::prost::alloc::string::String>,
}
/// Spec for MetricX result - calculates the MetricX score for the given instance
/// using the version specified in the spec.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct MetricxResult {
    /// Output only. MetricX score. Range depends on version.
    #[prost(float, optional, tag = "1")]
    pub score: ::core::option::Option<f32>,
}
/// Pairwise prediction autorater preference.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum PairwiseChoice {
    /// Unspecified prediction choice.
    Unspecified = 0,
    /// Baseline prediction wins
    Baseline = 1,
    /// Candidate prediction wins
    Candidate = 2,
    /// Winner cannot be determined
    Tie = 3,
}
impl PairwiseChoice {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "PAIRWISE_CHOICE_UNSPECIFIED",
            Self::Baseline => "BASELINE",
            Self::Candidate => "CANDIDATE",
            Self::Tie => "TIE",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "PAIRWISE_CHOICE_UNSPECIFIED" => Some(Self::Unspecified),
            "BASELINE" => Some(Self::Baseline),
            "CANDIDATE" => Some(Self::Candidate),
            "TIE" => Some(Self::Tie),
            _ => None,
        }
    }
}
/// Generated client implementations.
pub mod evaluation_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// Vertex AI Online Evaluation Service.
    #[derive(Debug, Clone)]
    pub struct EvaluationServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl EvaluationServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> EvaluationServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> EvaluationServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            EvaluationServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Evaluates instances based on a given metric.
        pub async fn evaluate_instances(
            &mut self,
            request: impl tonic::IntoRequest<super::EvaluateInstancesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::EvaluateInstancesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.EvaluationService/EvaluateInstances",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.EvaluationService",
                        "EvaluateInstances",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// An edge describing the relationship between an Artifact and an Execution in
/// a lineage graph.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Event {
    /// Required. The relative resource name of the Artifact in the Event.
    #[prost(string, tag = "1")]
    pub artifact: ::prost::alloc::string::String,
    /// Output only. The relative resource name of the Execution in the Event.
    #[prost(string, tag = "2")]
    pub execution: ::prost::alloc::string::String,
    /// Output only. Time the Event occurred.
    #[prost(message, optional, tag = "3")]
    pub event_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Required. The type of the Event.
    #[prost(enumeration = "event::Type", tag = "4")]
    pub r#type: i32,
    /// The labels with user-defined metadata to annotate Events.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// No more than 64 user labels can be associated with one Event (System
    /// labels are excluded).
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable.
    #[prost(map = "string, string", tag = "5")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
/// Nested message and enum types in `Event`.
pub mod event {
    /// Describes whether an Event's Artifact is the Execution's input or output.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Type {
        /// Unspecified whether input or output of the Execution.
        Unspecified = 0,
        /// An input of the Execution.
        Input = 1,
        /// An output of the Execution.
        Output = 2,
    }
    impl Type {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "TYPE_UNSPECIFIED",
                Self::Input => "INPUT",
                Self::Output => "OUTPUT",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "INPUT" => Some(Self::Input),
                "OUTPUT" => Some(Self::Output),
                _ => None,
            }
        }
    }
}
/// Instance of a general execution.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Execution {
    /// Output only. The resource name of the Execution.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// User provided display name of the Execution.
    /// May be up to 128 Unicode characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// The state of this Execution. This is a property of the Execution, and does
    /// not imply or capture any ongoing process. This property is managed by
    /// clients (such as Vertex AI Pipelines) and the system does not prescribe
    /// or check the validity of state transitions.
    #[prost(enumeration = "execution::State", tag = "6")]
    pub state: i32,
    /// An eTag used to perform consistent read-modify-write updates. If not set, a
    /// blind "overwrite" update happens.
    #[prost(string, tag = "9")]
    pub etag: ::prost::alloc::string::String,
    /// The labels with user-defined metadata to organize your Executions.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// No more than 64 user labels can be associated with one Execution (System
    /// labels are excluded).
    #[prost(map = "string, string", tag = "10")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Output only. Timestamp when this Execution was created.
    #[prost(message, optional, tag = "11")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Execution was last updated.
    #[prost(message, optional, tag = "12")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// The title of the schema describing the metadata.
    ///
    /// Schema title and version is expected to be registered in earlier Create
    /// Schema calls. And both are used together as unique identifiers to identify
    /// schemas within the local metadata store.
    #[prost(string, tag = "13")]
    pub schema_title: ::prost::alloc::string::String,
    /// The version of the schema in `schema_title` to use.
    ///
    /// Schema title and version is expected to be registered in earlier Create
    /// Schema calls. And both are used together as unique identifiers to identify
    /// schemas within the local metadata store.
    #[prost(string, tag = "14")]
    pub schema_version: ::prost::alloc::string::String,
    /// Properties of the Execution.
    /// Top level metadata keys' heading and trailing spaces will be trimmed.
    /// The size of this field should not exceed 200KB.
    #[prost(message, optional, tag = "15")]
    pub metadata: ::core::option::Option<::prost_types::Struct>,
    /// Description of the Execution
    #[prost(string, tag = "16")]
    pub description: ::prost::alloc::string::String,
}
/// Nested message and enum types in `Execution`.
pub mod execution {
    /// Describes the state of the Execution.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// Unspecified Execution state
        Unspecified = 0,
        /// The Execution is new
        New = 1,
        /// The Execution is running
        Running = 2,
        /// The Execution has finished running
        Complete = 3,
        /// The Execution has failed
        Failed = 4,
        /// The Execution completed through Cache hit.
        Cached = 5,
        /// The Execution was cancelled.
        Cancelled = 6,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STATE_UNSPECIFIED",
                Self::New => "NEW",
                Self::Running => "RUNNING",
                Self::Complete => "COMPLETE",
                Self::Failed => "FAILED",
                Self::Cached => "CACHED",
                Self::Cancelled => "CANCELLED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "NEW" => Some(Self::New),
                "RUNNING" => Some(Self::Running),
                "COMPLETE" => Some(Self::Complete),
                "FAILED" => Some(Self::Failed),
                "CACHED" => Some(Self::Cached),
                "CANCELLED" => Some(Self::Cancelled),
                _ => None,
            }
        }
    }
}
/// Stats and Anomaly generated at specific timestamp for specific Feature.
/// The start_time and end_time are used to define the time range of the dataset
/// that current stats belongs to, e.g. prediction traffic is bucketed into
/// prediction datasets by time window. If the Dataset is not defined by time
/// window, start_time = end_time. Timestamp of the stats and anomalies always
/// refers to end_time. Raw stats and anomalies are stored in stats_uri or
/// anomaly_uri in the tensorflow defined protos. Field data_stats contains
/// almost identical information with the raw stats in Vertex AI
/// defined proto, for UI to display.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FeatureStatsAnomaly {
    /// Feature importance score, only populated when cross-feature monitoring is
    /// enabled. For now only used to represent feature attribution score within
    /// range \[0, 1\] for
    /// [ModelDeploymentMonitoringObjectiveType.FEATURE_ATTRIBUTION_SKEW][google.cloud.aiplatform.v1.ModelDeploymentMonitoringObjectiveType.FEATURE_ATTRIBUTION_SKEW]
    /// and
    /// [ModelDeploymentMonitoringObjectiveType.FEATURE_ATTRIBUTION_DRIFT][google.cloud.aiplatform.v1.ModelDeploymentMonitoringObjectiveType.FEATURE_ATTRIBUTION_DRIFT].
    #[prost(double, tag = "1")]
    pub score: f64,
    /// Path of the stats file for current feature values in Cloud Storage bucket.
    /// Format: gs://<bucket_name>/<object_name>/stats.
    /// Example: gs://monitoring_bucket/feature_name/stats.
    /// Stats are stored as binary format with Protobuf message
    /// [tensorflow.metadata.v0.FeatureNameStatistics](<https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/statistics.proto>).
    #[prost(string, tag = "3")]
    pub stats_uri: ::prost::alloc::string::String,
    /// Path of the anomaly file for current feature values in Cloud Storage
    /// bucket.
    /// Format: gs://<bucket_name>/<object_name>/anomalies.
    /// Example: gs://monitoring_bucket/feature_name/anomalies.
    /// Stats are stored as binary format with Protobuf message
    /// Anoamlies are stored as binary format with Protobuf message
    /// \[tensorflow.metadata.v0.AnomalyInfo\]
    /// (<https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/anomalies.proto>).
    #[prost(string, tag = "4")]
    pub anomaly_uri: ::prost::alloc::string::String,
    /// Deviation from the current stats to baseline stats.
    ///    1. For categorical feature, the distribution distance is calculated by
    ///       L-inifinity norm.
    ///    2. For numerical feature, the distribution distance is calculated by
    ///       Jensen–Shannon divergence.
    #[prost(double, tag = "5")]
    pub distribution_deviation: f64,
    /// This is the threshold used when detecting anomalies.
    /// The threshold can be changed by user, so this one might be different from
    /// [ThresholdConfig.value][google.cloud.aiplatform.v1.ThresholdConfig.value].
    #[prost(double, tag = "9")]
    pub anomaly_detection_threshold: f64,
    /// The start timestamp of window where stats were generated.
    /// For objectives where time window doesn't make sense (e.g. Featurestore
    /// Snapshot Monitoring), start_time is only used to indicate the monitoring
    /// intervals, so it always equals to (end_time - monitoring_interval).
    #[prost(message, optional, tag = "7")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// The end timestamp of window where stats were generated.
    /// For objectives where time window doesn't make sense (e.g. Featurestore
    /// Snapshot Monitoring), end_time indicates the timestamp of the data used to
    /// generate stats (e.g. timestamp we take snapshots for feature values).
    #[prost(message, optional, tag = "8")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
}
/// Feature Metadata information.
/// For example, color is a feature that describes an apple.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Feature {
    /// Immutable. Name of the Feature.
    /// Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity_type}/features/{feature}`
    /// `projects/{project}/locations/{location}/featureGroups/{feature_group}/features/{feature}`
    ///
    /// The last part feature is assigned by the client. The feature can be up to
    /// 64 characters long and can consist only of ASCII Latin letters A-Z and a-z,
    /// underscore(_), and ASCII digits 0-9 starting with a letter. The value will
    /// be unique given an entity type.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Description of the Feature.
    #[prost(string, tag = "2")]
    pub description: ::prost::alloc::string::String,
    /// Immutable. Only applicable for Vertex AI Feature Store (Legacy).
    /// Type of Feature value.
    #[prost(enumeration = "feature::ValueType", tag = "3")]
    pub value_type: i32,
    /// Output only. Only applicable for Vertex AI Feature Store (Legacy).
    /// Timestamp when this EntityType was created.
    #[prost(message, optional, tag = "4")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Only applicable for Vertex AI Feature Store (Legacy).
    /// Timestamp when this EntityType was most recently updated.
    #[prost(message, optional, tag = "5")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. The labels with user-defined metadata to organize your Features.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information on and examples of labels.
    /// No more than 64 user labels can be associated with one Feature (System
    /// labels are excluded)."
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable.
    #[prost(map = "string, string", tag = "6")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Used to perform a consistent read-modify-write updates. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "7")]
    pub etag: ::prost::alloc::string::String,
    /// Optional. Only applicable for Vertex AI Feature Store (Legacy).
    /// If not set, use the monitoring_config defined for the EntityType this
    /// Feature belongs to.
    /// Only Features with type
    /// ([Feature.ValueType][google.cloud.aiplatform.v1.Feature.ValueType]) BOOL,
    /// STRING, DOUBLE or INT64 can enable monitoring.
    ///
    /// If set to true, all types of data monitoring are disabled despite the
    /// config on EntityType.
    #[prost(bool, tag = "12")]
    pub disable_monitoring: bool,
    /// Output only. Only applicable for Vertex AI Feature Store (Legacy).
    /// The list of historical stats and anomalies with specified objectives.
    #[prost(message, repeated, tag = "11")]
    pub monitoring_stats_anomalies: ::prost::alloc::vec::Vec<
        feature::MonitoringStatsAnomaly,
    >,
    /// Only applicable for Vertex AI Feature Store.
    /// The name of the BigQuery Table/View column hosting data for this version.
    /// If no value is provided, will use feature_id.
    #[prost(string, tag = "106")]
    pub version_column_name: ::prost::alloc::string::String,
    /// Entity responsible for maintaining this feature. Can be comma separated
    /// list of email addresses or URIs.
    #[prost(string, tag = "107")]
    pub point_of_contact: ::prost::alloc::string::String,
}
/// Nested message and enum types in `Feature`.
pub mod feature {
    /// A list of historical
    /// [SnapshotAnalysis][google.cloud.aiplatform.v1.FeaturestoreMonitoringConfig.SnapshotAnalysis]
    /// or
    /// [ImportFeaturesAnalysis][google.cloud.aiplatform.v1.FeaturestoreMonitoringConfig.ImportFeaturesAnalysis]
    /// stats requested by user, sorted by
    /// [FeatureStatsAnomaly.start_time][google.cloud.aiplatform.v1.FeatureStatsAnomaly.start_time]
    /// descending.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct MonitoringStatsAnomaly {
        /// Output only. The objective for each stats.
        #[prost(enumeration = "monitoring_stats_anomaly::Objective", tag = "1")]
        pub objective: i32,
        /// Output only. The stats and anomalies generated at specific timestamp.
        #[prost(message, optional, tag = "2")]
        pub feature_stats_anomaly: ::core::option::Option<super::FeatureStatsAnomaly>,
    }
    /// Nested message and enum types in `MonitoringStatsAnomaly`.
    pub mod monitoring_stats_anomaly {
        /// If the objective in the request is both
        /// Import Feature Analysis and Snapshot Analysis, this objective could be
        /// one of them. Otherwise, this objective should be the same as the
        /// objective in the request.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum Objective {
            /// If it's OBJECTIVE_UNSPECIFIED, monitoring_stats will be empty.
            Unspecified = 0,
            /// Stats are generated by Import Feature Analysis.
            ImportFeatureAnalysis = 1,
            /// Stats are generated by Snapshot Analysis.
            SnapshotAnalysis = 2,
        }
        impl Objective {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "OBJECTIVE_UNSPECIFIED",
                    Self::ImportFeatureAnalysis => "IMPORT_FEATURE_ANALYSIS",
                    Self::SnapshotAnalysis => "SNAPSHOT_ANALYSIS",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "OBJECTIVE_UNSPECIFIED" => Some(Self::Unspecified),
                    "IMPORT_FEATURE_ANALYSIS" => Some(Self::ImportFeatureAnalysis),
                    "SNAPSHOT_ANALYSIS" => Some(Self::SnapshotAnalysis),
                    _ => None,
                }
            }
        }
    }
    /// Only applicable for Vertex AI Legacy Feature Store.
    /// An enum representing the value type of a feature.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum ValueType {
        /// The value type is unspecified.
        Unspecified = 0,
        /// Used for Feature that is a boolean.
        Bool = 1,
        /// Used for Feature that is a list of boolean.
        BoolArray = 2,
        /// Used for Feature that is double.
        Double = 3,
        /// Used for Feature that is a list of double.
        DoubleArray = 4,
        /// Used for Feature that is INT64.
        Int64 = 9,
        /// Used for Feature that is a list of INT64.
        Int64Array = 10,
        /// Used for Feature that is string.
        String = 11,
        /// Used for Feature that is a list of String.
        StringArray = 12,
        /// Used for Feature that is bytes.
        Bytes = 13,
        /// Used for Feature that is struct.
        Struct = 14,
    }
    impl ValueType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "VALUE_TYPE_UNSPECIFIED",
                Self::Bool => "BOOL",
                Self::BoolArray => "BOOL_ARRAY",
                Self::Double => "DOUBLE",
                Self::DoubleArray => "DOUBLE_ARRAY",
                Self::Int64 => "INT64",
                Self::Int64Array => "INT64_ARRAY",
                Self::String => "STRING",
                Self::StringArray => "STRING_ARRAY",
                Self::Bytes => "BYTES",
                Self::Struct => "STRUCT",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "VALUE_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "BOOL" => Some(Self::Bool),
                "BOOL_ARRAY" => Some(Self::BoolArray),
                "DOUBLE" => Some(Self::Double),
                "DOUBLE_ARRAY" => Some(Self::DoubleArray),
                "INT64" => Some(Self::Int64),
                "INT64_ARRAY" => Some(Self::Int64Array),
                "STRING" => Some(Self::String),
                "STRING_ARRAY" => Some(Self::StringArray),
                "BYTES" => Some(Self::Bytes),
                "STRUCT" => Some(Self::Struct),
                _ => None,
            }
        }
    }
}
/// Vertex AI Feature Group.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FeatureGroup {
    /// Identifier. Name of the FeatureGroup. Format:
    /// `projects/{project}/locations/{location}/featureGroups/{featureGroup}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. Timestamp when this FeatureGroup was created.
    #[prost(message, optional, tag = "2")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this FeatureGroup was last updated.
    #[prost(message, optional, tag = "3")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. Used to perform consistent read-modify-write updates. If not set,
    /// a blind "overwrite" update happens.
    #[prost(string, tag = "4")]
    pub etag: ::prost::alloc::string::String,
    /// Optional. The labels with user-defined metadata to organize your
    /// FeatureGroup.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information on and examples of labels.
    /// No more than 64 user labels can be associated with one
    /// FeatureGroup(System labels are excluded)." System reserved label keys
    /// are prefixed with "aiplatform.googleapis.com/" and are immutable.
    #[prost(map = "string, string", tag = "5")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. Description of the FeatureGroup.
    #[prost(string, tag = "6")]
    pub description: ::prost::alloc::string::String,
    #[prost(oneof = "feature_group::Source", tags = "7")]
    pub source: ::core::option::Option<feature_group::Source>,
}
/// Nested message and enum types in `FeatureGroup`.
pub mod feature_group {
    /// Input source type for BigQuery Tables and Views.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct BigQuery {
        /// Required. Immutable. The BigQuery source URI that points to either a
        /// BigQuery Table or View.
        #[prost(message, optional, tag = "1")]
        pub big_query_source: ::core::option::Option<super::BigQuerySource>,
        /// Optional. Columns to construct entity_id / row keys.
        /// If not provided defaults to `entity_id`.
        #[prost(string, repeated, tag = "2")]
        pub entity_id_columns: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        /// Optional. Set if the data source is not a time-series.
        #[prost(bool, tag = "3")]
        pub static_data_source: bool,
        /// Optional. If the source is a time-series source, this can be set to
        /// control how downstream sources (ex:
        /// [FeatureView][google.cloud.aiplatform.v1.FeatureView] ) will treat
        /// time-series sources. If not set, will treat the source as a time-series
        /// source with `feature_timestamp` as timestamp column and no scan boundary.
        #[prost(message, optional, tag = "4")]
        pub time_series: ::core::option::Option<big_query::TimeSeries>,
        /// Optional. If set, all feature values will be fetched
        /// from a single row per unique entityId including nulls.
        /// If not set, will collapse all rows for each unique entityId into a singe
        /// row with any non-null values if present, if no non-null values are
        /// present will sync null.
        /// ex: If source has schema
        /// `(entity_id, feature_timestamp, f0, f1)` and the following rows:
        /// `(e1, 2020-01-01T10:00:00.123Z, 10, 15)`
        /// `(e1, 2020-02-01T10:00:00.123Z, 20, null)`
        /// If dense is set, `(e1, 20, null)` is synced to online stores. If dense is
        /// not set, `(e1, 20, 15)` is synced to online stores.
        #[prost(bool, tag = "5")]
        pub dense: bool,
    }
    /// Nested message and enum types in `BigQuery`.
    pub mod big_query {
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct TimeSeries {
            /// Optional. Column hosting timestamp values for a time-series source.
            /// Will be used to determine the latest `feature_values` for each entity.
            /// Optional. If not provided, column named `feature_timestamp` of
            /// type `TIMESTAMP` will be used.
            #[prost(string, tag = "1")]
            pub timestamp_column: ::prost::alloc::string::String,
        }
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Source {
        /// Indicates that features for this group come from BigQuery Table/View.
        /// By default treats the source as a sparse time series source. The BigQuery
        /// source table or view must have at least one entity ID column and a column
        /// named `feature_timestamp`.
        #[prost(message, tag = "7")]
        BigQuery(BigQuery),
    }
}
/// Vertex AI Feature Online Store provides a centralized repository for serving
/// ML features and embedding indexes at low latency. The Feature Online Store is
/// a top-level container.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FeatureOnlineStore {
    /// Identifier. Name of the FeatureOnlineStore. Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{featureOnlineStore}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. Timestamp when this FeatureOnlineStore was created.
    #[prost(message, optional, tag = "3")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this FeatureOnlineStore was last updated.
    #[prost(message, optional, tag = "4")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. Used to perform consistent read-modify-write updates. If not set,
    /// a blind "overwrite" update happens.
    #[prost(string, tag = "5")]
    pub etag: ::prost::alloc::string::String,
    /// Optional. The labels with user-defined metadata to organize your
    /// FeatureOnlineStore.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information on and examples of labels.
    /// No more than 64 user labels can be associated with one
    /// FeatureOnlineStore(System labels are excluded)." System reserved label keys
    /// are prefixed with "aiplatform.googleapis.com/" and are immutable.
    #[prost(map = "string, string", tag = "6")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Output only. State of the featureOnlineStore.
    #[prost(enumeration = "feature_online_store::State", tag = "7")]
    pub state: i32,
    /// Optional. The dedicated serving endpoint for this FeatureOnlineStore, which
    /// is different from common Vertex service endpoint.
    #[prost(message, optional, tag = "10")]
    pub dedicated_serving_endpoint: ::core::option::Option<
        feature_online_store::DedicatedServingEndpoint,
    >,
    /// Optional. Customer-managed encryption key spec for data storage. If set,
    /// online store will be secured by this key.
    #[prost(message, optional, tag = "13")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "15")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "16")]
    pub satisfies_pzi: bool,
    #[prost(oneof = "feature_online_store::StorageType", tags = "8, 12")]
    pub storage_type: ::core::option::Option<feature_online_store::StorageType>,
}
/// Nested message and enum types in `FeatureOnlineStore`.
pub mod feature_online_store {
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Bigtable {
        /// Required. Autoscaling config applied to Bigtable Instance.
        #[prost(message, optional, tag = "1")]
        pub auto_scaling: ::core::option::Option<bigtable::AutoScaling>,
    }
    /// Nested message and enum types in `Bigtable`.
    pub mod bigtable {
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct AutoScaling {
            /// Required. The minimum number of nodes to scale down to. Must be greater
            /// than or equal to 1.
            #[prost(int32, tag = "1")]
            pub min_node_count: i32,
            /// Required. The maximum number of nodes to scale up to. Must be greater
            /// than or equal to min_node_count, and less than or equal to 10 times of
            /// 'min_node_count'.
            #[prost(int32, tag = "2")]
            pub max_node_count: i32,
            /// Optional. A percentage of the cluster's CPU capacity. Can be from 10%
            /// to 80%. When a cluster's CPU utilization exceeds the target that you
            /// have set, Bigtable immediately adds nodes to the cluster. When CPU
            /// utilization is substantially lower than the target, Bigtable removes
            /// nodes. If not set will default to 50%.
            #[prost(int32, tag = "3")]
            pub cpu_utilization_target: i32,
        }
    }
    /// Optimized storage type
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Optimized {}
    /// The dedicated serving endpoint for this FeatureOnlineStore. Only need to
    /// set when you choose Optimized storage type. Public endpoint is provisioned
    /// by default.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct DedicatedServingEndpoint {
        /// Output only. This field will be populated with the domain name to use for
        /// this FeatureOnlineStore
        #[prost(string, tag = "2")]
        pub public_endpoint_domain_name: ::prost::alloc::string::String,
        /// Optional. Private service connect config. The private service connection
        /// is available only for Optimized storage type, not for embedding
        /// management now. If
        /// [PrivateServiceConnectConfig.enable_private_service_connect][google.cloud.aiplatform.v1.PrivateServiceConnectConfig.enable_private_service_connect]
        /// set to true, customers will use private service connection to send
        /// request. Otherwise, the connection will set to public endpoint.
        #[prost(message, optional, tag = "3")]
        pub private_service_connect_config: ::core::option::Option<
            super::PrivateServiceConnectConfig,
        >,
        /// Output only. The name of the service attachment resource. Populated if
        /// private service connect is enabled and after FeatureViewSync is created.
        #[prost(string, tag = "4")]
        pub service_attachment: ::prost::alloc::string::String,
    }
    /// Possible states a featureOnlineStore can have.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// Default value. This value is unused.
        Unspecified = 0,
        /// State when the featureOnlineStore configuration is not being updated and
        /// the fields reflect the current configuration of the featureOnlineStore.
        /// The featureOnlineStore is usable in this state.
        Stable = 1,
        /// The state of the featureOnlineStore configuration when it is being
        /// updated. During an update, the fields reflect either the original
        /// configuration or the updated configuration of the featureOnlineStore. The
        /// featureOnlineStore is still usable in this state.
        Updating = 2,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STATE_UNSPECIFIED",
                Self::Stable => "STABLE",
                Self::Updating => "UPDATING",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "STABLE" => Some(Self::Stable),
                "UPDATING" => Some(Self::Updating),
                _ => None,
            }
        }
    }
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum StorageType {
        /// Contains settings for the Cloud Bigtable instance that will be created
        /// to serve featureValues for all FeatureViews under this
        /// FeatureOnlineStore.
        #[prost(message, tag = "8")]
        Bigtable(Bigtable),
        /// Contains settings for the Optimized store that will be created
        /// to serve featureValues for all FeatureViews under this
        /// FeatureOnlineStore. When choose Optimized storage type, need to set
        /// [PrivateServiceConnectConfig.enable_private_service_connect][google.cloud.aiplatform.v1.PrivateServiceConnectConfig.enable_private_service_connect]
        /// to use private endpoint. Otherwise will use public endpoint by default.
        #[prost(message, tag = "12")]
        Optimized(Optimized),
    }
}
/// FeatureView is representation of values that the FeatureOnlineStore will
/// serve based on its syncConfig.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FeatureView {
    /// Identifier. Name of the FeatureView. Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}/featureViews/{feature_view}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. Timestamp when this FeatureView was created.
    #[prost(message, optional, tag = "2")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this FeatureView was last updated.
    #[prost(message, optional, tag = "3")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. Used to perform consistent read-modify-write updates. If not set,
    /// a blind "overwrite" update happens.
    #[prost(string, tag = "4")]
    pub etag: ::prost::alloc::string::String,
    /// Optional. The labels with user-defined metadata to organize your
    /// FeatureViews.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information on and examples of labels.
    /// No more than 64 user labels can be associated with one
    /// FeatureOnlineStore(System labels are excluded)." System reserved label keys
    /// are prefixed with "aiplatform.googleapis.com/" and are immutable.
    #[prost(map = "string, string", tag = "5")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Configures when data is to be synced/updated for this FeatureView. At the
    /// end of the sync the latest featureValues for each entityId of this
    /// FeatureView are made ready for online serving.
    #[prost(message, optional, tag = "7")]
    pub sync_config: ::core::option::Option<feature_view::SyncConfig>,
    /// Optional. Configuration for index preparation for vector search. It
    /// contains the required configurations to create an index from source data,
    /// so that approximate nearest neighbor (a.k.a ANN) algorithms search can be
    /// performed during online serving.
    #[prost(message, optional, tag = "15")]
    pub index_config: ::core::option::Option<feature_view::IndexConfig>,
    /// Optional. Configuration for FeatureView created under Optimized
    /// FeatureOnlineStore.
    #[prost(message, optional, tag = "16")]
    pub optimized_config: ::core::option::Option<feature_view::OptimizedConfig>,
    /// Optional. Service agent type used during data sync. By default, the Vertex
    /// AI Service Agent is used. When using an IAM Policy to isolate this
    /// FeatureView within a project, a separate service account should be
    /// provisioned by setting this field to `SERVICE_AGENT_TYPE_FEATURE_VIEW`.
    /// This will generate a separate service account to access the BigQuery source
    /// table.
    #[prost(enumeration = "feature_view::ServiceAgentType", tag = "14")]
    pub service_agent_type: i32,
    /// Output only. A Service Account unique to this FeatureView. The role
    /// bigquery.dataViewer should be granted to this service account to allow
    /// Vertex AI Feature Store to sync data to the online store.
    #[prost(string, tag = "13")]
    pub service_account_email: ::prost::alloc::string::String,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "19")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "20")]
    pub satisfies_pzi: bool,
    #[prost(oneof = "feature_view::Source", tags = "6, 9, 18")]
    pub source: ::core::option::Option<feature_view::Source>,
}
/// Nested message and enum types in `FeatureView`.
pub mod feature_view {
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct BigQuerySource {
        /// Required. The BigQuery view URI that will be materialized on each sync
        /// trigger based on FeatureView.SyncConfig.
        #[prost(string, tag = "1")]
        pub uri: ::prost::alloc::string::String,
        /// Required. Columns to construct entity_id / row keys.
        #[prost(string, repeated, tag = "2")]
        pub entity_id_columns: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    }
    /// Configuration for Sync. Only one option is set.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct SyncConfig {
        /// Cron schedule (<https://en.wikipedia.org/wiki/Cron>) to launch scheduled
        /// runs. To explicitly set a timezone to the cron tab, apply a prefix in
        /// the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}".
        /// The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone
        /// database. For example, "CRON_TZ=America/New_York 1 * * * *", or
        /// "TZ=America/New_York 1 * * * *".
        #[prost(string, tag = "1")]
        pub cron: ::prost::alloc::string::String,
        /// Optional. If true, syncs the FeatureView in a continuous manner to Online
        /// Store.
        #[prost(bool, tag = "2")]
        pub continuous: bool,
    }
    /// Configuration for vector indexing.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct IndexConfig {
        /// Optional. Column of embedding. This column contains the source data to
        /// create index for vector search. embedding_column must be set when using
        /// vector search.
        #[prost(string, tag = "1")]
        pub embedding_column: ::prost::alloc::string::String,
        /// Optional. Columns of features that're used to filter vector search
        /// results.
        #[prost(string, repeated, tag = "2")]
        pub filter_columns: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        /// Optional. Column of crowding. This column contains crowding attribute
        /// which is a constraint on a neighbor list produced by
        /// [FeatureOnlineStoreService.SearchNearestEntities][google.cloud.aiplatform.v1.FeatureOnlineStoreService.SearchNearestEntities]
        /// to diversify search results. If
        /// [NearestNeighborQuery.per_crowding_attribute_neighbor_count][google.cloud.aiplatform.v1.NearestNeighborQuery.per_crowding_attribute_neighbor_count]
        /// is set to K in
        /// [SearchNearestEntitiesRequest][google.cloud.aiplatform.v1.SearchNearestEntitiesRequest],
        /// it's guaranteed that no more than K entities of the same crowding
        /// attribute are returned in the response.
        #[prost(string, tag = "3")]
        pub crowding_column: ::prost::alloc::string::String,
        /// Optional. The number of dimensions of the input embedding.
        #[prost(int32, optional, tag = "4")]
        pub embedding_dimension: ::core::option::Option<i32>,
        /// Optional. The distance measure used in nearest neighbor search.
        #[prost(enumeration = "index_config::DistanceMeasureType", tag = "5")]
        pub distance_measure_type: i32,
        /// The configuration with regard to the algorithms used for efficient
        /// search.
        #[prost(oneof = "index_config::AlgorithmConfig", tags = "6, 7")]
        pub algorithm_config: ::core::option::Option<index_config::AlgorithmConfig>,
    }
    /// Nested message and enum types in `IndexConfig`.
    pub mod index_config {
        /// Configuration options for using brute force search.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct BruteForceConfig {}
        /// Configuration options for the tree-AH algorithm.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct TreeAhConfig {
            /// Optional. Number of embeddings on each leaf node. The default value is
            /// 1000 if not set.
            #[prost(int64, optional, tag = "1")]
            pub leaf_node_embedding_count: ::core::option::Option<i64>,
        }
        /// The distance measure used in nearest neighbor search.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum DistanceMeasureType {
            /// Should not be set.
            Unspecified = 0,
            /// Euclidean (L_2) Distance.
            SquaredL2Distance = 1,
            /// Cosine Distance. Defined as 1 - cosine similarity.
            ///
            /// We strongly suggest using DOT_PRODUCT_DISTANCE + UNIT_L2_NORM instead
            /// of COSINE distance. Our algorithms have been more optimized for
            /// DOT_PRODUCT distance which, when combined with UNIT_L2_NORM, is
            /// mathematically equivalent to COSINE distance and results in the same
            /// ranking.
            CosineDistance = 2,
            /// Dot Product Distance. Defined as a negative of the dot product.
            DotProductDistance = 3,
        }
        impl DistanceMeasureType {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "DISTANCE_MEASURE_TYPE_UNSPECIFIED",
                    Self::SquaredL2Distance => "SQUARED_L2_DISTANCE",
                    Self::CosineDistance => "COSINE_DISTANCE",
                    Self::DotProductDistance => "DOT_PRODUCT_DISTANCE",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "DISTANCE_MEASURE_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                    "SQUARED_L2_DISTANCE" => Some(Self::SquaredL2Distance),
                    "COSINE_DISTANCE" => Some(Self::CosineDistance),
                    "DOT_PRODUCT_DISTANCE" => Some(Self::DotProductDistance),
                    _ => None,
                }
            }
        }
        /// The configuration with regard to the algorithms used for efficient
        /// search.
        #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
        pub enum AlgorithmConfig {
            /// Optional. Configuration options for the tree-AH algorithm (Shallow tree
            /// + Asymmetric Hashing). Please refer to this paper for more details:
            /// <https://arxiv.org/abs/1908.10396>
            #[prost(message, tag = "6")]
            TreeAhConfig(TreeAhConfig),
            /// Optional. Configuration options for using brute force search, which
            /// simply implements the standard linear search in the database for each
            /// query. It is primarily meant for benchmarking and to generate the
            /// ground truth for approximate search.
            #[prost(message, tag = "7")]
            BruteForceConfig(BruteForceConfig),
        }
    }
    /// A Feature Registry source for features that need to be synced to Online
    /// Store.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct FeatureRegistrySource {
        /// Required. List of features that need to be synced to Online Store.
        #[prost(message, repeated, tag = "1")]
        pub feature_groups: ::prost::alloc::vec::Vec<
            feature_registry_source::FeatureGroup,
        >,
        /// Optional. The project number of the parent project of the Feature Groups.
        #[prost(int64, optional, tag = "2")]
        pub project_number: ::core::option::Option<i64>,
    }
    /// Nested message and enum types in `FeatureRegistrySource`.
    pub mod feature_registry_source {
        /// Features belonging to a single feature group that will be
        /// synced to Online Store.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct FeatureGroup {
            /// Required. Identifier of the feature group.
            #[prost(string, tag = "1")]
            pub feature_group_id: ::prost::alloc::string::String,
            /// Required. Identifiers of features under the feature group.
            #[prost(string, repeated, tag = "2")]
            pub feature_ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        }
    }
    /// A Vertex Rag source for features that need to be synced to Online
    /// Store.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct VertexRagSource {
        /// Required. The BigQuery view/table URI that will be materialized on each
        /// manual sync trigger. The table/view is expected to have the following
        /// columns and types at least:
        ///   - `corpus_id` (STRING, NULLABLE/REQUIRED)
        ///   - `file_id` (STRING, NULLABLE/REQUIRED)
        ///   - `chunk_id` (STRING, NULLABLE/REQUIRED)
        ///   - `chunk_data_type` (STRING, NULLABLE/REQUIRED)
        ///   - `chunk_data` (STRING, NULLABLE/REQUIRED)
        ///   - `embeddings` (FLOAT, REPEATED)
        ///   - `file_original_uri` (STRING, NULLABLE/REQUIRED)
        #[prost(string, tag = "1")]
        pub uri: ::prost::alloc::string::String,
        /// Optional. The RAG corpus id corresponding to this FeatureView.
        #[prost(int64, tag = "2")]
        pub rag_corpus_id: i64,
    }
    /// Configuration for FeatureViews created in Optimized FeatureOnlineStore.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct OptimizedConfig {
        /// Optional. A description of resources that the FeatureView uses, which to
        /// large degree are decided by Vertex AI, and optionally allows only a
        /// modest additional configuration. If min_replica_count is not set, the
        /// default value is 2. If max_replica_count is not set, the default value
        /// is 6. The max allowed replica count is 1000.
        #[prost(message, optional, tag = "7")]
        pub automatic_resources: ::core::option::Option<super::AutomaticResources>,
    }
    /// Service agent type used during data sync.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum ServiceAgentType {
        /// By default, the project-level Vertex AI Service Agent is enabled.
        Unspecified = 0,
        /// Indicates the project-level Vertex AI Service Agent
        /// (<https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents>)
        /// will be used during sync jobs.
        Project = 1,
        /// Enable a FeatureView service account to be created by Vertex AI and
        /// output in the field `service_account_email`. This service account will
        /// be used to read from the source BigQuery table during sync.
        FeatureView = 2,
    }
    impl ServiceAgentType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "SERVICE_AGENT_TYPE_UNSPECIFIED",
                Self::Project => "SERVICE_AGENT_TYPE_PROJECT",
                Self::FeatureView => "SERVICE_AGENT_TYPE_FEATURE_VIEW",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "SERVICE_AGENT_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "SERVICE_AGENT_TYPE_PROJECT" => Some(Self::Project),
                "SERVICE_AGENT_TYPE_FEATURE_VIEW" => Some(Self::FeatureView),
                _ => None,
            }
        }
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Source {
        /// Optional. Configures how data is supposed to be extracted from a BigQuery
        /// source to be loaded onto the FeatureOnlineStore.
        #[prost(message, tag = "6")]
        BigQuerySource(BigQuerySource),
        /// Optional. Configures the features from a Feature Registry source that
        /// need to be loaded onto the FeatureOnlineStore.
        #[prost(message, tag = "9")]
        FeatureRegistrySource(FeatureRegistrySource),
        /// Optional. The Vertex RAG Source that the FeatureView is linked to.
        #[prost(message, tag = "18")]
        VertexRagSource(VertexRagSource),
    }
}
/// FeatureViewSync is a representation of sync operation which copies data from
/// data source to Feature View in Online Store.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FeatureViewSync {
    /// Identifier. Name of the FeatureViewSync. Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}/featureViews/{feature_view}/featureViewSyncs/{feature_view_sync}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. Time when this FeatureViewSync is created. Creation of a
    /// FeatureViewSync means that the job is pending / waiting for sufficient
    /// resources but may not have started the actual data transfer yet.
    #[prost(message, optional, tag = "2")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when this FeatureViewSync is finished.
    #[prost(message, optional, tag = "5")]
    pub run_time: ::core::option::Option<super::super::super::r#type::Interval>,
    /// Output only. Final status of the FeatureViewSync.
    #[prost(message, optional, tag = "4")]
    pub final_status: ::core::option::Option<super::super::super::rpc::Status>,
    /// Output only. Summary of the sync job.
    #[prost(message, optional, tag = "6")]
    pub sync_summary: ::core::option::Option<feature_view_sync::SyncSummary>,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "7")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "8")]
    pub satisfies_pzi: bool,
}
/// Nested message and enum types in `FeatureViewSync`.
pub mod feature_view_sync {
    /// Summary from the Sync job. For continuous syncs, the summary is updated
    /// periodically. For batch syncs, it gets updated on completion of the sync.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct SyncSummary {
        /// Output only. Total number of rows synced.
        #[prost(int64, tag = "1")]
        pub row_synced: i64,
        /// Output only. BigQuery slot milliseconds consumed for the sync job.
        #[prost(int64, tag = "2")]
        pub total_slot: i64,
        /// Lower bound of the system time watermark for the sync job. This is only
        /// set for continuously syncing feature views.
        #[prost(message, optional, tag = "5")]
        pub system_watermark_time: ::core::option::Option<::prost_types::Timestamp>,
    }
}
/// Request message for
/// [FeatureOnlineStoreAdminService.CreateFeatureOnlineStore][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.CreateFeatureOnlineStore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateFeatureOnlineStoreRequest {
    /// Required. The resource name of the Location to create FeatureOnlineStores.
    /// Format:
    /// `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The FeatureOnlineStore to create.
    #[prost(message, optional, tag = "2")]
    pub feature_online_store: ::core::option::Option<FeatureOnlineStore>,
    /// Required. The ID to use for this FeatureOnlineStore, which will become the
    /// final component of the FeatureOnlineStore's resource name.
    ///
    /// This value may be up to 60 characters, and valid characters are
    /// `\[a-z0-9_\]`. The first character cannot be a number.
    ///
    /// The value must be unique within the project and location.
    #[prost(string, tag = "3")]
    pub feature_online_store_id: ::prost::alloc::string::String,
}
/// Request message for
/// [FeatureOnlineStoreAdminService.GetFeatureOnlineStore][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.GetFeatureOnlineStore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetFeatureOnlineStoreRequest {
    /// Required. The name of the FeatureOnlineStore resource.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [FeatureOnlineStoreAdminService.ListFeatureOnlineStores][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.ListFeatureOnlineStores].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFeatureOnlineStoresRequest {
    /// Required. The resource name of the Location to list FeatureOnlineStores.
    /// Format:
    /// `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the FeatureOnlineStores that match the filter expression. The
    /// following fields are supported:
    ///
    /// * `create_time`: Supports `=`, `!=`, `<`, `>`, `<=`, and `>=` comparisons.
    /// Values must be
    ///    in RFC 3339 format.
    /// * `update_time`: Supports `=`, `!=`, `<`, `>`, `<=`, and `>=` comparisons.
    /// Values must be
    ///    in RFC 3339 format.
    /// * `labels`: Supports key-value equality and key presence.
    ///
    /// Examples:
    ///
    /// * `create_time > "2020-01-01" OR update_time > "2020-01-01"`
    ///     FeatureOnlineStores created or updated after 2020-01-01.
    /// * `labels.env = "prod"`
    ///     FeatureOnlineStores with label "env" set to "prod".
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The maximum number of FeatureOnlineStores to return. The service may return
    /// fewer than this value. If unspecified, at most 100 FeatureOnlineStores will
    /// be returned. The maximum value is 100; any value greater than 100 will be
    /// coerced to 100.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [FeatureOnlineStoreAdminService.ListFeatureOnlineStores][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.ListFeatureOnlineStores]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [FeatureOnlineStoreAdminService.ListFeatureOnlineStores][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.ListFeatureOnlineStores]
    /// must match the call that provided the page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    /// Supported Fields:
    ///
    ///    * `create_time`
    ///    * `update_time`
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [FeatureOnlineStoreAdminService.ListFeatureOnlineStores][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.ListFeatureOnlineStores].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFeatureOnlineStoresResponse {
    /// The FeatureOnlineStores matching the request.
    #[prost(message, repeated, tag = "1")]
    pub feature_online_stores: ::prost::alloc::vec::Vec<FeatureOnlineStore>,
    /// A token, which can be sent as
    /// [ListFeatureOnlineStoresRequest.page_token][google.cloud.aiplatform.v1.ListFeatureOnlineStoresRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [FeatureOnlineStoreAdminService.UpdateFeatureOnlineStore][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.UpdateFeatureOnlineStore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateFeatureOnlineStoreRequest {
    /// Required. The FeatureOnlineStore's `name` field is used to identify the
    /// FeatureOnlineStore to be updated. Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}`
    #[prost(message, optional, tag = "1")]
    pub feature_online_store: ::core::option::Option<FeatureOnlineStore>,
    /// Field mask is used to specify the fields to be overwritten in the
    /// FeatureOnlineStore resource by the update.
    /// The fields specified in the update_mask are relative to the resource, not
    /// the full request. A field will be overwritten if it is in the mask. If the
    /// user does not provide a mask then only the non-empty fields present in the
    /// request will be overwritten. Set the update_mask to `*` to override all
    /// fields.
    ///
    /// Updatable fields:
    ///
    ///    * `labels`
    ///    * `description`
    ///    * `bigtable`
    ///    * `bigtable.auto_scaling`
    ///    * `bigtable.enable_multi_region_replica`
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [FeatureOnlineStoreAdminService.DeleteFeatureOnlineStore][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.DeleteFeatureOnlineStore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteFeatureOnlineStoreRequest {
    /// Required. The name of the FeatureOnlineStore to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// If set to true, any FeatureViews and Features for this FeatureOnlineStore
    /// will also be deleted. (Otherwise, the request will only work if the
    /// FeatureOnlineStore has no FeatureViews.)
    #[prost(bool, tag = "2")]
    pub force: bool,
}
/// Request message for
/// [FeatureOnlineStoreAdminService.CreateFeatureView][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.CreateFeatureView].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateFeatureViewRequest {
    /// Required. The resource name of the FeatureOnlineStore to create
    /// FeatureViews. Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The FeatureView to create.
    #[prost(message, optional, tag = "2")]
    pub feature_view: ::core::option::Option<FeatureView>,
    /// Required. The ID to use for the FeatureView, which will become the final
    /// component of the FeatureView's resource name.
    ///
    /// This value may be up to 60 characters, and valid characters are
    /// `\[a-z0-9_\]`. The first character cannot be a number.
    ///
    /// The value must be unique within a FeatureOnlineStore.
    #[prost(string, tag = "3")]
    pub feature_view_id: ::prost::alloc::string::String,
    /// Immutable. If set to true, one on demand sync will be run immediately,
    /// regardless whether the
    /// [FeatureView.sync_config][google.cloud.aiplatform.v1.FeatureView.sync_config]
    /// is configured or not.
    #[prost(bool, tag = "4")]
    pub run_sync_immediately: bool,
}
/// Request message for
/// [FeatureOnlineStoreAdminService.GetFeatureView][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.GetFeatureView].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetFeatureViewRequest {
    /// Required. The name of the FeatureView resource.
    /// Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}/featureViews/{feature_view}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [FeatureOnlineStoreAdminService.ListFeatureViews][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.ListFeatureViews].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFeatureViewsRequest {
    /// Required. The resource name of the FeatureOnlineStore to list FeatureViews.
    /// Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the FeatureViews that match the filter expression. The following
    /// filters are supported:
    ///
    /// * `create_time`: Supports `=`, `!=`, `<`, `>`, `>=`, and `<=` comparisons.
    /// Values must be in RFC 3339 format.
    /// * `update_time`: Supports `=`, `!=`, `<`, `>`, `>=`, and `<=` comparisons.
    /// Values must be in RFC 3339 format.
    /// * `labels`: Supports key-value equality as well as key presence.
    ///
    /// Examples:
    ///
    /// * `create_time > \"2020-01-31T15:30:00.000000Z\" OR
    ///       update_time > \"2020-01-31T15:30:00.000000Z\"` --> FeatureViews
    ///       created or updated after 2020-01-31T15:30:00.000000Z.
    /// * `labels.active = yes AND labels.env = prod` --> FeatureViews having both
    ///      (active: yes) and (env: prod) labels.
    /// * `labels.env: *` --> Any FeatureView which has a label with 'env' as the
    ///    key.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The maximum number of FeatureViews to return. The service may return fewer
    /// than this value. If unspecified, at most 1000 FeatureViews will be
    /// returned. The maximum value is 1000; any value greater than 1000 will be
    /// coerced to 1000.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [FeatureOnlineStoreAdminService.ListFeatureViews][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.ListFeatureViews]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [FeatureOnlineStoreAdminService.ListFeatureViews][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.ListFeatureViews]
    /// must match the call that provided the page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    ///
    /// Supported fields:
    ///
    ///    * `feature_view_id`
    ///    * `create_time`
    ///    * `update_time`
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [FeatureOnlineStoreAdminService.ListFeatureViews][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.ListFeatureViews].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFeatureViewsResponse {
    /// The FeatureViews matching the request.
    #[prost(message, repeated, tag = "1")]
    pub feature_views: ::prost::alloc::vec::Vec<FeatureView>,
    /// A token, which can be sent as
    /// [ListFeatureViewsRequest.page_token][google.cloud.aiplatform.v1.ListFeatureViewsRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [FeatureOnlineStoreAdminService.UpdateFeatureView][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.UpdateFeatureView].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateFeatureViewRequest {
    /// Required. The FeatureView's `name` field is used to identify the
    /// FeatureView to be updated. Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}/featureViews/{feature_view}`
    #[prost(message, optional, tag = "1")]
    pub feature_view: ::core::option::Option<FeatureView>,
    /// Field mask is used to specify the fields to be overwritten in the
    /// FeatureView resource by the update.
    /// The fields specified in the update_mask are relative to the resource, not
    /// the full request. A field will be overwritten if it is in the mask. If the
    /// user does not provide a mask then only the non-empty fields present in the
    /// request will be overwritten. Set the update_mask to `*` to override all
    /// fields.
    ///
    /// Updatable fields:
    ///
    ///    * `labels`
    ///    * `service_agent_type`
    ///    * `big_query_source`
    ///    * `big_query_source.uri`
    ///    * `big_query_source.entity_id_columns`
    ///    * `feature_registry_source`
    ///    * `feature_registry_source.feature_groups`
    ///    * `sync_config`
    ///    * `sync_config.cron`
    ///    * `optimized_config.automatic_resources`
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [FeatureOnlineStoreAdminService.DeleteFeatureView][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.DeleteFeatureView].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteFeatureViewRequest {
    /// Required. The name of the FeatureView to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}/featureViews/{feature_view}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Details of operations that perform create FeatureOnlineStore.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateFeatureOnlineStoreOperationMetadata {
    /// Operation metadata for FeatureOnlineStore.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that perform update FeatureOnlineStore.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateFeatureOnlineStoreOperationMetadata {
    /// Operation metadata for FeatureOnlineStore.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that perform create FeatureView.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateFeatureViewOperationMetadata {
    /// Operation metadata for FeatureView Create.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that perform update FeatureView.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateFeatureViewOperationMetadata {
    /// Operation metadata for FeatureView Update.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [FeatureOnlineStoreAdminService.SyncFeatureView][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.SyncFeatureView].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SyncFeatureViewRequest {
    /// Required. Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}/featureViews/{feature_view}`
    #[prost(string, tag = "1")]
    pub feature_view: ::prost::alloc::string::String,
}
/// Response message for
/// [FeatureOnlineStoreAdminService.SyncFeatureView][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.SyncFeatureView].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SyncFeatureViewResponse {
    /// Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}/featureViews/{feature_view}/featureViewSyncs/{feature_view_sync}`
    #[prost(string, tag = "1")]
    pub feature_view_sync: ::prost::alloc::string::String,
}
/// Request message for
/// [FeatureOnlineStoreAdminService.GetFeatureViewSync][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.GetFeatureViewSync].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetFeatureViewSyncRequest {
    /// Required. The name of the FeatureViewSync resource.
    /// Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}/featureViews/{feature_view}/featureViewSyncs/{feature_view_sync}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [FeatureOnlineStoreAdminService.ListFeatureViewSyncs][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.ListFeatureViewSyncs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFeatureViewSyncsRequest {
    /// Required. The resource name of the FeatureView to list FeatureViewSyncs.
    /// Format:
    /// `projects/{project}/locations/{location}/featureOnlineStores/{feature_online_store}/featureViews/{feature_view}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the FeatureViewSyncs that match the filter expression. The following
    /// filters are supported:
    ///
    /// * `create_time`: Supports `=`, `!=`, `<`, `>`, `>=`, and `<=` comparisons.
    /// Values must be in RFC 3339 format.
    ///
    /// Examples:
    ///
    /// * `create_time > \"2020-01-31T15:30:00.000000Z\"` --> FeatureViewSyncs
    ///       created after 2020-01-31T15:30:00.000000Z.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The maximum number of FeatureViewSyncs to return. The service may return
    /// fewer than this value. If unspecified, at most 1000 FeatureViewSyncs will
    /// be returned. The maximum value is 1000; any value greater than 1000 will be
    /// coerced to 1000.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [FeatureOnlineStoreAdminService.ListFeatureViewSyncs][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.ListFeatureViewSyncs]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [FeatureOnlineStoreAdminService.ListFeatureViewSyncs][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.ListFeatureViewSyncs]
    /// must match the call that provided the page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    ///
    /// Supported fields:
    ///
    ///    * `create_time`
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [FeatureOnlineStoreAdminService.ListFeatureViewSyncs][google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService.ListFeatureViewSyncs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFeatureViewSyncsResponse {
    /// The FeatureViewSyncs matching the request.
    #[prost(message, repeated, tag = "1")]
    pub feature_view_syncs: ::prost::alloc::vec::Vec<FeatureViewSync>,
    /// A token, which can be sent as
    /// [ListFeatureViewSyncsRequest.page_token][google.cloud.aiplatform.v1.ListFeatureViewSyncsRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod feature_online_store_admin_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// The service that handles CRUD and List for resources for
    /// FeatureOnlineStore.
    #[derive(Debug, Clone)]
    pub struct FeatureOnlineStoreAdminServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl FeatureOnlineStoreAdminServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> FeatureOnlineStoreAdminServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> FeatureOnlineStoreAdminServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            FeatureOnlineStoreAdminServiceClient::new(
                InterceptedService::new(inner, interceptor),
            )
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a new FeatureOnlineStore in a given project and location.
        pub async fn create_feature_online_store(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateFeatureOnlineStoreRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/CreateFeatureOnlineStore",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "CreateFeatureOnlineStore",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets details of a single FeatureOnlineStore.
        pub async fn get_feature_online_store(
            &mut self,
            request: impl tonic::IntoRequest<super::GetFeatureOnlineStoreRequest>,
        ) -> std::result::Result<
            tonic::Response<super::FeatureOnlineStore>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/GetFeatureOnlineStore",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "GetFeatureOnlineStore",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists FeatureOnlineStores in a given project and location.
        pub async fn list_feature_online_stores(
            &mut self,
            request: impl tonic::IntoRequest<super::ListFeatureOnlineStoresRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListFeatureOnlineStoresResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/ListFeatureOnlineStores",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "ListFeatureOnlineStores",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates the parameters of a single FeatureOnlineStore.
        pub async fn update_feature_online_store(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateFeatureOnlineStoreRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/UpdateFeatureOnlineStore",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "UpdateFeatureOnlineStore",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a single FeatureOnlineStore. The FeatureOnlineStore must not
        /// contain any FeatureViews.
        pub async fn delete_feature_online_store(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteFeatureOnlineStoreRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/DeleteFeatureOnlineStore",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "DeleteFeatureOnlineStore",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a new FeatureView in a given FeatureOnlineStore.
        pub async fn create_feature_view(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateFeatureViewRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/CreateFeatureView",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "CreateFeatureView",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets details of a single FeatureView.
        pub async fn get_feature_view(
            &mut self,
            request: impl tonic::IntoRequest<super::GetFeatureViewRequest>,
        ) -> std::result::Result<tonic::Response<super::FeatureView>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/GetFeatureView",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "GetFeatureView",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists FeatureViews in a given FeatureOnlineStore.
        pub async fn list_feature_views(
            &mut self,
            request: impl tonic::IntoRequest<super::ListFeatureViewsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListFeatureViewsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/ListFeatureViews",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "ListFeatureViews",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates the parameters of a single FeatureView.
        pub async fn update_feature_view(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateFeatureViewRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/UpdateFeatureView",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "UpdateFeatureView",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a single FeatureView.
        pub async fn delete_feature_view(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteFeatureViewRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/DeleteFeatureView",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "DeleteFeatureView",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Triggers on-demand sync for the FeatureView.
        pub async fn sync_feature_view(
            &mut self,
            request: impl tonic::IntoRequest<super::SyncFeatureViewRequest>,
        ) -> std::result::Result<
            tonic::Response<super::SyncFeatureViewResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/SyncFeatureView",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "SyncFeatureView",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets details of a single FeatureViewSync.
        pub async fn get_feature_view_sync(
            &mut self,
            request: impl tonic::IntoRequest<super::GetFeatureViewSyncRequest>,
        ) -> std::result::Result<
            tonic::Response<super::FeatureViewSync>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/GetFeatureViewSync",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "GetFeatureViewSync",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists FeatureViewSyncs in a given FeatureView.
        pub async fn list_feature_view_syncs(
            &mut self,
            request: impl tonic::IntoRequest<super::ListFeatureViewSyncsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListFeatureViewSyncsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService/ListFeatureViewSyncs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreAdminService",
                        "ListFeatureViewSyncs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Matcher for Features of an EntityType by Feature ID.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct IdMatcher {
    /// Required. The following are accepted as `ids`:
    ///
    ///   * A single-element list containing only `*`, which selects all Features
    ///   in the target EntityType, or
    ///   * A list containing only Feature IDs, which selects only Features with
    ///   those IDs in the target EntityType.
    #[prost(string, repeated, tag = "1")]
    pub ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Selector for Features of an EntityType.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FeatureSelector {
    /// Required. Matches Features based on ID.
    #[prost(message, optional, tag = "1")]
    pub id_matcher: ::core::option::Option<IdMatcher>,
}
/// A list of boolean values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BoolArray {
    /// A list of bool values.
    #[prost(bool, repeated, tag = "1")]
    pub values: ::prost::alloc::vec::Vec<bool>,
}
/// A list of double values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DoubleArray {
    /// A list of double values.
    #[prost(double, repeated, tag = "1")]
    pub values: ::prost::alloc::vec::Vec<f64>,
}
/// A list of int64 values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Int64Array {
    /// A list of int64 values.
    #[prost(int64, repeated, tag = "1")]
    pub values: ::prost::alloc::vec::Vec<i64>,
}
/// A list of string values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StringArray {
    /// A list of string values.
    #[prost(string, repeated, tag = "1")]
    pub values: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// A tensor value type.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Tensor {
    /// The data type of tensor.
    #[prost(enumeration = "tensor::DataType", tag = "1")]
    pub dtype: i32,
    /// Shape of the tensor.
    #[prost(int64, repeated, tag = "2")]
    pub shape: ::prost::alloc::vec::Vec<i64>,
    /// Type specific representations that make it easy to create tensor protos in
    /// all languages.  Only the representation corresponding to "dtype" can
    /// be set.  The values hold the flattened representation of the tensor in
    /// row major order.
    ///
    /// [BOOL][google.cloud.aiplatform.v1.Tensor.DataType.BOOL]
    #[prost(bool, repeated, tag = "3")]
    pub bool_val: ::prost::alloc::vec::Vec<bool>,
    /// [STRING][google.cloud.aiplatform.v1.Tensor.DataType.STRING]
    #[prost(string, repeated, tag = "14")]
    pub string_val: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// [STRING][google.cloud.aiplatform.v1.Tensor.DataType.STRING]
    #[prost(bytes = "vec", repeated, tag = "15")]
    pub bytes_val: ::prost::alloc::vec::Vec<::prost::alloc::vec::Vec<u8>>,
    /// [FLOAT][google.cloud.aiplatform.v1.Tensor.DataType.FLOAT]
    #[prost(float, repeated, tag = "5")]
    pub float_val: ::prost::alloc::vec::Vec<f32>,
    /// [DOUBLE][google.cloud.aiplatform.v1.Tensor.DataType.DOUBLE]
    #[prost(double, repeated, tag = "6")]
    pub double_val: ::prost::alloc::vec::Vec<f64>,
    /// [INT_8][google.cloud.aiplatform.v1.Tensor.DataType.INT8]
    /// [INT_16][google.cloud.aiplatform.v1.Tensor.DataType.INT16]
    /// [INT_32][google.cloud.aiplatform.v1.Tensor.DataType.INT32]
    #[prost(int32, repeated, tag = "7")]
    pub int_val: ::prost::alloc::vec::Vec<i32>,
    /// [INT64][google.cloud.aiplatform.v1.Tensor.DataType.INT64]
    #[prost(int64, repeated, tag = "8")]
    pub int64_val: ::prost::alloc::vec::Vec<i64>,
    /// [UINT8][google.cloud.aiplatform.v1.Tensor.DataType.UINT8]
    /// [UINT16][google.cloud.aiplatform.v1.Tensor.DataType.UINT16]
    /// [UINT32][google.cloud.aiplatform.v1.Tensor.DataType.UINT32]
    #[prost(uint32, repeated, tag = "9")]
    pub uint_val: ::prost::alloc::vec::Vec<u32>,
    /// [UINT64][google.cloud.aiplatform.v1.Tensor.DataType.UINT64]
    #[prost(uint64, repeated, tag = "10")]
    pub uint64_val: ::prost::alloc::vec::Vec<u64>,
    /// A list of tensor values.
    #[prost(message, repeated, tag = "11")]
    pub list_val: ::prost::alloc::vec::Vec<Tensor>,
    /// A map of string to tensor.
    #[prost(map = "string, message", tag = "12")]
    pub struct_val: ::std::collections::HashMap<::prost::alloc::string::String, Tensor>,
    /// Serialized raw tensor content.
    #[prost(bytes = "vec", tag = "13")]
    pub tensor_val: ::prost::alloc::vec::Vec<u8>,
}
/// Nested message and enum types in `Tensor`.
pub mod tensor {
    /// Data type of the tensor.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum DataType {
        /// Not a legal value for DataType. Used to indicate a DataType field has not
        /// been set.
        Unspecified = 0,
        /// Data types that all computation devices are expected to be
        /// capable to support.
        Bool = 1,
        String = 2,
        Float = 3,
        Double = 4,
        Int8 = 5,
        Int16 = 6,
        Int32 = 7,
        Int64 = 8,
        Uint8 = 9,
        Uint16 = 10,
        Uint32 = 11,
        Uint64 = 12,
    }
    impl DataType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "DATA_TYPE_UNSPECIFIED",
                Self::Bool => "BOOL",
                Self::String => "STRING",
                Self::Float => "FLOAT",
                Self::Double => "DOUBLE",
                Self::Int8 => "INT8",
                Self::Int16 => "INT16",
                Self::Int32 => "INT32",
                Self::Int64 => "INT64",
                Self::Uint8 => "UINT8",
                Self::Uint16 => "UINT16",
                Self::Uint32 => "UINT32",
                Self::Uint64 => "UINT64",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "DATA_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "BOOL" => Some(Self::Bool),
                "STRING" => Some(Self::String),
                "FLOAT" => Some(Self::Float),
                "DOUBLE" => Some(Self::Double),
                "INT8" => Some(Self::Int8),
                "INT16" => Some(Self::Int16),
                "INT32" => Some(Self::Int32),
                "INT64" => Some(Self::Int64),
                "UINT8" => Some(Self::Uint8),
                "UINT16" => Some(Self::Uint16),
                "UINT32" => Some(Self::Uint32),
                "UINT64" => Some(Self::Uint64),
                _ => None,
            }
        }
    }
}
/// Request message for
/// [FeaturestoreOnlineServingService.WriteFeatureValues][google.cloud.aiplatform.v1.FeaturestoreOnlineServingService.WriteFeatureValues].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct WriteFeatureValuesRequest {
    /// Required. The resource name of the EntityType for the entities being
    /// written. Value format:
    /// `projects/{project}/locations/{location}/featurestores/
    /// {featurestore}/entityTypes/{entityType}`. For example,
    /// for a machine learning model predicting user clicks on a website, an
    /// EntityType ID could be `user`.
    #[prost(string, tag = "1")]
    pub entity_type: ::prost::alloc::string::String,
    /// Required. The entities to be written. Up to 100,000 feature values can be
    /// written across all `payloads`.
    #[prost(message, repeated, tag = "2")]
    pub payloads: ::prost::alloc::vec::Vec<WriteFeatureValuesPayload>,
}
/// Contains Feature values to be written for a specific entity.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct WriteFeatureValuesPayload {
    /// Required. The ID of the entity.
    #[prost(string, tag = "1")]
    pub entity_id: ::prost::alloc::string::String,
    /// Required. Feature values to be written, mapping from Feature ID to value.
    /// Up to 100,000 `feature_values` entries may be written across all payloads.
    /// The feature generation time, aligned by days, must be no older than five
    /// years (1825 days) and no later than one year (366 days) in the future.
    #[prost(map = "string, message", tag = "2")]
    pub feature_values: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        FeatureValue,
    >,
}
/// Response message for
/// [FeaturestoreOnlineServingService.WriteFeatureValues][google.cloud.aiplatform.v1.FeaturestoreOnlineServingService.WriteFeatureValues].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct WriteFeatureValuesResponse {}
/// Request message for
/// [FeaturestoreOnlineServingService.ReadFeatureValues][google.cloud.aiplatform.v1.FeaturestoreOnlineServingService.ReadFeatureValues].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadFeatureValuesRequest {
    /// Required. The resource name of the EntityType for the entity being read.
    /// Value format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entityType}`.
    /// For example, for a machine learning model predicting user clicks on a
    /// website, an EntityType ID could be `user`.
    #[prost(string, tag = "1")]
    pub entity_type: ::prost::alloc::string::String,
    /// Required. ID for a specific entity. For example,
    /// for a machine learning model predicting user clicks on a website, an entity
    /// ID could be `user_123`.
    #[prost(string, tag = "2")]
    pub entity_id: ::prost::alloc::string::String,
    /// Required. Selector choosing Features of the target EntityType.
    #[prost(message, optional, tag = "3")]
    pub feature_selector: ::core::option::Option<FeatureSelector>,
}
/// Response message for
/// [FeaturestoreOnlineServingService.ReadFeatureValues][google.cloud.aiplatform.v1.FeaturestoreOnlineServingService.ReadFeatureValues].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadFeatureValuesResponse {
    /// Response header.
    #[prost(message, optional, tag = "1")]
    pub header: ::core::option::Option<read_feature_values_response::Header>,
    /// Entity view with Feature values. This may be the entity in the
    /// Featurestore if values for all Features were requested, or a projection
    /// of the entity in the Featurestore if values for only some Features were
    /// requested.
    #[prost(message, optional, tag = "2")]
    pub entity_view: ::core::option::Option<read_feature_values_response::EntityView>,
}
/// Nested message and enum types in `ReadFeatureValuesResponse`.
pub mod read_feature_values_response {
    /// Metadata for requested Features.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct FeatureDescriptor {
        /// Feature ID.
        #[prost(string, tag = "1")]
        pub id: ::prost::alloc::string::String,
    }
    /// Response header with metadata for the requested
    /// [ReadFeatureValuesRequest.entity_type][google.cloud.aiplatform.v1.ReadFeatureValuesRequest.entity_type]
    /// and Features.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Header {
        /// The resource name of the EntityType from the
        /// [ReadFeatureValuesRequest][google.cloud.aiplatform.v1.ReadFeatureValuesRequest].
        /// Value format:
        /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entityType}`.
        #[prost(string, tag = "1")]
        pub entity_type: ::prost::alloc::string::String,
        /// List of Feature metadata corresponding to each piece of
        /// [ReadFeatureValuesResponse.EntityView.data][google.cloud.aiplatform.v1.ReadFeatureValuesResponse.EntityView.data].
        #[prost(message, repeated, tag = "2")]
        pub feature_descriptors: ::prost::alloc::vec::Vec<FeatureDescriptor>,
    }
    /// Entity view with Feature values.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct EntityView {
        /// ID of the requested entity.
        #[prost(string, tag = "1")]
        pub entity_id: ::prost::alloc::string::String,
        /// Each piece of data holds the k
        /// requested values for one requested Feature. If no values
        /// for the requested Feature exist, the corresponding cell will be empty.
        /// This has the same size and is in the same order as the features from the
        /// header
        /// [ReadFeatureValuesResponse.header][google.cloud.aiplatform.v1.ReadFeatureValuesResponse.header].
        #[prost(message, repeated, tag = "2")]
        pub data: ::prost::alloc::vec::Vec<entity_view::Data>,
    }
    /// Nested message and enum types in `EntityView`.
    pub mod entity_view {
        /// Container to hold value(s), successive in time, for one Feature from the
        /// request.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct Data {
            #[prost(oneof = "data::Data", tags = "1, 2")]
            pub data: ::core::option::Option<data::Data>,
        }
        /// Nested message and enum types in `Data`.
        pub mod data {
            #[derive(Clone, PartialEq, ::prost::Oneof)]
            pub enum Data {
                /// Feature value if a single value is requested.
                #[prost(message, tag = "1")]
                Value(super::super::super::FeatureValue),
                /// Feature values list if values, successive in time, are requested.
                /// If the requested number of values is greater than the number of
                /// existing Feature values, nonexistent values are omitted instead of
                /// being returned as empty.
                #[prost(message, tag = "2")]
                Values(super::super::super::FeatureValueList),
            }
        }
    }
}
/// Request message for
/// [FeaturestoreOnlineServingService.StreamingReadFeatureValues][google.cloud.aiplatform.v1.FeaturestoreOnlineServingService.StreamingReadFeatureValues].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StreamingReadFeatureValuesRequest {
    /// Required. The resource name of the entities' type.
    /// Value format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entityType}`.
    /// For example,
    /// for a machine learning model predicting user clicks on a website, an
    /// EntityType ID could be `user`.
    #[prost(string, tag = "1")]
    pub entity_type: ::prost::alloc::string::String,
    /// Required. IDs of entities to read Feature values of. The maximum number of
    /// IDs is 100. For example, for a machine learning model predicting user
    /// clicks on a website, an entity ID could be `user_123`.
    #[prost(string, repeated, tag = "2")]
    pub entity_ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Required. Selector choosing Features of the target EntityType. Feature IDs
    /// will be deduplicated.
    #[prost(message, optional, tag = "3")]
    pub feature_selector: ::core::option::Option<FeatureSelector>,
}
/// Value for a feature.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FeatureValue {
    /// Metadata of feature value.
    #[prost(message, optional, tag = "14")]
    pub metadata: ::core::option::Option<feature_value::Metadata>,
    /// Value for the feature.
    #[prost(oneof = "feature_value::Value", tags = "1, 2, 5, 6, 7, 8, 11, 12, 13, 15")]
    pub value: ::core::option::Option<feature_value::Value>,
}
/// Nested message and enum types in `FeatureValue`.
pub mod feature_value {
    /// Metadata of feature value.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Metadata {
        /// Feature generation timestamp. Typically, it is provided by user at
        /// feature ingestion time. If not, feature store
        /// will use the system timestamp when the data is ingested into feature
        /// store. For streaming ingestion, the time, aligned by days, must be no
        /// older than five years (1825 days) and no later than one year (366 days)
        /// in the future.
        #[prost(message, optional, tag = "1")]
        pub generate_time: ::core::option::Option<::prost_types::Timestamp>,
    }
    /// Value for the feature.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Value {
        /// Bool type feature value.
        #[prost(bool, tag = "1")]
        BoolValue(bool),
        /// Double type feature value.
        #[prost(double, tag = "2")]
        DoubleValue(f64),
        /// Int64 feature value.
        #[prost(int64, tag = "5")]
        Int64Value(i64),
        /// String feature value.
        #[prost(string, tag = "6")]
        StringValue(::prost::alloc::string::String),
        /// A list of bool type feature value.
        #[prost(message, tag = "7")]
        BoolArrayValue(super::BoolArray),
        /// A list of double type feature value.
        #[prost(message, tag = "8")]
        DoubleArrayValue(super::DoubleArray),
        /// A list of int64 type feature value.
        #[prost(message, tag = "11")]
        Int64ArrayValue(super::Int64Array),
        /// A list of string type feature value.
        #[prost(message, tag = "12")]
        StringArrayValue(super::StringArray),
        /// Bytes feature value.
        #[prost(bytes, tag = "13")]
        BytesValue(::prost::alloc::vec::Vec<u8>),
        /// A struct type feature value.
        #[prost(message, tag = "15")]
        StructValue(super::StructValue),
    }
}
/// Struct (or object) type feature value.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StructValue {
    /// A list of field values.
    #[prost(message, repeated, tag = "1")]
    pub values: ::prost::alloc::vec::Vec<StructFieldValue>,
}
/// One field of a Struct (or object) type feature value.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StructFieldValue {
    /// Name of the field in the struct feature.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// The value for this field.
    #[prost(message, optional, tag = "2")]
    pub value: ::core::option::Option<FeatureValue>,
}
/// Container for list of values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FeatureValueList {
    /// A list of feature values. All of them should be the same data type.
    #[prost(message, repeated, tag = "1")]
    pub values: ::prost::alloc::vec::Vec<FeatureValue>,
}
/// Generated client implementations.
pub mod featurestore_online_serving_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for serving online feature values.
    #[derive(Debug, Clone)]
    pub struct FeaturestoreOnlineServingServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl FeaturestoreOnlineServingServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> FeaturestoreOnlineServingServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> FeaturestoreOnlineServingServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            FeaturestoreOnlineServingServiceClient::new(
                InterceptedService::new(inner, interceptor),
            )
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Reads Feature values of a specific entity of an EntityType. For reading
        /// feature values of multiple entities of an EntityType, please use
        /// StreamingReadFeatureValues.
        pub async fn read_feature_values(
            &mut self,
            request: impl tonic::IntoRequest<super::ReadFeatureValuesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ReadFeatureValuesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreOnlineServingService/ReadFeatureValues",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreOnlineServingService",
                        "ReadFeatureValues",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Reads Feature values for multiple entities. Depending on their size, data
        /// for different entities may be broken
        /// up across multiple responses.
        pub async fn streaming_read_feature_values(
            &mut self,
            request: impl tonic::IntoRequest<super::StreamingReadFeatureValuesRequest>,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::ReadFeatureValuesResponse>>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreOnlineServingService/StreamingReadFeatureValues",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreOnlineServingService",
                        "StreamingReadFeatureValues",
                    ),
                );
            self.inner.server_streaming(req, path, codec).await
        }
        /// Writes Feature values of one or more entities of an EntityType.
        ///
        /// The Feature values are merged into existing entities if any. The Feature
        /// values to be written must have timestamp within the online storage
        /// retention.
        pub async fn write_feature_values(
            &mut self,
            request: impl tonic::IntoRequest<super::WriteFeatureValuesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::WriteFeatureValuesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreOnlineServingService/WriteFeatureValues",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreOnlineServingService",
                        "WriteFeatureValues",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Lookup key for a feature view.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FeatureViewDataKey {
    #[prost(oneof = "feature_view_data_key::KeyOneof", tags = "1, 2")]
    pub key_oneof: ::core::option::Option<feature_view_data_key::KeyOneof>,
}
/// Nested message and enum types in `FeatureViewDataKey`.
pub mod feature_view_data_key {
    /// ID that is comprised from several parts (columns).
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct CompositeKey {
        /// Parts to construct Entity ID. Should match with the same ID columns as
        /// defined in FeatureView in the same order.
        #[prost(string, repeated, tag = "1")]
        pub parts: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum KeyOneof {
        /// String key to use for lookup.
        #[prost(string, tag = "1")]
        Key(::prost::alloc::string::String),
        /// The actual Entity ID will be composed from this struct. This should match
        /// with the way ID is defined in the FeatureView spec.
        #[prost(message, tag = "2")]
        CompositeKey(CompositeKey),
    }
}
/// Request message for
/// [FeatureOnlineStoreService.FetchFeatureValues][google.cloud.aiplatform.v1.FeatureOnlineStoreService.FetchFeatureValues].
/// All the features under the requested feature view will be returned.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FetchFeatureValuesRequest {
    /// Required. FeatureView resource format
    /// `projects/{project}/locations/{location}/featureOnlineStores/{featureOnlineStore}/featureViews/{featureView}`
    #[prost(string, tag = "1")]
    pub feature_view: ::prost::alloc::string::String,
    /// Optional. The request key to fetch feature values for.
    #[prost(message, optional, tag = "6")]
    pub data_key: ::core::option::Option<FeatureViewDataKey>,
    /// Optional. Response data format. If not set,
    /// [FeatureViewDataFormat.KEY_VALUE][google.cloud.aiplatform.v1.FeatureViewDataFormat.KEY_VALUE]
    /// will be used.
    #[prost(enumeration = "FeatureViewDataFormat", tag = "7")]
    pub data_format: i32,
}
/// Response message for
/// [FeatureOnlineStoreService.FetchFeatureValues][google.cloud.aiplatform.v1.FeatureOnlineStoreService.FetchFeatureValues]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FetchFeatureValuesResponse {
    /// The data key associated with this response.
    /// Will only be populated for
    /// [FeatureOnlineStoreService.StreamingFetchFeatureValues][] RPCs.
    #[prost(message, optional, tag = "4")]
    pub data_key: ::core::option::Option<FeatureViewDataKey>,
    #[prost(oneof = "fetch_feature_values_response::Format", tags = "3, 2")]
    pub format: ::core::option::Option<fetch_feature_values_response::Format>,
}
/// Nested message and enum types in `FetchFeatureValuesResponse`.
pub mod fetch_feature_values_response {
    /// Response structure in the format of key (feature name) and (feature) value
    /// pair.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct FeatureNameValuePairList {
        /// List of feature names and values.
        #[prost(message, repeated, tag = "1")]
        pub features: ::prost::alloc::vec::Vec<
            feature_name_value_pair_list::FeatureNameValuePair,
        >,
    }
    /// Nested message and enum types in `FeatureNameValuePairList`.
    pub mod feature_name_value_pair_list {
        /// Feature name & value pair.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct FeatureNameValuePair {
            /// Feature short name.
            #[prost(string, tag = "1")]
            pub name: ::prost::alloc::string::String,
            #[prost(oneof = "feature_name_value_pair::Data", tags = "2")]
            pub data: ::core::option::Option<feature_name_value_pair::Data>,
        }
        /// Nested message and enum types in `FeatureNameValuePair`.
        pub mod feature_name_value_pair {
            #[derive(Clone, PartialEq, ::prost::Oneof)]
            pub enum Data {
                /// Feature value.
                #[prost(message, tag = "2")]
                Value(super::super::super::FeatureValue),
            }
        }
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Format {
        /// Feature values in KeyValue format.
        #[prost(message, tag = "3")]
        KeyValues(FeatureNameValuePairList),
        /// Feature values in proto Struct format.
        #[prost(message, tag = "2")]
        ProtoStruct(::prost_types::Struct),
    }
}
/// A query to find a number of similar entities.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NearestNeighborQuery {
    /// Optional. The number of similar entities to be retrieved from feature view
    /// for each query.
    #[prost(int32, tag = "3")]
    pub neighbor_count: i32,
    /// Optional. The list of string filters.
    #[prost(message, repeated, tag = "4")]
    pub string_filters: ::prost::alloc::vec::Vec<nearest_neighbor_query::StringFilter>,
    /// Optional. The list of numeric filters.
    #[prost(message, repeated, tag = "8")]
    pub numeric_filters: ::prost::alloc::vec::Vec<nearest_neighbor_query::NumericFilter>,
    /// Optional. Crowding is a constraint on a neighbor list produced by nearest
    /// neighbor search requiring that no more than
    /// sper_crowding_attribute_neighbor_count of the k neighbors returned have the
    /// same value of crowding_attribute. It's used for improving result diversity.
    #[prost(int32, tag = "5")]
    pub per_crowding_attribute_neighbor_count: i32,
    /// Optional. Parameters that can be set to tune query on the fly.
    #[prost(message, optional, tag = "7")]
    pub parameters: ::core::option::Option<nearest_neighbor_query::Parameters>,
    #[prost(oneof = "nearest_neighbor_query::Instance", tags = "1, 2")]
    pub instance: ::core::option::Option<nearest_neighbor_query::Instance>,
}
/// Nested message and enum types in `NearestNeighborQuery`.
pub mod nearest_neighbor_query {
    /// The embedding vector.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Embedding {
        /// Optional. Individual value in the embedding.
        #[prost(float, repeated, packed = "false", tag = "1")]
        pub value: ::prost::alloc::vec::Vec<f32>,
    }
    /// String filter is used to search a subset of the entities by using boolean
    /// rules on string columns.
    /// For example: if a query specifies string filter
    /// with 'name = color, allow_tokens = {red, blue}, deny_tokens = {purple}','
    /// then that query will match entities that are red or blue, but if those
    /// points are also purple, then they will be excluded even if they are
    /// red/blue. Only string filter is supported for now, numeric filter will be
    /// supported in the near future.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct StringFilter {
        /// Required. Column names in BigQuery that used as filters.
        #[prost(string, tag = "1")]
        pub name: ::prost::alloc::string::String,
        /// Optional. The allowed tokens.
        #[prost(string, repeated, tag = "2")]
        pub allow_tokens: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        /// Optional. The denied tokens.
        #[prost(string, repeated, tag = "3")]
        pub deny_tokens: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    }
    /// Numeric filter is used to search a subset of the entities by using boolean
    /// rules on numeric columns.
    /// For example:
    /// Database Point 0: {name: "a" value_int: 42} {name: "b" value_float: 1.0}
    /// Database Point 1:  {name: "a" value_int: 10} {name: "b" value_float: 2.0}
    /// Database Point 2: {name: "a" value_int: -1} {name: "b" value_float: 3.0}
    /// Query: {name: "a" value_int: 12 operator: LESS}    // Matches Point 1, 2
    /// {name: "b" value_float: 2.0 operator: EQUAL} // Matches Point 1
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct NumericFilter {
        /// Required. Column name in BigQuery that used as filters.
        #[prost(string, tag = "1")]
        pub name: ::prost::alloc::string::String,
        /// Optional. This MUST be specified for queries and must NOT be specified
        /// for database points.
        #[prost(enumeration = "numeric_filter::Operator", optional, tag = "5")]
        pub op: ::core::option::Option<i32>,
        /// The type of Value must be consistent for all datapoints with a given
        /// name.  This is verified at runtime.
        #[prost(oneof = "numeric_filter::Value", tags = "2, 3, 4")]
        pub value: ::core::option::Option<numeric_filter::Value>,
    }
    /// Nested message and enum types in `NumericFilter`.
    pub mod numeric_filter {
        /// Datapoints for which Operator is true relative to the query's Value
        /// field will be allowlisted.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum Operator {
            /// Unspecified operator.
            Unspecified = 0,
            /// Entities are eligible if their value is < the query's.
            Less = 1,
            /// Entities are eligible if their value is <= the query's.
            LessEqual = 2,
            /// Entities are eligible if their value is == the query's.
            Equal = 3,
            /// Entities are eligible if their value is >= the query's.
            GreaterEqual = 4,
            /// Entities are eligible if their value is > the query's.
            Greater = 5,
            /// Entities are eligible if their value is != the query's.
            NotEqual = 6,
        }
        impl Operator {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "OPERATOR_UNSPECIFIED",
                    Self::Less => "LESS",
                    Self::LessEqual => "LESS_EQUAL",
                    Self::Equal => "EQUAL",
                    Self::GreaterEqual => "GREATER_EQUAL",
                    Self::Greater => "GREATER",
                    Self::NotEqual => "NOT_EQUAL",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "OPERATOR_UNSPECIFIED" => Some(Self::Unspecified),
                    "LESS" => Some(Self::Less),
                    "LESS_EQUAL" => Some(Self::LessEqual),
                    "EQUAL" => Some(Self::Equal),
                    "GREATER_EQUAL" => Some(Self::GreaterEqual),
                    "GREATER" => Some(Self::Greater),
                    "NOT_EQUAL" => Some(Self::NotEqual),
                    _ => None,
                }
            }
        }
        /// The type of Value must be consistent for all datapoints with a given
        /// name.  This is verified at runtime.
        #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
        pub enum Value {
            /// int value type.
            #[prost(int64, tag = "2")]
            ValueInt(i64),
            /// float value type.
            #[prost(float, tag = "3")]
            ValueFloat(f32),
            /// double value type.
            #[prost(double, tag = "4")]
            ValueDouble(f64),
        }
    }
    /// Parameters that can be overrided in each query to tune query latency and
    /// recall.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Parameters {
        /// Optional. The number of neighbors to find via approximate search before
        /// exact reordering is performed; if set, this value must be >
        /// neighbor_count.
        #[prost(int32, tag = "1")]
        pub approximate_neighbor_candidates: i32,
        /// Optional. The fraction of the number of leaves to search, set at query
        /// time allows user to tune search performance. This value increase result
        /// in both search accuracy and latency increase. The value should be between
        /// 0.0 and 1.0.
        #[prost(double, tag = "2")]
        pub leaf_nodes_search_fraction: f64,
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Instance {
        /// Optional. The entity id whose similar entities should be searched for.
        /// If embedding is set, search will use embedding instead of
        /// entity_id.
        #[prost(string, tag = "1")]
        EntityId(::prost::alloc::string::String),
        /// Optional. The embedding vector that be used for similar search.
        #[prost(message, tag = "2")]
        Embedding(Embedding),
    }
}
/// The request message for
/// [FeatureOnlineStoreService.SearchNearestEntities][google.cloud.aiplatform.v1.FeatureOnlineStoreService.SearchNearestEntities].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchNearestEntitiesRequest {
    /// Required. FeatureView resource format
    /// `projects/{project}/locations/{location}/featureOnlineStores/{featureOnlineStore}/featureViews/{featureView}`
    #[prost(string, tag = "1")]
    pub feature_view: ::prost::alloc::string::String,
    /// Required. The query.
    #[prost(message, optional, tag = "2")]
    pub query: ::core::option::Option<NearestNeighborQuery>,
    /// Optional. If set to true, the full entities (including all vector values
    /// and metadata) of the nearest neighbors are returned; otherwise only entity
    /// id of the nearest neighbors will be returned. Note that returning full
    /// entities will significantly increase the latency and cost of the query.
    #[prost(bool, tag = "3")]
    pub return_full_entity: bool,
}
/// Nearest neighbors for one query.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NearestNeighbors {
    /// All its neighbors.
    #[prost(message, repeated, tag = "1")]
    pub neighbors: ::prost::alloc::vec::Vec<nearest_neighbors::Neighbor>,
}
/// Nested message and enum types in `NearestNeighbors`.
pub mod nearest_neighbors {
    /// A neighbor of the query vector.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Neighbor {
        /// The id of the similar entity.
        #[prost(string, tag = "1")]
        pub entity_id: ::prost::alloc::string::String,
        /// The distance between the neighbor and the query vector.
        #[prost(double, tag = "2")]
        pub distance: f64,
        /// The attributes of the neighbor, e.g. filters, crowding and metadata
        /// Note that full entities are returned only when "return_full_entity"
        /// is set to true. Otherwise, only the "entity_id" and "distance" fields
        /// are populated.
        #[prost(message, optional, tag = "3")]
        pub entity_key_values: ::core::option::Option<super::FetchFeatureValuesResponse>,
    }
}
/// Response message for
/// [FeatureOnlineStoreService.SearchNearestEntities][google.cloud.aiplatform.v1.FeatureOnlineStoreService.SearchNearestEntities]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchNearestEntitiesResponse {
    /// The nearest neighbors of the query entity.
    #[prost(message, optional, tag = "1")]
    pub nearest_neighbors: ::core::option::Option<NearestNeighbors>,
}
/// Format of the data in the Feature View.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum FeatureViewDataFormat {
    /// Not set. Will be treated as the KeyValue format.
    Unspecified = 0,
    /// Return response data in key-value format.
    KeyValue = 1,
    /// Return response data in proto Struct format.
    ProtoStruct = 2,
}
impl FeatureViewDataFormat {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "FEATURE_VIEW_DATA_FORMAT_UNSPECIFIED",
            Self::KeyValue => "KEY_VALUE",
            Self::ProtoStruct => "PROTO_STRUCT",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "FEATURE_VIEW_DATA_FORMAT_UNSPECIFIED" => Some(Self::Unspecified),
            "KEY_VALUE" => Some(Self::KeyValue),
            "PROTO_STRUCT" => Some(Self::ProtoStruct),
            _ => None,
        }
    }
}
/// Generated client implementations.
pub mod feature_online_store_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for fetching feature values from the online store.
    #[derive(Debug, Clone)]
    pub struct FeatureOnlineStoreServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl FeatureOnlineStoreServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> FeatureOnlineStoreServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> FeatureOnlineStoreServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            FeatureOnlineStoreServiceClient::new(
                InterceptedService::new(inner, interceptor),
            )
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Fetch feature values under a FeatureView.
        pub async fn fetch_feature_values(
            &mut self,
            request: impl tonic::IntoRequest<super::FetchFeatureValuesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::FetchFeatureValuesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreService/FetchFeatureValues",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreService",
                        "FetchFeatureValues",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Search the nearest entities under a FeatureView.
        /// Search only works for indexable feature view; if a feature view isn't
        /// indexable, returns Invalid argument response.
        pub async fn search_nearest_entities(
            &mut self,
            request: impl tonic::IntoRequest<super::SearchNearestEntitiesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::SearchNearestEntitiesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureOnlineStoreService/SearchNearestEntities",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureOnlineStoreService",
                        "SearchNearestEntities",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Vertex AI Feature Store provides a centralized repository for organizing,
/// storing, and serving ML features. The Featurestore is a top-level container
/// for your features and their values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Featurestore {
    /// Output only. Name of the Featurestore. Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. Timestamp when this Featurestore was created.
    #[prost(message, optional, tag = "3")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Featurestore was last updated.
    #[prost(message, optional, tag = "4")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. Used to perform consistent read-modify-write updates. If not set,
    /// a blind "overwrite" update happens.
    #[prost(string, tag = "5")]
    pub etag: ::prost::alloc::string::String,
    /// Optional. The labels with user-defined metadata to organize your
    /// Featurestore.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information on and examples of labels.
    /// No more than 64 user labels can be associated with one Featurestore(System
    /// labels are excluded)."
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable.
    #[prost(map = "string, string", tag = "6")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. Config for online storage resources. The field should not
    /// co-exist with the field of `OnlineStoreReplicationConfig`. If both of it
    /// and OnlineStoreReplicationConfig are unset, the feature store will not have
    /// an online store and cannot be used for online serving.
    #[prost(message, optional, tag = "7")]
    pub online_serving_config: ::core::option::Option<featurestore::OnlineServingConfig>,
    /// Output only. State of the featurestore.
    #[prost(enumeration = "featurestore::State", tag = "8")]
    pub state: i32,
    /// Optional. TTL in days for feature values that will be stored in online
    /// serving storage. The Feature Store online storage periodically removes
    /// obsolete feature values older than `online_storage_ttl_days` since the
    /// feature generation time. Note that `online_storage_ttl_days` should be less
    /// than or equal to `offline_storage_ttl_days` for each EntityType under a
    /// featurestore. If not set, default to 4000 days
    #[prost(int32, tag = "13")]
    pub online_storage_ttl_days: i32,
    /// Optional. Customer-managed encryption key spec for data storage. If set,
    /// both of the online and offline data storage will be secured by this key.
    #[prost(message, optional, tag = "10")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "14")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "15")]
    pub satisfies_pzi: bool,
}
/// Nested message and enum types in `Featurestore`.
pub mod featurestore {
    /// OnlineServingConfig specifies the details for provisioning online serving
    /// resources.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct OnlineServingConfig {
        /// The number of nodes for the online store. The number of nodes doesn't
        /// scale automatically, but you can manually update the number of
        /// nodes. If set to 0, the featurestore will not have an
        /// online store and cannot be used for online serving.
        #[prost(int32, tag = "2")]
        pub fixed_node_count: i32,
        /// Online serving scaling configuration.
        /// Only one of `fixed_node_count` and `scaling` can be set. Setting one will
        /// reset the other.
        #[prost(message, optional, tag = "4")]
        pub scaling: ::core::option::Option<online_serving_config::Scaling>,
    }
    /// Nested message and enum types in `OnlineServingConfig`.
    pub mod online_serving_config {
        /// Online serving scaling configuration. If min_node_count and
        /// max_node_count are set to the same value, the cluster will be configured
        /// with the fixed number of node (no auto-scaling).
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct Scaling {
            /// Required. The minimum number of nodes to scale down to. Must be greater
            /// than or equal to 1.
            #[prost(int32, tag = "1")]
            pub min_node_count: i32,
            /// The maximum number of nodes to scale up to. Must be greater than
            /// min_node_count, and less than or equal to 10 times of 'min_node_count'.
            #[prost(int32, tag = "2")]
            pub max_node_count: i32,
            /// Optional. The cpu utilization that the Autoscaler should be trying to
            /// achieve. This number is on a scale from 0 (no utilization) to 100
            /// (total utilization), and is limited between 10 and 80. When a cluster's
            /// CPU utilization exceeds the target that you have set, Bigtable
            /// immediately adds nodes to the cluster. When CPU utilization is
            /// substantially lower than the target, Bigtable removes nodes. If not set
            /// or set to 0, default to 50.
            #[prost(int32, tag = "3")]
            pub cpu_utilization_target: i32,
        }
    }
    /// Possible states a featurestore can have.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// Default value. This value is unused.
        Unspecified = 0,
        /// State when the featurestore configuration is not being updated and the
        /// fields reflect the current configuration of the featurestore. The
        /// featurestore is usable in this state.
        Stable = 1,
        /// The state of the featurestore configuration when it is being updated.
        /// During an update, the fields reflect either the original configuration
        /// or the updated configuration of the featurestore. For example,
        /// `online_serving_config.fixed_node_count` can take minutes to update.
        /// While the update is in progress, the featurestore is in the UPDATING
        /// state, and the value of `fixed_node_count` can be the original value or
        /// the updated value, depending on the progress of the operation. Until the
        /// update completes, the actual number of nodes can still be the original
        /// value of `fixed_node_count`. The featurestore is still usable in this
        /// state.
        Updating = 2,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STATE_UNSPECIFIED",
                Self::Stable => "STABLE",
                Self::Updating => "UPDATING",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "STABLE" => Some(Self::Stable),
                "UPDATING" => Some(Self::Updating),
                _ => None,
            }
        }
    }
}
/// Request message for
/// [FeaturestoreService.CreateFeaturestore][google.cloud.aiplatform.v1.FeaturestoreService.CreateFeaturestore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateFeaturestoreRequest {
    /// Required. The resource name of the Location to create Featurestores.
    /// Format:
    /// `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The Featurestore to create.
    #[prost(message, optional, tag = "2")]
    pub featurestore: ::core::option::Option<Featurestore>,
    /// Required. The ID to use for this Featurestore, which will become the final
    /// component of the Featurestore's resource name.
    ///
    /// This value may be up to 60 characters, and valid characters are
    /// `\[a-z0-9_\]`. The first character cannot be a number.
    ///
    /// The value must be unique within the project and location.
    #[prost(string, tag = "3")]
    pub featurestore_id: ::prost::alloc::string::String,
}
/// Request message for
/// [FeaturestoreService.GetFeaturestore][google.cloud.aiplatform.v1.FeaturestoreService.GetFeaturestore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetFeaturestoreRequest {
    /// Required. The name of the Featurestore resource.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [FeaturestoreService.ListFeaturestores][google.cloud.aiplatform.v1.FeaturestoreService.ListFeaturestores].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFeaturestoresRequest {
    /// Required. The resource name of the Location to list Featurestores.
    /// Format:
    /// `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the featurestores that match the filter expression. The following
    /// fields are supported:
    ///
    /// * `create_time`: Supports `=`, `!=`, `<`, `>`, `<=`, and `>=` comparisons.
    /// Values must be
    ///    in RFC 3339 format.
    /// * `update_time`: Supports `=`, `!=`, `<`, `>`, `<=`, and `>=` comparisons.
    /// Values must be
    ///    in RFC 3339 format.
    /// * `online_serving_config.fixed_node_count`: Supports `=`, `!=`, `<`, `>`,
    /// `<=`, and `>=` comparisons.
    /// * `labels`: Supports key-value equality and key presence.
    ///
    /// Examples:
    ///
    /// * `create_time > "2020-01-01" OR update_time > "2020-01-01"`
    ///     Featurestores created or updated after 2020-01-01.
    /// * `labels.env = "prod"`
    ///     Featurestores with label "env" set to "prod".
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The maximum number of Featurestores to return. The service may return fewer
    /// than this value. If unspecified, at most 100 Featurestores will be
    /// returned. The maximum value is 100; any value greater than 100 will be
    /// coerced to 100.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [FeaturestoreService.ListFeaturestores][google.cloud.aiplatform.v1.FeaturestoreService.ListFeaturestores]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [FeaturestoreService.ListFeaturestores][google.cloud.aiplatform.v1.FeaturestoreService.ListFeaturestores]
    /// must match the call that provided the page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    /// Supported Fields:
    ///
    ///    * `create_time`
    ///    * `update_time`
    ///    * `online_serving_config.fixed_node_count`
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "6")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [FeaturestoreService.ListFeaturestores][google.cloud.aiplatform.v1.FeaturestoreService.ListFeaturestores].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFeaturestoresResponse {
    /// The Featurestores matching the request.
    #[prost(message, repeated, tag = "1")]
    pub featurestores: ::prost::alloc::vec::Vec<Featurestore>,
    /// A token, which can be sent as
    /// [ListFeaturestoresRequest.page_token][google.cloud.aiplatform.v1.ListFeaturestoresRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [FeaturestoreService.UpdateFeaturestore][google.cloud.aiplatform.v1.FeaturestoreService.UpdateFeaturestore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateFeaturestoreRequest {
    /// Required. The Featurestore's `name` field is used to identify the
    /// Featurestore to be updated. Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}`
    #[prost(message, optional, tag = "1")]
    pub featurestore: ::core::option::Option<Featurestore>,
    /// Field mask is used to specify the fields to be overwritten in the
    /// Featurestore resource by the update.
    /// The fields specified in the update_mask are relative to the resource, not
    /// the full request. A field will be overwritten if it is in the mask. If the
    /// user does not provide a mask then only the non-empty fields present in the
    /// request will be overwritten. Set the update_mask to `*` to override all
    /// fields.
    ///
    /// Updatable fields:
    ///
    ///    * `labels`
    ///    * `online_serving_config.fixed_node_count`
    ///    * `online_serving_config.scaling`
    ///    * `online_storage_ttl_days`
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [FeaturestoreService.DeleteFeaturestore][google.cloud.aiplatform.v1.FeaturestoreService.DeleteFeaturestore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteFeaturestoreRequest {
    /// Required. The name of the Featurestore to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// If set to true, any EntityTypes and Features for this Featurestore will
    /// also be deleted. (Otherwise, the request will only work if the Featurestore
    /// has no EntityTypes.)
    #[prost(bool, tag = "2")]
    pub force: bool,
}
/// Request message for
/// [FeaturestoreService.ImportFeatureValues][google.cloud.aiplatform.v1.FeaturestoreService.ImportFeatureValues].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ImportFeatureValuesRequest {
    /// Required. The resource name of the EntityType grouping the Features for
    /// which values are being imported. Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entityType}`
    #[prost(string, tag = "1")]
    pub entity_type: ::prost::alloc::string::String,
    /// Source column that holds entity IDs. If not provided, entity IDs are
    /// extracted from the column named entity_id.
    #[prost(string, tag = "5")]
    pub entity_id_field: ::prost::alloc::string::String,
    /// Required. Specifications defining which Feature values to import from the
    /// entity. The request fails if no feature_specs are provided, and having
    /// multiple feature_specs for one Feature is not allowed.
    #[prost(message, repeated, tag = "8")]
    pub feature_specs: ::prost::alloc::vec::Vec<
        import_feature_values_request::FeatureSpec,
    >,
    /// If set, data will not be imported for online serving. This
    /// is typically used for backfilling, where Feature generation timestamps are
    /// not in the timestamp range needed for online serving.
    #[prost(bool, tag = "9")]
    pub disable_online_serving: bool,
    /// Specifies the number of workers that are used to write data to the
    /// Featurestore. Consider the online serving capacity that you require to
    /// achieve the desired import throughput without interfering with online
    /// serving. The value must be positive, and less than or equal to 100.
    /// If not set, defaults to using 1 worker. The low count ensures minimal
    /// impact on online serving performance.
    #[prost(int32, tag = "11")]
    pub worker_count: i32,
    /// If true, API doesn't start ingestion analysis pipeline.
    #[prost(bool, tag = "12")]
    pub disable_ingestion_analysis: bool,
    /// Details about the source data, including the location of the storage and
    /// the format.
    #[prost(oneof = "import_feature_values_request::Source", tags = "2, 3, 4")]
    pub source: ::core::option::Option<import_feature_values_request::Source>,
    /// Source of Feature timestamp for all Feature values of each entity.
    /// Timestamps must be millisecond-aligned.
    #[prost(oneof = "import_feature_values_request::FeatureTimeSource", tags = "6, 7")]
    pub feature_time_source: ::core::option::Option<
        import_feature_values_request::FeatureTimeSource,
    >,
}
/// Nested message and enum types in `ImportFeatureValuesRequest`.
pub mod import_feature_values_request {
    /// Defines the Feature value(s) to import.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct FeatureSpec {
        /// Required. ID of the Feature to import values of. This Feature must exist
        /// in the target EntityType, or the request will fail.
        #[prost(string, tag = "1")]
        pub id: ::prost::alloc::string::String,
        /// Source column to get the Feature values from. If not set, uses the column
        /// with the same name as the Feature ID.
        #[prost(string, tag = "2")]
        pub source_field: ::prost::alloc::string::String,
    }
    /// Details about the source data, including the location of the storage and
    /// the format.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Source {
        #[prost(message, tag = "2")]
        AvroSource(super::AvroSource),
        #[prost(message, tag = "3")]
        BigquerySource(super::BigQuerySource),
        #[prost(message, tag = "4")]
        CsvSource(super::CsvSource),
    }
    /// Source of Feature timestamp for all Feature values of each entity.
    /// Timestamps must be millisecond-aligned.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum FeatureTimeSource {
        /// Source column that holds the Feature timestamp for all Feature
        /// values in each entity.
        #[prost(string, tag = "6")]
        FeatureTimeField(::prost::alloc::string::String),
        /// Single Feature timestamp for all entities being imported. The
        /// timestamp must not have higher than millisecond precision.
        #[prost(message, tag = "7")]
        FeatureTime(::prost_types::Timestamp),
    }
}
/// Response message for
/// [FeaturestoreService.ImportFeatureValues][google.cloud.aiplatform.v1.FeaturestoreService.ImportFeatureValues].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ImportFeatureValuesResponse {
    /// Number of entities that have been imported by the operation.
    #[prost(int64, tag = "1")]
    pub imported_entity_count: i64,
    /// Number of Feature values that have been imported by the operation.
    #[prost(int64, tag = "2")]
    pub imported_feature_value_count: i64,
    /// The number of rows in input source that weren't imported due to either
    /// * Not having any featureValues.
    /// * Having a null entityId.
    /// * Having a null timestamp.
    /// * Not being parsable (applicable for CSV sources).
    #[prost(int64, tag = "6")]
    pub invalid_row_count: i64,
    /// The number rows that weren't ingested due to having feature timestamps
    /// outside the retention boundary.
    #[prost(int64, tag = "4")]
    pub timestamp_outside_retention_rows_count: i64,
}
/// Request message for
/// [FeaturestoreService.BatchReadFeatureValues][google.cloud.aiplatform.v1.FeaturestoreService.BatchReadFeatureValues].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchReadFeatureValuesRequest {
    /// Required. The resource name of the Featurestore from which to query Feature
    /// values. Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}`
    #[prost(string, tag = "1")]
    pub featurestore: ::prost::alloc::string::String,
    /// Required. Specifies output location and format.
    #[prost(message, optional, tag = "4")]
    pub destination: ::core::option::Option<FeatureValueDestination>,
    /// When not empty, the specified fields in the *_read_instances source will be
    /// joined as-is in the output, in addition to those fields from the
    /// Featurestore Entity.
    ///
    /// For BigQuery source, the type of the pass-through values will be
    /// automatically inferred. For CSV source, the pass-through values will be
    /// passed as opaque bytes.
    #[prost(message, repeated, tag = "8")]
    pub pass_through_fields: ::prost::alloc::vec::Vec<
        batch_read_feature_values_request::PassThroughField,
    >,
    /// Required. Specifies EntityType grouping Features to read values of and
    /// settings.
    #[prost(message, repeated, tag = "7")]
    pub entity_type_specs: ::prost::alloc::vec::Vec<
        batch_read_feature_values_request::EntityTypeSpec,
    >,
    /// Optional. Excludes Feature values with feature generation timestamp before
    /// this timestamp. If not set, retrieve oldest values kept in Feature Store.
    /// Timestamp, if present, must not have higher than millisecond precision.
    #[prost(message, optional, tag = "11")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    #[prost(oneof = "batch_read_feature_values_request::ReadOption", tags = "3, 5")]
    pub read_option: ::core::option::Option<
        batch_read_feature_values_request::ReadOption,
    >,
}
/// Nested message and enum types in `BatchReadFeatureValuesRequest`.
pub mod batch_read_feature_values_request {
    /// Describe pass-through fields in read_instance source.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct PassThroughField {
        /// Required. The name of the field in the CSV header or the name of the
        /// column in BigQuery table. The naming restriction is the same as
        /// [Feature.name][google.cloud.aiplatform.v1.Feature.name].
        #[prost(string, tag = "1")]
        pub field_name: ::prost::alloc::string::String,
    }
    /// Selects Features of an EntityType to read values of and specifies read
    /// settings.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct EntityTypeSpec {
        /// Required. ID of the EntityType to select Features. The EntityType id is
        /// the
        /// [entity_type_id][google.cloud.aiplatform.v1.CreateEntityTypeRequest.entity_type_id]
        /// specified during EntityType creation.
        #[prost(string, tag = "1")]
        pub entity_type_id: ::prost::alloc::string::String,
        /// Required. Selectors choosing which Feature values to read from the
        /// EntityType.
        #[prost(message, optional, tag = "2")]
        pub feature_selector: ::core::option::Option<super::FeatureSelector>,
        /// Per-Feature settings for the batch read.
        #[prost(message, repeated, tag = "3")]
        pub settings: ::prost::alloc::vec::Vec<super::DestinationFeatureSetting>,
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum ReadOption {
        /// Each read instance consists of exactly one read timestamp and one or more
        /// entity IDs identifying entities of the corresponding EntityTypes whose
        /// Features are requested.
        ///
        /// Each output instance contains Feature values of requested entities
        /// concatenated together as of the read time.
        ///
        /// An example read instance may be `foo_entity_id, bar_entity_id,
        /// 2020-01-01T10:00:00.123Z`.
        ///
        /// An example output instance may be `foo_entity_id, bar_entity_id,
        /// 2020-01-01T10:00:00.123Z, foo_entity_feature1_value,
        /// bar_entity_feature2_value`.
        ///
        /// Timestamp in each read instance must be millisecond-aligned.
        ///
        /// `csv_read_instances` are read instances stored in a plain-text CSV file.
        /// The header should be:
        ///      \[ENTITY_TYPE_ID1\], \[ENTITY_TYPE_ID2\], ..., timestamp
        ///
        /// The columns can be in any order.
        ///
        /// Values in the timestamp column must use the RFC 3339 format, e.g.
        /// `2012-07-30T10:43:17.123Z`.
        #[prost(message, tag = "3")]
        CsvReadInstances(super::CsvSource),
        /// Similar to csv_read_instances, but from BigQuery source.
        #[prost(message, tag = "5")]
        BigqueryReadInstances(super::BigQuerySource),
    }
}
/// Request message for
/// [FeaturestoreService.ExportFeatureValues][google.cloud.aiplatform.v1.FeaturestoreService.ExportFeatureValues].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExportFeatureValuesRequest {
    /// Required. The resource name of the EntityType from which to export Feature
    /// values. Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity_type}`
    #[prost(string, tag = "1")]
    pub entity_type: ::prost::alloc::string::String,
    /// Required. Specifies destination location and format.
    #[prost(message, optional, tag = "4")]
    pub destination: ::core::option::Option<FeatureValueDestination>,
    /// Required. Selects Features to export values of.
    #[prost(message, optional, tag = "5")]
    pub feature_selector: ::core::option::Option<FeatureSelector>,
    /// Per-Feature export settings.
    #[prost(message, repeated, tag = "6")]
    pub settings: ::prost::alloc::vec::Vec<DestinationFeatureSetting>,
    /// Required. The mode in which Feature values are exported.
    #[prost(oneof = "export_feature_values_request::Mode", tags = "3, 7")]
    pub mode: ::core::option::Option<export_feature_values_request::Mode>,
}
/// Nested message and enum types in `ExportFeatureValuesRequest`.
pub mod export_feature_values_request {
    /// Describes exporting the latest Feature values of all entities of the
    /// EntityType between \[start_time, snapshot_time\].
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct SnapshotExport {
        /// Exports Feature values as of this timestamp. If not set,
        /// retrieve values as of now. Timestamp, if present, must not have higher
        /// than millisecond precision.
        #[prost(message, optional, tag = "1")]
        pub snapshot_time: ::core::option::Option<::prost_types::Timestamp>,
        /// Excludes Feature values with feature generation timestamp before this
        /// timestamp. If not set, retrieve oldest values kept in Feature Store.
        /// Timestamp, if present, must not have higher than millisecond precision.
        #[prost(message, optional, tag = "2")]
        pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    }
    /// Describes exporting all historical Feature values of all entities of the
    /// EntityType between \[start_time, end_time\].
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct FullExport {
        /// Excludes Feature values with feature generation timestamp before this
        /// timestamp. If not set, retrieve oldest values kept in Feature Store.
        /// Timestamp, if present, must not have higher than millisecond precision.
        #[prost(message, optional, tag = "2")]
        pub start_time: ::core::option::Option<::prost_types::Timestamp>,
        /// Exports Feature values as of this timestamp. If not set,
        /// retrieve values as of now. Timestamp, if present, must not have higher
        /// than millisecond precision.
        #[prost(message, optional, tag = "1")]
        pub end_time: ::core::option::Option<::prost_types::Timestamp>,
    }
    /// Required. The mode in which Feature values are exported.
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum Mode {
        /// Exports the latest Feature values of all entities of the EntityType
        /// within a time range.
        #[prost(message, tag = "3")]
        SnapshotExport(SnapshotExport),
        /// Exports all historical values of all entities of the EntityType within a
        /// time range
        #[prost(message, tag = "7")]
        FullExport(FullExport),
    }
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DestinationFeatureSetting {
    /// Required. The ID of the Feature to apply the setting to.
    #[prost(string, tag = "1")]
    pub feature_id: ::prost::alloc::string::String,
    /// Specify the field name in the export destination. If not specified,
    /// Feature ID is used.
    #[prost(string, tag = "2")]
    pub destination_field: ::prost::alloc::string::String,
}
/// A destination location for Feature values and format.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FeatureValueDestination {
    #[prost(oneof = "feature_value_destination::Destination", tags = "1, 2, 3")]
    pub destination: ::core::option::Option<feature_value_destination::Destination>,
}
/// Nested message and enum types in `FeatureValueDestination`.
pub mod feature_value_destination {
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Destination {
        /// Output in BigQuery format.
        /// [BigQueryDestination.output_uri][google.cloud.aiplatform.v1.BigQueryDestination.output_uri]
        /// in
        /// [FeatureValueDestination.bigquery_destination][google.cloud.aiplatform.v1.FeatureValueDestination.bigquery_destination]
        /// must refer to a table.
        #[prost(message, tag = "1")]
        BigqueryDestination(super::BigQueryDestination),
        /// Output in TFRecord format.
        ///
        /// Below are the mapping from Feature value type
        /// in Featurestore to Feature value type in TFRecord:
        ///
        ///      Value type in Featurestore                 | Value type in TFRecord
        ///      DOUBLE, DOUBLE_ARRAY                       | FLOAT_LIST
        ///      INT64, INT64_ARRAY                         | INT64_LIST
        ///      STRING, STRING_ARRAY, BYTES                | BYTES_LIST
        ///      true -> byte_string("true"), false -> byte_string("false")
        ///      BOOL, BOOL_ARRAY (true, false)             | BYTES_LIST
        #[prost(message, tag = "2")]
        TfrecordDestination(super::TfRecordDestination),
        /// Output in CSV format. Array Feature value types are not allowed in CSV
        /// format.
        #[prost(message, tag = "3")]
        CsvDestination(super::CsvDestination),
    }
}
/// Response message for
/// [FeaturestoreService.ExportFeatureValues][google.cloud.aiplatform.v1.FeaturestoreService.ExportFeatureValues].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ExportFeatureValuesResponse {}
/// Response message for
/// [FeaturestoreService.BatchReadFeatureValues][google.cloud.aiplatform.v1.FeaturestoreService.BatchReadFeatureValues].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct BatchReadFeatureValuesResponse {}
/// Request message for
/// [FeaturestoreService.CreateEntityType][google.cloud.aiplatform.v1.FeaturestoreService.CreateEntityType].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateEntityTypeRequest {
    /// Required. The resource name of the Featurestore to create EntityTypes.
    /// Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The EntityType to create.
    #[prost(message, optional, tag = "2")]
    pub entity_type: ::core::option::Option<EntityType>,
    /// Required. The ID to use for the EntityType, which will become the final
    /// component of the EntityType's resource name.
    ///
    /// This value may be up to 60 characters, and valid characters are
    /// `\[a-z0-9_\]`. The first character cannot be a number.
    ///
    /// The value must be unique within a featurestore.
    #[prost(string, tag = "3")]
    pub entity_type_id: ::prost::alloc::string::String,
}
/// Request message for
/// [FeaturestoreService.GetEntityType][google.cloud.aiplatform.v1.FeaturestoreService.GetEntityType].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetEntityTypeRequest {
    /// Required. The name of the EntityType resource.
    /// Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity_type}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [FeaturestoreService.ListEntityTypes][google.cloud.aiplatform.v1.FeaturestoreService.ListEntityTypes].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListEntityTypesRequest {
    /// Required. The resource name of the Featurestore to list EntityTypes.
    /// Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the EntityTypes that match the filter expression. The following
    /// filters are supported:
    ///
    /// * `create_time`: Supports `=`, `!=`, `<`, `>`, `>=`, and `<=` comparisons.
    /// Values must be in RFC 3339 format.
    /// * `update_time`: Supports `=`, `!=`, `<`, `>`, `>=`, and `<=` comparisons.
    /// Values must be in RFC 3339 format.
    /// * `labels`: Supports key-value equality as well as key presence.
    ///
    /// Examples:
    ///
    /// * `create_time > \"2020-01-31T15:30:00.000000Z\" OR
    ///       update_time > \"2020-01-31T15:30:00.000000Z\"` --> EntityTypes created
    ///       or updated after 2020-01-31T15:30:00.000000Z.
    /// * `labels.active = yes AND labels.env = prod` --> EntityTypes having both
    ///      (active: yes) and (env: prod) labels.
    /// * `labels.env: *` --> Any EntityType which has a label with 'env' as the
    ///    key.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The maximum number of EntityTypes to return. The service may return fewer
    /// than this value. If unspecified, at most 1000 EntityTypes will be returned.
    /// The maximum value is 1000; any value greater than 1000 will be coerced to
    /// 1000.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [FeaturestoreService.ListEntityTypes][google.cloud.aiplatform.v1.FeaturestoreService.ListEntityTypes]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [FeaturestoreService.ListEntityTypes][google.cloud.aiplatform.v1.FeaturestoreService.ListEntityTypes]
    /// must match the call that provided the page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    ///
    /// Supported fields:
    ///
    ///    * `entity_type_id`
    ///    * `create_time`
    ///    * `update_time`
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "6")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [FeaturestoreService.ListEntityTypes][google.cloud.aiplatform.v1.FeaturestoreService.ListEntityTypes].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListEntityTypesResponse {
    /// The EntityTypes matching the request.
    #[prost(message, repeated, tag = "1")]
    pub entity_types: ::prost::alloc::vec::Vec<EntityType>,
    /// A token, which can be sent as
    /// [ListEntityTypesRequest.page_token][google.cloud.aiplatform.v1.ListEntityTypesRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [FeaturestoreService.UpdateEntityType][google.cloud.aiplatform.v1.FeaturestoreService.UpdateEntityType].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateEntityTypeRequest {
    /// Required. The EntityType's `name` field is used to identify the EntityType
    /// to be updated. Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity_type}`
    #[prost(message, optional, tag = "1")]
    pub entity_type: ::core::option::Option<EntityType>,
    /// Field mask is used to specify the fields to be overwritten in the
    /// EntityType resource by the update.
    /// The fields specified in the update_mask are relative to the resource, not
    /// the full request. A field will be overwritten if it is in the mask. If the
    /// user does not provide a mask then only the non-empty fields present in the
    /// request will be overwritten. Set the update_mask to `*` to override all
    /// fields.
    ///
    /// Updatable fields:
    ///
    ///    * `description`
    ///    * `labels`
    ///    * `monitoring_config.snapshot_analysis.disabled`
    ///    * `monitoring_config.snapshot_analysis.monitoring_interval_days`
    ///    * `monitoring_config.snapshot_analysis.staleness_days`
    ///    * `monitoring_config.import_features_analysis.state`
    ///    * `monitoring_config.import_features_analysis.anomaly_detection_baseline`
    ///    * `monitoring_config.numerical_threshold_config.value`
    ///    * `monitoring_config.categorical_threshold_config.value`
    ///    * `offline_storage_ttl_days`
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [FeaturestoreService.DeleteEntityType][google.cloud.aiplatform.v1.FeaturestoreService.DeleteEntityType].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteEntityTypeRequest {
    /// Required. The name of the EntityType to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity_type}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// If set to true, any Features for this EntityType will also be deleted.
    /// (Otherwise, the request will only work if the EntityType has no Features.)
    #[prost(bool, tag = "2")]
    pub force: bool,
}
/// Request message for
/// [FeaturestoreService.CreateFeature][google.cloud.aiplatform.v1.FeaturestoreService.CreateFeature].
/// Request message for
/// [FeatureRegistryService.CreateFeature][google.cloud.aiplatform.v1.FeatureRegistryService.CreateFeature].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateFeatureRequest {
    /// Required. The resource name of the EntityType or FeatureGroup to create a
    /// Feature. Format for entity_type as parent:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity_type}`
    /// Format for feature_group as parent:
    /// `projects/{project}/locations/{location}/featureGroups/{feature_group}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The Feature to create.
    #[prost(message, optional, tag = "2")]
    pub feature: ::core::option::Option<Feature>,
    /// Required. The ID to use for the Feature, which will become the final
    /// component of the Feature's resource name.
    ///
    /// This value may be up to 128 characters, and valid characters are
    /// `\[a-z0-9_\]`. The first character cannot be a number.
    ///
    /// The value must be unique within an EntityType/FeatureGroup.
    #[prost(string, tag = "3")]
    pub feature_id: ::prost::alloc::string::String,
}
/// Request message for
/// [FeaturestoreService.BatchCreateFeatures][google.cloud.aiplatform.v1.FeaturestoreService.BatchCreateFeatures].
/// Request message for
/// [FeatureRegistryService.BatchCreateFeatures][google.cloud.aiplatform.v1.FeatureRegistryService.BatchCreateFeatures].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchCreateFeaturesRequest {
    /// Required. The resource name of the EntityType/FeatureGroup to create the
    /// batch of Features under. Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity_type}`
    /// `projects/{project}/locations/{location}/featureGroups/{feature_group}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The request message specifying the Features to create. All
    /// Features must be created under the same parent EntityType / FeatureGroup.
    /// The `parent` field in each child request message can be omitted. If
    /// `parent` is set in a child request, then the value must match the `parent`
    /// value in this request message.
    #[prost(message, repeated, tag = "2")]
    pub requests: ::prost::alloc::vec::Vec<CreateFeatureRequest>,
}
/// Response message for
/// [FeaturestoreService.BatchCreateFeatures][google.cloud.aiplatform.v1.FeaturestoreService.BatchCreateFeatures].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchCreateFeaturesResponse {
    /// The Features created.
    #[prost(message, repeated, tag = "1")]
    pub features: ::prost::alloc::vec::Vec<Feature>,
}
/// Request message for
/// [FeaturestoreService.GetFeature][google.cloud.aiplatform.v1.FeaturestoreService.GetFeature].
/// Request message for
/// [FeatureRegistryService.GetFeature][google.cloud.aiplatform.v1.FeatureRegistryService.GetFeature].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetFeatureRequest {
    /// Required. The name of the Feature resource.
    /// Format for entity_type as parent:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity_type}`
    /// Format for feature_group as parent:
    /// `projects/{project}/locations/{location}/featureGroups/{feature_group}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [FeaturestoreService.ListFeatures][google.cloud.aiplatform.v1.FeaturestoreService.ListFeatures].
/// Request message for
/// [FeatureRegistryService.ListFeatures][google.cloud.aiplatform.v1.FeatureRegistryService.ListFeatures].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFeaturesRequest {
    /// Required. The resource name of the Location to list Features.
    /// Format for entity_type as parent:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity_type}`
    /// Format for feature_group as parent:
    /// `projects/{project}/locations/{location}/featureGroups/{feature_group}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the Features that match the filter expression. The following
    /// filters are supported:
    ///
    /// * `value_type`: Supports = and != comparisons.
    /// * `create_time`: Supports =, !=, <, >, >=, and <= comparisons. Values must
    /// be in RFC 3339 format.
    /// * `update_time`: Supports =, !=, <, >, >=, and <= comparisons. Values must
    /// be in RFC 3339 format.
    /// * `labels`: Supports key-value equality as well as key presence.
    ///
    /// Examples:
    ///
    /// * `value_type = DOUBLE` --> Features whose type is DOUBLE.
    /// * `create_time > \"2020-01-31T15:30:00.000000Z\" OR
    ///       update_time > \"2020-01-31T15:30:00.000000Z\"` --> EntityTypes created
    ///       or updated after 2020-01-31T15:30:00.000000Z.
    /// * `labels.active = yes AND labels.env = prod` --> Features having both
    ///      (active: yes) and (env: prod) labels.
    /// * `labels.env: *` --> Any Feature which has a label with 'env' as the
    ///    key.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The maximum number of Features to return. The service may return fewer
    /// than this value. If unspecified, at most 1000 Features will be returned.
    /// The maximum value is 1000; any value greater than 1000 will be coerced to
    /// 1000.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [FeaturestoreService.ListFeatures][google.cloud.aiplatform.v1.FeaturestoreService.ListFeatures]
    /// call or
    /// [FeatureRegistryService.ListFeatures][google.cloud.aiplatform.v1.FeatureRegistryService.ListFeatures]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [FeaturestoreService.ListFeatures][google.cloud.aiplatform.v1.FeaturestoreService.ListFeatures]
    /// or
    /// [FeatureRegistryService.ListFeatures][google.cloud.aiplatform.v1.FeatureRegistryService.ListFeatures]
    /// must match the call that provided the page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    /// Supported fields:
    ///
    ///    * `feature_id`
    ///    * `value_type` (Not supported for FeatureRegistry Feature)
    ///    * `create_time`
    ///    * `update_time`
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "6")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// Only applicable for Vertex AI Feature Store (Legacy).
    /// If set, return the most recent
    /// [ListFeaturesRequest.latest_stats_count][google.cloud.aiplatform.v1.ListFeaturesRequest.latest_stats_count]
    /// of stats for each Feature in response. Valid value is \[0, 10\]. If number of
    /// stats exists <
    /// [ListFeaturesRequest.latest_stats_count][google.cloud.aiplatform.v1.ListFeaturesRequest.latest_stats_count],
    /// return all existing stats.
    #[prost(int32, tag = "7")]
    pub latest_stats_count: i32,
}
/// Response message for
/// [FeaturestoreService.ListFeatures][google.cloud.aiplatform.v1.FeaturestoreService.ListFeatures].
/// Response message for
/// [FeatureRegistryService.ListFeatures][google.cloud.aiplatform.v1.FeatureRegistryService.ListFeatures].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFeaturesResponse {
    /// The Features matching the request.
    #[prost(message, repeated, tag = "1")]
    pub features: ::prost::alloc::vec::Vec<Feature>,
    /// A token, which can be sent as
    /// [ListFeaturesRequest.page_token][google.cloud.aiplatform.v1.ListFeaturesRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [FeaturestoreService.SearchFeatures][google.cloud.aiplatform.v1.FeaturestoreService.SearchFeatures].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchFeaturesRequest {
    /// Required. The resource name of the Location to search Features.
    /// Format:
    /// `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub location: ::prost::alloc::string::String,
    /// Query string that is a conjunction of field-restricted queries and/or
    /// field-restricted filters.  Field-restricted queries and filters can be
    /// combined using `AND` to form a conjunction.
    ///
    /// A field query is in the form FIELD:QUERY. This implicitly checks if QUERY
    /// exists as a substring within Feature's FIELD. The QUERY
    /// and the FIELD are converted to a sequence of words (i.e. tokens) for
    /// comparison. This is done by:
    ///
    ///    * Removing leading/trailing whitespace and tokenizing the search value.
    ///    Characters that are not one of alphanumeric `\[a-zA-Z0-9\]`, underscore
    ///    `_`, or asterisk `*` are treated as delimiters for tokens. `*` is treated
    ///    as a wildcard that matches characters within a token.
    ///    * Ignoring case.
    ///    * Prepending an asterisk to the first and appending an asterisk to the
    ///    last token in QUERY.
    ///
    /// A QUERY must be either a singular token or a phrase. A phrase is one or
    /// multiple words enclosed in double quotation marks ("). With phrases, the
    /// order of the words is important. Words in the phrase must be matching in
    /// order and consecutively.
    ///
    /// Supported FIELDs for field-restricted queries:
    ///
    /// * `feature_id`
    /// * `description`
    /// * `entity_type_id`
    ///
    /// Examples:
    ///
    /// * `feature_id: foo` --> Matches a Feature with ID containing the substring
    /// `foo` (eg. `foo`, `foofeature`, `barfoo`).
    /// * `feature_id: foo*feature` --> Matches a Feature with ID containing the
    /// substring `foo*feature` (eg. `foobarfeature`).
    /// * `feature_id: foo AND description: bar` --> Matches a Feature with ID
    /// containing the substring `foo` and description containing the substring
    /// `bar`.
    ///
    ///
    /// Besides field queries, the following exact-match filters are
    /// supported. The exact-match filters do not support wildcards. Unlike
    /// field-restricted queries, exact-match filters are case-sensitive.
    ///
    /// * `feature_id`: Supports = comparisons.
    /// * `description`: Supports = comparisons. Multi-token filters should be
    /// enclosed in quotes.
    /// * `entity_type_id`: Supports = comparisons.
    /// * `value_type`: Supports = and != comparisons.
    /// * `labels`: Supports key-value equality as well as key presence.
    /// * `featurestore_id`: Supports = comparisons.
    ///
    /// Examples:
    ///
    /// * `description = "foo bar"` --> Any Feature with description exactly equal
    /// to `foo bar`
    /// * `value_type = DOUBLE` --> Features whose type is DOUBLE.
    /// * `labels.active = yes AND labels.env = prod` --> Features having both
    ///      (active: yes) and (env: prod) labels.
    /// * `labels.env: *` --> Any Feature which has a label with `env` as the
    ///    key.
    #[prost(string, tag = "3")]
    pub query: ::prost::alloc::string::String,
    /// The maximum number of Features to return. The service may return fewer
    /// than this value. If unspecified, at most 100 Features will be returned.
    /// The maximum value is 100; any value greater than 100 will be coerced to
    /// 100.
    #[prost(int32, tag = "4")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [FeaturestoreService.SearchFeatures][google.cloud.aiplatform.v1.FeaturestoreService.SearchFeatures]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [FeaturestoreService.SearchFeatures][google.cloud.aiplatform.v1.FeaturestoreService.SearchFeatures],
    /// except `page_size`, must match the call that provided the page token.
    #[prost(string, tag = "5")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response message for
/// [FeaturestoreService.SearchFeatures][google.cloud.aiplatform.v1.FeaturestoreService.SearchFeatures].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchFeaturesResponse {
    /// The Features matching the request.
    ///
    /// Fields returned:
    ///
    ///   * `name`
    ///   * `description`
    ///   * `labels`
    ///   * `create_time`
    ///   * `update_time`
    #[prost(message, repeated, tag = "1")]
    pub features: ::prost::alloc::vec::Vec<Feature>,
    /// A token, which can be sent as
    /// [SearchFeaturesRequest.page_token][google.cloud.aiplatform.v1.SearchFeaturesRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [FeaturestoreService.UpdateFeature][google.cloud.aiplatform.v1.FeaturestoreService.UpdateFeature].
/// Request message for
/// [FeatureRegistryService.UpdateFeature][google.cloud.aiplatform.v1.FeatureRegistryService.UpdateFeature].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateFeatureRequest {
    /// Required. The Feature's `name` field is used to identify the Feature to be
    /// updated.
    /// Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity_type}/features/{feature}`
    /// `projects/{project}/locations/{location}/featureGroups/{feature_group}/features/{feature}`
    #[prost(message, optional, tag = "1")]
    pub feature: ::core::option::Option<Feature>,
    /// Field mask is used to specify the fields to be overwritten in the
    /// Features resource by the update.
    /// The fields specified in the update_mask are relative to the resource, not
    /// the full request. A field will be overwritten if it is in the mask. If the
    /// user does not provide a mask then only the non-empty fields present in the
    /// request will be overwritten. Set the update_mask to `*` to override all
    /// fields.
    ///
    /// Updatable fields:
    ///
    ///    * `description`
    ///    * `labels`
    ///    * `disable_monitoring` (Not supported for FeatureRegistryService Feature)
    ///    * `point_of_contact` (Not supported for FeaturestoreService FeatureStore)
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [FeaturestoreService.DeleteFeature][google.cloud.aiplatform.v1.FeaturestoreService.DeleteFeature].
/// Request message for
/// [FeatureRegistryService.DeleteFeature][google.cloud.aiplatform.v1.FeatureRegistryService.DeleteFeature].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteFeatureRequest {
    /// Required. The name of the Features to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entity_type}/features/{feature}`
    /// `projects/{project}/locations/{location}/featureGroups/{feature_group}/features/{feature}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Details of operations that perform create Featurestore.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateFeaturestoreOperationMetadata {
    /// Operation metadata for Featurestore.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that perform update Featurestore.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateFeaturestoreOperationMetadata {
    /// Operation metadata for Featurestore.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that perform import Feature values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ImportFeatureValuesOperationMetadata {
    /// Operation metadata for Featurestore import Feature values.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// Number of entities that have been imported by the operation.
    #[prost(int64, tag = "2")]
    pub imported_entity_count: i64,
    /// Number of Feature values that have been imported by the operation.
    #[prost(int64, tag = "3")]
    pub imported_feature_value_count: i64,
    /// The source URI from where Feature values are imported.
    #[prost(string, repeated, tag = "4")]
    pub source_uris: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// The number of rows in input source that weren't imported due to either
    /// * Not having any featureValues.
    /// * Having a null entityId.
    /// * Having a null timestamp.
    /// * Not being parsable (applicable for CSV sources).
    #[prost(int64, tag = "6")]
    pub invalid_row_count: i64,
    /// The number rows that weren't ingested due to having timestamps outside the
    /// retention boundary.
    #[prost(int64, tag = "7")]
    pub timestamp_outside_retention_rows_count: i64,
    /// List of ImportFeatureValues operations running under a single EntityType
    /// that are blocking this operation.
    #[prost(int64, repeated, tag = "8")]
    pub blocking_operation_ids: ::prost::alloc::vec::Vec<i64>,
}
/// Details of operations that exports Features values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExportFeatureValuesOperationMetadata {
    /// Operation metadata for Featurestore export Feature values.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that batch reads Feature values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchReadFeatureValuesOperationMetadata {
    /// Operation metadata for Featurestore batch read Features values.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that delete Feature values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteFeatureValuesOperationMetadata {
    /// Operation metadata for Featurestore delete Features values.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that perform create EntityType.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateEntityTypeOperationMetadata {
    /// Operation metadata for EntityType.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that perform create Feature.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateFeatureOperationMetadata {
    /// Operation metadata for Feature.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that perform batch create Features.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchCreateFeaturesOperationMetadata {
    /// Operation metadata for Feature.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [FeaturestoreService.DeleteFeatureValues][google.cloud.aiplatform.v1.FeaturestoreService.DeleteFeatureValues].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteFeatureValuesRequest {
    /// Required. The resource name of the EntityType grouping the Features for
    /// which values are being deleted from. Format:
    /// `projects/{project}/locations/{location}/featurestores/{featurestore}/entityTypes/{entityType}`
    #[prost(string, tag = "1")]
    pub entity_type: ::prost::alloc::string::String,
    /// Defines options to select feature values to be deleted.
    #[prost(oneof = "delete_feature_values_request::DeleteOption", tags = "2, 3")]
    pub delete_option: ::core::option::Option<
        delete_feature_values_request::DeleteOption,
    >,
}
/// Nested message and enum types in `DeleteFeatureValuesRequest`.
pub mod delete_feature_values_request {
    /// Message to select entity.
    /// If an entity id is selected, all the feature values corresponding to the
    /// entity id will be deleted, including the entityId.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct SelectEntity {
        /// Required. Selectors choosing feature values of which entity id to be
        /// deleted from the EntityType.
        #[prost(message, optional, tag = "1")]
        pub entity_id_selector: ::core::option::Option<super::EntityIdSelector>,
    }
    /// Message to select time range and feature.
    /// Values of the selected feature generated within an inclusive time range
    /// will be deleted. Using this option permanently deletes the feature values
    /// from the specified feature IDs within the specified time range.
    /// This might include data from the online storage. If you want to retain
    /// any deleted historical data in the online storage, you must re-ingest it.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct SelectTimeRangeAndFeature {
        /// Required. Select feature generated within a half-inclusive time range.
        /// The time range is lower inclusive and upper exclusive.
        #[prost(message, optional, tag = "1")]
        pub time_range: ::core::option::Option<
            super::super::super::super::r#type::Interval,
        >,
        /// Required. Selectors choosing which feature values to be deleted from the
        /// EntityType.
        #[prost(message, optional, tag = "2")]
        pub feature_selector: ::core::option::Option<super::FeatureSelector>,
        /// If set, data will not be deleted from online storage.
        /// When time range is older than the data in online storage, setting this to
        /// be true will make the deletion have no impact on online serving.
        #[prost(bool, tag = "3")]
        pub skip_online_storage_delete: bool,
    }
    /// Defines options to select feature values to be deleted.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum DeleteOption {
        /// Select feature values to be deleted by specifying entities.
        #[prost(message, tag = "2")]
        SelectEntity(SelectEntity),
        /// Select feature values to be deleted by specifying time range and
        /// features.
        #[prost(message, tag = "3")]
        SelectTimeRangeAndFeature(SelectTimeRangeAndFeature),
    }
}
/// Response message for
/// [FeaturestoreService.DeleteFeatureValues][google.cloud.aiplatform.v1.FeaturestoreService.DeleteFeatureValues].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct DeleteFeatureValuesResponse {
    /// Response based on which delete option is specified in the
    /// request
    #[prost(oneof = "delete_feature_values_response::Response", tags = "1, 2")]
    pub response: ::core::option::Option<delete_feature_values_response::Response>,
}
/// Nested message and enum types in `DeleteFeatureValuesResponse`.
pub mod delete_feature_values_response {
    /// Response message if the request uses the SelectEntity option.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct SelectEntity {
        /// The count of deleted entity rows in the offline storage.
        /// Each row corresponds to the combination of an entity ID and a timestamp.
        /// One entity ID can have multiple rows in the offline storage.
        #[prost(int64, tag = "1")]
        pub offline_storage_deleted_entity_row_count: i64,
        /// The count of deleted entities in the online storage.
        /// Each entity ID corresponds to one entity.
        #[prost(int64, tag = "2")]
        pub online_storage_deleted_entity_count: i64,
    }
    /// Response message if the request uses the SelectTimeRangeAndFeature option.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct SelectTimeRangeAndFeature {
        /// The count of the features or columns impacted.
        /// This is the same as the feature count in the request.
        #[prost(int64, tag = "1")]
        pub impacted_feature_count: i64,
        /// The count of modified entity rows in the offline storage.
        /// Each row corresponds to the combination of an entity ID and a timestamp.
        /// One entity ID can have multiple rows in the offline storage.
        /// Within each row, only the features specified in the request are
        /// deleted.
        #[prost(int64, tag = "2")]
        pub offline_storage_modified_entity_row_count: i64,
        /// The count of modified entities in the online storage.
        /// Each entity ID corresponds to one entity.
        /// Within each entity, only the features specified in the request are
        /// deleted.
        #[prost(int64, tag = "3")]
        pub online_storage_modified_entity_count: i64,
    }
    /// Response based on which delete option is specified in the
    /// request
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum Response {
        /// Response for request specifying the entities to delete
        #[prost(message, tag = "1")]
        SelectEntity(SelectEntity),
        /// Response for request specifying time range and feature
        #[prost(message, tag = "2")]
        SelectTimeRangeAndFeature(SelectTimeRangeAndFeature),
    }
}
/// Selector for entityId. Getting ids from the given source.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EntityIdSelector {
    /// Source column that holds entity IDs. If not provided, entity IDs are
    /// extracted from the column named entity_id.
    #[prost(string, tag = "5")]
    pub entity_id_field: ::prost::alloc::string::String,
    /// Details about the source data, including the location of the storage and
    /// the format.
    #[prost(oneof = "entity_id_selector::EntityIdsSource", tags = "3")]
    pub entity_ids_source: ::core::option::Option<entity_id_selector::EntityIdsSource>,
}
/// Nested message and enum types in `EntityIdSelector`.
pub mod entity_id_selector {
    /// Details about the source data, including the location of the storage and
    /// the format.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum EntityIdsSource {
        /// Source of Csv
        #[prost(message, tag = "3")]
        CsvSource(super::CsvSource),
    }
}
/// Generated client implementations.
pub mod featurestore_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// The service that handles CRUD and List for resources for Featurestore.
    #[derive(Debug, Clone)]
    pub struct FeaturestoreServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl FeaturestoreServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> FeaturestoreServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> FeaturestoreServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            FeaturestoreServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a new Featurestore in a given project and location.
        pub async fn create_featurestore(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateFeaturestoreRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/CreateFeaturestore",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "CreateFeaturestore",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets details of a single Featurestore.
        pub async fn get_featurestore(
            &mut self,
            request: impl tonic::IntoRequest<super::GetFeaturestoreRequest>,
        ) -> std::result::Result<tonic::Response<super::Featurestore>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/GetFeaturestore",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "GetFeaturestore",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Featurestores in a given project and location.
        pub async fn list_featurestores(
            &mut self,
            request: impl tonic::IntoRequest<super::ListFeaturestoresRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListFeaturestoresResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/ListFeaturestores",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "ListFeaturestores",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates the parameters of a single Featurestore.
        pub async fn update_featurestore(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateFeaturestoreRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/UpdateFeaturestore",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "UpdateFeaturestore",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a single Featurestore. The Featurestore must not contain any
        /// EntityTypes or `force` must be set to true for the request to succeed.
        pub async fn delete_featurestore(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteFeaturestoreRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/DeleteFeaturestore",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "DeleteFeaturestore",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a new EntityType in a given Featurestore.
        pub async fn create_entity_type(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateEntityTypeRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/CreateEntityType",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "CreateEntityType",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets details of a single EntityType.
        pub async fn get_entity_type(
            &mut self,
            request: impl tonic::IntoRequest<super::GetEntityTypeRequest>,
        ) -> std::result::Result<tonic::Response<super::EntityType>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/GetEntityType",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "GetEntityType",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists EntityTypes in a given Featurestore.
        pub async fn list_entity_types(
            &mut self,
            request: impl tonic::IntoRequest<super::ListEntityTypesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListEntityTypesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/ListEntityTypes",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "ListEntityTypes",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates the parameters of a single EntityType.
        pub async fn update_entity_type(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateEntityTypeRequest>,
        ) -> std::result::Result<tonic::Response<super::EntityType>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/UpdateEntityType",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "UpdateEntityType",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a single EntityType. The EntityType must not have any Features
        /// or `force` must be set to true for the request to succeed.
        pub async fn delete_entity_type(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteEntityTypeRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/DeleteEntityType",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "DeleteEntityType",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a new Feature in a given EntityType.
        pub async fn create_feature(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateFeatureRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/CreateFeature",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "CreateFeature",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a batch of Features in a given EntityType.
        pub async fn batch_create_features(
            &mut self,
            request: impl tonic::IntoRequest<super::BatchCreateFeaturesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/BatchCreateFeatures",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "BatchCreateFeatures",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets details of a single Feature.
        pub async fn get_feature(
            &mut self,
            request: impl tonic::IntoRequest<super::GetFeatureRequest>,
        ) -> std::result::Result<tonic::Response<super::Feature>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/GetFeature",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "GetFeature",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Features in a given EntityType.
        pub async fn list_features(
            &mut self,
            request: impl tonic::IntoRequest<super::ListFeaturesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListFeaturesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/ListFeatures",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "ListFeatures",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates the parameters of a single Feature.
        pub async fn update_feature(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateFeatureRequest>,
        ) -> std::result::Result<tonic::Response<super::Feature>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/UpdateFeature",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "UpdateFeature",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a single Feature.
        pub async fn delete_feature(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteFeatureRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/DeleteFeature",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "DeleteFeature",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Imports Feature values into the Featurestore from a source storage.
        ///
        /// The progress of the import is tracked by the returned operation. The
        /// imported features are guaranteed to be visible to subsequent read
        /// operations after the operation is marked as successfully done.
        ///
        /// If an import operation fails, the Feature values returned from
        /// reads and exports may be inconsistent. If consistency is
        /// required, the caller must retry the same import request again and wait till
        /// the new operation returned is marked as successfully done.
        ///
        /// There are also scenarios where the caller can cause inconsistency.
        ///
        ///  - Source data for import contains multiple distinct Feature values for
        ///    the same entity ID and timestamp.
        ///  - Source is modified during an import. This includes adding, updating, or
        ///  removing source data and/or metadata. Examples of updating metadata
        ///  include but are not limited to changing storage location, storage class,
        ///  or retention policy.
        ///  - Online serving cluster is under-provisioned.
        pub async fn import_feature_values(
            &mut self,
            request: impl tonic::IntoRequest<super::ImportFeatureValuesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/ImportFeatureValues",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "ImportFeatureValues",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Batch reads Feature values from a Featurestore.
        ///
        /// This API enables batch reading Feature values, where each read
        /// instance in the batch may read Feature values of entities from one or
        /// more EntityTypes. Point-in-time correctness is guaranteed for Feature
        /// values of each read instance as of each instance's read timestamp.
        pub async fn batch_read_feature_values(
            &mut self,
            request: impl tonic::IntoRequest<super::BatchReadFeatureValuesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/BatchReadFeatureValues",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "BatchReadFeatureValues",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Exports Feature values from all the entities of a target EntityType.
        pub async fn export_feature_values(
            &mut self,
            request: impl tonic::IntoRequest<super::ExportFeatureValuesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/ExportFeatureValues",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "ExportFeatureValues",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Delete Feature values from Featurestore.
        ///
        /// The progress of the deletion is tracked by the returned operation. The
        /// deleted feature values are guaranteed to be invisible to subsequent read
        /// operations after the operation is marked as successfully done.
        ///
        /// If a delete feature values operation fails, the feature values
        /// returned from reads and exports may be inconsistent. If consistency is
        /// required, the caller must retry the same delete request again and wait till
        /// the new operation returned is marked as successfully done.
        pub async fn delete_feature_values(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteFeatureValuesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/DeleteFeatureValues",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "DeleteFeatureValues",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Searches Features matching a query in a given project.
        pub async fn search_features(
            &mut self,
            request: impl tonic::IntoRequest<super::SearchFeaturesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::SearchFeaturesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeaturestoreService/SearchFeatures",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeaturestoreService",
                        "SearchFeatures",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Request message for
/// [FeatureRegistryService.CreateFeatureGroup][google.cloud.aiplatform.v1.FeatureRegistryService.CreateFeatureGroup].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateFeatureGroupRequest {
    /// Required. The resource name of the Location to create FeatureGroups.
    /// Format:
    /// `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The FeatureGroup to create.
    #[prost(message, optional, tag = "2")]
    pub feature_group: ::core::option::Option<FeatureGroup>,
    /// Required. The ID to use for this FeatureGroup, which will become the final
    /// component of the FeatureGroup's resource name.
    ///
    /// This value may be up to 128 characters, and valid characters are
    /// `\[a-z0-9_\]`. The first character cannot be a number.
    ///
    /// The value must be unique within the project and location.
    #[prost(string, tag = "3")]
    pub feature_group_id: ::prost::alloc::string::String,
}
/// Request message for
/// [FeatureRegistryService.GetFeatureGroup][google.cloud.aiplatform.v1.FeatureRegistryService.GetFeatureGroup].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetFeatureGroupRequest {
    /// Required. The name of the FeatureGroup resource.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [FeatureRegistryService.ListFeatureGroups][google.cloud.aiplatform.v1.FeatureRegistryService.ListFeatureGroups].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFeatureGroupsRequest {
    /// Required. The resource name of the Location to list FeatureGroups.
    /// Format:
    /// `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the FeatureGroups that match the filter expression. The
    /// following fields are supported:
    ///
    /// * `create_time`: Supports `=`, `!=`, `<`, `>`, `<=`, and `>=` comparisons.
    /// Values must be
    ///    in RFC 3339 format.
    /// * `update_time`: Supports `=`, `!=`, `<`, `>`, `<=`, and `>=` comparisons.
    /// Values must be
    ///    in RFC 3339 format.
    /// * `labels`: Supports key-value equality and key presence.
    ///
    /// Examples:
    ///
    /// * `create_time > "2020-01-01" OR update_time > "2020-01-01"`
    ///     FeatureGroups created or updated after 2020-01-01.
    /// * `labels.env = "prod"`
    ///     FeatureGroups with label "env" set to "prod".
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The maximum number of FeatureGroups to return. The service may return
    /// fewer than this value. If unspecified, at most 100 FeatureGroups will
    /// be returned. The maximum value is 100; any value greater than 100 will be
    /// coerced to 100.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [FeatureRegistryService.ListFeatureGroups][google.cloud.aiplatform.v1.FeatureRegistryService.ListFeatureGroups]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [FeatureRegistryService.ListFeatureGroups][google.cloud.aiplatform.v1.FeatureRegistryService.ListFeatureGroups]
    /// must match the call that provided the page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    /// Supported Fields:
    ///
    ///    * `create_time`
    ///    * `update_time`
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [FeatureRegistryService.ListFeatureGroups][google.cloud.aiplatform.v1.FeatureRegistryService.ListFeatureGroups].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListFeatureGroupsResponse {
    /// The FeatureGroups matching the request.
    #[prost(message, repeated, tag = "1")]
    pub feature_groups: ::prost::alloc::vec::Vec<FeatureGroup>,
    /// A token, which can be sent as
    /// [ListFeatureGroupsRequest.page_token][google.cloud.aiplatform.v1.ListFeatureGroupsRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [FeatureRegistryService.UpdateFeatureGroup][google.cloud.aiplatform.v1.FeatureRegistryService.UpdateFeatureGroup].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateFeatureGroupRequest {
    /// Required. The FeatureGroup's `name` field is used to identify the
    /// FeatureGroup to be updated. Format:
    /// `projects/{project}/locations/{location}/featureGroups/{feature_group}`
    #[prost(message, optional, tag = "1")]
    pub feature_group: ::core::option::Option<FeatureGroup>,
    /// Field mask is used to specify the fields to be overwritten in the
    /// FeatureGroup resource by the update.
    /// The fields specified in the update_mask are relative to the resource, not
    /// the full request. A field will be overwritten if it is in the mask. If the
    /// user does not provide a mask then only the non-empty fields present in the
    /// request will be overwritten. Set the update_mask to `*` to override all
    /// fields.
    ///
    /// Updatable fields:
    ///
    ///    * `labels`
    ///    * `description`
    ///    * `big_query`
    ///    * `big_query.entity_id_columns`
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [FeatureRegistryService.DeleteFeatureGroup][google.cloud.aiplatform.v1.FeatureRegistryService.DeleteFeatureGroup].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteFeatureGroupRequest {
    /// Required. The name of the FeatureGroup to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/featureGroups/{feature_group}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// If set to true, any Features under this FeatureGroup
    /// will also be deleted. (Otherwise, the request will only work if the
    /// FeatureGroup has no Features.)
    #[prost(bool, tag = "2")]
    pub force: bool,
}
/// Details of operations that perform create FeatureGroup.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateFeatureGroupOperationMetadata {
    /// Operation metadata for FeatureGroup.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that perform update FeatureGroup.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateFeatureGroupOperationMetadata {
    /// Operation metadata for FeatureGroup.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that perform create FeatureGroup.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateRegistryFeatureOperationMetadata {
    /// Operation metadata for Feature.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that perform update Feature.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateFeatureOperationMetadata {
    /// Operation metadata for Feature Update.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Generated client implementations.
pub mod feature_registry_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// The service that handles CRUD and List for resources for
    /// FeatureRegistry.
    #[derive(Debug, Clone)]
    pub struct FeatureRegistryServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl FeatureRegistryServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> FeatureRegistryServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> FeatureRegistryServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            FeatureRegistryServiceClient::new(
                InterceptedService::new(inner, interceptor),
            )
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a new FeatureGroup in a given project and location.
        pub async fn create_feature_group(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateFeatureGroupRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureRegistryService/CreateFeatureGroup",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureRegistryService",
                        "CreateFeatureGroup",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets details of a single FeatureGroup.
        pub async fn get_feature_group(
            &mut self,
            request: impl tonic::IntoRequest<super::GetFeatureGroupRequest>,
        ) -> std::result::Result<tonic::Response<super::FeatureGroup>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureRegistryService/GetFeatureGroup",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureRegistryService",
                        "GetFeatureGroup",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists FeatureGroups in a given project and location.
        pub async fn list_feature_groups(
            &mut self,
            request: impl tonic::IntoRequest<super::ListFeatureGroupsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListFeatureGroupsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureRegistryService/ListFeatureGroups",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureRegistryService",
                        "ListFeatureGroups",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates the parameters of a single FeatureGroup.
        pub async fn update_feature_group(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateFeatureGroupRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureRegistryService/UpdateFeatureGroup",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureRegistryService",
                        "UpdateFeatureGroup",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a single FeatureGroup.
        pub async fn delete_feature_group(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteFeatureGroupRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureRegistryService/DeleteFeatureGroup",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureRegistryService",
                        "DeleteFeatureGroup",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a new Feature in a given FeatureGroup.
        pub async fn create_feature(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateFeatureRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureRegistryService/CreateFeature",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureRegistryService",
                        "CreateFeature",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a batch of Features in a given FeatureGroup.
        pub async fn batch_create_features(
            &mut self,
            request: impl tonic::IntoRequest<super::BatchCreateFeaturesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureRegistryService/BatchCreateFeatures",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureRegistryService",
                        "BatchCreateFeatures",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets details of a single Feature.
        pub async fn get_feature(
            &mut self,
            request: impl tonic::IntoRequest<super::GetFeatureRequest>,
        ) -> std::result::Result<tonic::Response<super::Feature>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureRegistryService/GetFeature",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureRegistryService",
                        "GetFeature",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Features in a given FeatureGroup.
        pub async fn list_features(
            &mut self,
            request: impl tonic::IntoRequest<super::ListFeaturesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListFeaturesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureRegistryService/ListFeatures",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureRegistryService",
                        "ListFeatures",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates the parameters of a single Feature.
        pub async fn update_feature(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateFeatureRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureRegistryService/UpdateFeature",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureRegistryService",
                        "UpdateFeature",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a single Feature.
        pub async fn delete_feature(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteFeatureRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.FeatureRegistryService/DeleteFeature",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.FeatureRegistryService",
                        "DeleteFeature",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Request message for
/// [GenAiCacheService.CreateCachedContent][google.cloud.aiplatform.v1.GenAiCacheService.CreateCachedContent].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateCachedContentRequest {
    /// Required. The parent resource where the cached content will be created
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The cached content to create
    #[prost(message, optional, tag = "2")]
    pub cached_content: ::core::option::Option<CachedContent>,
}
/// Request message for
/// [GenAiCacheService.GetCachedContent][google.cloud.aiplatform.v1.GenAiCacheService.GetCachedContent].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetCachedContentRequest {
    /// Required. The resource name referring to the cached content
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [GenAiCacheService.UpdateCachedContent][google.cloud.aiplatform.v1.GenAiCacheService.UpdateCachedContent].
/// Only expire_time or ttl can be updated.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateCachedContentRequest {
    /// Required. The cached content to update
    #[prost(message, optional, tag = "1")]
    pub cached_content: ::core::option::Option<CachedContent>,
    /// Required. The list of fields to update.
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [GenAiCacheService.DeleteCachedContent][google.cloud.aiplatform.v1.GenAiCacheService.DeleteCachedContent].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteCachedContentRequest {
    /// Required. The resource name referring to the cached content
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request to list CachedContents.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListCachedContentsRequest {
    /// Required. The parent, which owns this collection of cached contents.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. The maximum number of cached contents to return. The service may
    /// return fewer than this value. If unspecified, some default (under maximum)
    /// number of items will be returned. The maximum value is 1000; values above
    /// 1000 will be coerced to 1000.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// Optional. A page token, received from a previous `ListCachedContents` call.
    /// Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to `ListCachedContents` must
    /// match the call that provided the page token.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response with a list of CachedContents.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListCachedContentsResponse {
    /// List of cached contents.
    #[prost(message, repeated, tag = "1")]
    pub cached_contents: ::prost::alloc::vec::Vec<CachedContent>,
    /// A token, which can be sent as `page_token` to retrieve the next page.
    /// If this field is omitted, there are no subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod gen_ai_cache_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// Service for managing Vertex AI's CachedContent resource.
    #[derive(Debug, Clone)]
    pub struct GenAiCacheServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl GenAiCacheServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> GenAiCacheServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> GenAiCacheServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            GenAiCacheServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates cached content, this call will initialize the cached content in the
        /// data storage, and users need to pay for the cache data storage.
        pub async fn create_cached_content(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateCachedContentRequest>,
        ) -> std::result::Result<tonic::Response<super::CachedContent>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.GenAiCacheService/CreateCachedContent",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.GenAiCacheService",
                        "CreateCachedContent",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets cached content configurations
        pub async fn get_cached_content(
            &mut self,
            request: impl tonic::IntoRequest<super::GetCachedContentRequest>,
        ) -> std::result::Result<tonic::Response<super::CachedContent>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.GenAiCacheService/GetCachedContent",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.GenAiCacheService",
                        "GetCachedContent",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates cached content configurations
        pub async fn update_cached_content(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateCachedContentRequest>,
        ) -> std::result::Result<tonic::Response<super::CachedContent>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.GenAiCacheService/UpdateCachedContent",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.GenAiCacheService",
                        "UpdateCachedContent",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes cached content
        pub async fn delete_cached_content(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteCachedContentRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.GenAiCacheService/DeleteCachedContent",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.GenAiCacheService",
                        "DeleteCachedContent",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists cached contents in a project
        pub async fn list_cached_contents(
            &mut self,
            request: impl tonic::IntoRequest<super::ListCachedContentsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListCachedContentsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.GenAiCacheService/ListCachedContents",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.GenAiCacheService",
                        "ListCachedContents",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Represents a TuningJob that runs with Google owned models.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TuningJob {
    /// Output only. Identifier. Resource name of a TuningJob. Format:
    /// `projects/{project}/locations/{location}/tuningJobs/{tuning_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The display name of the
    /// [TunedModel][google.cloud.aiplatform.v1.Model]. The name can be up to 128
    /// characters long and can consist of any UTF-8 characters.
    #[prost(string, tag = "2")]
    pub tuned_model_display_name: ::prost::alloc::string::String,
    /// Optional. The description of the
    /// [TuningJob][google.cloud.aiplatform.v1.TuningJob].
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// Output only. The detailed state of the job.
    #[prost(enumeration = "JobState", tag = "6")]
    pub state: i32,
    /// Output only. Time when the
    /// [TuningJob][google.cloud.aiplatform.v1.TuningJob] was created.
    #[prost(message, optional, tag = "7")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the
    /// [TuningJob][google.cloud.aiplatform.v1.TuningJob] for the first time
    /// entered the `JOB_STATE_RUNNING` state.
    #[prost(message, optional, tag = "8")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the TuningJob entered any of the following
    /// [JobStates][google.cloud.aiplatform.v1.JobState]: `JOB_STATE_SUCCEEDED`,
    /// `JOB_STATE_FAILED`, `JOB_STATE_CANCELLED`, `JOB_STATE_EXPIRED`.
    #[prost(message, optional, tag = "9")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the
    /// [TuningJob][google.cloud.aiplatform.v1.TuningJob] was most recently
    /// updated.
    #[prost(message, optional, tag = "10")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Only populated when job's state is `JOB_STATE_FAILED` or
    /// `JOB_STATE_CANCELLED`.
    #[prost(message, optional, tag = "11")]
    pub error: ::core::option::Option<super::super::super::rpc::Status>,
    /// Optional. The labels with user-defined metadata to organize
    /// [TuningJob][google.cloud.aiplatform.v1.TuningJob] and generated resources
    /// such as [Model][google.cloud.aiplatform.v1.Model] and
    /// [Endpoint][google.cloud.aiplatform.v1.Endpoint].
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "12")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Output only. The Experiment associated with this
    /// [TuningJob][google.cloud.aiplatform.v1.TuningJob].
    #[prost(string, tag = "13")]
    pub experiment: ::prost::alloc::string::String,
    /// Output only. The tuned model resources assiociated with this
    /// [TuningJob][google.cloud.aiplatform.v1.TuningJob].
    #[prost(message, optional, tag = "14")]
    pub tuned_model: ::core::option::Option<TunedModel>,
    /// Output only. The tuning data statistics associated with this
    /// [TuningJob][google.cloud.aiplatform.v1.TuningJob].
    #[prost(message, optional, tag = "15")]
    pub tuning_data_stats: ::core::option::Option<TuningDataStats>,
    /// Customer-managed encryption key options for a TuningJob. If this is set,
    /// then all resources created by the TuningJob will be encrypted with the
    /// provided encryption key.
    #[prost(message, optional, tag = "16")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// The service account that the tuningJob workload runs as.
    /// If not specified, the Vertex AI Secure Fine-Tuned Service Agent in the
    /// project will be used. See
    /// <https://cloud.google.com/iam/docs/service-agents#vertex-ai-secure-fine-tuning-service-agent>
    ///
    /// Users starting the pipeline must have the `iam.serviceAccounts.actAs`
    /// permission on this service account.
    #[prost(string, tag = "22")]
    pub service_account: ::prost::alloc::string::String,
    #[prost(oneof = "tuning_job::SourceModel", tags = "4")]
    pub source_model: ::core::option::Option<tuning_job::SourceModel>,
    #[prost(oneof = "tuning_job::TuningSpec", tags = "5")]
    pub tuning_spec: ::core::option::Option<tuning_job::TuningSpec>,
}
/// Nested message and enum types in `TuningJob`.
pub mod tuning_job {
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum SourceModel {
        /// The base model that is being tuned, e.g., "gemini-1.0-pro-002".
        #[prost(string, tag = "4")]
        BaseModel(::prost::alloc::string::String),
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum TuningSpec {
        /// Tuning Spec for Supervised Fine Tuning.
        #[prost(message, tag = "5")]
        SupervisedTuningSpec(super::SupervisedTuningSpec),
    }
}
/// The Model Registry Model and Online Prediction Endpoint assiociated with
/// this [TuningJob][google.cloud.aiplatform.v1.TuningJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TunedModel {
    /// Output only. The resource name of the TunedModel. Format:
    /// `projects/{project}/locations/{location}/models/{model}`.
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Output only. A resource name of an Endpoint. Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`.
    #[prost(string, tag = "2")]
    pub endpoint: ::prost::alloc::string::String,
}
/// Dataset distribution for Supervised Tuning.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SupervisedTuningDatasetDistribution {
    /// Output only. Sum of a given population of values.
    #[prost(int64, tag = "1")]
    pub sum: i64,
    /// Output only. Sum of a given population of values that are billable.
    #[prost(int64, tag = "9")]
    pub billable_sum: i64,
    /// Output only. The minimum of the population values.
    #[prost(double, tag = "2")]
    pub min: f64,
    /// Output only. The maximum of the population values.
    #[prost(double, tag = "3")]
    pub max: f64,
    /// Output only. The arithmetic mean of the values in the population.
    #[prost(double, tag = "4")]
    pub mean: f64,
    /// Output only. The median of the values in the population.
    #[prost(double, tag = "5")]
    pub median: f64,
    /// Output only. The 5th percentile of the values in the population.
    #[prost(double, tag = "6")]
    pub p5: f64,
    /// Output only. The 95th percentile of the values in the population.
    #[prost(double, tag = "7")]
    pub p95: f64,
    /// Output only. Defines the histogram bucket.
    #[prost(message, repeated, tag = "8")]
    pub buckets: ::prost::alloc::vec::Vec<
        supervised_tuning_dataset_distribution::DatasetBucket,
    >,
}
/// Nested message and enum types in `SupervisedTuningDatasetDistribution`.
pub mod supervised_tuning_dataset_distribution {
    /// Dataset bucket used to create a histogram for the distribution given a
    /// population of values.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct DatasetBucket {
        /// Output only. Number of values in the bucket.
        #[prost(double, tag = "1")]
        pub count: f64,
        /// Output only. Left bound of the bucket.
        #[prost(double, tag = "2")]
        pub left: f64,
        /// Output only. Right bound of the bucket.
        #[prost(double, tag = "3")]
        pub right: f64,
    }
}
/// Tuning data statistics for Supervised Tuning.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SupervisedTuningDataStats {
    /// Output only. Number of examples in the tuning dataset.
    #[prost(int64, tag = "1")]
    pub tuning_dataset_example_count: i64,
    /// Output only. Number of tuning characters in the tuning dataset.
    #[prost(int64, tag = "2")]
    pub total_tuning_character_count: i64,
    /// Output only. Number of billable characters in the tuning dataset.
    #[deprecated]
    #[prost(int64, tag = "3")]
    pub total_billable_character_count: i64,
    /// Output only. Number of billable tokens in the tuning dataset.
    #[prost(int64, tag = "9")]
    pub total_billable_token_count: i64,
    /// Output only. Number of tuning steps for this Tuning Job.
    #[prost(int64, tag = "4")]
    pub tuning_step_count: i64,
    /// Output only. Dataset distributions for the user input tokens.
    #[prost(message, optional, tag = "5")]
    pub user_input_token_distribution: ::core::option::Option<
        SupervisedTuningDatasetDistribution,
    >,
    /// Output only. Dataset distributions for the user output tokens.
    #[prost(message, optional, tag = "6")]
    pub user_output_token_distribution: ::core::option::Option<
        SupervisedTuningDatasetDistribution,
    >,
    /// Output only. Dataset distributions for the messages per example.
    #[prost(message, optional, tag = "7")]
    pub user_message_per_example_distribution: ::core::option::Option<
        SupervisedTuningDatasetDistribution,
    >,
    /// Output only. Sample user messages in the training dataset uri.
    #[prost(message, repeated, tag = "8")]
    pub user_dataset_examples: ::prost::alloc::vec::Vec<Content>,
    /// The number of examples in the dataset that have been truncated by any
    /// amount.
    #[prost(int64, tag = "10")]
    pub total_truncated_example_count: i64,
    /// A partial sample of the indices (starting from 1) of the truncated
    /// examples.
    #[prost(int64, repeated, tag = "11")]
    pub truncated_example_indices: ::prost::alloc::vec::Vec<i64>,
}
/// The tuning data statistic values for
/// [TuningJob][google.cloud.aiplatform.v1.TuningJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TuningDataStats {
    #[prost(oneof = "tuning_data_stats::TuningDataStats", tags = "1")]
    pub tuning_data_stats: ::core::option::Option<tuning_data_stats::TuningDataStats>,
}
/// Nested message and enum types in `TuningDataStats`.
pub mod tuning_data_stats {
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum TuningDataStats {
        /// The SFT Tuning data stats.
        #[prost(message, tag = "1")]
        SupervisedTuningDataStats(super::SupervisedTuningDataStats),
    }
}
/// Hyperparameters for SFT.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SupervisedHyperParameters {
    /// Optional. Number of complete passes the model makes over the entire
    /// training dataset during training.
    #[prost(int64, tag = "1")]
    pub epoch_count: i64,
    /// Optional. Multiplier for adjusting the default learning rate.
    #[prost(double, tag = "2")]
    pub learning_rate_multiplier: f64,
    /// Optional. Adapter size for tuning.
    #[prost(enumeration = "supervised_hyper_parameters::AdapterSize", tag = "3")]
    pub adapter_size: i32,
}
/// Nested message and enum types in `SupervisedHyperParameters`.
pub mod supervised_hyper_parameters {
    /// Supported adapter sizes for tuning.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum AdapterSize {
        /// Adapter size is unspecified.
        Unspecified = 0,
        /// Adapter size 1.
        One = 1,
        /// Adapter size 4.
        Four = 2,
        /// Adapter size 8.
        Eight = 3,
        /// Adapter size 16.
        Sixteen = 4,
    }
    impl AdapterSize {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "ADAPTER_SIZE_UNSPECIFIED",
                Self::One => "ADAPTER_SIZE_ONE",
                Self::Four => "ADAPTER_SIZE_FOUR",
                Self::Eight => "ADAPTER_SIZE_EIGHT",
                Self::Sixteen => "ADAPTER_SIZE_SIXTEEN",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "ADAPTER_SIZE_UNSPECIFIED" => Some(Self::Unspecified),
                "ADAPTER_SIZE_ONE" => Some(Self::One),
                "ADAPTER_SIZE_FOUR" => Some(Self::Four),
                "ADAPTER_SIZE_EIGHT" => Some(Self::Eight),
                "ADAPTER_SIZE_SIXTEEN" => Some(Self::Sixteen),
                _ => None,
            }
        }
    }
}
/// Tuning Spec for Supervised Tuning for first party models.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SupervisedTuningSpec {
    /// Required. Cloud Storage path to file containing training dataset for
    /// tuning. The dataset must be formatted as a JSONL file.
    #[prost(string, tag = "1")]
    pub training_dataset_uri: ::prost::alloc::string::String,
    /// Optional. Cloud Storage path to file containing validation dataset for
    /// tuning. The dataset must be formatted as a JSONL file.
    #[prost(string, tag = "2")]
    pub validation_dataset_uri: ::prost::alloc::string::String,
    /// Optional. Hyperparameters for SFT.
    #[prost(message, optional, tag = "3")]
    pub hyper_parameters: ::core::option::Option<SupervisedHyperParameters>,
}
/// TunedModel Reference for legacy model migration.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TunedModelRef {
    /// The Tuned Model Reference for the model.
    #[prost(oneof = "tuned_model_ref::TunedModelRef", tags = "1, 2, 3")]
    pub tuned_model_ref: ::core::option::Option<tuned_model_ref::TunedModelRef>,
}
/// Nested message and enum types in `TunedModelRef`.
pub mod tuned_model_ref {
    /// The Tuned Model Reference for the model.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum TunedModelRef {
        /// Support migration from model registry.
        #[prost(string, tag = "1")]
        TunedModel(::prost::alloc::string::String),
        /// Support migration from tuning job list page, from gemini-1.0-pro-002
        /// to 1.5 and above.
        #[prost(string, tag = "2")]
        TuningJob(::prost::alloc::string::String),
        /// Support migration from tuning job list page, from bison model to gemini
        /// model.
        #[prost(string, tag = "3")]
        PipelineJob(::prost::alloc::string::String),
    }
}
/// Request message for
/// [GenAiTuningService.CreateTuningJob][google.cloud.aiplatform.v1.GenAiTuningService.CreateTuningJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTuningJobRequest {
    /// Required. The resource name of the Location to create the TuningJob in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The TuningJob to create.
    #[prost(message, optional, tag = "2")]
    pub tuning_job: ::core::option::Option<TuningJob>,
}
/// Request message for
/// [GenAiTuningService.GetTuningJob][google.cloud.aiplatform.v1.GenAiTuningService.GetTuningJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetTuningJobRequest {
    /// Required. The name of the TuningJob resource. Format:
    /// `projects/{project}/locations/{location}/tuningJobs/{tuning_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [GenAiTuningService.ListTuningJobs][google.cloud.aiplatform.v1.GenAiTuningService.ListTuningJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTuningJobsRequest {
    /// Required. The resource name of the Location to list the TuningJobs from.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. The standard list filter.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// Optional. The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// Optional. The standard list page token.
    /// Typically obtained via
    /// [ListTuningJobsResponse.next_page_token][google.cloud.aiplatform.v1.ListTuningJobsResponse.next_page_token]
    /// of the previous GenAiTuningService.ListTuningJob][] call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response message for
/// [GenAiTuningService.ListTuningJobs][google.cloud.aiplatform.v1.GenAiTuningService.ListTuningJobs]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTuningJobsResponse {
    /// List of TuningJobs in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub tuning_jobs: ::prost::alloc::vec::Vec<TuningJob>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListTuningJobsRequest.page_token][google.cloud.aiplatform.v1.ListTuningJobsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [GenAiTuningService.CancelTuningJob][google.cloud.aiplatform.v1.GenAiTuningService.CancelTuningJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CancelTuningJobRequest {
    /// Required. The name of the TuningJob to cancel. Format:
    /// `projects/{project}/locations/{location}/tuningJobs/{tuning_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [GenAiTuningService.RebaseTunedModel][google.cloud.aiplatform.v1.GenAiTuningService.RebaseTunedModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RebaseTunedModelRequest {
    /// Required. The resource name of the Location into which to rebase the Model.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. TunedModel reference to retrieve the legacy model information.
    #[prost(message, optional, tag = "2")]
    pub tuned_model_ref: ::core::option::Option<TunedModelRef>,
    /// Optional. The TuningJob to be updated. Users can use this TuningJob field
    /// to overwrite tuning configs.
    #[prost(message, optional, tag = "3")]
    pub tuning_job: ::core::option::Option<TuningJob>,
    /// Optional. The Google Cloud Storage location to write the artifacts.
    #[prost(message, optional, tag = "4")]
    pub artifact_destination: ::core::option::Option<GcsDestination>,
    /// Optional. By default, bison to gemini migration will always create new
    /// model/endpoint, but for gemini-1.0 to gemini-1.5 migration, we default
    /// deploy to the same endpoint. See details in this Section.
    #[prost(bool, tag = "5")]
    pub deploy_to_same_endpoint: bool,
}
/// Runtime operation information for
/// [GenAiTuningService.RebaseTunedModel][google.cloud.aiplatform.v1.GenAiTuningService.RebaseTunedModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RebaseTunedModelOperationMetadata {
    /// The common part of the operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Generated client implementations.
pub mod gen_ai_tuning_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for creating and managing GenAI Tuning Jobs.
    #[derive(Debug, Clone)]
    pub struct GenAiTuningServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl GenAiTuningServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> GenAiTuningServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> GenAiTuningServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            GenAiTuningServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a TuningJob. A created TuningJob right away will be attempted to
        /// be run.
        pub async fn create_tuning_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateTuningJobRequest>,
        ) -> std::result::Result<tonic::Response<super::TuningJob>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.GenAiTuningService/CreateTuningJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.GenAiTuningService",
                        "CreateTuningJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a TuningJob.
        pub async fn get_tuning_job(
            &mut self,
            request: impl tonic::IntoRequest<super::GetTuningJobRequest>,
        ) -> std::result::Result<tonic::Response<super::TuningJob>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.GenAiTuningService/GetTuningJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.GenAiTuningService",
                        "GetTuningJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists TuningJobs in a Location.
        pub async fn list_tuning_jobs(
            &mut self,
            request: impl tonic::IntoRequest<super::ListTuningJobsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListTuningJobsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.GenAiTuningService/ListTuningJobs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.GenAiTuningService",
                        "ListTuningJobs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Cancels a TuningJob.
        /// Starts asynchronous cancellation on the TuningJob. The server makes a best
        /// effort to cancel the job, but success is not guaranteed. Clients can use
        /// [GenAiTuningService.GetTuningJob][google.cloud.aiplatform.v1.GenAiTuningService.GetTuningJob]
        /// or other methods to check whether the cancellation succeeded or whether the
        /// job completed despite cancellation. On successful cancellation, the
        /// TuningJob is not deleted; instead it becomes a job with a
        /// [TuningJob.error][google.cloud.aiplatform.v1.TuningJob.error] value with a
        /// [google.rpc.Status.code][google.rpc.Status.code] of 1, corresponding to
        /// `Code.CANCELLED`, and
        /// [TuningJob.state][google.cloud.aiplatform.v1.TuningJob.state] is set to
        /// `CANCELLED`.
        pub async fn cancel_tuning_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CancelTuningJobRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.GenAiTuningService/CancelTuningJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.GenAiTuningService",
                        "CancelTuningJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Rebase a TunedModel.
        pub async fn rebase_tuned_model(
            &mut self,
            request: impl tonic::IntoRequest<super::RebaseTunedModelRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.GenAiTuningService/RebaseTunedModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.GenAiTuningService",
                        "RebaseTunedModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// A message representing a Study.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Study {
    /// Output only. The name of a study. The study's globally unique identifier.
    /// Format: `projects/{project}/locations/{location}/studies/{study}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. Describes the Study, default value is empty string.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Required. Configuration of the Study.
    #[prost(message, optional, tag = "3")]
    pub study_spec: ::core::option::Option<StudySpec>,
    /// Output only. The detailed state of a Study.
    #[prost(enumeration = "study::State", tag = "4")]
    pub state: i32,
    /// Output only. Time at which the study was created.
    #[prost(message, optional, tag = "5")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. A human readable reason why the Study is inactive.
    /// This should be empty if a study is ACTIVE or COMPLETED.
    #[prost(string, tag = "6")]
    pub inactive_reason: ::prost::alloc::string::String,
}
/// Nested message and enum types in `Study`.
pub mod study {
    /// Describes the Study state.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// The study state is unspecified.
        Unspecified = 0,
        /// The study is active.
        Active = 1,
        /// The study is stopped due to an internal error.
        Inactive = 2,
        /// The study is done when the service exhausts the parameter search space
        /// or max_trial_count is reached.
        Completed = 3,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STATE_UNSPECIFIED",
                Self::Active => "ACTIVE",
                Self::Inactive => "INACTIVE",
                Self::Completed => "COMPLETED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "ACTIVE" => Some(Self::Active),
                "INACTIVE" => Some(Self::Inactive),
                "COMPLETED" => Some(Self::Completed),
                _ => None,
            }
        }
    }
}
/// A message representing a Trial. A Trial contains a unique set of Parameters
/// that has been or will be evaluated, along with the objective metrics got by
/// running the Trial.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Trial {
    /// Output only. Resource name of the Trial assigned by the service.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. The identifier of the Trial assigned by the service.
    #[prost(string, tag = "2")]
    pub id: ::prost::alloc::string::String,
    /// Output only. The detailed state of the Trial.
    #[prost(enumeration = "trial::State", tag = "3")]
    pub state: i32,
    /// Output only. The parameters of the Trial.
    #[prost(message, repeated, tag = "4")]
    pub parameters: ::prost::alloc::vec::Vec<trial::Parameter>,
    /// Output only. The final measurement containing the objective value.
    #[prost(message, optional, tag = "5")]
    pub final_measurement: ::core::option::Option<Measurement>,
    /// Output only. A list of measurements that are strictly lexicographically
    /// ordered by their induced tuples (steps, elapsed_duration).
    /// These are used for early stopping computations.
    #[prost(message, repeated, tag = "6")]
    pub measurements: ::prost::alloc::vec::Vec<Measurement>,
    /// Output only. Time when the Trial was started.
    #[prost(message, optional, tag = "7")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the Trial's status changed to `SUCCEEDED` or
    /// `INFEASIBLE`.
    #[prost(message, optional, tag = "8")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The identifier of the client that originally requested this
    /// Trial. Each client is identified by a unique client_id. When a client asks
    /// for a suggestion, Vertex AI Vizier will assign it a Trial. The client
    /// should evaluate the Trial, complete it, and report back to Vertex AI
    /// Vizier. If suggestion is asked again by same client_id before the Trial is
    /// completed, the same Trial will be returned. Multiple clients with
    /// different client_ids can ask for suggestions simultaneously, each of them
    /// will get their own Trial.
    #[prost(string, tag = "9")]
    pub client_id: ::prost::alloc::string::String,
    /// Output only. A human readable string describing why the Trial is
    /// infeasible. This is set only if Trial state is `INFEASIBLE`.
    #[prost(string, tag = "10")]
    pub infeasible_reason: ::prost::alloc::string::String,
    /// Output only. The CustomJob name linked to the Trial.
    /// It's set for a HyperparameterTuningJob's Trial.
    #[prost(string, tag = "11")]
    pub custom_job: ::prost::alloc::string::String,
    /// Output only. URIs for accessing [interactive
    /// shells](<https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell>)
    /// (one URI for each training node). Only available if this trial is part of
    /// a
    /// [HyperparameterTuningJob][google.cloud.aiplatform.v1.HyperparameterTuningJob]
    /// and the job's
    /// [trial_job_spec.enable_web_access][google.cloud.aiplatform.v1.CustomJobSpec.enable_web_access]
    /// field is `true`.
    ///
    /// The keys are names of each node used for the trial; for example,
    /// `workerpool0-0` for the primary node, `workerpool1-0` for the first node in
    /// the second worker pool, and `workerpool1-1` for the second node in the
    /// second worker pool.
    ///
    /// The values are the URIs for each node's interactive shell.
    #[prost(map = "string, string", tag = "12")]
    pub web_access_uris: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
/// Nested message and enum types in `Trial`.
pub mod trial {
    /// A message representing a parameter to be tuned.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Parameter {
        /// Output only. The ID of the parameter. The parameter should be defined in
        /// [StudySpec's
        /// Parameters][google.cloud.aiplatform.v1.StudySpec.parameters].
        #[prost(string, tag = "1")]
        pub parameter_id: ::prost::alloc::string::String,
        /// Output only. The value of the parameter.
        /// `number_value` will be set if a parameter defined in StudySpec is
        /// in type 'INTEGER', 'DOUBLE' or 'DISCRETE'.
        /// `string_value` will be set if a parameter defined in StudySpec is
        /// in type 'CATEGORICAL'.
        #[prost(message, optional, tag = "2")]
        pub value: ::core::option::Option<::prost_types::Value>,
    }
    /// Describes a Trial state.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// The Trial state is unspecified.
        Unspecified = 0,
        /// Indicates that a specific Trial has been requested, but it has not yet
        /// been suggested by the service.
        Requested = 1,
        /// Indicates that the Trial has been suggested.
        Active = 2,
        /// Indicates that the Trial should stop according to the service.
        Stopping = 3,
        /// Indicates that the Trial is completed successfully.
        Succeeded = 4,
        /// Indicates that the Trial should not be attempted again.
        /// The service will set a Trial to INFEASIBLE when it's done but missing
        /// the final_measurement.
        Infeasible = 5,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STATE_UNSPECIFIED",
                Self::Requested => "REQUESTED",
                Self::Active => "ACTIVE",
                Self::Stopping => "STOPPING",
                Self::Succeeded => "SUCCEEDED",
                Self::Infeasible => "INFEASIBLE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "REQUESTED" => Some(Self::Requested),
                "ACTIVE" => Some(Self::Active),
                "STOPPING" => Some(Self::Stopping),
                "SUCCEEDED" => Some(Self::Succeeded),
                "INFEASIBLE" => Some(Self::Infeasible),
                _ => None,
            }
        }
    }
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TrialContext {
    /// A human-readable field which can store a description of this context.
    /// This will become part of the resulting Trial's description field.
    #[prost(string, tag = "1")]
    pub description: ::prost::alloc::string::String,
    /// If/when a Trial is generated or selected from this Context,
    /// its Parameters will match any parameters specified here.
    /// (I.e. if this context specifies parameter name:'a' int_value:3,
    /// then a resulting Trial will have int_value:3 for its parameter named
    /// 'a'.) Note that we first attempt to match existing REQUESTED Trials with
    /// contexts, and if there are no matches, we generate suggestions in the
    /// subspace defined by the parameters specified here.
    /// NOTE: a Context without any Parameters matches the entire feasible search
    ///    space.
    #[prost(message, repeated, tag = "2")]
    pub parameters: ::prost::alloc::vec::Vec<trial::Parameter>,
}
/// Time-based Constraint for Study
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct StudyTimeConstraint {
    #[prost(oneof = "study_time_constraint::Constraint", tags = "1, 2")]
    pub constraint: ::core::option::Option<study_time_constraint::Constraint>,
}
/// Nested message and enum types in `StudyTimeConstraint`.
pub mod study_time_constraint {
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum Constraint {
        /// Counts the wallclock time passed since the creation of this Study.
        #[prost(message, tag = "1")]
        MaxDuration(::prost_types::Duration),
        /// Compares the wallclock time to this time. Must use UTC timezone.
        #[prost(message, tag = "2")]
        EndTime(::prost_types::Timestamp),
    }
}
/// Represents specification of a Study.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StudySpec {
    /// Required. Metric specs for the Study.
    #[prost(message, repeated, tag = "1")]
    pub metrics: ::prost::alloc::vec::Vec<study_spec::MetricSpec>,
    /// Required. The set of parameters to tune.
    #[prost(message, repeated, tag = "2")]
    pub parameters: ::prost::alloc::vec::Vec<study_spec::ParameterSpec>,
    /// The search algorithm specified for the Study.
    #[prost(enumeration = "study_spec::Algorithm", tag = "3")]
    pub algorithm: i32,
    /// The observation noise level of the study.
    /// Currently only supported by the Vertex AI Vizier service. Not supported by
    /// HyperparameterTuningJob or TrainingPipeline.
    #[prost(enumeration = "study_spec::ObservationNoise", tag = "6")]
    pub observation_noise: i32,
    /// Describe which measurement selection type will be used
    #[prost(enumeration = "study_spec::MeasurementSelectionType", tag = "7")]
    pub measurement_selection_type: i32,
    /// Conditions for automated stopping of a Study. Enable automated stopping by
    /// configuring at least one condition.
    #[prost(message, optional, tag = "11")]
    pub study_stopping_config: ::core::option::Option<study_spec::StudyStoppingConfig>,
    #[prost(oneof = "study_spec::AutomatedStoppingSpec", tags = "4, 5, 9")]
    pub automated_stopping_spec: ::core::option::Option<
        study_spec::AutomatedStoppingSpec,
    >,
}
/// Nested message and enum types in `StudySpec`.
pub mod study_spec {
    /// Represents a metric to optimize.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct MetricSpec {
        /// Required. The ID of the metric. Must not contain whitespaces and must be
        /// unique amongst all MetricSpecs.
        #[prost(string, tag = "1")]
        pub metric_id: ::prost::alloc::string::String,
        /// Required. The optimization goal of the metric.
        #[prost(enumeration = "metric_spec::GoalType", tag = "2")]
        pub goal: i32,
        /// Used for safe search. In the case, the metric will be a safety
        /// metric. You must provide a separate metric for objective metric.
        #[prost(message, optional, tag = "3")]
        pub safety_config: ::core::option::Option<metric_spec::SafetyMetricConfig>,
    }
    /// Nested message and enum types in `MetricSpec`.
    pub mod metric_spec {
        /// Used in safe optimization to specify threshold levels and risk tolerance.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct SafetyMetricConfig {
            /// Safety threshold (boundary value between safe and unsafe). NOTE that if
            /// you leave SafetyMetricConfig unset, a default value of 0 will be used.
            #[prost(double, tag = "1")]
            pub safety_threshold: f64,
            /// Desired minimum fraction of safe trials (over total number of trials)
            /// that should be targeted by the algorithm at any time during the
            /// study (best effort). This should be between 0.0 and 1.0 and a value of
            /// 0.0 means that there is no minimum and an algorithm proceeds without
            /// targeting any specific fraction. A value of 1.0 means that the
            /// algorithm attempts to only Suggest safe Trials.
            #[prost(double, optional, tag = "2")]
            pub desired_min_safe_trials_fraction: ::core::option::Option<f64>,
        }
        /// The available types of optimization goals.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum GoalType {
            /// Goal Type will default to maximize.
            Unspecified = 0,
            /// Maximize the goal metric.
            Maximize = 1,
            /// Minimize the goal metric.
            Minimize = 2,
        }
        impl GoalType {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "GOAL_TYPE_UNSPECIFIED",
                    Self::Maximize => "MAXIMIZE",
                    Self::Minimize => "MINIMIZE",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "GOAL_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                    "MAXIMIZE" => Some(Self::Maximize),
                    "MINIMIZE" => Some(Self::Minimize),
                    _ => None,
                }
            }
        }
    }
    /// Represents a single parameter to optimize.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ParameterSpec {
        /// Required. The ID of the parameter. Must not contain whitespaces and must
        /// be unique amongst all ParameterSpecs.
        #[prost(string, tag = "1")]
        pub parameter_id: ::prost::alloc::string::String,
        /// How the parameter should be scaled.
        /// Leave unset for `CATEGORICAL` parameters.
        #[prost(enumeration = "parameter_spec::ScaleType", tag = "6")]
        pub scale_type: i32,
        /// A conditional parameter node is active if the parameter's value matches
        /// the conditional node's parent_value_condition.
        ///
        /// If two items in conditional_parameter_specs have the same name, they
        /// must have disjoint parent_value_condition.
        #[prost(message, repeated, tag = "10")]
        pub conditional_parameter_specs: ::prost::alloc::vec::Vec<
            parameter_spec::ConditionalParameterSpec,
        >,
        #[prost(oneof = "parameter_spec::ParameterValueSpec", tags = "2, 3, 4, 5")]
        pub parameter_value_spec: ::core::option::Option<
            parameter_spec::ParameterValueSpec,
        >,
    }
    /// Nested message and enum types in `ParameterSpec`.
    pub mod parameter_spec {
        /// Value specification for a parameter in `DOUBLE` type.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct DoubleValueSpec {
            /// Required. Inclusive minimum value of the parameter.
            #[prost(double, tag = "1")]
            pub min_value: f64,
            /// Required. Inclusive maximum value of the parameter.
            #[prost(double, tag = "2")]
            pub max_value: f64,
            /// A default value for a `DOUBLE` parameter that is assumed to be a
            /// relatively good starting point.  Unset value signals that there is no
            /// offered starting point.
            ///
            /// Currently only supported by the Vertex AI Vizier service. Not supported
            /// by HyperparameterTuningJob or TrainingPipeline.
            #[prost(double, optional, tag = "4")]
            pub default_value: ::core::option::Option<f64>,
        }
        /// Value specification for a parameter in `INTEGER` type.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct IntegerValueSpec {
            /// Required. Inclusive minimum value of the parameter.
            #[prost(int64, tag = "1")]
            pub min_value: i64,
            /// Required. Inclusive maximum value of the parameter.
            #[prost(int64, tag = "2")]
            pub max_value: i64,
            /// A default value for an `INTEGER` parameter that is assumed to be a
            /// relatively good starting point.  Unset value signals that there is no
            /// offered starting point.
            ///
            /// Currently only supported by the Vertex AI Vizier service. Not supported
            /// by HyperparameterTuningJob or TrainingPipeline.
            #[prost(int64, optional, tag = "4")]
            pub default_value: ::core::option::Option<i64>,
        }
        /// Value specification for a parameter in `CATEGORICAL` type.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct CategoricalValueSpec {
            /// Required. The list of possible categories.
            #[prost(string, repeated, tag = "1")]
            pub values: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
            /// A default value for a `CATEGORICAL` parameter that is assumed to be a
            /// relatively good starting point.  Unset value signals that there is no
            /// offered starting point.
            ///
            /// Currently only supported by the Vertex AI Vizier service. Not supported
            /// by HyperparameterTuningJob or TrainingPipeline.
            #[prost(string, optional, tag = "3")]
            pub default_value: ::core::option::Option<::prost::alloc::string::String>,
        }
        /// Value specification for a parameter in `DISCRETE` type.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct DiscreteValueSpec {
            /// Required. A list of possible values.
            /// The list should be in increasing order and at least 1e-10 apart.
            /// For instance, this parameter might have possible settings of 1.5, 2.5,
            /// and 4.0. This list should not contain more than 1,000 values.
            #[prost(double, repeated, packed = "false", tag = "1")]
            pub values: ::prost::alloc::vec::Vec<f64>,
            /// A default value for a `DISCRETE` parameter that is assumed to be a
            /// relatively good starting point.  Unset value signals that there is no
            /// offered starting point.  It automatically rounds to the
            /// nearest feasible discrete point.
            ///
            /// Currently only supported by the Vertex AI Vizier service. Not supported
            /// by HyperparameterTuningJob or TrainingPipeline.
            #[prost(double, optional, tag = "3")]
            pub default_value: ::core::option::Option<f64>,
        }
        /// Represents a parameter spec with condition from its parent parameter.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct ConditionalParameterSpec {
            /// Required. The spec for a conditional parameter.
            #[prost(message, optional, tag = "1")]
            pub parameter_spec: ::core::option::Option<super::ParameterSpec>,
            /// A set of parameter values from the parent ParameterSpec's feasible
            /// space.
            #[prost(
                oneof = "conditional_parameter_spec::ParentValueCondition",
                tags = "2, 3, 4"
            )]
            pub parent_value_condition: ::core::option::Option<
                conditional_parameter_spec::ParentValueCondition,
            >,
        }
        /// Nested message and enum types in `ConditionalParameterSpec`.
        pub mod conditional_parameter_spec {
            /// Represents the spec to match discrete values from parent parameter.
            #[derive(Clone, PartialEq, ::prost::Message)]
            pub struct DiscreteValueCondition {
                /// Required. Matches values of the parent parameter of 'DISCRETE' type.
                /// All values must exist in `discrete_value_spec` of parent parameter.
                ///
                /// The Epsilon of the value matching is 1e-10.
                #[prost(double, repeated, packed = "false", tag = "1")]
                pub values: ::prost::alloc::vec::Vec<f64>,
            }
            /// Represents the spec to match integer values from parent parameter.
            #[derive(Clone, PartialEq, ::prost::Message)]
            pub struct IntValueCondition {
                /// Required. Matches values of the parent parameter of 'INTEGER' type.
                /// All values must lie in `integer_value_spec` of parent parameter.
                #[prost(int64, repeated, packed = "false", tag = "1")]
                pub values: ::prost::alloc::vec::Vec<i64>,
            }
            /// Represents the spec to match categorical values from parent parameter.
            #[derive(Clone, PartialEq, ::prost::Message)]
            pub struct CategoricalValueCondition {
                /// Required. Matches values of the parent parameter of 'CATEGORICAL'
                /// type. All values must exist in `categorical_value_spec` of parent
                /// parameter.
                #[prost(string, repeated, tag = "1")]
                pub values: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
            }
            /// A set of parameter values from the parent ParameterSpec's feasible
            /// space.
            #[derive(Clone, PartialEq, ::prost::Oneof)]
            pub enum ParentValueCondition {
                /// The spec for matching values from a parent parameter of
                /// `DISCRETE` type.
                #[prost(message, tag = "2")]
                ParentDiscreteValues(DiscreteValueCondition),
                /// The spec for matching values from a parent parameter of `INTEGER`
                /// type.
                #[prost(message, tag = "3")]
                ParentIntValues(IntValueCondition),
                /// The spec for matching values from a parent parameter of
                /// `CATEGORICAL` type.
                #[prost(message, tag = "4")]
                ParentCategoricalValues(CategoricalValueCondition),
            }
        }
        /// The type of scaling that should be applied to this parameter.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum ScaleType {
            /// By default, no scaling is applied.
            Unspecified = 0,
            /// Scales the feasible space to (0, 1) linearly.
            UnitLinearScale = 1,
            /// Scales the feasible space logarithmically to (0, 1). The entire
            /// feasible space must be strictly positive.
            UnitLogScale = 2,
            /// Scales the feasible space "reverse" logarithmically to (0, 1). The
            /// result is that values close to the top of the feasible space are spread
            /// out more than points near the bottom. The entire feasible space must be
            /// strictly positive.
            UnitReverseLogScale = 3,
        }
        impl ScaleType {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "SCALE_TYPE_UNSPECIFIED",
                    Self::UnitLinearScale => "UNIT_LINEAR_SCALE",
                    Self::UnitLogScale => "UNIT_LOG_SCALE",
                    Self::UnitReverseLogScale => "UNIT_REVERSE_LOG_SCALE",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "SCALE_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                    "UNIT_LINEAR_SCALE" => Some(Self::UnitLinearScale),
                    "UNIT_LOG_SCALE" => Some(Self::UnitLogScale),
                    "UNIT_REVERSE_LOG_SCALE" => Some(Self::UnitReverseLogScale),
                    _ => None,
                }
            }
        }
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum ParameterValueSpec {
            /// The value spec for a 'DOUBLE' parameter.
            #[prost(message, tag = "2")]
            DoubleValueSpec(DoubleValueSpec),
            /// The value spec for an 'INTEGER' parameter.
            #[prost(message, tag = "3")]
            IntegerValueSpec(IntegerValueSpec),
            /// The value spec for a 'CATEGORICAL' parameter.
            #[prost(message, tag = "4")]
            CategoricalValueSpec(CategoricalValueSpec),
            /// The value spec for a 'DISCRETE' parameter.
            #[prost(message, tag = "5")]
            DiscreteValueSpec(DiscreteValueSpec),
        }
    }
    /// The decay curve automated stopping rule builds a Gaussian Process
    /// Regressor to predict the final objective value of a Trial based on the
    /// already completed Trials and the intermediate measurements of the current
    /// Trial. Early stopping is requested for the current Trial if there is very
    /// low probability to exceed the optimal value found so far.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct DecayCurveAutomatedStoppingSpec {
        /// True if
        /// [Measurement.elapsed_duration][google.cloud.aiplatform.v1.Measurement.elapsed_duration]
        /// is used as the x-axis of each Trials Decay Curve. Otherwise,
        /// [Measurement.step_count][google.cloud.aiplatform.v1.Measurement.step_count]
        /// will be used as the x-axis.
        #[prost(bool, tag = "1")]
        pub use_elapsed_duration: bool,
    }
    /// The median automated stopping rule stops a pending Trial if the Trial's
    /// best objective_value is strictly below the median 'performance' of all
    /// completed Trials reported up to the Trial's last measurement.
    /// Currently, 'performance' refers to the running average of the objective
    /// values reported by the Trial in each measurement.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct MedianAutomatedStoppingSpec {
        /// True if median automated stopping rule applies on
        /// [Measurement.elapsed_duration][google.cloud.aiplatform.v1.Measurement.elapsed_duration].
        /// It means that elapsed_duration field of latest measurement of current
        /// Trial is used to compute median objective value for each completed
        /// Trials.
        #[prost(bool, tag = "1")]
        pub use_elapsed_duration: bool,
    }
    /// Configuration for ConvexAutomatedStoppingSpec.
    /// When there are enough completed trials (configured by
    /// min_measurement_count), for pending trials with enough measurements and
    /// steps, the policy first computes an overestimate of the objective value at
    /// max_num_steps according to the slope of the incomplete objective value
    /// curve. No prediction can be made if the curve is completely flat. If the
    /// overestimation is worse than the best objective value of the completed
    /// trials, this pending trial will be early-stopped, but a last measurement
    /// will be added to the pending trial with max_num_steps and predicted
    /// objective value from the autoregression model.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ConvexAutomatedStoppingSpec {
        /// Steps used in predicting the final objective for early stopped trials. In
        /// general, it's set to be the same as the defined steps in training /
        /// tuning. If not defined, it will learn it from the completed trials. When
        /// use_steps is false, this field is set to the maximum elapsed seconds.
        #[prost(int64, tag = "1")]
        pub max_step_count: i64,
        /// Minimum number of steps for a trial to complete. Trials which do not have
        /// a measurement with step_count > min_step_count won't be considered for
        /// early stopping. It's ok to set it to 0, and a trial can be early stopped
        /// at any stage. By default, min_step_count is set to be one-tenth of the
        /// max_step_count.
        /// When use_elapsed_duration is true, this field is set to the minimum
        /// elapsed seconds.
        #[prost(int64, tag = "2")]
        pub min_step_count: i64,
        /// The minimal number of measurements in a Trial.  Early-stopping checks
        /// will not trigger if less than min_measurement_count+1 completed trials or
        /// pending trials with less than min_measurement_count measurements. If not
        /// defined, the default value is 5.
        #[prost(int64, tag = "3")]
        pub min_measurement_count: i64,
        /// The hyper-parameter name used in the tuning job that stands for learning
        /// rate. Leave it blank if learning rate is not in a parameter in tuning.
        /// The learning_rate is used to estimate the objective value of the ongoing
        /// trial.
        #[prost(string, tag = "4")]
        pub learning_rate_parameter_name: ::prost::alloc::string::String,
        /// This bool determines whether or not the rule is applied based on
        /// elapsed_secs or steps. If use_elapsed_duration==false, the early stopping
        /// decision is made according to the predicted objective values according to
        /// the target steps. If use_elapsed_duration==true, elapsed_secs is used
        /// instead of steps. Also, in this case, the parameters max_num_steps and
        /// min_num_steps are overloaded to contain max_elapsed_seconds and
        /// min_elapsed_seconds.
        #[prost(bool, tag = "5")]
        pub use_elapsed_duration: bool,
        /// ConvexAutomatedStoppingSpec by default only updates the trials that needs
        /// to be early stopped using a newly trained auto-regressive model. When
        /// this flag is set to True, all stopped trials from the beginning are
        /// potentially updated in terms of their `final_measurement`. Also, note
        /// that the training logic of autoregressive models is different in this
        /// case. Enabling this option has shown better results and this may be the
        /// default option in the future.
        #[prost(bool, optional, tag = "6")]
        pub update_all_stopped_trials: ::core::option::Option<bool>,
    }
    /// The configuration (stopping conditions) for automated stopping of a Study.
    /// Conditions include trial budgets, time budgets, and convergence detection.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct StudyStoppingConfig {
        /// If true, a Study enters STOPPING_ASAP whenever it would normally enters
        /// STOPPING state.
        ///
        /// The bottom line is: set to true if you want to interrupt on-going
        /// evaluations of Trials as soon as the study stopping condition is met.
        /// (Please see Study.State documentation for the source of truth).
        #[prost(message, optional, tag = "1")]
        pub should_stop_asap: ::core::option::Option<bool>,
        /// Each "stopping rule" in this proto specifies an "if" condition. Before
        /// Vizier would generate a new suggestion, it first checks each specified
        /// stopping rule, from top to bottom in this list.
        /// Note that the first few rules (e.g. minimum_runtime_constraint,
        /// min_num_trials) will prevent other stopping rules from being evaluated
        /// until they are met. For example, setting `min_num_trials=5` and
        /// `always_stop_after= 1 hour` means that the Study will ONLY stop after it
        /// has 5 COMPLETED trials, even if more than an hour has passed since its
        /// creation. It follows the first applicable rule (whose "if" condition is
        /// satisfied) to make a stopping decision. If none of the specified rules
        /// are applicable, then Vizier decides that the study should not stop.
        /// If Vizier decides that the study should stop, the study enters
        /// STOPPING state (or STOPPING_ASAP if should_stop_asap = true).
        /// IMPORTANT: The automatic study state transition happens precisely as
        /// described above; that is, deleting trials or updating StudyConfig NEVER
        /// automatically moves the study state back to ACTIVE. If you want to
        /// _resume_ a Study that was stopped, 1) change the stopping conditions if
        /// necessary, 2) activate the study, and then 3) ask for suggestions.
        /// If the specified time or duration has not passed, do not stop the
        /// study.
        #[prost(message, optional, tag = "2")]
        pub minimum_runtime_constraint: ::core::option::Option<
            super::StudyTimeConstraint,
        >,
        /// If the specified time or duration has passed, stop the study.
        #[prost(message, optional, tag = "3")]
        pub maximum_runtime_constraint: ::core::option::Option<
            super::StudyTimeConstraint,
        >,
        /// If there are fewer than this many COMPLETED trials, do not stop the
        /// study.
        #[prost(message, optional, tag = "4")]
        pub min_num_trials: ::core::option::Option<i32>,
        /// If there are more than this many trials, stop the study.
        #[prost(message, optional, tag = "5")]
        pub max_num_trials: ::core::option::Option<i32>,
        /// If the objective value has not improved for this many consecutive
        /// trials, stop the study.
        ///
        /// WARNING: Effective only for single-objective studies.
        #[prost(message, optional, tag = "6")]
        pub max_num_trials_no_progress: ::core::option::Option<i32>,
        /// If the objective value has not improved for this much time, stop the
        /// study.
        ///
        /// WARNING: Effective only for single-objective studies.
        #[prost(message, optional, tag = "7")]
        pub max_duration_no_progress: ::core::option::Option<::prost_types::Duration>,
    }
    /// The available search algorithms for the Study.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum Algorithm {
        /// The default algorithm used by Vertex AI for [hyperparameter
        /// tuning](<https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview>)
        /// and [Vertex AI Vizier](<https://cloud.google.com/vertex-ai/docs/vizier>).
        Unspecified = 0,
        /// Simple grid search within the feasible space. To use grid search,
        /// all parameters must be `INTEGER`, `CATEGORICAL`, or `DISCRETE`.
        GridSearch = 2,
        /// Simple random search within the feasible space.
        RandomSearch = 3,
    }
    impl Algorithm {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "ALGORITHM_UNSPECIFIED",
                Self::GridSearch => "GRID_SEARCH",
                Self::RandomSearch => "RANDOM_SEARCH",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "ALGORITHM_UNSPECIFIED" => Some(Self::Unspecified),
                "GRID_SEARCH" => Some(Self::GridSearch),
                "RANDOM_SEARCH" => Some(Self::RandomSearch),
                _ => None,
            }
        }
    }
    /// Describes the noise level of the repeated observations.
    ///
    /// "Noisy" means that the repeated observations with the same Trial parameters
    /// may lead to different metric evaluations.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum ObservationNoise {
        /// The default noise level chosen by Vertex AI.
        Unspecified = 0,
        /// Vertex AI assumes that the objective function is (nearly)
        /// perfectly reproducible, and will never repeat the same Trial
        /// parameters.
        Low = 1,
        /// Vertex AI will estimate the amount of noise in metric
        /// evaluations, it may repeat the same Trial parameters more than once.
        High = 2,
    }
    impl ObservationNoise {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "OBSERVATION_NOISE_UNSPECIFIED",
                Self::Low => "LOW",
                Self::High => "HIGH",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "OBSERVATION_NOISE_UNSPECIFIED" => Some(Self::Unspecified),
                "LOW" => Some(Self::Low),
                "HIGH" => Some(Self::High),
                _ => None,
            }
        }
    }
    /// This indicates which measurement to use if/when the service automatically
    /// selects the final measurement from previously reported intermediate
    /// measurements. Choose this based on two considerations:
    ///   A) Do you expect your measurements to monotonically improve?
    ///      If so, choose LAST_MEASUREMENT. On the other hand, if you're in a
    ///      situation where your system can "over-train" and you expect the
    ///      performance to get better for a while but then start declining,
    ///      choose BEST_MEASUREMENT.
    ///   B) Are your measurements significantly noisy and/or irreproducible?
    ///      If so, BEST_MEASUREMENT will tend to be over-optimistic, and it
    ///      may be better to choose LAST_MEASUREMENT.
    ///   If both or neither of (A) and (B) apply, it doesn't matter which
    ///   selection type is chosen.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum MeasurementSelectionType {
        /// Will be treated as LAST_MEASUREMENT.
        Unspecified = 0,
        /// Use the last measurement reported.
        LastMeasurement = 1,
        /// Use the best measurement reported.
        BestMeasurement = 2,
    }
    impl MeasurementSelectionType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "MEASUREMENT_SELECTION_TYPE_UNSPECIFIED",
                Self::LastMeasurement => "LAST_MEASUREMENT",
                Self::BestMeasurement => "BEST_MEASUREMENT",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "MEASUREMENT_SELECTION_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "LAST_MEASUREMENT" => Some(Self::LastMeasurement),
                "BEST_MEASUREMENT" => Some(Self::BestMeasurement),
                _ => None,
            }
        }
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum AutomatedStoppingSpec {
        /// The automated early stopping spec using decay curve rule.
        #[prost(message, tag = "4")]
        DecayCurveStoppingSpec(DecayCurveAutomatedStoppingSpec),
        /// The automated early stopping spec using median rule.
        #[prost(message, tag = "5")]
        MedianAutomatedStoppingSpec(MedianAutomatedStoppingSpec),
        /// The automated early stopping spec using convex stopping rule.
        #[prost(message, tag = "9")]
        ConvexAutomatedStoppingSpec(ConvexAutomatedStoppingSpec),
    }
}
/// A message representing a Measurement of a Trial. A Measurement contains
/// the Metrics got by executing a Trial using suggested hyperparameter
/// values.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Measurement {
    /// Output only. Time that the Trial has been running at the point of this
    /// Measurement.
    #[prost(message, optional, tag = "1")]
    pub elapsed_duration: ::core::option::Option<::prost_types::Duration>,
    /// Output only. The number of steps the machine learning model has been
    /// trained for. Must be non-negative.
    #[prost(int64, tag = "2")]
    pub step_count: i64,
    /// Output only. A list of metrics got by evaluating the objective functions
    /// using suggested Parameter values.
    #[prost(message, repeated, tag = "3")]
    pub metrics: ::prost::alloc::vec::Vec<measurement::Metric>,
}
/// Nested message and enum types in `Measurement`.
pub mod measurement {
    /// A message representing a metric in the measurement.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Metric {
        /// Output only. The ID of the Metric. The Metric should be defined in
        /// [StudySpec's Metrics][google.cloud.aiplatform.v1.StudySpec.metrics].
        #[prost(string, tag = "1")]
        pub metric_id: ::prost::alloc::string::String,
        /// Output only. The value for this metric.
        #[prost(double, tag = "2")]
        pub value: f64,
    }
}
/// Represents a HyperparameterTuningJob. A HyperparameterTuningJob
/// has a Study specification and multiple CustomJobs with identical
/// CustomJob specification.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct HyperparameterTuningJob {
    /// Output only. Resource name of the HyperparameterTuningJob.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The display name of the HyperparameterTuningJob.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Required. Study configuration of the HyperparameterTuningJob.
    #[prost(message, optional, tag = "4")]
    pub study_spec: ::core::option::Option<StudySpec>,
    /// Required. The desired total number of Trials.
    #[prost(int32, tag = "5")]
    pub max_trial_count: i32,
    /// Required. The desired number of Trials to run in parallel.
    #[prost(int32, tag = "6")]
    pub parallel_trial_count: i32,
    /// The number of failed Trials that need to be seen before failing
    /// the HyperparameterTuningJob.
    ///
    /// If set to 0, Vertex AI decides how many Trials must fail
    /// before the whole job fails.
    #[prost(int32, tag = "7")]
    pub max_failed_trial_count: i32,
    /// Required. The spec of a trial job. The same spec applies to the CustomJobs
    /// created in all the trials.
    #[prost(message, optional, tag = "8")]
    pub trial_job_spec: ::core::option::Option<CustomJobSpec>,
    /// Output only. Trials of the HyperparameterTuningJob.
    #[prost(message, repeated, tag = "9")]
    pub trials: ::prost::alloc::vec::Vec<Trial>,
    /// Output only. The detailed state of the job.
    #[prost(enumeration = "JobState", tag = "10")]
    pub state: i32,
    /// Output only. Time when the HyperparameterTuningJob was created.
    #[prost(message, optional, tag = "11")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the HyperparameterTuningJob for the first time
    /// entered the `JOB_STATE_RUNNING` state.
    #[prost(message, optional, tag = "12")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the HyperparameterTuningJob entered any of the
    /// following states: `JOB_STATE_SUCCEEDED`, `JOB_STATE_FAILED`,
    /// `JOB_STATE_CANCELLED`.
    #[prost(message, optional, tag = "13")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the HyperparameterTuningJob was most recently
    /// updated.
    #[prost(message, optional, tag = "14")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Only populated when job's state is JOB_STATE_FAILED or
    /// JOB_STATE_CANCELLED.
    #[prost(message, optional, tag = "15")]
    pub error: ::core::option::Option<super::super::super::rpc::Status>,
    /// The labels with user-defined metadata to organize HyperparameterTuningJobs.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "16")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Customer-managed encryption key options for a HyperparameterTuningJob.
    /// If this is set, then all resources created by the HyperparameterTuningJob
    /// will be encrypted with the provided encryption key.
    #[prost(message, optional, tag = "17")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "19")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "20")]
    pub satisfies_pzi: bool,
}
/// A representation of a collection of database items organized in a way that
/// allows for approximate nearest neighbor (a.k.a ANN) algorithms search.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Index {
    /// Output only. The resource name of the Index.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The display name of the Index.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// The description of the Index.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// Immutable. Points to a YAML file stored on Google Cloud Storage describing
    /// additional information about the Index, that is specific to it. Unset if
    /// the Index does not have any additional information. The schema is defined
    /// as an OpenAPI 3.0.2 [Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    /// Note: The URI given on output will be immutable and probably different,
    /// including the URI scheme, than the one given on input. The output URI will
    /// point to a location where the user only has a read access.
    #[prost(string, tag = "4")]
    pub metadata_schema_uri: ::prost::alloc::string::String,
    /// An additional information about the Index; the schema of the metadata can
    /// be found in
    /// [metadata_schema][google.cloud.aiplatform.v1.Index.metadata_schema_uri].
    #[prost(message, optional, tag = "6")]
    pub metadata: ::core::option::Option<::prost_types::Value>,
    /// Output only. The pointers to DeployedIndexes created from this Index.
    /// An Index can be only deleted if all its DeployedIndexes had been undeployed
    /// first.
    #[prost(message, repeated, tag = "7")]
    pub deployed_indexes: ::prost::alloc::vec::Vec<DeployedIndexRef>,
    /// Used to perform consistent read-modify-write updates. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "8")]
    pub etag: ::prost::alloc::string::String,
    /// The labels with user-defined metadata to organize your Indexes.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "9")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Output only. Timestamp when this Index was created.
    #[prost(message, optional, tag = "10")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Index was most recently updated.
    /// This also includes any update to the contents of the Index.
    /// Note that Operations working on this Index may have their
    /// \[Operations.metadata.generic_metadata.update_time\]
    /// \[google.cloud.aiplatform.v1.GenericOperationMetadata.update_time\] a little
    /// after the value of this timestamp, yet that does not mean their results are
    /// not already reflected in the Index. Result of any successfully completed
    /// Operation on the Index is reflected in it.
    #[prost(message, optional, tag = "11")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Stats of the index resource.
    #[prost(message, optional, tag = "14")]
    pub index_stats: ::core::option::Option<IndexStats>,
    /// Immutable. The update method to use with this Index. If not set,
    /// BATCH_UPDATE will be used by default.
    #[prost(enumeration = "index::IndexUpdateMethod", tag = "16")]
    pub index_update_method: i32,
    /// Immutable. Customer-managed encryption key spec for an Index. If set, this
    /// Index and all sub-resources of this Index will be secured by this key.
    #[prost(message, optional, tag = "17")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "18")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "19")]
    pub satisfies_pzi: bool,
}
/// Nested message and enum types in `Index`.
pub mod index {
    /// The update method of an Index.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum IndexUpdateMethod {
        /// Should not be used.
        Unspecified = 0,
        /// BatchUpdate: user can call UpdateIndex with files on Cloud Storage of
        /// Datapoints to update.
        BatchUpdate = 1,
        /// StreamUpdate: user can call UpsertDatapoints/DeleteDatapoints to update
        /// the Index and the updates will be applied in corresponding
        /// DeployedIndexes in nearly real-time.
        StreamUpdate = 2,
    }
    impl IndexUpdateMethod {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "INDEX_UPDATE_METHOD_UNSPECIFIED",
                Self::BatchUpdate => "BATCH_UPDATE",
                Self::StreamUpdate => "STREAM_UPDATE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "INDEX_UPDATE_METHOD_UNSPECIFIED" => Some(Self::Unspecified),
                "BATCH_UPDATE" => Some(Self::BatchUpdate),
                "STREAM_UPDATE" => Some(Self::StreamUpdate),
                _ => None,
            }
        }
    }
}
/// A datapoint of Index.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct IndexDatapoint {
    /// Required. Unique identifier of the datapoint.
    #[prost(string, tag = "1")]
    pub datapoint_id: ::prost::alloc::string::String,
    /// Required. Feature embedding vector for dense index. An array of numbers
    /// with the length of \[NearestNeighborSearchConfig.dimensions\].
    #[prost(float, repeated, packed = "false", tag = "2")]
    pub feature_vector: ::prost::alloc::vec::Vec<f32>,
    /// Optional. Feature embedding vector for sparse index.
    #[prost(message, optional, tag = "7")]
    pub sparse_embedding: ::core::option::Option<index_datapoint::SparseEmbedding>,
    /// Optional. List of Restrict of the datapoint, used to perform "restricted
    /// searches" where boolean rule are used to filter the subset of the database
    /// eligible for matching. This uses categorical tokens. See:
    /// <https://cloud.google.com/vertex-ai/docs/matching-engine/filtering>
    #[prost(message, repeated, tag = "4")]
    pub restricts: ::prost::alloc::vec::Vec<index_datapoint::Restriction>,
    /// Optional. List of Restrict of the datapoint, used to perform "restricted
    /// searches" where boolean rule are used to filter the subset of the database
    /// eligible for matching. This uses numeric comparisons.
    #[prost(message, repeated, tag = "6")]
    pub numeric_restricts: ::prost::alloc::vec::Vec<index_datapoint::NumericRestriction>,
    /// Optional. CrowdingTag of the datapoint, the number of neighbors to return
    /// in each crowding can be configured during query.
    #[prost(message, optional, tag = "5")]
    pub crowding_tag: ::core::option::Option<index_datapoint::CrowdingTag>,
}
/// Nested message and enum types in `IndexDatapoint`.
pub mod index_datapoint {
    /// Feature embedding vector for sparse index. An array of numbers whose values
    /// are located in the specified dimensions.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct SparseEmbedding {
        /// Required. The list of embedding values of the sparse vector.
        #[prost(float, repeated, packed = "false", tag = "1")]
        pub values: ::prost::alloc::vec::Vec<f32>,
        /// Required. The list of indexes for the embedding values of the sparse
        /// vector.
        #[prost(int64, repeated, packed = "false", tag = "2")]
        pub dimensions: ::prost::alloc::vec::Vec<i64>,
    }
    /// Restriction of a datapoint which describe its attributes(tokens) from each
    /// of several attribute categories(namespaces).
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Restriction {
        /// The namespace of this restriction. e.g.: color.
        #[prost(string, tag = "1")]
        pub namespace: ::prost::alloc::string::String,
        /// The attributes to allow in this namespace. e.g.: 'red'
        #[prost(string, repeated, tag = "2")]
        pub allow_list: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        /// The attributes to deny in this namespace. e.g.: 'blue'
        #[prost(string, repeated, tag = "3")]
        pub deny_list: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    }
    /// This field allows restricts to be based on numeric comparisons rather
    /// than categorical tokens.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct NumericRestriction {
        /// The namespace of this restriction. e.g.: cost.
        #[prost(string, tag = "1")]
        pub namespace: ::prost::alloc::string::String,
        /// This MUST be specified for queries and must NOT be specified for
        /// datapoints.
        #[prost(enumeration = "numeric_restriction::Operator", tag = "5")]
        pub op: i32,
        /// The type of Value must be consistent for all datapoints with a given
        /// namespace name. This is verified at runtime.
        #[prost(oneof = "numeric_restriction::Value", tags = "2, 3, 4")]
        pub value: ::core::option::Option<numeric_restriction::Value>,
    }
    /// Nested message and enum types in `NumericRestriction`.
    pub mod numeric_restriction {
        /// Which comparison operator to use.  Should be specified for queries only;
        /// specifying this for a datapoint is an error.
        ///
        /// Datapoints for which Operator is true relative to the query's Value
        /// field will be allowlisted.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum Operator {
            /// Default value of the enum.
            Unspecified = 0,
            /// Datapoints are eligible iff their value is < the query's.
            Less = 1,
            /// Datapoints are eligible iff their value is <= the query's.
            LessEqual = 2,
            /// Datapoints are eligible iff their value is == the query's.
            Equal = 3,
            /// Datapoints are eligible iff their value is >= the query's.
            GreaterEqual = 4,
            /// Datapoints are eligible iff their value is > the query's.
            Greater = 5,
            /// Datapoints are eligible iff their value is != the query's.
            NotEqual = 6,
        }
        impl Operator {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "OPERATOR_UNSPECIFIED",
                    Self::Less => "LESS",
                    Self::LessEqual => "LESS_EQUAL",
                    Self::Equal => "EQUAL",
                    Self::GreaterEqual => "GREATER_EQUAL",
                    Self::Greater => "GREATER",
                    Self::NotEqual => "NOT_EQUAL",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "OPERATOR_UNSPECIFIED" => Some(Self::Unspecified),
                    "LESS" => Some(Self::Less),
                    "LESS_EQUAL" => Some(Self::LessEqual),
                    "EQUAL" => Some(Self::Equal),
                    "GREATER_EQUAL" => Some(Self::GreaterEqual),
                    "GREATER" => Some(Self::Greater),
                    "NOT_EQUAL" => Some(Self::NotEqual),
                    _ => None,
                }
            }
        }
        /// The type of Value must be consistent for all datapoints with a given
        /// namespace name. This is verified at runtime.
        #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
        pub enum Value {
            /// Represents 64 bit integer.
            #[prost(int64, tag = "2")]
            ValueInt(i64),
            /// Represents 32 bit float.
            #[prost(float, tag = "3")]
            ValueFloat(f32),
            /// Represents 64 bit float.
            #[prost(double, tag = "4")]
            ValueDouble(f64),
        }
    }
    /// Crowding tag is a constraint on a neighbor list produced by nearest
    /// neighbor search requiring that no more than some value k' of the k
    /// neighbors returned have the same value of crowding_attribute.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct CrowdingTag {
        /// The attribute value used for crowding.  The maximum number of neighbors
        /// to return per crowding attribute value
        /// (per_crowding_attribute_num_neighbors) is configured per-query. This
        /// field is ignored if per_crowding_attribute_num_neighbors is larger than
        /// the total number of neighbors to return for a given query.
        #[prost(string, tag = "1")]
        pub crowding_attribute: ::prost::alloc::string::String,
    }
}
/// Stats of the Index.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct IndexStats {
    /// Output only. The number of dense vectors in the Index.
    #[prost(int64, tag = "1")]
    pub vectors_count: i64,
    /// Output only. The number of sparse vectors in the Index.
    #[prost(int64, tag = "3")]
    pub sparse_vectors_count: i64,
    /// Output only. The number of shards in the Index.
    #[prost(int32, tag = "2")]
    pub shards_count: i32,
}
/// Indexes are deployed into it. An IndexEndpoint can have multiple
/// DeployedIndexes.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct IndexEndpoint {
    /// Output only. The resource name of the IndexEndpoint.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The display name of the IndexEndpoint.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// The description of the IndexEndpoint.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// Output only. The indexes deployed in this endpoint.
    #[prost(message, repeated, tag = "4")]
    pub deployed_indexes: ::prost::alloc::vec::Vec<DeployedIndex>,
    /// Used to perform consistent read-modify-write updates. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "5")]
    pub etag: ::prost::alloc::string::String,
    /// The labels with user-defined metadata to organize your IndexEndpoints.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "6")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Output only. Timestamp when this IndexEndpoint was created.
    #[prost(message, optional, tag = "7")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this IndexEndpoint was last updated.
    /// This timestamp is not updated when the endpoint's DeployedIndexes are
    /// updated, e.g. due to updates of the original Indexes they are the
    /// deployments of.
    #[prost(message, optional, tag = "8")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. The full name of the Google Compute Engine
    /// [network](<https://cloud.google.com/compute/docs/networks-and-firewalls#networks>)
    /// to which the IndexEndpoint should be peered.
    ///
    /// Private services access must already be configured for the network. If left
    /// unspecified, the Endpoint is not peered with any network.
    ///
    /// [network][google.cloud.aiplatform.v1.IndexEndpoint.network] and
    /// [private_service_connect_config][google.cloud.aiplatform.v1.IndexEndpoint.private_service_connect_config]
    /// are mutually exclusive.
    ///
    /// [Format](<https://cloud.google.com/compute/docs/reference/rest/v1/networks/insert>):
    /// `projects/{project}/global/networks/{network}`.
    /// Where {project} is a project number, as in '12345', and {network} is
    /// network name.
    #[prost(string, tag = "9")]
    pub network: ::prost::alloc::string::String,
    /// Optional. Deprecated: If true, expose the IndexEndpoint via private service
    /// connect.
    ///
    /// Only one of the fields,
    /// [network][google.cloud.aiplatform.v1.IndexEndpoint.network] or
    /// [enable_private_service_connect][google.cloud.aiplatform.v1.IndexEndpoint.enable_private_service_connect],
    /// can be set.
    #[deprecated]
    #[prost(bool, tag = "10")]
    pub enable_private_service_connect: bool,
    /// Optional. Configuration for private service connect.
    ///
    /// [network][google.cloud.aiplatform.v1.IndexEndpoint.network] and
    /// [private_service_connect_config][google.cloud.aiplatform.v1.IndexEndpoint.private_service_connect_config]
    /// are mutually exclusive.
    #[prost(message, optional, tag = "12")]
    pub private_service_connect_config: ::core::option::Option<
        PrivateServiceConnectConfig,
    >,
    /// Optional. If true, the deployed index will be accessible through public
    /// endpoint.
    #[prost(bool, tag = "13")]
    pub public_endpoint_enabled: bool,
    /// Output only. If
    /// [public_endpoint_enabled][google.cloud.aiplatform.v1.IndexEndpoint.public_endpoint_enabled]
    /// is true, this field will be populated with the domain name to use for this
    /// index endpoint.
    #[prost(string, tag = "14")]
    pub public_endpoint_domain_name: ::prost::alloc::string::String,
    /// Immutable. Customer-managed encryption key spec for an IndexEndpoint. If
    /// set, this IndexEndpoint and all sub-resources of this IndexEndpoint will be
    /// secured by this key.
    #[prost(message, optional, tag = "15")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "17")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "18")]
    pub satisfies_pzi: bool,
}
/// A deployment of an Index. IndexEndpoints contain one or more DeployedIndexes.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeployedIndex {
    /// Required. The user specified ID of the DeployedIndex.
    /// The ID can be up to 128 characters long and must start with a letter and
    /// only contain letters, numbers, and underscores.
    /// The ID must be unique within the project it is created in.
    #[prost(string, tag = "1")]
    pub id: ::prost::alloc::string::String,
    /// Required. The name of the Index this is the deployment of.
    /// We may refer to this Index as the DeployedIndex's "original" Index.
    #[prost(string, tag = "2")]
    pub index: ::prost::alloc::string::String,
    /// The display name of the DeployedIndex. If not provided upon creation,
    /// the Index's display_name is used.
    #[prost(string, tag = "3")]
    pub display_name: ::prost::alloc::string::String,
    /// Output only. Timestamp when the DeployedIndex was created.
    #[prost(message, optional, tag = "4")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Provides paths for users to send requests directly to the
    /// deployed index services running on Cloud via private services access. This
    /// field is populated if
    /// [network][google.cloud.aiplatform.v1.IndexEndpoint.network] is configured.
    #[prost(message, optional, tag = "5")]
    pub private_endpoints: ::core::option::Option<IndexPrivateEndpoints>,
    /// Output only. The DeployedIndex may depend on various data on its original
    /// Index. Additionally when certain changes to the original Index are being
    /// done (e.g. when what the Index contains is being changed) the DeployedIndex
    /// may be asynchronously updated in the background to reflect these changes.
    /// If this timestamp's value is at least the
    /// [Index.update_time][google.cloud.aiplatform.v1.Index.update_time] of the
    /// original Index, it means that this DeployedIndex and the original Index are
    /// in sync. If this timestamp is older, then to see which updates this
    /// DeployedIndex already contains (and which it does not), one must
    /// [list][google.longrunning.Operations.ListOperations] the operations that
    /// are running on the original Index. Only the successfully completed
    /// Operations with
    /// [update_time][google.cloud.aiplatform.v1.GenericOperationMetadata.update_time]
    /// equal or before this sync time are contained in this DeployedIndex.
    #[prost(message, optional, tag = "6")]
    pub index_sync_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. A description of resources that the DeployedIndex uses, which to
    /// large degree are decided by Vertex AI, and optionally allows only a modest
    /// additional configuration.
    /// If min_replica_count is not set, the default value is 2 (we don't provide
    /// SLA when min_replica_count=1). If max_replica_count is not set, the
    /// default value is min_replica_count. The max allowed replica count is
    /// 1000.
    #[prost(message, optional, tag = "7")]
    pub automatic_resources: ::core::option::Option<AutomaticResources>,
    /// Optional. A description of resources that are dedicated to the
    /// DeployedIndex, and that need a higher degree of manual configuration. The
    /// field min_replica_count must be set to a value strictly greater than 0, or
    /// else validation will fail. We don't provide SLA when min_replica_count=1.
    /// If max_replica_count is not set, the default value is min_replica_count.
    /// The max allowed replica count is 1000.
    ///
    /// Available machine types for SMALL shard:
    /// e2-standard-2 and all machine types available for MEDIUM and LARGE shard.
    ///
    /// Available machine types for MEDIUM shard:
    /// e2-standard-16 and all machine types available for LARGE shard.
    ///
    /// Available machine types for LARGE shard:
    /// e2-highmem-16, n2d-standard-32.
    ///
    /// n1-standard-16 and n1-standard-32 are still available, but we recommend
    /// e2-standard-16 and e2-highmem-16 for cost efficiency.
    #[prost(message, optional, tag = "16")]
    pub dedicated_resources: ::core::option::Option<DedicatedResources>,
    /// Optional. If true, private endpoint's access logs are sent to Cloud
    /// Logging.
    ///
    /// These logs are like standard server access logs, containing
    /// information like timestamp and latency for each MatchRequest.
    ///
    /// Note that logs may incur a cost, especially if the deployed
    /// index receives a high queries per second rate (QPS).
    /// Estimate your costs before enabling this option.
    #[prost(bool, tag = "8")]
    pub enable_access_logging: bool,
    /// Optional. If set, the authentication is enabled for the private endpoint.
    #[prost(message, optional, tag = "9")]
    pub deployed_index_auth_config: ::core::option::Option<DeployedIndexAuthConfig>,
    /// Optional. A list of reserved ip ranges under the VPC network that can be
    /// used for this DeployedIndex.
    ///
    /// If set, we will deploy the index within the provided ip ranges. Otherwise,
    /// the index might be deployed to any ip ranges under the provided VPC
    /// network.
    ///
    /// The value should be the name of the address
    /// (<https://cloud.google.com/compute/docs/reference/rest/v1/addresses>)
    /// Example: \['vertex-ai-ip-range'\].
    ///
    /// For more information about subnets and network IP ranges, please see
    /// <https://cloud.google.com/vpc/docs/subnets#manually_created_subnet_ip_ranges.>
    #[prost(string, repeated, tag = "10")]
    pub reserved_ip_ranges: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. The deployment group can be no longer than 64 characters (eg:
    /// 'test', 'prod'). If not set, we will use the 'default' deployment group.
    ///
    /// Creating `deployment_groups` with `reserved_ip_ranges` is a recommended
    /// practice when the peered network has multiple peering ranges. This creates
    /// your deployments from predictable IP spaces for easier traffic
    /// administration. Also, one deployment_group (except 'default') can only be
    /// used with the same reserved_ip_ranges which means if the deployment_group
    /// has been used with reserved_ip_ranges: \[a, b, c\], using it with \[a, b\] or
    /// \[d, e\] is disallowed.
    ///
    /// Note: we only support up to 5 deployment groups(not including 'default').
    #[prost(string, tag = "11")]
    pub deployment_group: ::prost::alloc::string::String,
    /// Optional. If set for PSC deployed index, PSC connection will be
    /// automatically created after deployment is done and the endpoint information
    /// is populated in private_endpoints.psc_automated_endpoints.
    #[prost(message, repeated, tag = "19")]
    pub psc_automation_configs: ::prost::alloc::vec::Vec<PscAutomationConfig>,
}
/// Used to set up the auth on the DeployedIndex's private endpoint.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeployedIndexAuthConfig {
    /// Defines the authentication provider that the DeployedIndex uses.
    #[prost(message, optional, tag = "1")]
    pub auth_provider: ::core::option::Option<deployed_index_auth_config::AuthProvider>,
}
/// Nested message and enum types in `DeployedIndexAuthConfig`.
pub mod deployed_index_auth_config {
    /// Configuration for an authentication provider, including support for
    /// [JSON Web Token
    /// (JWT)](<https://tools.ietf.org/html/draft-ietf-oauth-json-web-token-32>).
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct AuthProvider {
        /// The list of JWT
        /// [audiences](<https://tools.ietf.org/html/draft-ietf-oauth-json-web-token-32#section-4.1.3>).
        /// that are allowed to access. A JWT containing any of these audiences will
        /// be accepted.
        #[prost(string, repeated, tag = "1")]
        pub audiences: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        /// A list of allowed JWT issuers. Each entry must be a valid Google
        /// service account, in the following format:
        ///
        /// `service-account-name@project-id.iam.gserviceaccount.com`
        #[prost(string, repeated, tag = "2")]
        pub allowed_issuers: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    }
}
/// IndexPrivateEndpoints proto is used to provide paths for users to send
/// requests via private endpoints (e.g. private service access, private service
/// connect).
/// To send request via private service access, use match_grpc_address.
/// To send request via private service connect, use service_attachment.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct IndexPrivateEndpoints {
    /// Output only. The ip address used to send match gRPC requests.
    #[prost(string, tag = "1")]
    pub match_grpc_address: ::prost::alloc::string::String,
    /// Output only. The name of the service attachment resource. Populated if
    /// private service connect is enabled.
    #[prost(string, tag = "2")]
    pub service_attachment: ::prost::alloc::string::String,
    /// Output only. PscAutomatedEndpoints is populated if private service connect
    /// is enabled if PscAutomatedConfig is set.
    #[prost(message, repeated, tag = "3")]
    pub psc_automated_endpoints: ::prost::alloc::vec::Vec<PscAutomatedEndpoints>,
}
/// Request message for
/// [IndexEndpointService.CreateIndexEndpoint][google.cloud.aiplatform.v1.IndexEndpointService.CreateIndexEndpoint].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateIndexEndpointRequest {
    /// Required. The resource name of the Location to create the IndexEndpoint in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The IndexEndpoint to create.
    #[prost(message, optional, tag = "2")]
    pub index_endpoint: ::core::option::Option<IndexEndpoint>,
}
/// Runtime operation information for
/// [IndexEndpointService.CreateIndexEndpoint][google.cloud.aiplatform.v1.IndexEndpointService.CreateIndexEndpoint].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateIndexEndpointOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [IndexEndpointService.GetIndexEndpoint][google.cloud.aiplatform.v1.IndexEndpointService.GetIndexEndpoint]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetIndexEndpointRequest {
    /// Required. The name of the IndexEndpoint resource.
    /// Format:
    /// `projects/{project}/locations/{location}/indexEndpoints/{index_endpoint}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [IndexEndpointService.ListIndexEndpoints][google.cloud.aiplatform.v1.IndexEndpointService.ListIndexEndpoints].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListIndexEndpointsRequest {
    /// Required. The resource name of the Location from which to list the
    /// IndexEndpoints. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. An expression for filtering the results of the request. For field
    /// names both snake_case and camelCase are supported.
    ///
    ///    * `index_endpoint` supports = and !=. `index_endpoint` represents the
    ///       IndexEndpoint ID, ie. the last segment of the IndexEndpoint's
    ///       [resourcename][google.cloud.aiplatform.v1.IndexEndpoint.name].
    ///    * `display_name` supports =, != and regex()
    ///              (uses [re2](<https://github.com/google/re2/wiki/Syntax>) syntax)
    ///    * `labels` supports general map functions that is:
    ///              `labels.key=value` - key:value equality
    ///              `labels.key:* or labels:key - key existence
    ///               A key including a space must be quoted. `labels."a key"`.
    ///
    /// Some examples:
    ///    * `index_endpoint="1"`
    ///    * `display_name="myDisplayName"`
    ///    * `regex(display_name, "^A") -> The display name starts with an A.
    ///    * `labels.myKey="myValue"`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// Optional. The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// Optional. The standard list page token.
    /// Typically obtained via
    /// [ListIndexEndpointsResponse.next_page_token][google.cloud.aiplatform.v1.ListIndexEndpointsResponse.next_page_token]
    /// of the previous
    /// [IndexEndpointService.ListIndexEndpoints][google.cloud.aiplatform.v1.IndexEndpointService.ListIndexEndpoints]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Optional. Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [IndexEndpointService.ListIndexEndpoints][google.cloud.aiplatform.v1.IndexEndpointService.ListIndexEndpoints].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListIndexEndpointsResponse {
    /// List of IndexEndpoints in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub index_endpoints: ::prost::alloc::vec::Vec<IndexEndpoint>,
    /// A token to retrieve next page of results.
    /// Pass to
    /// [ListIndexEndpointsRequest.page_token][google.cloud.aiplatform.v1.ListIndexEndpointsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [IndexEndpointService.UpdateIndexEndpoint][google.cloud.aiplatform.v1.IndexEndpointService.UpdateIndexEndpoint].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateIndexEndpointRequest {
    /// Required. The IndexEndpoint which replaces the resource on the server.
    #[prost(message, optional, tag = "1")]
    pub index_endpoint: ::core::option::Option<IndexEndpoint>,
    /// Required. The update mask applies to the resource. See
    /// [google.protobuf.FieldMask][google.protobuf.FieldMask].
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [IndexEndpointService.DeleteIndexEndpoint][google.cloud.aiplatform.v1.IndexEndpointService.DeleteIndexEndpoint].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteIndexEndpointRequest {
    /// Required. The name of the IndexEndpoint resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/indexEndpoints/{index_endpoint}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [IndexEndpointService.DeployIndex][google.cloud.aiplatform.v1.IndexEndpointService.DeployIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeployIndexRequest {
    /// Required. The name of the IndexEndpoint resource into which to deploy an
    /// Index. Format:
    /// `projects/{project}/locations/{location}/indexEndpoints/{index_endpoint}`
    #[prost(string, tag = "1")]
    pub index_endpoint: ::prost::alloc::string::String,
    /// Required. The DeployedIndex to be created within the IndexEndpoint.
    #[prost(message, optional, tag = "2")]
    pub deployed_index: ::core::option::Option<DeployedIndex>,
}
/// Response message for
/// [IndexEndpointService.DeployIndex][google.cloud.aiplatform.v1.IndexEndpointService.DeployIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeployIndexResponse {
    /// The DeployedIndex that had been deployed in the IndexEndpoint.
    #[prost(message, optional, tag = "1")]
    pub deployed_index: ::core::option::Option<DeployedIndex>,
}
/// Runtime operation information for
/// [IndexEndpointService.DeployIndex][google.cloud.aiplatform.v1.IndexEndpointService.DeployIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeployIndexOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// The unique index id specified by user
    #[prost(string, tag = "2")]
    pub deployed_index_id: ::prost::alloc::string::String,
}
/// Request message for
/// [IndexEndpointService.UndeployIndex][google.cloud.aiplatform.v1.IndexEndpointService.UndeployIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UndeployIndexRequest {
    /// Required. The name of the IndexEndpoint resource from which to undeploy an
    /// Index. Format:
    /// `projects/{project}/locations/{location}/indexEndpoints/{index_endpoint}`
    #[prost(string, tag = "1")]
    pub index_endpoint: ::prost::alloc::string::String,
    /// Required. The ID of the DeployedIndex to be undeployed from the
    /// IndexEndpoint.
    #[prost(string, tag = "2")]
    pub deployed_index_id: ::prost::alloc::string::String,
}
/// Response message for
/// [IndexEndpointService.UndeployIndex][google.cloud.aiplatform.v1.IndexEndpointService.UndeployIndex].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct UndeployIndexResponse {}
/// Runtime operation information for
/// [IndexEndpointService.UndeployIndex][google.cloud.aiplatform.v1.IndexEndpointService.UndeployIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UndeployIndexOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [IndexEndpointService.MutateDeployedIndex][google.cloud.aiplatform.v1.IndexEndpointService.MutateDeployedIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MutateDeployedIndexRequest {
    /// Required. The name of the IndexEndpoint resource into which to deploy an
    /// Index. Format:
    /// `projects/{project}/locations/{location}/indexEndpoints/{index_endpoint}`
    #[prost(string, tag = "1")]
    pub index_endpoint: ::prost::alloc::string::String,
    /// Required. The DeployedIndex to be updated within the IndexEndpoint.
    /// Currently, the updatable fields are
    /// [DeployedIndex.automatic_resources][google.cloud.aiplatform.v1.DeployedIndex.automatic_resources]
    /// and
    /// [DeployedIndex.dedicated_resources][google.cloud.aiplatform.v1.DeployedIndex.dedicated_resources]
    #[prost(message, optional, tag = "2")]
    pub deployed_index: ::core::option::Option<DeployedIndex>,
}
/// Response message for
/// [IndexEndpointService.MutateDeployedIndex][google.cloud.aiplatform.v1.IndexEndpointService.MutateDeployedIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MutateDeployedIndexResponse {
    /// The DeployedIndex that had been updated in the IndexEndpoint.
    #[prost(message, optional, tag = "1")]
    pub deployed_index: ::core::option::Option<DeployedIndex>,
}
/// Runtime operation information for
/// [IndexEndpointService.MutateDeployedIndex][google.cloud.aiplatform.v1.IndexEndpointService.MutateDeployedIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MutateDeployedIndexOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// The unique index id specified by user
    #[prost(string, tag = "2")]
    pub deployed_index_id: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod index_endpoint_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for managing Vertex AI's IndexEndpoints.
    #[derive(Debug, Clone)]
    pub struct IndexEndpointServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl IndexEndpointServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> IndexEndpointServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> IndexEndpointServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            IndexEndpointServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates an IndexEndpoint.
        pub async fn create_index_endpoint(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateIndexEndpointRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexEndpointService/CreateIndexEndpoint",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexEndpointService",
                        "CreateIndexEndpoint",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets an IndexEndpoint.
        pub async fn get_index_endpoint(
            &mut self,
            request: impl tonic::IntoRequest<super::GetIndexEndpointRequest>,
        ) -> std::result::Result<tonic::Response<super::IndexEndpoint>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexEndpointService/GetIndexEndpoint",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexEndpointService",
                        "GetIndexEndpoint",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists IndexEndpoints in a Location.
        pub async fn list_index_endpoints(
            &mut self,
            request: impl tonic::IntoRequest<super::ListIndexEndpointsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListIndexEndpointsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexEndpointService/ListIndexEndpoints",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexEndpointService",
                        "ListIndexEndpoints",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates an IndexEndpoint.
        pub async fn update_index_endpoint(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateIndexEndpointRequest>,
        ) -> std::result::Result<tonic::Response<super::IndexEndpoint>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexEndpointService/UpdateIndexEndpoint",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexEndpointService",
                        "UpdateIndexEndpoint",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes an IndexEndpoint.
        pub async fn delete_index_endpoint(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteIndexEndpointRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexEndpointService/DeleteIndexEndpoint",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexEndpointService",
                        "DeleteIndexEndpoint",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deploys an Index into this IndexEndpoint, creating a DeployedIndex within
        /// it.
        /// Only non-empty Indexes can be deployed.
        pub async fn deploy_index(
            &mut self,
            request: impl tonic::IntoRequest<super::DeployIndexRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexEndpointService/DeployIndex",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexEndpointService",
                        "DeployIndex",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Undeploys an Index from an IndexEndpoint, removing a DeployedIndex from it,
        /// and freeing all resources it's using.
        pub async fn undeploy_index(
            &mut self,
            request: impl tonic::IntoRequest<super::UndeployIndexRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexEndpointService/UndeployIndex",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexEndpointService",
                        "UndeployIndex",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Update an existing DeployedIndex under an IndexEndpoint.
        pub async fn mutate_deployed_index(
            &mut self,
            request: impl tonic::IntoRequest<super::MutateDeployedIndexRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexEndpointService/MutateDeployedIndex",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexEndpointService",
                        "MutateDeployedIndex",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Request message for
/// [IndexService.CreateIndex][google.cloud.aiplatform.v1.IndexService.CreateIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateIndexRequest {
    /// Required. The resource name of the Location to create the Index in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The Index to create.
    #[prost(message, optional, tag = "2")]
    pub index: ::core::option::Option<Index>,
}
/// Runtime operation information for
/// [IndexService.CreateIndex][google.cloud.aiplatform.v1.IndexService.CreateIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateIndexOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// The operation metadata with regard to Matching Engine Index operation.
    #[prost(message, optional, tag = "2")]
    pub nearest_neighbor_search_operation_metadata: ::core::option::Option<
        NearestNeighborSearchOperationMetadata,
    >,
}
/// Request message for
/// [IndexService.GetIndex][google.cloud.aiplatform.v1.IndexService.GetIndex]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetIndexRequest {
    /// Required. The name of the Index resource.
    /// Format:
    /// `projects/{project}/locations/{location}/indexes/{index}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [IndexService.ListIndexes][google.cloud.aiplatform.v1.IndexService.ListIndexes].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListIndexesRequest {
    /// Required. The resource name of the Location from which to list the Indexes.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [ListIndexesResponse.next_page_token][google.cloud.aiplatform.v1.ListIndexesResponse.next_page_token]
    /// of the previous
    /// [IndexService.ListIndexes][google.cloud.aiplatform.v1.IndexService.ListIndexes]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [IndexService.ListIndexes][google.cloud.aiplatform.v1.IndexService.ListIndexes].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListIndexesResponse {
    /// List of indexes in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub indexes: ::prost::alloc::vec::Vec<Index>,
    /// A token to retrieve next page of results.
    /// Pass to
    /// [ListIndexesRequest.page_token][google.cloud.aiplatform.v1.ListIndexesRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [IndexService.UpdateIndex][google.cloud.aiplatform.v1.IndexService.UpdateIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateIndexRequest {
    /// Required. The Index which updates the resource on the server.
    #[prost(message, optional, tag = "1")]
    pub index: ::core::option::Option<Index>,
    /// The update mask applies to the resource.
    /// For the `FieldMask` definition, see
    /// [google.protobuf.FieldMask][google.protobuf.FieldMask].
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Runtime operation information for
/// [IndexService.UpdateIndex][google.cloud.aiplatform.v1.IndexService.UpdateIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateIndexOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// The operation metadata with regard to Matching Engine Index operation.
    #[prost(message, optional, tag = "2")]
    pub nearest_neighbor_search_operation_metadata: ::core::option::Option<
        NearestNeighborSearchOperationMetadata,
    >,
}
/// Request message for
/// [IndexService.DeleteIndex][google.cloud.aiplatform.v1.IndexService.DeleteIndex].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteIndexRequest {
    /// Required. The name of the Index resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/indexes/{index}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [IndexService.UpsertDatapoints][google.cloud.aiplatform.v1.IndexService.UpsertDatapoints]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpsertDatapointsRequest {
    /// Required. The name of the Index resource to be updated.
    /// Format:
    /// `projects/{project}/locations/{location}/indexes/{index}`
    #[prost(string, tag = "1")]
    pub index: ::prost::alloc::string::String,
    /// A list of datapoints to be created/updated.
    #[prost(message, repeated, tag = "2")]
    pub datapoints: ::prost::alloc::vec::Vec<IndexDatapoint>,
    /// Optional. Update mask is used to specify the fields to be overwritten in
    /// the datapoints by the update. The fields specified in the update_mask are
    /// relative to each IndexDatapoint inside datapoints, not the full request.
    ///
    /// Updatable fields:
    ///
    ///    * Use `all_restricts` to update both restricts and numeric_restricts.
    #[prost(message, optional, tag = "3")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [IndexService.UpsertDatapoints][google.cloud.aiplatform.v1.IndexService.UpsertDatapoints]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct UpsertDatapointsResponse {}
/// Request message for
/// [IndexService.RemoveDatapoints][google.cloud.aiplatform.v1.IndexService.RemoveDatapoints]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RemoveDatapointsRequest {
    /// Required. The name of the Index resource to be updated.
    /// Format:
    /// `projects/{project}/locations/{location}/indexes/{index}`
    #[prost(string, tag = "1")]
    pub index: ::prost::alloc::string::String,
    /// A list of datapoint ids to be deleted.
    #[prost(string, repeated, tag = "2")]
    pub datapoint_ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Response message for
/// [IndexService.RemoveDatapoints][google.cloud.aiplatform.v1.IndexService.RemoveDatapoints]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RemoveDatapointsResponse {}
/// Runtime operation metadata with regard to Matching Engine Index.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NearestNeighborSearchOperationMetadata {
    /// The validation stats of the content (per file) to be inserted or
    /// updated on the Matching Engine Index resource. Populated if
    /// contentsDeltaUri is provided as part of
    /// [Index.metadata][google.cloud.aiplatform.v1.Index.metadata]. Please note
    /// that, currently for those files that are broken or has unsupported file
    /// format, we will not have the stats for those files.
    #[prost(message, repeated, tag = "1")]
    pub content_validation_stats: ::prost::alloc::vec::Vec<
        nearest_neighbor_search_operation_metadata::ContentValidationStats,
    >,
    /// The ingested data size in bytes.
    #[prost(int64, tag = "2")]
    pub data_bytes_count: i64,
}
/// Nested message and enum types in `NearestNeighborSearchOperationMetadata`.
pub mod nearest_neighbor_search_operation_metadata {
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct RecordError {
        /// The error type of this record.
        #[prost(enumeration = "record_error::RecordErrorType", tag = "1")]
        pub error_type: i32,
        /// A human-readable message that is shown to the user to help them fix the
        /// error. Note that this message may change from time to time, your code
        /// should check against error_type as the source of truth.
        #[prost(string, tag = "2")]
        pub error_message: ::prost::alloc::string::String,
        /// Cloud Storage URI pointing to the original file in user's bucket.
        #[prost(string, tag = "3")]
        pub source_gcs_uri: ::prost::alloc::string::String,
        /// Empty if the embedding id is failed to parse.
        #[prost(string, tag = "4")]
        pub embedding_id: ::prost::alloc::string::String,
        /// The original content of this record.
        #[prost(string, tag = "5")]
        pub raw_record: ::prost::alloc::string::String,
    }
    /// Nested message and enum types in `RecordError`.
    pub mod record_error {
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum RecordErrorType {
            /// Default, shall not be used.
            ErrorTypeUnspecified = 0,
            /// The record is empty.
            EmptyLine = 1,
            /// Invalid json format.
            InvalidJsonSyntax = 2,
            /// Invalid csv format.
            InvalidCsvSyntax = 3,
            /// Invalid avro format.
            InvalidAvroSyntax = 4,
            /// The embedding id is not valid.
            InvalidEmbeddingId = 5,
            /// The size of the dense embedding vectors does not match with the
            /// specified dimension.
            EmbeddingSizeMismatch = 6,
            /// The `namespace` field is missing.
            NamespaceMissing = 7,
            /// Generic catch-all error. Only used for validation failure where the
            /// root cause cannot be easily retrieved programmatically.
            ParsingError = 8,
            /// There are multiple restricts with the same `namespace` value.
            DuplicateNamespace = 9,
            /// Numeric restrict has operator specified in datapoint.
            OpInDatapoint = 10,
            /// Numeric restrict has multiple values specified.
            MultipleValues = 11,
            /// Numeric restrict has invalid numeric value specified.
            InvalidNumericValue = 12,
            /// File is not in UTF_8 format.
            InvalidEncoding = 13,
            /// Error parsing sparse dimensions field.
            InvalidSparseDimensions = 14,
            /// Token restrict value is invalid.
            InvalidTokenValue = 15,
            /// Invalid sparse embedding.
            InvalidSparseEmbedding = 16,
            /// Invalid dense embedding.
            InvalidEmbedding = 17,
        }
        impl RecordErrorType {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::ErrorTypeUnspecified => "ERROR_TYPE_UNSPECIFIED",
                    Self::EmptyLine => "EMPTY_LINE",
                    Self::InvalidJsonSyntax => "INVALID_JSON_SYNTAX",
                    Self::InvalidCsvSyntax => "INVALID_CSV_SYNTAX",
                    Self::InvalidAvroSyntax => "INVALID_AVRO_SYNTAX",
                    Self::InvalidEmbeddingId => "INVALID_EMBEDDING_ID",
                    Self::EmbeddingSizeMismatch => "EMBEDDING_SIZE_MISMATCH",
                    Self::NamespaceMissing => "NAMESPACE_MISSING",
                    Self::ParsingError => "PARSING_ERROR",
                    Self::DuplicateNamespace => "DUPLICATE_NAMESPACE",
                    Self::OpInDatapoint => "OP_IN_DATAPOINT",
                    Self::MultipleValues => "MULTIPLE_VALUES",
                    Self::InvalidNumericValue => "INVALID_NUMERIC_VALUE",
                    Self::InvalidEncoding => "INVALID_ENCODING",
                    Self::InvalidSparseDimensions => "INVALID_SPARSE_DIMENSIONS",
                    Self::InvalidTokenValue => "INVALID_TOKEN_VALUE",
                    Self::InvalidSparseEmbedding => "INVALID_SPARSE_EMBEDDING",
                    Self::InvalidEmbedding => "INVALID_EMBEDDING",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "ERROR_TYPE_UNSPECIFIED" => Some(Self::ErrorTypeUnspecified),
                    "EMPTY_LINE" => Some(Self::EmptyLine),
                    "INVALID_JSON_SYNTAX" => Some(Self::InvalidJsonSyntax),
                    "INVALID_CSV_SYNTAX" => Some(Self::InvalidCsvSyntax),
                    "INVALID_AVRO_SYNTAX" => Some(Self::InvalidAvroSyntax),
                    "INVALID_EMBEDDING_ID" => Some(Self::InvalidEmbeddingId),
                    "EMBEDDING_SIZE_MISMATCH" => Some(Self::EmbeddingSizeMismatch),
                    "NAMESPACE_MISSING" => Some(Self::NamespaceMissing),
                    "PARSING_ERROR" => Some(Self::ParsingError),
                    "DUPLICATE_NAMESPACE" => Some(Self::DuplicateNamespace),
                    "OP_IN_DATAPOINT" => Some(Self::OpInDatapoint),
                    "MULTIPLE_VALUES" => Some(Self::MultipleValues),
                    "INVALID_NUMERIC_VALUE" => Some(Self::InvalidNumericValue),
                    "INVALID_ENCODING" => Some(Self::InvalidEncoding),
                    "INVALID_SPARSE_DIMENSIONS" => Some(Self::InvalidSparseDimensions),
                    "INVALID_TOKEN_VALUE" => Some(Self::InvalidTokenValue),
                    "INVALID_SPARSE_EMBEDDING" => Some(Self::InvalidSparseEmbedding),
                    "INVALID_EMBEDDING" => Some(Self::InvalidEmbedding),
                    _ => None,
                }
            }
        }
    }
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ContentValidationStats {
        /// Cloud Storage URI pointing to the original file in user's bucket.
        #[prost(string, tag = "1")]
        pub source_gcs_uri: ::prost::alloc::string::String,
        /// Number of records in this file that were successfully processed.
        #[prost(int64, tag = "2")]
        pub valid_record_count: i64,
        /// Number of records in this file we skipped due to validate errors.
        #[prost(int64, tag = "3")]
        pub invalid_record_count: i64,
        /// The detail information of the partial failures encountered for those
        /// invalid records that couldn't be parsed.
        /// Up to 50 partial errors will be reported.
        #[prost(message, repeated, tag = "4")]
        pub partial_errors: ::prost::alloc::vec::Vec<RecordError>,
        /// Number of sparse records in this file that were successfully processed.
        #[prost(int64, tag = "5")]
        pub valid_sparse_record_count: i64,
        /// Number of sparse records in this file we skipped due to validate errors.
        #[prost(int64, tag = "6")]
        pub invalid_sparse_record_count: i64,
    }
}
/// Generated client implementations.
pub mod index_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for creating and managing Vertex AI's Index resources.
    #[derive(Debug, Clone)]
    pub struct IndexServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl IndexServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> IndexServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> IndexServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            IndexServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates an Index.
        pub async fn create_index(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateIndexRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexService/CreateIndex",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexService",
                        "CreateIndex",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets an Index.
        pub async fn get_index(
            &mut self,
            request: impl tonic::IntoRequest<super::GetIndexRequest>,
        ) -> std::result::Result<tonic::Response<super::Index>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexService/GetIndex",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexService",
                        "GetIndex",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Indexes in a Location.
        pub async fn list_indexes(
            &mut self,
            request: impl tonic::IntoRequest<super::ListIndexesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListIndexesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexService/ListIndexes",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexService",
                        "ListIndexes",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates an Index.
        pub async fn update_index(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateIndexRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexService/UpdateIndex",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexService",
                        "UpdateIndex",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes an Index.
        /// An Index can only be deleted when all its
        /// [DeployedIndexes][google.cloud.aiplatform.v1.Index.deployed_indexes] had
        /// been undeployed.
        pub async fn delete_index(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteIndexRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexService/DeleteIndex",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexService",
                        "DeleteIndex",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Add/update Datapoints into an Index.
        pub async fn upsert_datapoints(
            &mut self,
            request: impl tonic::IntoRequest<super::UpsertDatapointsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::UpsertDatapointsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexService/UpsertDatapoints",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexService",
                        "UpsertDatapoints",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Remove Datapoints from an Index.
        pub async fn remove_datapoints(
            &mut self,
            request: impl tonic::IntoRequest<super::RemoveDatapointsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::RemoveDatapointsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.IndexService/RemoveDatapoints",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.IndexService",
                        "RemoveDatapoints",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// The objective configuration for model monitoring, including the information
/// needed to detect anomalies for one particular model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelMonitoringObjectiveConfig {
    /// Training dataset for models. This field has to be set only if
    /// TrainingPredictionSkewDetectionConfig is specified.
    #[prost(message, optional, tag = "1")]
    pub training_dataset: ::core::option::Option<
        model_monitoring_objective_config::TrainingDataset,
    >,
    /// The config for skew between training data and prediction data.
    #[prost(message, optional, tag = "2")]
    pub training_prediction_skew_detection_config: ::core::option::Option<
        model_monitoring_objective_config::TrainingPredictionSkewDetectionConfig,
    >,
    /// The config for drift of prediction data.
    #[prost(message, optional, tag = "3")]
    pub prediction_drift_detection_config: ::core::option::Option<
        model_monitoring_objective_config::PredictionDriftDetectionConfig,
    >,
    /// The config for integrating with Vertex Explainable AI.
    #[prost(message, optional, tag = "5")]
    pub explanation_config: ::core::option::Option<
        model_monitoring_objective_config::ExplanationConfig,
    >,
}
/// Nested message and enum types in `ModelMonitoringObjectiveConfig`.
pub mod model_monitoring_objective_config {
    /// Training Dataset information.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct TrainingDataset {
        /// Data format of the dataset, only applicable if the input is from
        /// Google Cloud Storage.
        /// The possible formats are:
        ///
        /// "tf-record"
        /// The source file is a TFRecord file.
        ///
        /// "csv"
        /// The source file is a CSV file.
        /// "jsonl"
        /// The source file is a JSONL file.
        #[prost(string, tag = "2")]
        pub data_format: ::prost::alloc::string::String,
        /// The target field name the model is to predict.
        /// This field will be excluded when doing Predict and (or) Explain for the
        /// training data.
        #[prost(string, tag = "6")]
        pub target_field: ::prost::alloc::string::String,
        /// Strategy to sample data from Training Dataset.
        /// If not set, we process the whole dataset.
        #[prost(message, optional, tag = "7")]
        pub logging_sampling_strategy: ::core::option::Option<super::SamplingStrategy>,
        #[prost(oneof = "training_dataset::DataSource", tags = "3, 4, 5")]
        pub data_source: ::core::option::Option<training_dataset::DataSource>,
    }
    /// Nested message and enum types in `TrainingDataset`.
    pub mod training_dataset {
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum DataSource {
            /// The resource name of the Dataset used to train this Model.
            #[prost(string, tag = "3")]
            Dataset(::prost::alloc::string::String),
            /// The Google Cloud Storage uri of the unmanaged Dataset used to train
            /// this Model.
            #[prost(message, tag = "4")]
            GcsSource(super::super::GcsSource),
            /// The BigQuery table of the unmanaged Dataset used to train this
            /// Model.
            #[prost(message, tag = "5")]
            BigquerySource(super::super::BigQuerySource),
        }
    }
    /// The config for Training & Prediction data skew detection. It specifies the
    /// training dataset sources and the skew detection parameters.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct TrainingPredictionSkewDetectionConfig {
        /// Key is the feature name and value is the threshold. If a feature needs to
        /// be monitored for skew, a value threshold must be configured for that
        /// feature. The threshold here is against feature distribution distance
        /// between the training and prediction feature.
        #[prost(map = "string, message", tag = "1")]
        pub skew_thresholds: ::std::collections::HashMap<
            ::prost::alloc::string::String,
            super::ThresholdConfig,
        >,
        /// Key is the feature name and value is the threshold. The threshold here is
        /// against attribution score distance between the training and prediction
        /// feature.
        #[prost(map = "string, message", tag = "2")]
        pub attribution_score_skew_thresholds: ::std::collections::HashMap<
            ::prost::alloc::string::String,
            super::ThresholdConfig,
        >,
        /// Skew anomaly detection threshold used by all features.
        /// When the per-feature thresholds are not set, this field can be used to
        /// specify a threshold for all features.
        #[prost(message, optional, tag = "6")]
        pub default_skew_threshold: ::core::option::Option<super::ThresholdConfig>,
    }
    /// The config for Prediction data drift detection.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct PredictionDriftDetectionConfig {
        /// Key is the feature name and value is the threshold. If a feature needs to
        /// be monitored for drift, a value threshold must be configured for that
        /// feature. The threshold here is against feature distribution distance
        /// between different time windws.
        #[prost(map = "string, message", tag = "1")]
        pub drift_thresholds: ::std::collections::HashMap<
            ::prost::alloc::string::String,
            super::ThresholdConfig,
        >,
        /// Key is the feature name and value is the threshold. The threshold here is
        /// against attribution score distance between different time windows.
        #[prost(map = "string, message", tag = "2")]
        pub attribution_score_drift_thresholds: ::std::collections::HashMap<
            ::prost::alloc::string::String,
            super::ThresholdConfig,
        >,
        /// Drift anomaly detection threshold used by all features.
        /// When the per-feature thresholds are not set, this field can be used to
        /// specify a threshold for all features.
        #[prost(message, optional, tag = "5")]
        pub default_drift_threshold: ::core::option::Option<super::ThresholdConfig>,
    }
    /// The config for integrating with Vertex Explainable AI. Only applicable if
    /// the Model has explanation_spec populated.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ExplanationConfig {
        /// If want to analyze the Vertex Explainable AI feature attribute scores or
        /// not. If set to true, Vertex AI will log the feature attributions from
        /// explain response and do the skew/drift detection for them.
        #[prost(bool, tag = "1")]
        pub enable_feature_attributes: bool,
        /// Predictions generated by the BatchPredictionJob using baseline dataset.
        #[prost(message, optional, tag = "2")]
        pub explanation_baseline: ::core::option::Option<
            explanation_config::ExplanationBaseline,
        >,
    }
    /// Nested message and enum types in `ExplanationConfig`.
    pub mod explanation_config {
        /// Output from
        /// [BatchPredictionJob][google.cloud.aiplatform.v1.BatchPredictionJob] for
        /// Model Monitoring baseline dataset, which can be used to generate baseline
        /// attribution scores.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct ExplanationBaseline {
            /// The storage format of the predictions generated BatchPrediction job.
            #[prost(enumeration = "explanation_baseline::PredictionFormat", tag = "1")]
            pub prediction_format: i32,
            /// The configuration specifying of BatchExplain job output. This can be
            /// used to generate the baseline of feature attribution scores.
            #[prost(oneof = "explanation_baseline::Destination", tags = "2, 3")]
            pub destination: ::core::option::Option<explanation_baseline::Destination>,
        }
        /// Nested message and enum types in `ExplanationBaseline`.
        pub mod explanation_baseline {
            /// The storage format of the predictions generated BatchPrediction job.
            #[derive(
                Clone,
                Copy,
                Debug,
                PartialEq,
                Eq,
                Hash,
                PartialOrd,
                Ord,
                ::prost::Enumeration
            )]
            #[repr(i32)]
            pub enum PredictionFormat {
                /// Should not be set.
                Unspecified = 0,
                /// Predictions are in JSONL files.
                Jsonl = 2,
                /// Predictions are in BigQuery.
                Bigquery = 3,
            }
            impl PredictionFormat {
                /// String value of the enum field names used in the ProtoBuf definition.
                ///
                /// The values are not transformed in any way and thus are considered stable
                /// (if the ProtoBuf definition does not change) and safe for programmatic use.
                pub fn as_str_name(&self) -> &'static str {
                    match self {
                        Self::Unspecified => "PREDICTION_FORMAT_UNSPECIFIED",
                        Self::Jsonl => "JSONL",
                        Self::Bigquery => "BIGQUERY",
                    }
                }
                /// Creates an enum from field names used in the ProtoBuf definition.
                pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                    match value {
                        "PREDICTION_FORMAT_UNSPECIFIED" => Some(Self::Unspecified),
                        "JSONL" => Some(Self::Jsonl),
                        "BIGQUERY" => Some(Self::Bigquery),
                        _ => None,
                    }
                }
            }
            /// The configuration specifying of BatchExplain job output. This can be
            /// used to generate the baseline of feature attribution scores.
            #[derive(Clone, PartialEq, ::prost::Oneof)]
            pub enum Destination {
                /// Cloud Storage location for BatchExplain output.
                #[prost(message, tag = "2")]
                Gcs(super::super::super::GcsDestination),
                /// BigQuery location for BatchExplain output.
                #[prost(message, tag = "3")]
                Bigquery(super::super::super::BigQueryDestination),
            }
        }
    }
}
/// The alert config for model monitoring.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelMonitoringAlertConfig {
    /// Dump the anomalies to Cloud Logging. The anomalies will be put to json
    /// payload encoded from proto
    /// [ModelMonitoringStatsAnomalies][google.cloud.aiplatform.v1.ModelMonitoringStatsAnomalies].
    /// This can be further synced to Pub/Sub or any other services supported by
    /// Cloud Logging.
    #[prost(bool, tag = "2")]
    pub enable_logging: bool,
    /// Resource names of the NotificationChannels to send alert.
    /// Must be of the format
    /// `projects/<project_id_or_number>/notificationChannels/<channel_id>`
    #[prost(string, repeated, tag = "3")]
    pub notification_channels: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    #[prost(oneof = "model_monitoring_alert_config::Alert", tags = "1")]
    pub alert: ::core::option::Option<model_monitoring_alert_config::Alert>,
}
/// Nested message and enum types in `ModelMonitoringAlertConfig`.
pub mod model_monitoring_alert_config {
    /// The config for email alert.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct EmailAlertConfig {
        /// The email addresses to send the alert.
        #[prost(string, repeated, tag = "1")]
        pub user_emails: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Alert {
        /// Email alert config.
        #[prost(message, tag = "1")]
        EmailAlertConfig(EmailAlertConfig),
    }
}
/// The config for feature monitoring threshold.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ThresholdConfig {
    #[prost(oneof = "threshold_config::Threshold", tags = "1")]
    pub threshold: ::core::option::Option<threshold_config::Threshold>,
}
/// Nested message and enum types in `ThresholdConfig`.
pub mod threshold_config {
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum Threshold {
        /// Specify a threshold value that can trigger the alert.
        /// If this threshold config is for feature distribution distance:
        ///    1. For categorical feature, the distribution distance is calculated by
        ///       L-inifinity norm.
        ///    2. For numerical feature, the distribution distance is calculated by
        ///       Jensen–Shannon divergence.
        /// Each feature must have a non-zero threshold if they need to be monitored.
        /// Otherwise no alert will be triggered for that feature.
        #[prost(double, tag = "1")]
        Value(f64),
    }
}
/// Sampling Strategy for logging, can be for both training and prediction
/// dataset.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SamplingStrategy {
    /// Random sample config. Will support more sampling strategies later.
    #[prost(message, optional, tag = "1")]
    pub random_sample_config: ::core::option::Option<
        sampling_strategy::RandomSampleConfig,
    >,
}
/// Nested message and enum types in `SamplingStrategy`.
pub mod sampling_strategy {
    /// Requests are randomly selected.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct RandomSampleConfig {
        /// Sample rate (0, 1]
        #[prost(double, tag = "1")]
        pub sample_rate: f64,
    }
}
/// Represents a job that runs periodically to monitor the deployed models in an
/// endpoint. It will analyze the logged training & prediction data to detect any
/// abnormal behaviors.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelDeploymentMonitoringJob {
    /// Output only. Resource name of a ModelDeploymentMonitoringJob.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The user-defined name of the ModelDeploymentMonitoringJob.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    /// Display name of a ModelDeploymentMonitoringJob.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Required. Endpoint resource name.
    /// Format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "3")]
    pub endpoint: ::prost::alloc::string::String,
    /// Output only. The detailed state of the monitoring job.
    /// When the job is still creating, the state will be 'PENDING'.
    /// Once the job is successfully created, the state will be 'RUNNING'.
    /// Pause the job, the state will be 'PAUSED'.
    /// Resume the job, the state will return to 'RUNNING'.
    #[prost(enumeration = "JobState", tag = "4")]
    pub state: i32,
    /// Output only. Schedule state when the monitoring job is in Running state.
    #[prost(
        enumeration = "model_deployment_monitoring_job::MonitoringScheduleState",
        tag = "5"
    )]
    pub schedule_state: i32,
    /// Output only. Latest triggered monitoring pipeline metadata.
    #[prost(message, optional, tag = "25")]
    pub latest_monitoring_pipeline_metadata: ::core::option::Option<
        model_deployment_monitoring_job::LatestMonitoringPipelineMetadata,
    >,
    /// Required. The config for monitoring objectives. This is a per DeployedModel
    /// config. Each DeployedModel needs to be configured separately.
    #[prost(message, repeated, tag = "6")]
    pub model_deployment_monitoring_objective_configs: ::prost::alloc::vec::Vec<
        ModelDeploymentMonitoringObjectiveConfig,
    >,
    /// Required. Schedule config for running the monitoring job.
    #[prost(message, optional, tag = "7")]
    pub model_deployment_monitoring_schedule_config: ::core::option::Option<
        ModelDeploymentMonitoringScheduleConfig,
    >,
    /// Required. Sample Strategy for logging.
    #[prost(message, optional, tag = "8")]
    pub logging_sampling_strategy: ::core::option::Option<SamplingStrategy>,
    /// Alert config for model monitoring.
    #[prost(message, optional, tag = "15")]
    pub model_monitoring_alert_config: ::core::option::Option<
        ModelMonitoringAlertConfig,
    >,
    /// YAML schema file uri describing the format of a single instance,
    /// which are given to format this Endpoint's prediction (and explanation).
    /// If not set, we will generate predict schema from collected predict
    /// requests.
    #[prost(string, tag = "9")]
    pub predict_instance_schema_uri: ::prost::alloc::string::String,
    /// Sample Predict instance, same format as
    /// [PredictRequest.instances][google.cloud.aiplatform.v1.PredictRequest.instances],
    /// this can be set as a replacement of
    /// [ModelDeploymentMonitoringJob.predict_instance_schema_uri][google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.predict_instance_schema_uri].
    /// If not set, we will generate predict schema from collected predict
    /// requests.
    #[prost(message, optional, tag = "19")]
    pub sample_predict_instance: ::core::option::Option<::prost_types::Value>,
    /// YAML schema file uri describing the format of a single instance that you
    /// want Tensorflow Data Validation (TFDV) to analyze.
    ///
    /// If this field is empty, all the feature data types are inferred from
    /// [predict_instance_schema_uri][google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.predict_instance_schema_uri],
    /// meaning that TFDV will use the data in the exact format(data type) as
    /// prediction request/response.
    /// If there are any data type differences between predict instance and TFDV
    /// instance, this field can be used to override the schema.
    /// For models trained with Vertex AI, this field must be set as all the
    /// fields in predict instance formatted as string.
    #[prost(string, tag = "16")]
    pub analysis_instance_schema_uri: ::prost::alloc::string::String,
    /// Output only. The created bigquery tables for the job under customer
    /// project. Customer could do their own query & analysis. There could be 4 log
    /// tables in maximum:
    /// 1. Training data logging predict request/response
    /// 2. Serving data logging predict request/response
    #[prost(message, repeated, tag = "10")]
    pub bigquery_tables: ::prost::alloc::vec::Vec<
        ModelDeploymentMonitoringBigQueryTable,
    >,
    /// The TTL of BigQuery tables in user projects which stores logs.
    /// A day is the basic unit of the TTL and we take the ceil of TTL/86400(a
    /// day). e.g. { second: 3600} indicates ttl = 1 day.
    #[prost(message, optional, tag = "17")]
    pub log_ttl: ::core::option::Option<::prost_types::Duration>,
    /// The labels with user-defined metadata to organize your
    /// ModelDeploymentMonitoringJob.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "11")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Output only. Timestamp when this ModelDeploymentMonitoringJob was created.
    #[prost(message, optional, tag = "12")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this ModelDeploymentMonitoringJob was updated
    /// most recently.
    #[prost(message, optional, tag = "13")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this monitoring pipeline will be scheduled to
    /// run for the next round.
    #[prost(message, optional, tag = "14")]
    pub next_schedule_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Stats anomalies base folder path.
    #[prost(message, optional, tag = "20")]
    pub stats_anomalies_base_directory: ::core::option::Option<GcsDestination>,
    /// Customer-managed encryption key spec for a ModelDeploymentMonitoringJob. If
    /// set, this ModelDeploymentMonitoringJob and all sub-resources of this
    /// ModelDeploymentMonitoringJob will be secured by this key.
    #[prost(message, optional, tag = "21")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// If true, the scheduled monitoring pipeline logs are sent to
    /// Google Cloud Logging, including pipeline status and anomalies detected.
    /// Please note the logs incur cost, which are subject to [Cloud Logging
    /// pricing](<https://cloud.google.com/logging#pricing>).
    #[prost(bool, tag = "22")]
    pub enable_monitoring_pipeline_logs: bool,
    /// Output only. Only populated when the job's state is `JOB_STATE_FAILED` or
    /// `JOB_STATE_CANCELLED`.
    #[prost(message, optional, tag = "23")]
    pub error: ::core::option::Option<super::super::super::rpc::Status>,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "26")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "27")]
    pub satisfies_pzi: bool,
}
/// Nested message and enum types in `ModelDeploymentMonitoringJob`.
pub mod model_deployment_monitoring_job {
    /// All metadata of most recent monitoring pipelines.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct LatestMonitoringPipelineMetadata {
        /// The time that most recent monitoring pipelines that is related to this
        /// run.
        #[prost(message, optional, tag = "1")]
        pub run_time: ::core::option::Option<::prost_types::Timestamp>,
        /// The status of the most recent monitoring pipeline.
        #[prost(message, optional, tag = "2")]
        pub status: ::core::option::Option<super::super::super::super::rpc::Status>,
    }
    /// The state to Specify the monitoring pipeline.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum MonitoringScheduleState {
        /// Unspecified state.
        Unspecified = 0,
        /// The pipeline is picked up and wait to run.
        Pending = 1,
        /// The pipeline is offline and will be scheduled for next run.
        Offline = 2,
        /// The pipeline is running.
        Running = 3,
    }
    impl MonitoringScheduleState {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "MONITORING_SCHEDULE_STATE_UNSPECIFIED",
                Self::Pending => "PENDING",
                Self::Offline => "OFFLINE",
                Self::Running => "RUNNING",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "MONITORING_SCHEDULE_STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "PENDING" => Some(Self::Pending),
                "OFFLINE" => Some(Self::Offline),
                "RUNNING" => Some(Self::Running),
                _ => None,
            }
        }
    }
}
/// ModelDeploymentMonitoringBigQueryTable specifies the BigQuery table name
/// as well as some information of the logs stored in this table.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelDeploymentMonitoringBigQueryTable {
    /// The source of log.
    #[prost(
        enumeration = "model_deployment_monitoring_big_query_table::LogSource",
        tag = "1"
    )]
    pub log_source: i32,
    /// The type of log.
    #[prost(
        enumeration = "model_deployment_monitoring_big_query_table::LogType",
        tag = "2"
    )]
    pub log_type: i32,
    /// The created BigQuery table to store logs. Customer could do their own query
    /// & analysis. Format:
    /// `bq://<project_id>.model_deployment_monitoring_<endpoint_id>.<tolower(log_source)>_<tolower(log_type)>`
    #[prost(string, tag = "3")]
    pub bigquery_table_path: ::prost::alloc::string::String,
    /// Output only. The schema version of the request/response logging BigQuery
    /// table. Default to v1 if unset.
    #[prost(string, tag = "4")]
    pub request_response_logging_schema_version: ::prost::alloc::string::String,
}
/// Nested message and enum types in `ModelDeploymentMonitoringBigQueryTable`.
pub mod model_deployment_monitoring_big_query_table {
    /// Indicates where does the log come from.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum LogSource {
        /// Unspecified source.
        Unspecified = 0,
        /// Logs coming from Training dataset.
        Training = 1,
        /// Logs coming from Serving traffic.
        Serving = 2,
    }
    impl LogSource {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "LOG_SOURCE_UNSPECIFIED",
                Self::Training => "TRAINING",
                Self::Serving => "SERVING",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "LOG_SOURCE_UNSPECIFIED" => Some(Self::Unspecified),
                "TRAINING" => Some(Self::Training),
                "SERVING" => Some(Self::Serving),
                _ => None,
            }
        }
    }
    /// Indicates what type of traffic does the log belong to.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum LogType {
        /// Unspecified type.
        Unspecified = 0,
        /// Predict logs.
        Predict = 1,
        /// Explain logs.
        Explain = 2,
    }
    impl LogType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "LOG_TYPE_UNSPECIFIED",
                Self::Predict => "PREDICT",
                Self::Explain => "EXPLAIN",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "LOG_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "PREDICT" => Some(Self::Predict),
                "EXPLAIN" => Some(Self::Explain),
                _ => None,
            }
        }
    }
}
/// ModelDeploymentMonitoringObjectiveConfig contains the pair of
/// deployed_model_id to ModelMonitoringObjectiveConfig.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelDeploymentMonitoringObjectiveConfig {
    /// The DeployedModel ID of the objective config.
    #[prost(string, tag = "1")]
    pub deployed_model_id: ::prost::alloc::string::String,
    /// The objective config of for the modelmonitoring job of this deployed model.
    #[prost(message, optional, tag = "2")]
    pub objective_config: ::core::option::Option<ModelMonitoringObjectiveConfig>,
}
/// The config for scheduling monitoring job.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ModelDeploymentMonitoringScheduleConfig {
    /// Required. The model monitoring job scheduling interval. It will be rounded
    /// up to next full hour. This defines how often the monitoring jobs are
    /// triggered.
    #[prost(message, optional, tag = "1")]
    pub monitor_interval: ::core::option::Option<::prost_types::Duration>,
    /// The time window of the prediction data being included in each prediction
    /// dataset. This window specifies how long the data should be collected from
    /// historical model results for each run. If not set,
    /// [ModelDeploymentMonitoringScheduleConfig.monitor_interval][google.cloud.aiplatform.v1.ModelDeploymentMonitoringScheduleConfig.monitor_interval]
    /// will be used. e.g. If currently the cutoff time is 2022-01-08 14:30:00 and
    /// the monitor_window is set to be 3600, then data from 2022-01-08 13:30:00 to
    /// 2022-01-08 14:30:00 will be retrieved and aggregated to calculate the
    /// monitoring statistics.
    #[prost(message, optional, tag = "2")]
    pub monitor_window: ::core::option::Option<::prost_types::Duration>,
}
/// Statistics and anomalies generated by Model Monitoring.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelMonitoringStatsAnomalies {
    /// Model Monitoring Objective those stats and anomalies belonging to.
    #[prost(enumeration = "ModelDeploymentMonitoringObjectiveType", tag = "1")]
    pub objective: i32,
    /// Deployed Model ID.
    #[prost(string, tag = "2")]
    pub deployed_model_id: ::prost::alloc::string::String,
    /// Number of anomalies within all stats.
    #[prost(int32, tag = "3")]
    pub anomaly_count: i32,
    /// A list of historical Stats and Anomalies generated for all Features.
    #[prost(message, repeated, tag = "4")]
    pub feature_stats: ::prost::alloc::vec::Vec<
        model_monitoring_stats_anomalies::FeatureHistoricStatsAnomalies,
    >,
}
/// Nested message and enum types in `ModelMonitoringStatsAnomalies`.
pub mod model_monitoring_stats_anomalies {
    /// Historical Stats (and Anomalies) for a specific Feature.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct FeatureHistoricStatsAnomalies {
        /// Display Name of the Feature.
        #[prost(string, tag = "1")]
        pub feature_display_name: ::prost::alloc::string::String,
        /// Threshold for anomaly detection.
        #[prost(message, optional, tag = "3")]
        pub threshold: ::core::option::Option<super::ThresholdConfig>,
        /// Stats calculated for the Training Dataset.
        #[prost(message, optional, tag = "4")]
        pub training_stats: ::core::option::Option<super::FeatureStatsAnomaly>,
        /// A list of historical stats generated by different time window's
        /// Prediction Dataset.
        #[prost(message, repeated, tag = "5")]
        pub prediction_stats: ::prost::alloc::vec::Vec<super::FeatureStatsAnomaly>,
    }
}
/// The Model Monitoring Objective types.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum ModelDeploymentMonitoringObjectiveType {
    /// Default value, should not be set.
    Unspecified = 0,
    /// Raw feature values' stats to detect skew between Training-Prediction
    /// datasets.
    RawFeatureSkew = 1,
    /// Raw feature values' stats to detect drift between Serving-Prediction
    /// datasets.
    RawFeatureDrift = 2,
    /// Feature attribution scores to detect skew between Training-Prediction
    /// datasets.
    FeatureAttributionSkew = 3,
    /// Feature attribution scores to detect skew between Prediction datasets
    /// collected within different time windows.
    FeatureAttributionDrift = 4,
}
impl ModelDeploymentMonitoringObjectiveType {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "MODEL_DEPLOYMENT_MONITORING_OBJECTIVE_TYPE_UNSPECIFIED",
            Self::RawFeatureSkew => "RAW_FEATURE_SKEW",
            Self::RawFeatureDrift => "RAW_FEATURE_DRIFT",
            Self::FeatureAttributionSkew => "FEATURE_ATTRIBUTION_SKEW",
            Self::FeatureAttributionDrift => "FEATURE_ATTRIBUTION_DRIFT",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "MODEL_DEPLOYMENT_MONITORING_OBJECTIVE_TYPE_UNSPECIFIED" => {
                Some(Self::Unspecified)
            }
            "RAW_FEATURE_SKEW" => Some(Self::RawFeatureSkew),
            "RAW_FEATURE_DRIFT" => Some(Self::RawFeatureDrift),
            "FEATURE_ATTRIBUTION_SKEW" => Some(Self::FeatureAttributionSkew),
            "FEATURE_ATTRIBUTION_DRIFT" => Some(Self::FeatureAttributionDrift),
            _ => None,
        }
    }
}
/// Represents a Neural Architecture Search (NAS) job.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NasJob {
    /// Output only. Resource name of the NasJob.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The display name of the NasJob.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Required. The specification of a NasJob.
    #[prost(message, optional, tag = "4")]
    pub nas_job_spec: ::core::option::Option<NasJobSpec>,
    /// Output only. Output of the NasJob.
    #[prost(message, optional, tag = "5")]
    pub nas_job_output: ::core::option::Option<NasJobOutput>,
    /// Output only. The detailed state of the job.
    #[prost(enumeration = "JobState", tag = "6")]
    pub state: i32,
    /// Output only. Time when the NasJob was created.
    #[prost(message, optional, tag = "7")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the NasJob for the first time entered the
    /// `JOB_STATE_RUNNING` state.
    #[prost(message, optional, tag = "8")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the NasJob entered any of the following states:
    /// `JOB_STATE_SUCCEEDED`, `JOB_STATE_FAILED`, `JOB_STATE_CANCELLED`.
    #[prost(message, optional, tag = "9")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the NasJob was most recently updated.
    #[prost(message, optional, tag = "10")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Only populated when job's state is JOB_STATE_FAILED or
    /// JOB_STATE_CANCELLED.
    #[prost(message, optional, tag = "11")]
    pub error: ::core::option::Option<super::super::super::rpc::Status>,
    /// The labels with user-defined metadata to organize NasJobs.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "12")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Customer-managed encryption key options for a NasJob.
    /// If this is set, then all resources created by the NasJob
    /// will be encrypted with the provided encryption key.
    #[prost(message, optional, tag = "13")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Optional. Enable a separation of Custom model training
    /// and restricted image training for tenant project.
    #[deprecated]
    #[prost(bool, tag = "14")]
    pub enable_restricted_image_training: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "15")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "16")]
    pub satisfies_pzi: bool,
}
/// Represents a NasTrial details along with its parameters. If there is a
/// corresponding train NasTrial, the train NasTrial is also returned.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NasTrialDetail {
    /// Output only. Resource name of the NasTrialDetail.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// The parameters for the NasJob NasTrial.
    #[prost(string, tag = "2")]
    pub parameters: ::prost::alloc::string::String,
    /// The requested search NasTrial.
    #[prost(message, optional, tag = "3")]
    pub search_trial: ::core::option::Option<NasTrial>,
    /// The train NasTrial corresponding to
    /// [search_trial][google.cloud.aiplatform.v1.NasTrialDetail.search_trial].
    /// Only populated if
    /// [search_trial][google.cloud.aiplatform.v1.NasTrialDetail.search_trial] is
    /// used for training.
    #[prost(message, optional, tag = "4")]
    pub train_trial: ::core::option::Option<NasTrial>,
}
/// Represents the spec of a NasJob.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NasJobSpec {
    /// The ID of the existing NasJob in the same Project and Location
    /// which will be used to resume search. search_space_spec and
    /// nas_algorithm_spec are obtained from previous NasJob hence should not
    /// provide them again for this NasJob.
    #[prost(string, tag = "3")]
    pub resume_nas_job_id: ::prost::alloc::string::String,
    /// It defines the search space for Neural Architecture Search (NAS).
    #[prost(string, tag = "1")]
    pub search_space_spec: ::prost::alloc::string::String,
    /// The Neural Architecture Search (NAS) algorithm specification.
    #[prost(oneof = "nas_job_spec::NasAlgorithmSpec", tags = "2")]
    pub nas_algorithm_spec: ::core::option::Option<nas_job_spec::NasAlgorithmSpec>,
}
/// Nested message and enum types in `NasJobSpec`.
pub mod nas_job_spec {
    /// The spec of multi-trial Neural Architecture Search (NAS).
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct MultiTrialAlgorithmSpec {
        /// The multi-trial Neural Architecture Search (NAS) algorithm
        /// type. Defaults to `REINFORCEMENT_LEARNING`.
        #[prost(
            enumeration = "multi_trial_algorithm_spec::MultiTrialAlgorithm",
            tag = "1"
        )]
        pub multi_trial_algorithm: i32,
        /// Metric specs for the NAS job.
        /// Validation for this field is done at `multi_trial_algorithm_spec` field.
        #[prost(message, optional, tag = "2")]
        pub metric: ::core::option::Option<multi_trial_algorithm_spec::MetricSpec>,
        /// Required. Spec for search trials.
        #[prost(message, optional, tag = "3")]
        pub search_trial_spec: ::core::option::Option<
            multi_trial_algorithm_spec::SearchTrialSpec,
        >,
        /// Spec for train trials. Top N \[TrainTrialSpec.max_parallel_trial_count\]
        /// search trials will be trained for every M
        /// \[TrainTrialSpec.frequency\] trials searched.
        #[prost(message, optional, tag = "4")]
        pub train_trial_spec: ::core::option::Option<
            multi_trial_algorithm_spec::TrainTrialSpec,
        >,
    }
    /// Nested message and enum types in `MultiTrialAlgorithmSpec`.
    pub mod multi_trial_algorithm_spec {
        /// Represents a metric to optimize.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct MetricSpec {
            /// Required. The ID of the metric. Must not contain whitespaces.
            #[prost(string, tag = "1")]
            pub metric_id: ::prost::alloc::string::String,
            /// Required. The optimization goal of the metric.
            #[prost(enumeration = "metric_spec::GoalType", tag = "2")]
            pub goal: i32,
        }
        /// Nested message and enum types in `MetricSpec`.
        pub mod metric_spec {
            /// The available types of optimization goals.
            #[derive(
                Clone,
                Copy,
                Debug,
                PartialEq,
                Eq,
                Hash,
                PartialOrd,
                Ord,
                ::prost::Enumeration
            )]
            #[repr(i32)]
            pub enum GoalType {
                /// Goal Type will default to maximize.
                Unspecified = 0,
                /// Maximize the goal metric.
                Maximize = 1,
                /// Minimize the goal metric.
                Minimize = 2,
            }
            impl GoalType {
                /// String value of the enum field names used in the ProtoBuf definition.
                ///
                /// The values are not transformed in any way and thus are considered stable
                /// (if the ProtoBuf definition does not change) and safe for programmatic use.
                pub fn as_str_name(&self) -> &'static str {
                    match self {
                        Self::Unspecified => "GOAL_TYPE_UNSPECIFIED",
                        Self::Maximize => "MAXIMIZE",
                        Self::Minimize => "MINIMIZE",
                    }
                }
                /// Creates an enum from field names used in the ProtoBuf definition.
                pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                    match value {
                        "GOAL_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                        "MAXIMIZE" => Some(Self::Maximize),
                        "MINIMIZE" => Some(Self::Minimize),
                        _ => None,
                    }
                }
            }
        }
        /// Represent spec for search trials.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct SearchTrialSpec {
            /// Required. The spec of a search trial job. The same spec applies to
            /// all search trials.
            #[prost(message, optional, tag = "1")]
            pub search_trial_job_spec: ::core::option::Option<
                super::super::CustomJobSpec,
            >,
            /// Required. The maximum number of Neural Architecture Search (NAS) trials
            /// to run.
            #[prost(int32, tag = "2")]
            pub max_trial_count: i32,
            /// Required. The maximum number of trials to run in parallel.
            #[prost(int32, tag = "3")]
            pub max_parallel_trial_count: i32,
            /// The number of failed trials that need to be seen before failing
            /// the NasJob.
            ///
            /// If set to 0, Vertex AI decides how many trials must fail
            /// before the whole job fails.
            #[prost(int32, tag = "4")]
            pub max_failed_trial_count: i32,
        }
        /// Represent spec for train trials.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct TrainTrialSpec {
            /// Required. The spec of a train trial job. The same spec applies to
            /// all train trials.
            #[prost(message, optional, tag = "1")]
            pub train_trial_job_spec: ::core::option::Option<
                super::super::CustomJobSpec,
            >,
            /// Required. The maximum number of trials to run in parallel.
            #[prost(int32, tag = "2")]
            pub max_parallel_trial_count: i32,
            /// Required. Frequency of search trials to start train stage. Top N
            /// \[TrainTrialSpec.max_parallel_trial_count\]
            /// search trials will be trained for every M
            /// \[TrainTrialSpec.frequency\] trials searched.
            #[prost(int32, tag = "3")]
            pub frequency: i32,
        }
        /// The available types of multi-trial algorithms.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum MultiTrialAlgorithm {
            /// Defaults to `REINFORCEMENT_LEARNING`.
            Unspecified = 0,
            /// The Reinforcement Learning Algorithm for Multi-trial Neural
            /// Architecture Search (NAS).
            ReinforcementLearning = 1,
            /// The Grid Search Algorithm for Multi-trial Neural
            /// Architecture Search (NAS).
            GridSearch = 2,
        }
        impl MultiTrialAlgorithm {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "MULTI_TRIAL_ALGORITHM_UNSPECIFIED",
                    Self::ReinforcementLearning => "REINFORCEMENT_LEARNING",
                    Self::GridSearch => "GRID_SEARCH",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "MULTI_TRIAL_ALGORITHM_UNSPECIFIED" => Some(Self::Unspecified),
                    "REINFORCEMENT_LEARNING" => Some(Self::ReinforcementLearning),
                    "GRID_SEARCH" => Some(Self::GridSearch),
                    _ => None,
                }
            }
        }
    }
    /// The Neural Architecture Search (NAS) algorithm specification.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum NasAlgorithmSpec {
        /// The spec of multi-trial algorithms.
        #[prost(message, tag = "2")]
        MultiTrialAlgorithmSpec(MultiTrialAlgorithmSpec),
    }
}
/// Represents a uCAIP NasJob output.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NasJobOutput {
    /// The output of this Neural Architecture Search (NAS) job.
    #[prost(oneof = "nas_job_output::Output", tags = "1")]
    pub output: ::core::option::Option<nas_job_output::Output>,
}
/// Nested message and enum types in `NasJobOutput`.
pub mod nas_job_output {
    /// The output of a multi-trial Neural Architecture Search (NAS) jobs.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct MultiTrialJobOutput {
        /// Output only. List of NasTrials that were started as part of search stage.
        #[prost(message, repeated, tag = "1")]
        pub search_trials: ::prost::alloc::vec::Vec<super::NasTrial>,
        /// Output only. List of NasTrials that were started as part of train stage.
        #[prost(message, repeated, tag = "2")]
        pub train_trials: ::prost::alloc::vec::Vec<super::NasTrial>,
    }
    /// The output of this Neural Architecture Search (NAS) job.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Output {
        /// Output only. The output of this multi-trial Neural Architecture Search
        /// (NAS) job.
        #[prost(message, tag = "1")]
        MultiTrialJobOutput(MultiTrialJobOutput),
    }
}
/// Represents a uCAIP NasJob trial.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NasTrial {
    /// Output only. The identifier of the NasTrial assigned by the service.
    #[prost(string, tag = "1")]
    pub id: ::prost::alloc::string::String,
    /// Output only. The detailed state of the NasTrial.
    #[prost(enumeration = "nas_trial::State", tag = "2")]
    pub state: i32,
    /// Output only. The final measurement containing the objective value.
    #[prost(message, optional, tag = "3")]
    pub final_measurement: ::core::option::Option<Measurement>,
    /// Output only. Time when the NasTrial was started.
    #[prost(message, optional, tag = "4")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the NasTrial's status changed to `SUCCEEDED` or
    /// `INFEASIBLE`.
    #[prost(message, optional, tag = "5")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
}
/// Nested message and enum types in `NasTrial`.
pub mod nas_trial {
    /// Describes a NasTrial state.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// The NasTrial state is unspecified.
        Unspecified = 0,
        /// Indicates that a specific NasTrial has been requested, but it has not yet
        /// been suggested by the service.
        Requested = 1,
        /// Indicates that the NasTrial has been suggested.
        Active = 2,
        /// Indicates that the NasTrial should stop according to the service.
        Stopping = 3,
        /// Indicates that the NasTrial is completed successfully.
        Succeeded = 4,
        /// Indicates that the NasTrial should not be attempted again.
        /// The service will set a NasTrial to INFEASIBLE when it's done but missing
        /// the final_measurement.
        Infeasible = 5,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STATE_UNSPECIFIED",
                Self::Requested => "REQUESTED",
                Self::Active => "ACTIVE",
                Self::Stopping => "STOPPING",
                Self::Succeeded => "SUCCEEDED",
                Self::Infeasible => "INFEASIBLE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "REQUESTED" => Some(Self::Requested),
                "ACTIVE" => Some(Self::Active),
                "STOPPING" => Some(Self::Stopping),
                "SUCCEEDED" => Some(Self::Succeeded),
                "INFEASIBLE" => Some(Self::Infeasible),
                _ => None,
            }
        }
    }
}
/// Request message for
/// [JobService.CreateCustomJob][google.cloud.aiplatform.v1.JobService.CreateCustomJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateCustomJobRequest {
    /// Required. The resource name of the Location to create the CustomJob in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The CustomJob to create.
    #[prost(message, optional, tag = "2")]
    pub custom_job: ::core::option::Option<CustomJob>,
}
/// Request message for
/// [JobService.GetCustomJob][google.cloud.aiplatform.v1.JobService.GetCustomJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetCustomJobRequest {
    /// Required. The name of the CustomJob resource.
    /// Format:
    /// `projects/{project}/locations/{location}/customJobs/{custom_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.ListCustomJobs][google.cloud.aiplatform.v1.JobService.ListCustomJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListCustomJobsRequest {
    /// Required. The resource name of the Location to list the CustomJobs from.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    ///
    /// Supported fields:
    ///
    ///    * `display_name` supports `=`, `!=` comparisons, and `:` wildcard.
    ///    * `state` supports `=`, `!=` comparisons.
    ///    * `create_time` supports `=`, `!=`,`<`, `<=`,`>`, `>=` comparisons.
    ///      `create_time` must be in RFC 3339 format.
    ///    * `labels` supports general map functions that is:
    ///      `labels.key=value` - key:value equality
    ///      `labels.key:* - key existence
    ///
    /// Some examples of using the filter are:
    ///
    ///    * `state="JOB_STATE_SUCCEEDED" AND display_name:"my_job_*"`
    ///    * `state!="JOB_STATE_FAILED" OR display_name="my_job"`
    ///    * `NOT display_name="my_job"`
    ///    * `create_time>"2021-05-18T00:00:00Z"`
    ///    * `labels.keyA=valueA`
    ///    * `labels.keyB:*`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [ListCustomJobsResponse.next_page_token][google.cloud.aiplatform.v1.ListCustomJobsResponse.next_page_token]
    /// of the previous
    /// [JobService.ListCustomJobs][google.cloud.aiplatform.v1.JobService.ListCustomJobs]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [JobService.ListCustomJobs][google.cloud.aiplatform.v1.JobService.ListCustomJobs]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListCustomJobsResponse {
    /// List of CustomJobs in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub custom_jobs: ::prost::alloc::vec::Vec<CustomJob>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListCustomJobsRequest.page_token][google.cloud.aiplatform.v1.ListCustomJobsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.DeleteCustomJob][google.cloud.aiplatform.v1.JobService.DeleteCustomJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteCustomJobRequest {
    /// Required. The name of the CustomJob resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/customJobs/{custom_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.CancelCustomJob][google.cloud.aiplatform.v1.JobService.CancelCustomJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CancelCustomJobRequest {
    /// Required. The name of the CustomJob to cancel.
    /// Format:
    /// `projects/{project}/locations/{location}/customJobs/{custom_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.CreateDataLabelingJob][google.cloud.aiplatform.v1.JobService.CreateDataLabelingJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateDataLabelingJobRequest {
    /// Required. The parent of the DataLabelingJob.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The DataLabelingJob to create.
    #[prost(message, optional, tag = "2")]
    pub data_labeling_job: ::core::option::Option<DataLabelingJob>,
}
/// Request message for
/// [JobService.GetDataLabelingJob][google.cloud.aiplatform.v1.JobService.GetDataLabelingJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetDataLabelingJobRequest {
    /// Required. The name of the DataLabelingJob.
    /// Format:
    /// `projects/{project}/locations/{location}/dataLabelingJobs/{data_labeling_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.ListDataLabelingJobs][google.cloud.aiplatform.v1.JobService.ListDataLabelingJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListDataLabelingJobsRequest {
    /// Required. The parent of the DataLabelingJob.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    ///
    /// Supported fields:
    ///
    ///    * `display_name` supports `=`, `!=` comparisons, and `:` wildcard.
    ///    * `state` supports `=`, `!=` comparisons.
    ///    * `create_time` supports `=`, `!=`,`<`, `<=`,`>`, `>=` comparisons.
    ///      `create_time` must be in RFC 3339 format.
    ///    * `labels` supports general map functions that is:
    ///      `labels.key=value` - key:value equality
    ///      `labels.key:* - key existence
    ///
    /// Some examples of using the filter are:
    ///
    ///    * `state="JOB_STATE_SUCCEEDED" AND display_name:"my_job_*"`
    ///    * `state!="JOB_STATE_FAILED" OR display_name="my_job"`
    ///    * `NOT display_name="my_job"`
    ///    * `create_time>"2021-05-18T00:00:00Z"`
    ///    * `labels.keyA=valueA`
    ///    * `labels.keyB:*`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read. FieldMask represents a set of
    /// symbolic field paths. For example, the mask can be `paths: "name"`. The
    /// "name" here is a field in DataLabelingJob.
    /// If this field is not set, all fields of the DataLabelingJob are returned.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// A comma-separated list of fields to order by, sorted in ascending order by
    /// default.
    /// Use `desc` after a field name for descending.
    #[prost(string, tag = "6")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [JobService.ListDataLabelingJobs][google.cloud.aiplatform.v1.JobService.ListDataLabelingJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListDataLabelingJobsResponse {
    /// A list of DataLabelingJobs that matches the specified filter in the
    /// request.
    #[prost(message, repeated, tag = "1")]
    pub data_labeling_jobs: ::prost::alloc::vec::Vec<DataLabelingJob>,
    /// The standard List next-page token.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.DeleteDataLabelingJob][google.cloud.aiplatform.v1.JobService.DeleteDataLabelingJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteDataLabelingJobRequest {
    /// Required. The name of the DataLabelingJob to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/dataLabelingJobs/{data_labeling_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.CancelDataLabelingJob][google.cloud.aiplatform.v1.JobService.CancelDataLabelingJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CancelDataLabelingJobRequest {
    /// Required. The name of the DataLabelingJob.
    /// Format:
    /// `projects/{project}/locations/{location}/dataLabelingJobs/{data_labeling_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.CreateHyperparameterTuningJob][google.cloud.aiplatform.v1.JobService.CreateHyperparameterTuningJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateHyperparameterTuningJobRequest {
    /// Required. The resource name of the Location to create the
    /// HyperparameterTuningJob in. Format:
    /// `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The HyperparameterTuningJob to create.
    #[prost(message, optional, tag = "2")]
    pub hyperparameter_tuning_job: ::core::option::Option<HyperparameterTuningJob>,
}
/// Request message for
/// [JobService.GetHyperparameterTuningJob][google.cloud.aiplatform.v1.JobService.GetHyperparameterTuningJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetHyperparameterTuningJobRequest {
    /// Required. The name of the HyperparameterTuningJob resource.
    /// Format:
    /// `projects/{project}/locations/{location}/hyperparameterTuningJobs/{hyperparameter_tuning_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.ListHyperparameterTuningJobs][google.cloud.aiplatform.v1.JobService.ListHyperparameterTuningJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListHyperparameterTuningJobsRequest {
    /// Required. The resource name of the Location to list the
    /// HyperparameterTuningJobs from. Format:
    /// `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    ///
    /// Supported fields:
    ///
    ///    * `display_name` supports `=`, `!=` comparisons, and `:` wildcard.
    ///    * `state` supports `=`, `!=` comparisons.
    ///    * `create_time` supports `=`, `!=`,`<`, `<=`,`>`, `>=` comparisons.
    ///      `create_time` must be in RFC 3339 format.
    ///    * `labels` supports general map functions that is:
    ///      `labels.key=value` - key:value equality
    ///      `labels.key:* - key existence
    ///
    /// Some examples of using the filter are:
    ///
    ///    * `state="JOB_STATE_SUCCEEDED" AND display_name:"my_job_*"`
    ///    * `state!="JOB_STATE_FAILED" OR display_name="my_job"`
    ///    * `NOT display_name="my_job"`
    ///    * `create_time>"2021-05-18T00:00:00Z"`
    ///    * `labels.keyA=valueA`
    ///    * `labels.keyB:*`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [ListHyperparameterTuningJobsResponse.next_page_token][google.cloud.aiplatform.v1.ListHyperparameterTuningJobsResponse.next_page_token]
    /// of the previous
    /// [JobService.ListHyperparameterTuningJobs][google.cloud.aiplatform.v1.JobService.ListHyperparameterTuningJobs]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [JobService.ListHyperparameterTuningJobs][google.cloud.aiplatform.v1.JobService.ListHyperparameterTuningJobs]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListHyperparameterTuningJobsResponse {
    /// List of HyperparameterTuningJobs in the requested page.
    /// [HyperparameterTuningJob.trials][google.cloud.aiplatform.v1.HyperparameterTuningJob.trials]
    /// of the jobs will be not be returned.
    #[prost(message, repeated, tag = "1")]
    pub hyperparameter_tuning_jobs: ::prost::alloc::vec::Vec<HyperparameterTuningJob>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListHyperparameterTuningJobsRequest.page_token][google.cloud.aiplatform.v1.ListHyperparameterTuningJobsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.DeleteHyperparameterTuningJob][google.cloud.aiplatform.v1.JobService.DeleteHyperparameterTuningJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteHyperparameterTuningJobRequest {
    /// Required. The name of the HyperparameterTuningJob resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/hyperparameterTuningJobs/{hyperparameter_tuning_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.CancelHyperparameterTuningJob][google.cloud.aiplatform.v1.JobService.CancelHyperparameterTuningJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CancelHyperparameterTuningJobRequest {
    /// Required. The name of the HyperparameterTuningJob to cancel.
    /// Format:
    /// `projects/{project}/locations/{location}/hyperparameterTuningJobs/{hyperparameter_tuning_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.CreateNasJob][google.cloud.aiplatform.v1.JobService.CreateNasJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateNasJobRequest {
    /// Required. The resource name of the Location to create the NasJob in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The NasJob to create.
    #[prost(message, optional, tag = "2")]
    pub nas_job: ::core::option::Option<NasJob>,
}
/// Request message for
/// [JobService.GetNasJob][google.cloud.aiplatform.v1.JobService.GetNasJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetNasJobRequest {
    /// Required. The name of the NasJob resource.
    /// Format:
    /// `projects/{project}/locations/{location}/nasJobs/{nas_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.ListNasJobs][google.cloud.aiplatform.v1.JobService.ListNasJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListNasJobsRequest {
    /// Required. The resource name of the Location to list the NasJobs
    /// from. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    ///
    /// Supported fields:
    ///
    ///    * `display_name` supports `=`, `!=` comparisons, and `:` wildcard.
    ///    * `state` supports `=`, `!=` comparisons.
    ///    * `create_time` supports `=`, `!=`,`<`, `<=`,`>`, `>=` comparisons.
    ///      `create_time` must be in RFC 3339 format.
    ///    * `labels` supports general map functions that is:
    ///      `labels.key=value` - key:value equality
    ///      `labels.key:* - key existence
    ///
    /// Some examples of using the filter are:
    ///
    ///    * `state="JOB_STATE_SUCCEEDED" AND display_name:"my_job_*"`
    ///    * `state!="JOB_STATE_FAILED" OR display_name="my_job"`
    ///    * `NOT display_name="my_job"`
    ///    * `create_time>"2021-05-18T00:00:00Z"`
    ///    * `labels.keyA=valueA`
    ///    * `labels.keyB:*`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [ListNasJobsResponse.next_page_token][google.cloud.aiplatform.v1.ListNasJobsResponse.next_page_token]
    /// of the previous
    /// [JobService.ListNasJobs][google.cloud.aiplatform.v1.JobService.ListNasJobs]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [JobService.ListNasJobs][google.cloud.aiplatform.v1.JobService.ListNasJobs]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListNasJobsResponse {
    /// List of NasJobs in the requested page.
    /// [NasJob.nas_job_output][google.cloud.aiplatform.v1.NasJob.nas_job_output]
    /// of the jobs will not be returned.
    #[prost(message, repeated, tag = "1")]
    pub nas_jobs: ::prost::alloc::vec::Vec<NasJob>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListNasJobsRequest.page_token][google.cloud.aiplatform.v1.ListNasJobsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.DeleteNasJob][google.cloud.aiplatform.v1.JobService.DeleteNasJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteNasJobRequest {
    /// Required. The name of the NasJob resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/nasJobs/{nas_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.CancelNasJob][google.cloud.aiplatform.v1.JobService.CancelNasJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CancelNasJobRequest {
    /// Required. The name of the NasJob to cancel.
    /// Format:
    /// `projects/{project}/locations/{location}/nasJobs/{nas_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.GetNasTrialDetail][google.cloud.aiplatform.v1.JobService.GetNasTrialDetail].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetNasTrialDetailRequest {
    /// Required. The name of the NasTrialDetail resource.
    /// Format:
    /// `projects/{project}/locations/{location}/nasJobs/{nas_job}/nasTrialDetails/{nas_trial_detail}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.ListNasTrialDetails][google.cloud.aiplatform.v1.JobService.ListNasTrialDetails].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListNasTrialDetailsRequest {
    /// Required. The name of the NasJob resource.
    /// Format:
    /// `projects/{project}/locations/{location}/nasJobs/{nas_job}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [ListNasTrialDetailsResponse.next_page_token][google.cloud.aiplatform.v1.ListNasTrialDetailsResponse.next_page_token]
    /// of the previous
    /// [JobService.ListNasTrialDetails][google.cloud.aiplatform.v1.JobService.ListNasTrialDetails]
    /// call.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response message for
/// [JobService.ListNasTrialDetails][google.cloud.aiplatform.v1.JobService.ListNasTrialDetails]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListNasTrialDetailsResponse {
    /// List of top NasTrials in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub nas_trial_details: ::prost::alloc::vec::Vec<NasTrialDetail>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListNasTrialDetailsRequest.page_token][google.cloud.aiplatform.v1.ListNasTrialDetailsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.CreateBatchPredictionJob][google.cloud.aiplatform.v1.JobService.CreateBatchPredictionJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateBatchPredictionJobRequest {
    /// Required. The resource name of the Location to create the
    /// BatchPredictionJob in. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The BatchPredictionJob to create.
    #[prost(message, optional, tag = "2")]
    pub batch_prediction_job: ::core::option::Option<BatchPredictionJob>,
}
/// Request message for
/// [JobService.GetBatchPredictionJob][google.cloud.aiplatform.v1.JobService.GetBatchPredictionJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetBatchPredictionJobRequest {
    /// Required. The name of the BatchPredictionJob resource.
    /// Format:
    /// `projects/{project}/locations/{location}/batchPredictionJobs/{batch_prediction_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.ListBatchPredictionJobs][google.cloud.aiplatform.v1.JobService.ListBatchPredictionJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListBatchPredictionJobsRequest {
    /// Required. The resource name of the Location to list the BatchPredictionJobs
    /// from. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    ///
    /// Supported fields:
    ///
    ///    * `display_name` supports `=`, `!=` comparisons, and `:` wildcard.
    ///    * `model_display_name` supports `=`, `!=` comparisons.
    ///    * `state` supports `=`, `!=` comparisons.
    ///    * `create_time` supports `=`, `!=`,`<`, `<=`,`>`, `>=` comparisons.
    ///      `create_time` must be in RFC 3339 format.
    ///    * `labels` supports general map functions that is:
    ///      `labels.key=value` - key:value equality
    ///      `labels.key:* - key existence
    ///
    /// Some examples of using the filter are:
    ///
    ///    * `state="JOB_STATE_SUCCEEDED" AND display_name:"my_job_*"`
    ///    * `state!="JOB_STATE_FAILED" OR display_name="my_job"`
    ///    * `NOT display_name="my_job"`
    ///    * `create_time>"2021-05-18T00:00:00Z"`
    ///    * `labels.keyA=valueA`
    ///    * `labels.keyB:*`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [ListBatchPredictionJobsResponse.next_page_token][google.cloud.aiplatform.v1.ListBatchPredictionJobsResponse.next_page_token]
    /// of the previous
    /// [JobService.ListBatchPredictionJobs][google.cloud.aiplatform.v1.JobService.ListBatchPredictionJobs]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [JobService.ListBatchPredictionJobs][google.cloud.aiplatform.v1.JobService.ListBatchPredictionJobs]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListBatchPredictionJobsResponse {
    /// List of BatchPredictionJobs in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub batch_prediction_jobs: ::prost::alloc::vec::Vec<BatchPredictionJob>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListBatchPredictionJobsRequest.page_token][google.cloud.aiplatform.v1.ListBatchPredictionJobsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.DeleteBatchPredictionJob][google.cloud.aiplatform.v1.JobService.DeleteBatchPredictionJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteBatchPredictionJobRequest {
    /// Required. The name of the BatchPredictionJob resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/batchPredictionJobs/{batch_prediction_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.CancelBatchPredictionJob][google.cloud.aiplatform.v1.JobService.CancelBatchPredictionJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CancelBatchPredictionJobRequest {
    /// Required. The name of the BatchPredictionJob to cancel.
    /// Format:
    /// `projects/{project}/locations/{location}/batchPredictionJobs/{batch_prediction_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.CreateModelDeploymentMonitoringJob][google.cloud.aiplatform.v1.JobService.CreateModelDeploymentMonitoringJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateModelDeploymentMonitoringJobRequest {
    /// Required. The parent of the ModelDeploymentMonitoringJob.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The ModelDeploymentMonitoringJob to create
    #[prost(message, optional, tag = "2")]
    pub model_deployment_monitoring_job: ::core::option::Option<
        ModelDeploymentMonitoringJob,
    >,
}
/// Request message for
/// [JobService.SearchModelDeploymentMonitoringStatsAnomalies][google.cloud.aiplatform.v1.JobService.SearchModelDeploymentMonitoringStatsAnomalies].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchModelDeploymentMonitoringStatsAnomaliesRequest {
    /// Required. ModelDeploymentMonitoring Job resource name.
    /// Format:
    /// `projects/{project}/locations/{location}/modelDeploymentMonitoringJobs/{model_deployment_monitoring_job}`
    #[prost(string, tag = "1")]
    pub model_deployment_monitoring_job: ::prost::alloc::string::String,
    /// Required. The DeployedModel ID of the
    /// \[ModelDeploymentMonitoringObjectiveConfig.deployed_model_id\].
    #[prost(string, tag = "2")]
    pub deployed_model_id: ::prost::alloc::string::String,
    /// The feature display name. If specified, only return the stats belonging to
    /// this feature. Format:
    /// [ModelMonitoringStatsAnomalies.FeatureHistoricStatsAnomalies.feature_display_name][google.cloud.aiplatform.v1.ModelMonitoringStatsAnomalies.FeatureHistoricStatsAnomalies.feature_display_name],
    /// example: "user_destination".
    #[prost(string, tag = "3")]
    pub feature_display_name: ::prost::alloc::string::String,
    /// Required. Objectives of the stats to retrieve.
    #[prost(message, repeated, tag = "4")]
    pub objectives: ::prost::alloc::vec::Vec<
        search_model_deployment_monitoring_stats_anomalies_request::StatsAnomaliesObjective,
    >,
    /// The standard list page size.
    #[prost(int32, tag = "5")]
    pub page_size: i32,
    /// A page token received from a previous
    /// [JobService.SearchModelDeploymentMonitoringStatsAnomalies][google.cloud.aiplatform.v1.JobService.SearchModelDeploymentMonitoringStatsAnomalies]
    /// call.
    #[prost(string, tag = "6")]
    pub page_token: ::prost::alloc::string::String,
    /// The earliest timestamp of stats being generated.
    /// If not set, indicates fetching stats till the earliest possible one.
    #[prost(message, optional, tag = "7")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// The latest timestamp of stats being generated.
    /// If not set, indicates feching stats till the latest possible one.
    #[prost(message, optional, tag = "8")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
}
/// Nested message and enum types in `SearchModelDeploymentMonitoringStatsAnomaliesRequest`.
pub mod search_model_deployment_monitoring_stats_anomalies_request {
    /// Stats requested for specific objective.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct StatsAnomaliesObjective {
        #[prost(
            enumeration = "super::ModelDeploymentMonitoringObjectiveType",
            tag = "1"
        )]
        pub r#type: i32,
        /// If set, all attribution scores between
        /// [SearchModelDeploymentMonitoringStatsAnomaliesRequest.start_time][google.cloud.aiplatform.v1.SearchModelDeploymentMonitoringStatsAnomaliesRequest.start_time]
        /// and
        /// [SearchModelDeploymentMonitoringStatsAnomaliesRequest.end_time][google.cloud.aiplatform.v1.SearchModelDeploymentMonitoringStatsAnomaliesRequest.end_time]
        /// are fetched, and page token doesn't take effect in this case. Only used
        /// to retrieve attribution score for the top Features which has the highest
        /// attribution score in the latest monitoring run.
        #[prost(int32, tag = "4")]
        pub top_feature_count: i32,
    }
}
/// Response message for
/// [JobService.SearchModelDeploymentMonitoringStatsAnomalies][google.cloud.aiplatform.v1.JobService.SearchModelDeploymentMonitoringStatsAnomalies].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchModelDeploymentMonitoringStatsAnomaliesResponse {
    /// Stats retrieved for requested objectives.
    /// There are at most 1000
    /// [ModelMonitoringStatsAnomalies.FeatureHistoricStatsAnomalies.prediction_stats][google.cloud.aiplatform.v1.ModelMonitoringStatsAnomalies.FeatureHistoricStatsAnomalies.prediction_stats]
    /// in the response.
    #[prost(message, repeated, tag = "1")]
    pub monitoring_stats: ::prost::alloc::vec::Vec<ModelMonitoringStatsAnomalies>,
    /// The page token that can be used by the next
    /// [JobService.SearchModelDeploymentMonitoringStatsAnomalies][google.cloud.aiplatform.v1.JobService.SearchModelDeploymentMonitoringStatsAnomalies]
    /// call.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.GetModelDeploymentMonitoringJob][google.cloud.aiplatform.v1.JobService.GetModelDeploymentMonitoringJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetModelDeploymentMonitoringJobRequest {
    /// Required. The resource name of the ModelDeploymentMonitoringJob.
    /// Format:
    /// `projects/{project}/locations/{location}/modelDeploymentMonitoringJobs/{model_deployment_monitoring_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.ListModelDeploymentMonitoringJobs][google.cloud.aiplatform.v1.JobService.ListModelDeploymentMonitoringJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelDeploymentMonitoringJobsRequest {
    /// Required. The parent of the ModelDeploymentMonitoringJob.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    ///
    /// Supported fields:
    ///
    ///    * `display_name` supports `=`, `!=` comparisons, and `:` wildcard.
    ///    * `state` supports `=`, `!=` comparisons.
    ///    * `create_time` supports `=`, `!=`,`<`, `<=`,`>`, `>=` comparisons.
    ///      `create_time` must be in RFC 3339 format.
    ///    * `labels` supports general map functions that is:
    ///      `labels.key=value` - key:value equality
    ///      `labels.key:* - key existence
    ///
    /// Some examples of using the filter are:
    ///
    ///    * `state="JOB_STATE_SUCCEEDED" AND display_name:"my_job_*"`
    ///    * `state!="JOB_STATE_FAILED" OR display_name="my_job"`
    ///    * `NOT display_name="my_job"`
    ///    * `create_time>"2021-05-18T00:00:00Z"`
    ///    * `labels.keyA=valueA`
    ///    * `labels.keyB:*`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [JobService.ListModelDeploymentMonitoringJobs][google.cloud.aiplatform.v1.JobService.ListModelDeploymentMonitoringJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelDeploymentMonitoringJobsResponse {
    /// A list of ModelDeploymentMonitoringJobs that matches the specified filter
    /// in the request.
    #[prost(message, repeated, tag = "1")]
    pub model_deployment_monitoring_jobs: ::prost::alloc::vec::Vec<
        ModelDeploymentMonitoringJob,
    >,
    /// The standard List next-page token.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.UpdateModelDeploymentMonitoringJob][google.cloud.aiplatform.v1.JobService.UpdateModelDeploymentMonitoringJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateModelDeploymentMonitoringJobRequest {
    /// Required. The model monitoring configuration which replaces the resource on
    /// the server.
    #[prost(message, optional, tag = "1")]
    pub model_deployment_monitoring_job: ::core::option::Option<
        ModelDeploymentMonitoringJob,
    >,
    /// Required. The update mask is used to specify the fields to be overwritten
    /// in the ModelDeploymentMonitoringJob resource by the update. The fields
    /// specified in the update_mask are relative to the resource, not the full
    /// request. A field will be overwritten if it is in the mask. If the user does
    /// not provide a mask then only the non-empty fields present in the request
    /// will be overwritten. Set the update_mask to `*` to override all fields. For
    /// the objective config, the user can either provide the update mask for
    /// model_deployment_monitoring_objective_configs or any combination of its
    /// nested fields, such as:
    /// model_deployment_monitoring_objective_configs.objective_config.training_dataset.
    ///
    /// Updatable fields:
    ///
    ///    * `display_name`
    ///    * `model_deployment_monitoring_schedule_config`
    ///    * `model_monitoring_alert_config`
    ///    * `logging_sampling_strategy`
    ///    * `labels`
    ///    * `log_ttl`
    ///    * `enable_monitoring_pipeline_logs`
    /// .  and
    ///    * `model_deployment_monitoring_objective_configs`
    /// .  or
    ///    * `model_deployment_monitoring_objective_configs.objective_config.training_dataset`
    ///    * `model_deployment_monitoring_objective_configs.objective_config.training_prediction_skew_detection_config`
    ///    * `model_deployment_monitoring_objective_configs.objective_config.prediction_drift_detection_config`
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [JobService.DeleteModelDeploymentMonitoringJob][google.cloud.aiplatform.v1.JobService.DeleteModelDeploymentMonitoringJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteModelDeploymentMonitoringJobRequest {
    /// Required. The resource name of the model monitoring job to delete.
    /// Format:
    /// `projects/{project}/locations/{location}/modelDeploymentMonitoringJobs/{model_deployment_monitoring_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.PauseModelDeploymentMonitoringJob][google.cloud.aiplatform.v1.JobService.PauseModelDeploymentMonitoringJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PauseModelDeploymentMonitoringJobRequest {
    /// Required. The resource name of the ModelDeploymentMonitoringJob to pause.
    /// Format:
    /// `projects/{project}/locations/{location}/modelDeploymentMonitoringJobs/{model_deployment_monitoring_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [JobService.ResumeModelDeploymentMonitoringJob][google.cloud.aiplatform.v1.JobService.ResumeModelDeploymentMonitoringJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ResumeModelDeploymentMonitoringJobRequest {
    /// Required. The resource name of the ModelDeploymentMonitoringJob to resume.
    /// Format:
    /// `projects/{project}/locations/{location}/modelDeploymentMonitoringJobs/{model_deployment_monitoring_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Runtime operation information for
/// [JobService.UpdateModelDeploymentMonitoringJob][google.cloud.aiplatform.v1.JobService.UpdateModelDeploymentMonitoringJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateModelDeploymentMonitoringJobOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Generated client implementations.
pub mod job_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for creating and managing Vertex AI's jobs.
    #[derive(Debug, Clone)]
    pub struct JobServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl JobServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> JobServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> JobServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            JobServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a CustomJob. A created CustomJob right away
        /// will be attempted to be run.
        pub async fn create_custom_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateCustomJobRequest>,
        ) -> std::result::Result<tonic::Response<super::CustomJob>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/CreateCustomJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "CreateCustomJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a CustomJob.
        pub async fn get_custom_job(
            &mut self,
            request: impl tonic::IntoRequest<super::GetCustomJobRequest>,
        ) -> std::result::Result<tonic::Response<super::CustomJob>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/GetCustomJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "GetCustomJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists CustomJobs in a Location.
        pub async fn list_custom_jobs(
            &mut self,
            request: impl tonic::IntoRequest<super::ListCustomJobsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListCustomJobsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/ListCustomJobs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "ListCustomJobs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a CustomJob.
        pub async fn delete_custom_job(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteCustomJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/DeleteCustomJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "DeleteCustomJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Cancels a CustomJob.
        /// Starts asynchronous cancellation on the CustomJob. The server
        /// makes a best effort to cancel the job, but success is not
        /// guaranteed. Clients can use
        /// [JobService.GetCustomJob][google.cloud.aiplatform.v1.JobService.GetCustomJob]
        /// or other methods to check whether the cancellation succeeded or whether the
        /// job completed despite cancellation. On successful cancellation,
        /// the CustomJob is not deleted; instead it becomes a job with
        /// a [CustomJob.error][google.cloud.aiplatform.v1.CustomJob.error] value with
        /// a [google.rpc.Status.code][google.rpc.Status.code] of 1, corresponding to
        /// `Code.CANCELLED`, and
        /// [CustomJob.state][google.cloud.aiplatform.v1.CustomJob.state] is set to
        /// `CANCELLED`.
        pub async fn cancel_custom_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CancelCustomJobRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/CancelCustomJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "CancelCustomJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a DataLabelingJob.
        pub async fn create_data_labeling_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateDataLabelingJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::DataLabelingJob>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/CreateDataLabelingJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "CreateDataLabelingJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a DataLabelingJob.
        pub async fn get_data_labeling_job(
            &mut self,
            request: impl tonic::IntoRequest<super::GetDataLabelingJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::DataLabelingJob>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/GetDataLabelingJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "GetDataLabelingJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists DataLabelingJobs in a Location.
        pub async fn list_data_labeling_jobs(
            &mut self,
            request: impl tonic::IntoRequest<super::ListDataLabelingJobsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListDataLabelingJobsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/ListDataLabelingJobs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "ListDataLabelingJobs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a DataLabelingJob.
        pub async fn delete_data_labeling_job(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteDataLabelingJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/DeleteDataLabelingJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "DeleteDataLabelingJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Cancels a DataLabelingJob. Success of cancellation is not guaranteed.
        pub async fn cancel_data_labeling_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CancelDataLabelingJobRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/CancelDataLabelingJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "CancelDataLabelingJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a HyperparameterTuningJob
        pub async fn create_hyperparameter_tuning_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateHyperparameterTuningJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::HyperparameterTuningJob>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/CreateHyperparameterTuningJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "CreateHyperparameterTuningJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a HyperparameterTuningJob
        pub async fn get_hyperparameter_tuning_job(
            &mut self,
            request: impl tonic::IntoRequest<super::GetHyperparameterTuningJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::HyperparameterTuningJob>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/GetHyperparameterTuningJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "GetHyperparameterTuningJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists HyperparameterTuningJobs in a Location.
        pub async fn list_hyperparameter_tuning_jobs(
            &mut self,
            request: impl tonic::IntoRequest<super::ListHyperparameterTuningJobsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListHyperparameterTuningJobsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/ListHyperparameterTuningJobs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "ListHyperparameterTuningJobs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a HyperparameterTuningJob.
        pub async fn delete_hyperparameter_tuning_job(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteHyperparameterTuningJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/DeleteHyperparameterTuningJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "DeleteHyperparameterTuningJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Cancels a HyperparameterTuningJob.
        /// Starts asynchronous cancellation on the HyperparameterTuningJob. The server
        /// makes a best effort to cancel the job, but success is not
        /// guaranteed. Clients can use
        /// [JobService.GetHyperparameterTuningJob][google.cloud.aiplatform.v1.JobService.GetHyperparameterTuningJob]
        /// or other methods to check whether the cancellation succeeded or whether the
        /// job completed despite cancellation. On successful cancellation,
        /// the HyperparameterTuningJob is not deleted; instead it becomes a job with
        /// a
        /// [HyperparameterTuningJob.error][google.cloud.aiplatform.v1.HyperparameterTuningJob.error]
        /// value with a [google.rpc.Status.code][google.rpc.Status.code] of 1,
        /// corresponding to `Code.CANCELLED`, and
        /// [HyperparameterTuningJob.state][google.cloud.aiplatform.v1.HyperparameterTuningJob.state]
        /// is set to `CANCELLED`.
        pub async fn cancel_hyperparameter_tuning_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CancelHyperparameterTuningJobRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/CancelHyperparameterTuningJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "CancelHyperparameterTuningJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a NasJob
        pub async fn create_nas_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateNasJobRequest>,
        ) -> std::result::Result<tonic::Response<super::NasJob>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/CreateNasJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "CreateNasJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a NasJob
        pub async fn get_nas_job(
            &mut self,
            request: impl tonic::IntoRequest<super::GetNasJobRequest>,
        ) -> std::result::Result<tonic::Response<super::NasJob>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/GetNasJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("google.cloud.aiplatform.v1.JobService", "GetNasJob"),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists NasJobs in a Location.
        pub async fn list_nas_jobs(
            &mut self,
            request: impl tonic::IntoRequest<super::ListNasJobsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListNasJobsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/ListNasJobs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "ListNasJobs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a NasJob.
        pub async fn delete_nas_job(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteNasJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/DeleteNasJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "DeleteNasJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Cancels a NasJob.
        /// Starts asynchronous cancellation on the NasJob. The server
        /// makes a best effort to cancel the job, but success is not
        /// guaranteed. Clients can use
        /// [JobService.GetNasJob][google.cloud.aiplatform.v1.JobService.GetNasJob] or
        /// other methods to check whether the cancellation succeeded or whether the
        /// job completed despite cancellation. On successful cancellation,
        /// the NasJob is not deleted; instead it becomes a job with
        /// a [NasJob.error][google.cloud.aiplatform.v1.NasJob.error] value with a
        /// [google.rpc.Status.code][google.rpc.Status.code] of 1, corresponding to
        /// `Code.CANCELLED`, and
        /// [NasJob.state][google.cloud.aiplatform.v1.NasJob.state] is set to
        /// `CANCELLED`.
        pub async fn cancel_nas_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CancelNasJobRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/CancelNasJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "CancelNasJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a NasTrialDetail.
        pub async fn get_nas_trial_detail(
            &mut self,
            request: impl tonic::IntoRequest<super::GetNasTrialDetailRequest>,
        ) -> std::result::Result<tonic::Response<super::NasTrialDetail>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/GetNasTrialDetail",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "GetNasTrialDetail",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// List top NasTrialDetails of a NasJob.
        pub async fn list_nas_trial_details(
            &mut self,
            request: impl tonic::IntoRequest<super::ListNasTrialDetailsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListNasTrialDetailsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/ListNasTrialDetails",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "ListNasTrialDetails",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a BatchPredictionJob. A BatchPredictionJob once created will
        /// right away be attempted to start.
        pub async fn create_batch_prediction_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateBatchPredictionJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::BatchPredictionJob>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/CreateBatchPredictionJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "CreateBatchPredictionJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a BatchPredictionJob
        pub async fn get_batch_prediction_job(
            &mut self,
            request: impl tonic::IntoRequest<super::GetBatchPredictionJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::BatchPredictionJob>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/GetBatchPredictionJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "GetBatchPredictionJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists BatchPredictionJobs in a Location.
        pub async fn list_batch_prediction_jobs(
            &mut self,
            request: impl tonic::IntoRequest<super::ListBatchPredictionJobsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListBatchPredictionJobsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/ListBatchPredictionJobs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "ListBatchPredictionJobs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a BatchPredictionJob. Can only be called on jobs that already
        /// finished.
        pub async fn delete_batch_prediction_job(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteBatchPredictionJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/DeleteBatchPredictionJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "DeleteBatchPredictionJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Cancels a BatchPredictionJob.
        ///
        /// Starts asynchronous cancellation on the BatchPredictionJob. The server
        /// makes the best effort to cancel the job, but success is not
        /// guaranteed. Clients can use
        /// [JobService.GetBatchPredictionJob][google.cloud.aiplatform.v1.JobService.GetBatchPredictionJob]
        /// or other methods to check whether the cancellation succeeded or whether the
        /// job completed despite cancellation. On a successful cancellation,
        /// the BatchPredictionJob is not deleted;instead its
        /// [BatchPredictionJob.state][google.cloud.aiplatform.v1.BatchPredictionJob.state]
        /// is set to `CANCELLED`. Any files already outputted by the job are not
        /// deleted.
        pub async fn cancel_batch_prediction_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CancelBatchPredictionJobRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/CancelBatchPredictionJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "CancelBatchPredictionJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a ModelDeploymentMonitoringJob. It will run periodically on a
        /// configured interval.
        pub async fn create_model_deployment_monitoring_job(
            &mut self,
            request: impl tonic::IntoRequest<
                super::CreateModelDeploymentMonitoringJobRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::ModelDeploymentMonitoringJob>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/CreateModelDeploymentMonitoringJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "CreateModelDeploymentMonitoringJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Searches Model Monitoring Statistics generated within a given time window.
        pub async fn search_model_deployment_monitoring_stats_anomalies(
            &mut self,
            request: impl tonic::IntoRequest<
                super::SearchModelDeploymentMonitoringStatsAnomaliesRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<
                super::SearchModelDeploymentMonitoringStatsAnomaliesResponse,
            >,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/SearchModelDeploymentMonitoringStatsAnomalies",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "SearchModelDeploymentMonitoringStatsAnomalies",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a ModelDeploymentMonitoringJob.
        pub async fn get_model_deployment_monitoring_job(
            &mut self,
            request: impl tonic::IntoRequest<
                super::GetModelDeploymentMonitoringJobRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::ModelDeploymentMonitoringJob>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/GetModelDeploymentMonitoringJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "GetModelDeploymentMonitoringJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists ModelDeploymentMonitoringJobs in a Location.
        pub async fn list_model_deployment_monitoring_jobs(
            &mut self,
            request: impl tonic::IntoRequest<
                super::ListModelDeploymentMonitoringJobsRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::ListModelDeploymentMonitoringJobsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/ListModelDeploymentMonitoringJobs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "ListModelDeploymentMonitoringJobs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a ModelDeploymentMonitoringJob.
        pub async fn update_model_deployment_monitoring_job(
            &mut self,
            request: impl tonic::IntoRequest<
                super::UpdateModelDeploymentMonitoringJobRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/UpdateModelDeploymentMonitoringJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "UpdateModelDeploymentMonitoringJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a ModelDeploymentMonitoringJob.
        pub async fn delete_model_deployment_monitoring_job(
            &mut self,
            request: impl tonic::IntoRequest<
                super::DeleteModelDeploymentMonitoringJobRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/DeleteModelDeploymentMonitoringJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "DeleteModelDeploymentMonitoringJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Pauses a ModelDeploymentMonitoringJob. If the job is running, the server
        /// makes a best effort to cancel the job. Will mark
        /// [ModelDeploymentMonitoringJob.state][google.cloud.aiplatform.v1.ModelDeploymentMonitoringJob.state]
        /// to 'PAUSED'.
        pub async fn pause_model_deployment_monitoring_job(
            &mut self,
            request: impl tonic::IntoRequest<
                super::PauseModelDeploymentMonitoringJobRequest,
            >,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/PauseModelDeploymentMonitoringJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "PauseModelDeploymentMonitoringJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Resumes a paused ModelDeploymentMonitoringJob. It will start to run from
        /// next scheduled time. A deleted ModelDeploymentMonitoringJob can't be
        /// resumed.
        pub async fn resume_model_deployment_monitoring_job(
            &mut self,
            request: impl tonic::IntoRequest<
                super::ResumeModelDeploymentMonitoringJobRequest,
            >,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.JobService/ResumeModelDeploymentMonitoringJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.JobService",
                        "ResumeModelDeploymentMonitoringJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// A subgraph of the overall lineage graph. Event edges connect Artifact and
/// Execution nodes.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct LineageSubgraph {
    /// The Artifact nodes in the subgraph.
    #[prost(message, repeated, tag = "1")]
    pub artifacts: ::prost::alloc::vec::Vec<Artifact>,
    /// The Execution nodes in the subgraph.
    #[prost(message, repeated, tag = "2")]
    pub executions: ::prost::alloc::vec::Vec<Execution>,
    /// The Event edges between Artifacts and Executions in the subgraph.
    #[prost(message, repeated, tag = "3")]
    pub events: ::prost::alloc::vec::Vec<Event>,
}
/// Request message for
/// [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PredictRequest {
    /// Required. The name of the Endpoint requested to serve the prediction.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// Required. The instances that are the input to the prediction call.
    /// A DeployedModel may have an upper limit on the number of instances it
    /// supports per request, and when it is exceeded the prediction call errors
    /// in case of AutoML Models, or, in case of customer created Models, the
    /// behaviour is as documented by that Model.
    /// The schema of any single instance may be specified via Endpoint's
    /// DeployedModels' [Model's][google.cloud.aiplatform.v1.DeployedModel.model]
    /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
    /// [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri].
    #[prost(message, repeated, tag = "2")]
    pub instances: ::prost::alloc::vec::Vec<::prost_types::Value>,
    /// The parameters that govern the prediction. The schema of the parameters may
    /// be specified via Endpoint's DeployedModels' [Model's
    /// ][google.cloud.aiplatform.v1.DeployedModel.model]
    /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
    /// [parameters_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri].
    #[prost(message, optional, tag = "3")]
    pub parameters: ::core::option::Option<::prost_types::Value>,
}
/// Response message for
/// [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PredictResponse {
    /// The predictions that are the output of the predictions call.
    /// The schema of any single prediction may be specified via Endpoint's
    /// DeployedModels' [Model's ][google.cloud.aiplatform.v1.DeployedModel.model]
    /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
    /// [prediction_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.prediction_schema_uri].
    #[prost(message, repeated, tag = "1")]
    pub predictions: ::prost::alloc::vec::Vec<::prost_types::Value>,
    /// ID of the Endpoint's DeployedModel that served this prediction.
    #[prost(string, tag = "2")]
    pub deployed_model_id: ::prost::alloc::string::String,
    /// Output only. The resource name of the Model which is deployed as the
    /// DeployedModel that this prediction hits.
    #[prost(string, tag = "3")]
    pub model: ::prost::alloc::string::String,
    /// Output only. The version ID of the Model which is deployed as the
    /// DeployedModel that this prediction hits.
    #[prost(string, tag = "5")]
    pub model_version_id: ::prost::alloc::string::String,
    /// Output only. The [display
    /// name][google.cloud.aiplatform.v1.Model.display_name] of the Model which is
    /// deployed as the DeployedModel that this prediction hits.
    #[prost(string, tag = "4")]
    pub model_display_name: ::prost::alloc::string::String,
    /// Output only. Request-level metadata returned by the model. The metadata
    /// type will be dependent upon the model implementation.
    #[prost(message, optional, tag = "6")]
    pub metadata: ::core::option::Option<::prost_types::Value>,
}
/// Request message for
/// [PredictionService.RawPredict][google.cloud.aiplatform.v1.PredictionService.RawPredict].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RawPredictRequest {
    /// Required. The name of the Endpoint requested to serve the prediction.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// The prediction input. Supports HTTP headers and arbitrary data payload.
    ///
    /// A [DeployedModel][google.cloud.aiplatform.v1.DeployedModel] may have an
    /// upper limit on the number of instances it supports per request. When this
    /// limit it is exceeded for an AutoML model, the
    /// [RawPredict][google.cloud.aiplatform.v1.PredictionService.RawPredict]
    /// method returns an error. When this limit is exceeded for a custom-trained
    /// model, the behavior varies depending on the model.
    ///
    /// You can specify the schema for each instance in the
    /// [predict_schemata.instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri]
    /// field when you create a [Model][google.cloud.aiplatform.v1.Model]. This
    /// schema applies when you deploy the `Model` as a `DeployedModel` to an
    /// [Endpoint][google.cloud.aiplatform.v1.Endpoint] and use the `RawPredict`
    /// method.
    #[prost(message, optional, tag = "2")]
    pub http_body: ::core::option::Option<super::super::super::api::HttpBody>,
}
/// Request message for
/// [PredictionService.StreamRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamRawPredict].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StreamRawPredictRequest {
    /// Required. The name of the Endpoint requested to serve the prediction.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// The prediction input. Supports HTTP headers and arbitrary data payload.
    #[prost(message, optional, tag = "2")]
    pub http_body: ::core::option::Option<super::super::super::api::HttpBody>,
}
/// Request message for
/// [PredictionService.DirectPredict][google.cloud.aiplatform.v1.PredictionService.DirectPredict].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DirectPredictRequest {
    /// Required. The name of the Endpoint requested to serve the prediction.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// The prediction input.
    #[prost(message, repeated, tag = "2")]
    pub inputs: ::prost::alloc::vec::Vec<Tensor>,
    /// The parameters that govern the prediction.
    #[prost(message, optional, tag = "3")]
    pub parameters: ::core::option::Option<Tensor>,
}
/// Response message for
/// [PredictionService.DirectPredict][google.cloud.aiplatform.v1.PredictionService.DirectPredict].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DirectPredictResponse {
    /// The prediction output.
    #[prost(message, repeated, tag = "1")]
    pub outputs: ::prost::alloc::vec::Vec<Tensor>,
    /// The parameters that govern the prediction.
    #[prost(message, optional, tag = "2")]
    pub parameters: ::core::option::Option<Tensor>,
}
/// Request message for
/// [PredictionService.DirectRawPredict][google.cloud.aiplatform.v1.PredictionService.DirectRawPredict].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DirectRawPredictRequest {
    /// Required. The name of the Endpoint requested to serve the prediction.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// Fully qualified name of the API method being invoked to perform
    /// predictions.
    ///
    /// Format:
    /// `/namespace.Service/Method/`
    /// Example:
    /// `/tensorflow.serving.PredictionService/Predict`
    #[prost(string, tag = "2")]
    pub method_name: ::prost::alloc::string::String,
    /// The prediction input.
    #[prost(bytes = "vec", tag = "3")]
    pub input: ::prost::alloc::vec::Vec<u8>,
}
/// Response message for
/// [PredictionService.DirectRawPredict][google.cloud.aiplatform.v1.PredictionService.DirectRawPredict].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DirectRawPredictResponse {
    /// The prediction output.
    #[prost(bytes = "vec", tag = "1")]
    pub output: ::prost::alloc::vec::Vec<u8>,
}
/// Request message for
/// [PredictionService.StreamDirectPredict][google.cloud.aiplatform.v1.PredictionService.StreamDirectPredict].
///
/// The first message must contain
/// [endpoint][google.cloud.aiplatform.v1.StreamDirectPredictRequest.endpoint]
/// field and optionally [input][]. The subsequent messages must contain
/// [input][].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StreamDirectPredictRequest {
    /// Required. The name of the Endpoint requested to serve the prediction.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// Optional. The prediction input.
    #[prost(message, repeated, tag = "2")]
    pub inputs: ::prost::alloc::vec::Vec<Tensor>,
    /// Optional. The parameters that govern the prediction.
    #[prost(message, optional, tag = "3")]
    pub parameters: ::core::option::Option<Tensor>,
}
/// Response message for
/// [PredictionService.StreamDirectPredict][google.cloud.aiplatform.v1.PredictionService.StreamDirectPredict].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StreamDirectPredictResponse {
    /// The prediction output.
    #[prost(message, repeated, tag = "1")]
    pub outputs: ::prost::alloc::vec::Vec<Tensor>,
    /// The parameters that govern the prediction.
    #[prost(message, optional, tag = "2")]
    pub parameters: ::core::option::Option<Tensor>,
}
/// Request message for
/// [PredictionService.StreamDirectRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamDirectRawPredict].
///
/// The first message must contain
/// [endpoint][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.endpoint]
/// and
/// [method_name][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.method_name]
/// fields and optionally
/// [input][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.input]. The
/// subsequent messages must contain
/// [input][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.input].
/// [method_name][google.cloud.aiplatform.v1.StreamDirectRawPredictRequest.method_name]
/// in the subsequent messages have no effect.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StreamDirectRawPredictRequest {
    /// Required. The name of the Endpoint requested to serve the prediction.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// Optional. Fully qualified name of the API method being invoked to perform
    /// predictions.
    ///
    /// Format:
    /// `/namespace.Service/Method/`
    /// Example:
    /// `/tensorflow.serving.PredictionService/Predict`
    #[prost(string, tag = "2")]
    pub method_name: ::prost::alloc::string::String,
    /// Optional. The prediction input.
    #[prost(bytes = "vec", tag = "3")]
    pub input: ::prost::alloc::vec::Vec<u8>,
}
/// Response message for
/// [PredictionService.StreamDirectRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamDirectRawPredict].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StreamDirectRawPredictResponse {
    /// The prediction output.
    #[prost(bytes = "vec", tag = "1")]
    pub output: ::prost::alloc::vec::Vec<u8>,
}
/// Request message for
/// [PredictionService.StreamingPredict][google.cloud.aiplatform.v1.PredictionService.StreamingPredict].
///
/// The first message must contain
/// [endpoint][google.cloud.aiplatform.v1.StreamingPredictRequest.endpoint] field
/// and optionally [input][]. The subsequent messages must contain [input][].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StreamingPredictRequest {
    /// Required. The name of the Endpoint requested to serve the prediction.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// The prediction input.
    #[prost(message, repeated, tag = "2")]
    pub inputs: ::prost::alloc::vec::Vec<Tensor>,
    /// The parameters that govern the prediction.
    #[prost(message, optional, tag = "3")]
    pub parameters: ::core::option::Option<Tensor>,
}
/// Response message for
/// [PredictionService.StreamingPredict][google.cloud.aiplatform.v1.PredictionService.StreamingPredict].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StreamingPredictResponse {
    /// The prediction output.
    #[prost(message, repeated, tag = "1")]
    pub outputs: ::prost::alloc::vec::Vec<Tensor>,
    /// The parameters that govern the prediction.
    #[prost(message, optional, tag = "2")]
    pub parameters: ::core::option::Option<Tensor>,
}
/// Request message for
/// [PredictionService.StreamingRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamingRawPredict].
///
/// The first message must contain
/// [endpoint][google.cloud.aiplatform.v1.StreamingRawPredictRequest.endpoint]
/// and
/// [method_name][google.cloud.aiplatform.v1.StreamingRawPredictRequest.method_name]
/// fields and optionally
/// [input][google.cloud.aiplatform.v1.StreamingRawPredictRequest.input]. The
/// subsequent messages must contain
/// [input][google.cloud.aiplatform.v1.StreamingRawPredictRequest.input].
/// [method_name][google.cloud.aiplatform.v1.StreamingRawPredictRequest.method_name]
/// in the subsequent messages have no effect.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StreamingRawPredictRequest {
    /// Required. The name of the Endpoint requested to serve the prediction.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// Fully qualified name of the API method being invoked to perform
    /// predictions.
    ///
    /// Format:
    /// `/namespace.Service/Method/`
    /// Example:
    /// `/tensorflow.serving.PredictionService/Predict`
    #[prost(string, tag = "2")]
    pub method_name: ::prost::alloc::string::String,
    /// The prediction input.
    #[prost(bytes = "vec", tag = "3")]
    pub input: ::prost::alloc::vec::Vec<u8>,
}
/// Response message for
/// [PredictionService.StreamingRawPredict][google.cloud.aiplatform.v1.PredictionService.StreamingRawPredict].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StreamingRawPredictResponse {
    /// The prediction output.
    #[prost(bytes = "vec", tag = "1")]
    pub output: ::prost::alloc::vec::Vec<u8>,
}
/// Request message for
/// [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExplainRequest {
    /// Required. The name of the Endpoint requested to serve the explanation.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// Required. The instances that are the input to the explanation call.
    /// A DeployedModel may have an upper limit on the number of instances it
    /// supports per request, and when it is exceeded the explanation call errors
    /// in case of AutoML Models, or, in case of customer created Models, the
    /// behaviour is as documented by that Model.
    /// The schema of any single instance may be specified via Endpoint's
    /// DeployedModels' [Model's][google.cloud.aiplatform.v1.DeployedModel.model]
    /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
    /// [instance_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.instance_schema_uri].
    #[prost(message, repeated, tag = "2")]
    pub instances: ::prost::alloc::vec::Vec<::prost_types::Value>,
    /// The parameters that govern the prediction. The schema of the parameters may
    /// be specified via Endpoint's DeployedModels' [Model's
    /// ][google.cloud.aiplatform.v1.DeployedModel.model]
    /// [PredictSchemata's][google.cloud.aiplatform.v1.Model.predict_schemata]
    /// [parameters_schema_uri][google.cloud.aiplatform.v1.PredictSchemata.parameters_schema_uri].
    #[prost(message, optional, tag = "4")]
    pub parameters: ::core::option::Option<::prost_types::Value>,
    /// If specified, overrides the
    /// [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
    /// of the DeployedModel. Can be used for explaining prediction results with
    /// different configurations, such as:
    ///   - Explaining top-5 predictions results as opposed to top-1;
    ///   - Increasing path count or step count of the attribution methods to reduce
    ///     approximate errors;
    ///   - Using different baselines for explaining the prediction results.
    #[prost(message, optional, tag = "5")]
    pub explanation_spec_override: ::core::option::Option<ExplanationSpecOverride>,
    /// If specified, this ExplainRequest will be served by the chosen
    /// DeployedModel, overriding
    /// [Endpoint.traffic_split][google.cloud.aiplatform.v1.Endpoint.traffic_split].
    #[prost(string, tag = "3")]
    pub deployed_model_id: ::prost::alloc::string::String,
}
/// Response message for
/// [PredictionService.Explain][google.cloud.aiplatform.v1.PredictionService.Explain].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExplainResponse {
    /// The explanations of the Model's
    /// [PredictResponse.predictions][google.cloud.aiplatform.v1.PredictResponse.predictions].
    ///
    /// It has the same number of elements as
    /// [instances][google.cloud.aiplatform.v1.ExplainRequest.instances] to be
    /// explained.
    #[prost(message, repeated, tag = "1")]
    pub explanations: ::prost::alloc::vec::Vec<Explanation>,
    /// ID of the Endpoint's DeployedModel that served this explanation.
    #[prost(string, tag = "2")]
    pub deployed_model_id: ::prost::alloc::string::String,
    /// The predictions that are the output of the predictions call.
    /// Same as
    /// [PredictResponse.predictions][google.cloud.aiplatform.v1.PredictResponse.predictions].
    #[prost(message, repeated, tag = "3")]
    pub predictions: ::prost::alloc::vec::Vec<::prost_types::Value>,
}
/// Request message for [PredictionService.CountTokens][].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CountTokensRequest {
    /// Required. The name of the Endpoint requested to perform token counting.
    /// Format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// Optional. The name of the publisher model requested to serve the
    /// prediction. Format:
    /// `projects/{project}/locations/{location}/publishers/*/models/*`
    #[prost(string, tag = "3")]
    pub model: ::prost::alloc::string::String,
    /// Optional. The instances that are the input to token counting call.
    /// Schema is identical to the prediction schema of the underlying model.
    #[prost(message, repeated, tag = "2")]
    pub instances: ::prost::alloc::vec::Vec<::prost_types::Value>,
    /// Optional. Input content.
    #[prost(message, repeated, tag = "4")]
    pub contents: ::prost::alloc::vec::Vec<Content>,
    /// Optional. The user provided system instructions for the model.
    /// Note: only text should be used in parts and content in each part will be in
    /// a separate paragraph.
    #[prost(message, optional, tag = "5")]
    pub system_instruction: ::core::option::Option<Content>,
    /// Optional. A list of `Tools` the model may use to generate the next
    /// response.
    ///
    /// A `Tool` is a piece of code that enables the system to interact with
    /// external systems to perform an action, or set of actions, outside of
    /// knowledge and scope of the model.
    #[prost(message, repeated, tag = "6")]
    pub tools: ::prost::alloc::vec::Vec<Tool>,
    /// Optional. Generation config that the model will use to generate the
    /// response.
    #[prost(message, optional, tag = "7")]
    pub generation_config: ::core::option::Option<GenerationConfig>,
}
/// Response message for [PredictionService.CountTokens][].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CountTokensResponse {
    /// The total number of tokens counted across all instances from the request.
    #[prost(int32, tag = "1")]
    pub total_tokens: i32,
    /// The total number of billable characters counted across all instances from
    /// the request.
    #[prost(int32, tag = "2")]
    pub total_billable_characters: i32,
    /// Output only. List of modalities that were processed in the request input.
    #[prost(message, repeated, tag = "3")]
    pub prompt_tokens_details: ::prost::alloc::vec::Vec<ModalityTokenCount>,
}
/// Request message for \[PredictionService.GenerateContent\].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateContentRequest {
    /// Required. The fully qualified name of the publisher model or tuned model
    /// endpoint to use.
    ///
    /// Publisher model format:
    /// `projects/{project}/locations/{location}/publishers/*/models/*`
    ///
    /// Tuned model endpoint format:
    /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
    #[prost(string, tag = "5")]
    pub model: ::prost::alloc::string::String,
    /// Required. The content of the current conversation with the model.
    ///
    /// For single-turn queries, this is a single instance. For multi-turn queries,
    /// this is a repeated field that contains conversation history + latest
    /// request.
    #[prost(message, repeated, tag = "2")]
    pub contents: ::prost::alloc::vec::Vec<Content>,
    /// Optional. The user provided system instructions for the model.
    /// Note: only text should be used in parts and content in each part will be in
    /// a separate paragraph.
    #[prost(message, optional, tag = "8")]
    pub system_instruction: ::core::option::Option<Content>,
    /// Optional. The name of the cached content used as context to serve the
    /// prediction. Note: only used in explicit caching, where users can have
    /// control over caching (e.g. what content to cache) and enjoy guaranteed cost
    /// savings. Format:
    /// `projects/{project}/locations/{location}/cachedContents/{cachedContent}`
    #[prost(string, tag = "9")]
    pub cached_content: ::prost::alloc::string::String,
    /// Optional. A list of `Tools` the model may use to generate the next
    /// response.
    ///
    /// A `Tool` is a piece of code that enables the system to interact with
    /// external systems to perform an action, or set of actions, outside of
    /// knowledge and scope of the model.
    #[prost(message, repeated, tag = "6")]
    pub tools: ::prost::alloc::vec::Vec<Tool>,
    /// Optional. Tool config. This config is shared for all tools provided in the
    /// request.
    #[prost(message, optional, tag = "7")]
    pub tool_config: ::core::option::Option<ToolConfig>,
    /// Optional. The labels with user-defined metadata for the request. It is used
    /// for billing and reporting only.
    ///
    /// Label keys and values can be no longer than 63 characters
    /// (Unicode codepoints) and can only contain lowercase letters, numeric
    /// characters, underscores, and dashes. International characters are allowed.
    /// Label values are optional. Label keys must start with a letter.
    #[prost(map = "string, string", tag = "10")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. Per request settings for blocking unsafe content.
    /// Enforced on GenerateContentResponse.candidates.
    #[prost(message, repeated, tag = "3")]
    pub safety_settings: ::prost::alloc::vec::Vec<SafetySetting>,
    /// Optional. Generation config.
    #[prost(message, optional, tag = "4")]
    pub generation_config: ::core::option::Option<GenerationConfig>,
}
/// Response message for \[PredictionService.GenerateContent\].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateContentResponse {
    /// Output only. Generated candidates.
    #[prost(message, repeated, tag = "2")]
    pub candidates: ::prost::alloc::vec::Vec<Candidate>,
    /// Output only. The model version used to generate the response.
    #[prost(string, tag = "11")]
    pub model_version: ::prost::alloc::string::String,
    /// Output only. Content filter results for a prompt sent in the request.
    /// Note: Sent only in the first stream chunk.
    /// Only happens when no candidates were generated due to content violations.
    #[prost(message, optional, tag = "3")]
    pub prompt_feedback: ::core::option::Option<
        generate_content_response::PromptFeedback,
    >,
    /// Usage metadata about the response(s).
    #[prost(message, optional, tag = "4")]
    pub usage_metadata: ::core::option::Option<generate_content_response::UsageMetadata>,
}
/// Nested message and enum types in `GenerateContentResponse`.
pub mod generate_content_response {
    /// Content filter results for a prompt sent in the request.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct PromptFeedback {
        /// Output only. Blocked reason.
        #[prost(enumeration = "prompt_feedback::BlockedReason", tag = "1")]
        pub block_reason: i32,
        /// Output only. Safety ratings.
        #[prost(message, repeated, tag = "2")]
        pub safety_ratings: ::prost::alloc::vec::Vec<super::SafetyRating>,
        /// Output only. A readable block reason message.
        #[prost(string, tag = "3")]
        pub block_reason_message: ::prost::alloc::string::String,
    }
    /// Nested message and enum types in `PromptFeedback`.
    pub mod prompt_feedback {
        /// Blocked reason enumeration.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum BlockedReason {
            /// Unspecified blocked reason.
            Unspecified = 0,
            /// Candidates blocked due to safety.
            Safety = 1,
            /// Candidates blocked due to other reason.
            Other = 2,
            /// Candidates blocked due to the terms which are included from the
            /// terminology blocklist.
            Blocklist = 3,
            /// Candidates blocked due to prohibited content.
            ProhibitedContent = 4,
        }
        impl BlockedReason {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "BLOCKED_REASON_UNSPECIFIED",
                    Self::Safety => "SAFETY",
                    Self::Other => "OTHER",
                    Self::Blocklist => "BLOCKLIST",
                    Self::ProhibitedContent => "PROHIBITED_CONTENT",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "BLOCKED_REASON_UNSPECIFIED" => Some(Self::Unspecified),
                    "SAFETY" => Some(Self::Safety),
                    "OTHER" => Some(Self::Other),
                    "BLOCKLIST" => Some(Self::Blocklist),
                    "PROHIBITED_CONTENT" => Some(Self::ProhibitedContent),
                    _ => None,
                }
            }
        }
    }
    /// Usage metadata about response(s).
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct UsageMetadata {
        /// Number of tokens in the request. When `cached_content` is set, this is
        /// still the total effective prompt size meaning this includes the number of
        /// tokens in the cached content.
        #[prost(int32, tag = "1")]
        pub prompt_token_count: i32,
        /// Number of tokens in the response(s).
        #[prost(int32, tag = "2")]
        pub candidates_token_count: i32,
        /// Total token count for prompt and response candidates.
        #[prost(int32, tag = "3")]
        pub total_token_count: i32,
        /// Output only. Number of tokens in the cached part in the input (the cached
        /// content).
        #[prost(int32, tag = "5")]
        pub cached_content_token_count: i32,
        /// Output only. List of modalities that were processed in the request input.
        #[prost(message, repeated, tag = "9")]
        pub prompt_tokens_details: ::prost::alloc::vec::Vec<super::ModalityTokenCount>,
        /// Output only. List of modalities of the cached content in the request
        /// input.
        #[prost(message, repeated, tag = "10")]
        pub cache_tokens_details: ::prost::alloc::vec::Vec<super::ModalityTokenCount>,
        /// Output only. List of modalities that were returned in the response.
        #[prost(message, repeated, tag = "11")]
        pub candidates_tokens_details: ::prost::alloc::vec::Vec<
            super::ModalityTokenCount,
        >,
    }
}
/// Generated client implementations.
pub mod prediction_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for online predictions and explanations.
    #[derive(Debug, Clone)]
    pub struct PredictionServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl PredictionServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> PredictionServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> PredictionServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            PredictionServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Perform an online prediction.
        pub async fn predict(
            &mut self,
            request: impl tonic::IntoRequest<super::PredictRequest>,
        ) -> std::result::Result<
            tonic::Response<super::PredictResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/Predict",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "Predict",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Perform an online prediction with an arbitrary HTTP payload.
        ///
        /// The response includes the following HTTP headers:
        ///
        /// * `X-Vertex-AI-Endpoint-Id`: ID of the
        /// [Endpoint][google.cloud.aiplatform.v1.Endpoint] that served this
        /// prediction.
        ///
        /// * `X-Vertex-AI-Deployed-Model-Id`: ID of the Endpoint's
        /// [DeployedModel][google.cloud.aiplatform.v1.DeployedModel] that served this
        /// prediction.
        pub async fn raw_predict(
            &mut self,
            request: impl tonic::IntoRequest<super::RawPredictRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::api::HttpBody>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/RawPredict",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "RawPredict",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Perform a streaming online prediction with an arbitrary HTTP payload.
        pub async fn stream_raw_predict(
            &mut self,
            request: impl tonic::IntoRequest<super::StreamRawPredictRequest>,
        ) -> std::result::Result<
            tonic::Response<
                tonic::codec::Streaming<super::super::super::super::api::HttpBody>,
            >,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/StreamRawPredict",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "StreamRawPredict",
                    ),
                );
            self.inner.server_streaming(req, path, codec).await
        }
        /// Perform an unary online prediction request to a gRPC model server for
        /// Vertex first-party products and frameworks.
        pub async fn direct_predict(
            &mut self,
            request: impl tonic::IntoRequest<super::DirectPredictRequest>,
        ) -> std::result::Result<
            tonic::Response<super::DirectPredictResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/DirectPredict",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "DirectPredict",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Perform an unary online prediction request to a gRPC model server for
        /// custom containers.
        pub async fn direct_raw_predict(
            &mut self,
            request: impl tonic::IntoRequest<super::DirectRawPredictRequest>,
        ) -> std::result::Result<
            tonic::Response<super::DirectRawPredictResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/DirectRawPredict",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "DirectRawPredict",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Perform a streaming online prediction request to a gRPC model server for
        /// Vertex first-party products and frameworks.
        pub async fn stream_direct_predict(
            &mut self,
            request: impl tonic::IntoStreamingRequest<
                Message = super::StreamDirectPredictRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::StreamDirectPredictResponse>>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/StreamDirectPredict",
            );
            let mut req = request.into_streaming_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "StreamDirectPredict",
                    ),
                );
            self.inner.streaming(req, path, codec).await
        }
        /// Perform a streaming online prediction request to a gRPC model server for
        /// custom containers.
        pub async fn stream_direct_raw_predict(
            &mut self,
            request: impl tonic::IntoStreamingRequest<
                Message = super::StreamDirectRawPredictRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<
                tonic::codec::Streaming<super::StreamDirectRawPredictResponse>,
            >,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/StreamDirectRawPredict",
            );
            let mut req = request.into_streaming_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "StreamDirectRawPredict",
                    ),
                );
            self.inner.streaming(req, path, codec).await
        }
        /// Perform a streaming online prediction request for Vertex first-party
        /// products and frameworks.
        pub async fn streaming_predict(
            &mut self,
            request: impl tonic::IntoStreamingRequest<
                Message = super::StreamingPredictRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::StreamingPredictResponse>>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/StreamingPredict",
            );
            let mut req = request.into_streaming_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "StreamingPredict",
                    ),
                );
            self.inner.streaming(req, path, codec).await
        }
        /// Perform a server-side streaming online prediction request for Vertex
        /// LLM streaming.
        pub async fn server_streaming_predict(
            &mut self,
            request: impl tonic::IntoRequest<super::StreamingPredictRequest>,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::StreamingPredictResponse>>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/ServerStreamingPredict",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "ServerStreamingPredict",
                    ),
                );
            self.inner.server_streaming(req, path, codec).await
        }
        /// Perform a streaming online prediction request through gRPC.
        pub async fn streaming_raw_predict(
            &mut self,
            request: impl tonic::IntoStreamingRequest<
                Message = super::StreamingRawPredictRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::StreamingRawPredictResponse>>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/StreamingRawPredict",
            );
            let mut req = request.into_streaming_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "StreamingRawPredict",
                    ),
                );
            self.inner.streaming(req, path, codec).await
        }
        /// Perform an online explanation.
        ///
        /// If
        /// [deployed_model_id][google.cloud.aiplatform.v1.ExplainRequest.deployed_model_id]
        /// is specified, the corresponding DeployModel must have
        /// [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
        /// populated. If
        /// [deployed_model_id][google.cloud.aiplatform.v1.ExplainRequest.deployed_model_id]
        /// is not specified, all DeployedModels must have
        /// [explanation_spec][google.cloud.aiplatform.v1.DeployedModel.explanation_spec]
        /// populated.
        pub async fn explain(
            &mut self,
            request: impl tonic::IntoRequest<super::ExplainRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ExplainResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/Explain",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "Explain",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Generate content with multimodal inputs.
        pub async fn generate_content(
            &mut self,
            request: impl tonic::IntoRequest<super::GenerateContentRequest>,
        ) -> std::result::Result<
            tonic::Response<super::GenerateContentResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/GenerateContent",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "GenerateContent",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Generate content with multimodal inputs with streaming support.
        pub async fn stream_generate_content(
            &mut self,
            request: impl tonic::IntoRequest<super::GenerateContentRequest>,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::GenerateContentResponse>>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PredictionService/StreamGenerateContent",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PredictionService",
                        "StreamGenerateContent",
                    ),
                );
            self.inner.server_streaming(req, path, codec).await
        }
    }
}
/// Request message for ComputeTokens RPC call.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ComputeTokensRequest {
    /// Required. The name of the Endpoint requested to get lists of tokens and
    /// token ids.
    #[prost(string, tag = "1")]
    pub endpoint: ::prost::alloc::string::String,
    /// Optional. The instances that are the input to token computing API call.
    /// Schema is identical to the prediction schema of the text model, even for
    /// the non-text models, like chat models, or Codey models.
    #[prost(message, repeated, tag = "2")]
    pub instances: ::prost::alloc::vec::Vec<::prost_types::Value>,
    /// Optional. The name of the publisher model requested to serve the
    /// prediction. Format:
    /// projects/{project}/locations/{location}/publishers/*/models/*
    #[prost(string, tag = "3")]
    pub model: ::prost::alloc::string::String,
    /// Optional. Input content.
    #[prost(message, repeated, tag = "4")]
    pub contents: ::prost::alloc::vec::Vec<Content>,
}
/// Tokens info with a list of tokens and the corresponding list of token ids.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TokensInfo {
    /// A list of tokens from the input.
    #[prost(bytes = "vec", repeated, tag = "1")]
    pub tokens: ::prost::alloc::vec::Vec<::prost::alloc::vec::Vec<u8>>,
    /// A list of token ids from the input.
    #[prost(int64, repeated, tag = "2")]
    pub token_ids: ::prost::alloc::vec::Vec<i64>,
    /// Optional. Optional fields for the role from the corresponding Content.
    #[prost(string, tag = "3")]
    pub role: ::prost::alloc::string::String,
}
/// Response message for ComputeTokens RPC call.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ComputeTokensResponse {
    /// Lists of tokens info from the input. A ComputeTokensRequest could have
    /// multiple instances with a prompt in each instance. We also need to return
    /// lists of tokens info for the request with multiple instances.
    #[prost(message, repeated, tag = "1")]
    pub tokens_info: ::prost::alloc::vec::Vec<TokensInfo>,
}
/// Generated client implementations.
pub mod llm_utility_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// Service for LLM related utility functions.
    #[derive(Debug, Clone)]
    pub struct LlmUtilityServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl LlmUtilityServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> LlmUtilityServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> LlmUtilityServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            LlmUtilityServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Perform a token counting.
        pub async fn count_tokens(
            &mut self,
            request: impl tonic::IntoRequest<super::CountTokensRequest>,
        ) -> std::result::Result<
            tonic::Response<super::CountTokensResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.LlmUtilityService/CountTokens",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.LlmUtilityService",
                        "CountTokens",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Return a list of tokens based on the input text.
        pub async fn compute_tokens(
            &mut self,
            request: impl tonic::IntoRequest<super::ComputeTokensRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ComputeTokensResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.LlmUtilityService/ComputeTokens",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.LlmUtilityService",
                        "ComputeTokens",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// The request message for
/// [MatchService.FindNeighbors][google.cloud.aiplatform.v1.MatchService.FindNeighbors].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FindNeighborsRequest {
    /// Required. The name of the index endpoint.
    /// Format:
    /// `projects/{project}/locations/{location}/indexEndpoints/{index_endpoint}`
    #[prost(string, tag = "1")]
    pub index_endpoint: ::prost::alloc::string::String,
    /// The ID of the DeployedIndex that will serve the request. This request is
    /// sent to a specific IndexEndpoint, as per the IndexEndpoint.network. That
    /// IndexEndpoint also has IndexEndpoint.deployed_indexes, and each such index
    /// has a DeployedIndex.id field.
    /// The value of the field below must equal one of the DeployedIndex.id
    /// fields of the IndexEndpoint that is being called for this request.
    #[prost(string, tag = "2")]
    pub deployed_index_id: ::prost::alloc::string::String,
    /// The list of queries.
    #[prost(message, repeated, tag = "3")]
    pub queries: ::prost::alloc::vec::Vec<find_neighbors_request::Query>,
    /// If set to true, the full datapoints (including all vector values and
    /// restricts) of the nearest neighbors are returned.
    /// Note that returning full datapoint will significantly increase the
    /// latency and cost of the query.
    #[prost(bool, tag = "4")]
    pub return_full_datapoint: bool,
}
/// Nested message and enum types in `FindNeighborsRequest`.
pub mod find_neighbors_request {
    /// A query to find a number of the nearest neighbors (most similar vectors)
    /// of a vector.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Query {
        /// Required. The datapoint/vector whose nearest neighbors should be searched
        /// for.
        #[prost(message, optional, tag = "1")]
        pub datapoint: ::core::option::Option<super::IndexDatapoint>,
        /// The number of nearest neighbors to be retrieved from database for each
        /// query. If not set, will use the default from the service configuration
        /// (<https://cloud.google.com/vertex-ai/docs/matching-engine/configuring-indexes#nearest-neighbor-search-config>).
        #[prost(int32, tag = "2")]
        pub neighbor_count: i32,
        /// Crowding is a constraint on a neighbor list produced by nearest neighbor
        /// search requiring that no more than some value k' of the k neighbors
        /// returned have the same value of crowding_attribute.
        /// It's used for improving result diversity.
        /// This field is the maximum number of matches with the same crowding tag.
        #[prost(int32, tag = "3")]
        pub per_crowding_attribute_neighbor_count: i32,
        /// The number of neighbors to find via approximate search before
        /// exact reordering is performed. If not set, the default value from scam
        /// config is used; if set, this value must be > 0.
        #[prost(int32, tag = "4")]
        pub approximate_neighbor_count: i32,
        /// The fraction of the number of leaves to search, set at query time allows
        /// user to tune search performance. This value increase result in both
        /// search accuracy and latency increase. The value should be between 0.0
        /// and 1.0. If not set or set to 0.0, query uses the default value specified
        /// in
        /// NearestNeighborSearchConfig.TreeAHConfig.fraction_leaf_nodes_to_search.
        #[prost(double, tag = "5")]
        pub fraction_leaf_nodes_to_search_override: f64,
        #[prost(oneof = "query::Ranking", tags = "6")]
        pub ranking: ::core::option::Option<query::Ranking>,
    }
    /// Nested message and enum types in `Query`.
    pub mod query {
        /// Parameters for RRF algorithm that combines search results.
        #[derive(Clone, Copy, PartialEq, ::prost::Message)]
        pub struct Rrf {
            /// Required. Users can provide an alpha value to give more weight to dense
            /// vs sparse results. For example, if the alpha is 0, we only return
            /// sparse and if the alpha is 1, we only return dense.
            #[prost(float, tag = "1")]
            pub alpha: f32,
        }
        #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
        pub enum Ranking {
            /// Optional. Represents RRF algorithm that combines search results.
            #[prost(message, tag = "6")]
            Rrf(Rrf),
        }
    }
}
/// The response message for
/// [MatchService.FindNeighbors][google.cloud.aiplatform.v1.MatchService.FindNeighbors].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FindNeighborsResponse {
    /// The nearest neighbors of the query datapoints.
    #[prost(message, repeated, tag = "1")]
    pub nearest_neighbors: ::prost::alloc::vec::Vec<
        find_neighbors_response::NearestNeighbors,
    >,
}
/// Nested message and enum types in `FindNeighborsResponse`.
pub mod find_neighbors_response {
    /// A neighbor of the query vector.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Neighbor {
        /// The datapoint of the neighbor.
        /// Note that full datapoints are returned only when "return_full_datapoint"
        /// is set to true. Otherwise, only the "datapoint_id" and "crowding_tag"
        /// fields are populated.
        #[prost(message, optional, tag = "1")]
        pub datapoint: ::core::option::Option<super::IndexDatapoint>,
        /// The distance between the neighbor and the dense embedding query.
        #[prost(double, tag = "2")]
        pub distance: f64,
        /// The distance between the neighbor and the query sparse_embedding.
        #[prost(double, tag = "3")]
        pub sparse_distance: f64,
    }
    /// Nearest neighbors for one query.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct NearestNeighbors {
        /// The ID of the query datapoint.
        #[prost(string, tag = "1")]
        pub id: ::prost::alloc::string::String,
        /// All its neighbors.
        #[prost(message, repeated, tag = "2")]
        pub neighbors: ::prost::alloc::vec::Vec<Neighbor>,
    }
}
/// The request message for
/// [MatchService.ReadIndexDatapoints][google.cloud.aiplatform.v1.MatchService.ReadIndexDatapoints].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadIndexDatapointsRequest {
    /// Required. The name of the index endpoint.
    /// Format:
    /// `projects/{project}/locations/{location}/indexEndpoints/{index_endpoint}`
    #[prost(string, tag = "1")]
    pub index_endpoint: ::prost::alloc::string::String,
    /// The ID of the DeployedIndex that will serve the request.
    #[prost(string, tag = "2")]
    pub deployed_index_id: ::prost::alloc::string::String,
    /// IDs of the datapoints to be searched for.
    #[prost(string, repeated, tag = "3")]
    pub ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// The response message for
/// [MatchService.ReadIndexDatapoints][google.cloud.aiplatform.v1.MatchService.ReadIndexDatapoints].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadIndexDatapointsResponse {
    /// The result list of datapoints.
    #[prost(message, repeated, tag = "1")]
    pub datapoints: ::prost::alloc::vec::Vec<IndexDatapoint>,
}
/// Generated client implementations.
pub mod match_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// MatchService is a Google managed service for efficient vector similarity
    /// search at scale.
    #[derive(Debug, Clone)]
    pub struct MatchServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl MatchServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> MatchServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> MatchServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            MatchServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Finds the nearest neighbors of each vector within the request.
        pub async fn find_neighbors(
            &mut self,
            request: impl tonic::IntoRequest<super::FindNeighborsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::FindNeighborsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MatchService/FindNeighbors",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MatchService",
                        "FindNeighbors",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Reads the datapoints/vectors of the given IDs.
        /// A maximum of 1000 datapoints can be retrieved in a batch.
        pub async fn read_index_datapoints(
            &mut self,
            request: impl tonic::IntoRequest<super::ReadIndexDatapointsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ReadIndexDatapointsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MatchService/ReadIndexDatapoints",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MatchService",
                        "ReadIndexDatapoints",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Instance of a general MetadataSchema.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MetadataSchema {
    /// Output only. The resource name of the MetadataSchema.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// The version of the MetadataSchema. The version's format must match
    /// the following regular expression: `^\[0-9\]+[.][0-9]+[.][0-9]+$`, which would
    /// allow to order/compare different versions. Example: 1.0.0, 1.0.1, etc.
    #[prost(string, tag = "2")]
    pub schema_version: ::prost::alloc::string::String,
    /// Required. The raw YAML string representation of the MetadataSchema. The
    /// combination of \[MetadataSchema.version\] and the schema name given by
    /// `title` in \[MetadataSchema.schema\] must be unique within a MetadataStore.
    ///
    /// The schema is defined as an OpenAPI 3.0.2
    /// [MetadataSchema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.2.md#schemaObject>)
    #[prost(string, tag = "3")]
    pub schema: ::prost::alloc::string::String,
    /// The type of the MetadataSchema. This is a property that identifies which
    /// metadata types will use the MetadataSchema.
    #[prost(enumeration = "metadata_schema::MetadataSchemaType", tag = "4")]
    pub schema_type: i32,
    /// Output only. Timestamp when this MetadataSchema was created.
    #[prost(message, optional, tag = "5")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Description of the Metadata Schema
    #[prost(string, tag = "6")]
    pub description: ::prost::alloc::string::String,
}
/// Nested message and enum types in `MetadataSchema`.
pub mod metadata_schema {
    /// Describes the type of the MetadataSchema.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum MetadataSchemaType {
        /// Unspecified type for the MetadataSchema.
        Unspecified = 0,
        /// A type indicating that the MetadataSchema will be used by Artifacts.
        ArtifactType = 1,
        /// A typee indicating that the MetadataSchema will be used by Executions.
        ExecutionType = 2,
        /// A state indicating that the MetadataSchema will be used by Contexts.
        ContextType = 3,
    }
    impl MetadataSchemaType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "METADATA_SCHEMA_TYPE_UNSPECIFIED",
                Self::ArtifactType => "ARTIFACT_TYPE",
                Self::ExecutionType => "EXECUTION_TYPE",
                Self::ContextType => "CONTEXT_TYPE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "METADATA_SCHEMA_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "ARTIFACT_TYPE" => Some(Self::ArtifactType),
                "EXECUTION_TYPE" => Some(Self::ExecutionType),
                "CONTEXT_TYPE" => Some(Self::ContextType),
                _ => None,
            }
        }
    }
}
/// Instance of a metadata store. Contains a set of metadata that can be
/// queried.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MetadataStore {
    /// Output only. The resource name of the MetadataStore instance.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. Timestamp when this MetadataStore was created.
    #[prost(message, optional, tag = "3")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this MetadataStore was last updated.
    #[prost(message, optional, tag = "4")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Customer-managed encryption key spec for a Metadata Store. If set, this
    /// Metadata Store and all sub-resources of this Metadata Store are secured
    /// using this key.
    #[prost(message, optional, tag = "5")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Description of the MetadataStore.
    #[prost(string, tag = "6")]
    pub description: ::prost::alloc::string::String,
    /// Output only. State information of the MetadataStore.
    #[prost(message, optional, tag = "7")]
    pub state: ::core::option::Option<metadata_store::MetadataStoreState>,
    /// Optional. Dataplex integration settings.
    #[prost(message, optional, tag = "8")]
    pub dataplex_config: ::core::option::Option<metadata_store::DataplexConfig>,
}
/// Nested message and enum types in `MetadataStore`.
pub mod metadata_store {
    /// Represents state information for a MetadataStore.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct MetadataStoreState {
        /// The disk utilization of the MetadataStore in bytes.
        #[prost(int64, tag = "1")]
        pub disk_utilization_bytes: i64,
    }
    /// Represents Dataplex integration settings.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct DataplexConfig {
        /// Optional. Whether or not Data Lineage synchronization is enabled for
        /// Vertex Pipelines.
        #[prost(bool, tag = "1")]
        pub enabled_pipelines_lineage: bool,
    }
}
/// Request message for
/// [MetadataService.CreateMetadataStore][google.cloud.aiplatform.v1.MetadataService.CreateMetadataStore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateMetadataStoreRequest {
    /// Required. The resource name of the Location where the MetadataStore should
    /// be created.
    /// Format: `projects/{project}/locations/{location}/`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The MetadataStore to create.
    #[prost(message, optional, tag = "2")]
    pub metadata_store: ::core::option::Option<MetadataStore>,
    /// The {metadatastore} portion of the resource name with the format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    /// If not provided, the MetadataStore's ID will be a UUID generated by the
    /// service.
    /// Must be 4-128 characters in length. Valid characters are `/[a-z][0-9]-/`.
    /// Must be unique across all MetadataStores in the parent Location.
    /// (Otherwise the request will fail with ALREADY_EXISTS, or PERMISSION_DENIED
    /// if the caller can't view the preexisting MetadataStore.)
    #[prost(string, tag = "3")]
    pub metadata_store_id: ::prost::alloc::string::String,
}
/// Details of operations that perform
/// [MetadataService.CreateMetadataStore][google.cloud.aiplatform.v1.MetadataService.CreateMetadataStore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateMetadataStoreOperationMetadata {
    /// Operation metadata for creating a MetadataStore.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [MetadataService.GetMetadataStore][google.cloud.aiplatform.v1.MetadataService.GetMetadataStore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetMetadataStoreRequest {
    /// Required. The resource name of the MetadataStore to retrieve.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.ListMetadataStores][google.cloud.aiplatform.v1.MetadataService.ListMetadataStores].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListMetadataStoresRequest {
    /// Required. The Location whose MetadataStores should be listed.
    /// Format:
    /// `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The maximum number of Metadata Stores to return. The service may return
    /// fewer.
    /// Must be in range 1-1000, inclusive. Defaults to 100.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [MetadataService.ListMetadataStores][google.cloud.aiplatform.v1.MetadataService.ListMetadataStores]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other provided parameters must match the call that
    /// provided the page token. (Otherwise the request will fail with
    /// INVALID_ARGUMENT error.)
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response message for
/// [MetadataService.ListMetadataStores][google.cloud.aiplatform.v1.MetadataService.ListMetadataStores].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListMetadataStoresResponse {
    /// The MetadataStores found for the Location.
    #[prost(message, repeated, tag = "1")]
    pub metadata_stores: ::prost::alloc::vec::Vec<MetadataStore>,
    /// A token, which can be sent as
    /// [ListMetadataStoresRequest.page_token][google.cloud.aiplatform.v1.ListMetadataStoresRequest.page_token]
    /// to retrieve the next page. If this field is not populated, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.DeleteMetadataStore][google.cloud.aiplatform.v1.MetadataService.DeleteMetadataStore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteMetadataStoreRequest {
    /// Required. The resource name of the MetadataStore to delete.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Deprecated: Field is no longer supported.
    #[deprecated]
    #[prost(bool, tag = "2")]
    pub force: bool,
}
/// Details of operations that perform
/// [MetadataService.DeleteMetadataStore][google.cloud.aiplatform.v1.MetadataService.DeleteMetadataStore].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteMetadataStoreOperationMetadata {
    /// Operation metadata for deleting a MetadataStore.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [MetadataService.CreateArtifact][google.cloud.aiplatform.v1.MetadataService.CreateArtifact].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateArtifactRequest {
    /// Required. The resource name of the MetadataStore where the Artifact should
    /// be created.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The Artifact to create.
    #[prost(message, optional, tag = "2")]
    pub artifact: ::core::option::Option<Artifact>,
    /// The {artifact} portion of the resource name with the format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/artifacts/{artifact}`
    /// If not provided, the Artifact's ID will be a UUID generated by the service.
    /// Must be 4-128 characters in length. Valid characters are `/[a-z][0-9]-/`.
    /// Must be unique across all Artifacts in the parent MetadataStore. (Otherwise
    /// the request will fail with ALREADY_EXISTS, or PERMISSION_DENIED if the
    /// caller can't view the preexisting Artifact.)
    #[prost(string, tag = "3")]
    pub artifact_id: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.GetArtifact][google.cloud.aiplatform.v1.MetadataService.GetArtifact].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetArtifactRequest {
    /// Required. The resource name of the Artifact to retrieve.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/artifacts/{artifact}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.ListArtifacts][google.cloud.aiplatform.v1.MetadataService.ListArtifacts].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListArtifactsRequest {
    /// Required. The MetadataStore whose Artifacts should be listed.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The maximum number of Artifacts to return. The service may return fewer.
    /// Must be in range 1-1000, inclusive. Defaults to 100.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [MetadataService.ListArtifacts][google.cloud.aiplatform.v1.MetadataService.ListArtifacts]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other provided parameters must match the call that
    /// provided the page token. (Otherwise the request will fail with
    /// INVALID_ARGUMENT error.)
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
    /// Filter specifying the boolean condition for the Artifacts to satisfy in
    /// order to be part of the result set.
    /// The syntax to define filter query is based on <https://google.aip.dev/160.>
    /// The supported set of filters include the following:
    ///
    /// *   **Attribute filtering**:
    ///      For example: `display_name = "test"`.
    ///      Supported fields include: `name`, `display_name`, `uri`, `state`,
    ///      `schema_title`, `create_time`, and `update_time`.
    ///      Time fields, such as `create_time` and `update_time`, require values
    ///      specified in RFC-3339 format.
    ///      For example: `create_time = "2020-11-19T11:30:00-04:00"`
    /// *   **Metadata field**:
    ///      To filter on metadata fields use traversal operation as follows:
    ///      `metadata.<field_name>.<type_value>`.
    ///      For example: `metadata.field_1.number_value = 10.0`
    ///      In case the field name contains special characters (such as colon), one
    ///      can embed it inside double quote.
    ///      For example: `metadata."field:1".number_value = 10.0`
    /// *   **Context based filtering**:
    ///      To filter Artifacts based on the contexts to which they belong, use the
    ///      function operator with the full resource name
    ///      `in_context(<context-name>)`.
    ///      For example:
    ///      `in_context("projects/<project_number>/locations/<location>/metadataStores/<metadatastore_name>/contexts/<context-id>")`
    ///
    /// Each of the above supported filter types can be combined together using
    /// logical operators (`AND` & `OR`). Maximum nested expression depth allowed
    /// is 5.
    ///
    /// For example: `display_name = "test" AND metadata.field1.bool_value = true`.
    #[prost(string, tag = "4")]
    pub filter: ::prost::alloc::string::String,
    /// How the list of messages is ordered. Specify the values to order by and an
    /// ordering operation. The default sorting order is ascending. To specify
    /// descending order for a field, users append a " desc" suffix; for example:
    /// "foo desc, bar".
    /// Subfields are specified with a `.` character, such as foo.bar.
    /// see <https://google.aip.dev/132#ordering> for more details.
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [MetadataService.ListArtifacts][google.cloud.aiplatform.v1.MetadataService.ListArtifacts].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListArtifactsResponse {
    /// The Artifacts retrieved from the MetadataStore.
    #[prost(message, repeated, tag = "1")]
    pub artifacts: ::prost::alloc::vec::Vec<Artifact>,
    /// A token, which can be sent as
    /// [ListArtifactsRequest.page_token][google.cloud.aiplatform.v1.ListArtifactsRequest.page_token]
    /// to retrieve the next page.
    /// If this field is not populated, there are no subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.UpdateArtifact][google.cloud.aiplatform.v1.MetadataService.UpdateArtifact].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateArtifactRequest {
    /// Required. The Artifact containing updates.
    /// The Artifact's [Artifact.name][google.cloud.aiplatform.v1.Artifact.name]
    /// field is used to identify the Artifact to be updated. Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/artifacts/{artifact}`
    #[prost(message, optional, tag = "1")]
    pub artifact: ::core::option::Option<Artifact>,
    /// Optional. A FieldMask indicating which fields should be updated.
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// If set to true, and the [Artifact][google.cloud.aiplatform.v1.Artifact] is
    /// not found, a new [Artifact][google.cloud.aiplatform.v1.Artifact] is
    /// created.
    #[prost(bool, tag = "3")]
    pub allow_missing: bool,
}
/// Request message for
/// [MetadataService.DeleteArtifact][google.cloud.aiplatform.v1.MetadataService.DeleteArtifact].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteArtifactRequest {
    /// Required. The resource name of the Artifact to delete.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/artifacts/{artifact}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The etag of the Artifact to delete.
    /// If this is provided, it must match the server's etag. Otherwise, the
    /// request will fail with a FAILED_PRECONDITION.
    #[prost(string, tag = "2")]
    pub etag: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.PurgeArtifacts][google.cloud.aiplatform.v1.MetadataService.PurgeArtifacts].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PurgeArtifactsRequest {
    /// Required. The metadata store to purge Artifacts from.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. A required filter matching the Artifacts to be purged.
    /// E.g., `update_time <= 2020-11-19T11:30:00-04:00`.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// Optional. Flag to indicate to actually perform the purge.
    /// If `force` is set to false, the method will return a sample of
    /// Artifact names that would be deleted.
    #[prost(bool, tag = "3")]
    pub force: bool,
}
/// Response message for
/// [MetadataService.PurgeArtifacts][google.cloud.aiplatform.v1.MetadataService.PurgeArtifacts].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PurgeArtifactsResponse {
    /// The number of Artifacts that this request deleted (or, if `force` is false,
    /// the number of Artifacts that will be deleted). This can be an estimate.
    #[prost(int64, tag = "1")]
    pub purge_count: i64,
    /// A sample of the Artifact names that will be deleted.
    /// Only populated if `force` is set to false. The maximum number of samples is
    /// 100 (it is possible to return fewer).
    #[prost(string, repeated, tag = "2")]
    pub purge_sample: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Details of operations that perform
/// [MetadataService.PurgeArtifacts][google.cloud.aiplatform.v1.MetadataService.PurgeArtifacts].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PurgeArtifactsMetadata {
    /// Operation metadata for purging Artifacts.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [MetadataService.CreateContext][google.cloud.aiplatform.v1.MetadataService.CreateContext].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateContextRequest {
    /// Required. The resource name of the MetadataStore where the Context should
    /// be created. Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The Context to create.
    #[prost(message, optional, tag = "2")]
    pub context: ::core::option::Option<Context>,
    /// The {context} portion of the resource name with the format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/contexts/{context}`.
    /// If not provided, the Context's ID will be a UUID generated by the service.
    /// Must be 4-128 characters in length. Valid characters are `/[a-z][0-9]-/`.
    /// Must be unique across all Contexts in the parent MetadataStore. (Otherwise
    /// the request will fail with ALREADY_EXISTS, or PERMISSION_DENIED if the
    /// caller can't view the preexisting Context.)
    #[prost(string, tag = "3")]
    pub context_id: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.GetContext][google.cloud.aiplatform.v1.MetadataService.GetContext].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetContextRequest {
    /// Required. The resource name of the Context to retrieve.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/contexts/{context}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.ListContexts][google.cloud.aiplatform.v1.MetadataService.ListContexts]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListContextsRequest {
    /// Required. The MetadataStore whose Contexts should be listed.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The maximum number of Contexts to return. The service may return fewer.
    /// Must be in range 1-1000, inclusive. Defaults to 100.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [MetadataService.ListContexts][google.cloud.aiplatform.v1.MetadataService.ListContexts]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other provided parameters must match the call that
    /// provided the page token. (Otherwise the request will fail with
    /// INVALID_ARGUMENT error.)
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
    /// Filter specifying the boolean condition for the Contexts to satisfy in
    /// order to be part of the result set.
    /// The syntax to define filter query is based on <https://google.aip.dev/160.>
    /// Following are the supported set of filters:
    ///
    /// *  **Attribute filtering**:
    ///     For example: `display_name = "test"`.
    ///     Supported fields include: `name`, `display_name`, `schema_title`,
    ///     `create_time`, and `update_time`.
    ///     Time fields, such as `create_time` and `update_time`, require values
    ///     specified in RFC-3339 format.
    ///     For example: `create_time = "2020-11-19T11:30:00-04:00"`.
    /// *  **Metadata field**:
    ///     To filter on metadata fields use traversal operation as follows:
    ///     `metadata.<field_name>.<type_value>`.
    ///     For example: `metadata.field_1.number_value = 10.0`.
    ///     In case the field name contains special characters (such as colon), one
    ///     can embed it inside double quote.
    ///     For example: `metadata."field:1".number_value = 10.0`
    /// *  **Parent Child filtering**:
    ///     To filter Contexts based on parent-child relationship use the HAS
    ///     operator as follows:
    ///
    ///     ```
    ///     parent_contexts:
    ///     "projects/<project_number>/locations/<location>/metadataStores/<metadatastore_name>/contexts/<context_id>"
    ///     child_contexts:
    ///     "projects/<project_number>/locations/<location>/metadataStores/<metadatastore_name>/contexts/<context_id>"
    ///     ```
    ///
    /// Each of the above supported filters can be combined together using
    /// logical operators (`AND` & `OR`). Maximum nested expression depth allowed
    /// is 5.
    ///
    /// For example: `display_name = "test" AND metadata.field1.bool_value = true`.
    #[prost(string, tag = "4")]
    pub filter: ::prost::alloc::string::String,
    /// How the list of messages is ordered. Specify the values to order by and an
    /// ordering operation. The default sorting order is ascending. To specify
    /// descending order for a field, users append a " desc" suffix; for example:
    /// "foo desc, bar".
    /// Subfields are specified with a `.` character, such as foo.bar.
    /// see <https://google.aip.dev/132#ordering> for more details.
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [MetadataService.ListContexts][google.cloud.aiplatform.v1.MetadataService.ListContexts].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListContextsResponse {
    /// The Contexts retrieved from the MetadataStore.
    #[prost(message, repeated, tag = "1")]
    pub contexts: ::prost::alloc::vec::Vec<Context>,
    /// A token, which can be sent as
    /// [ListContextsRequest.page_token][google.cloud.aiplatform.v1.ListContextsRequest.page_token]
    /// to retrieve the next page.
    /// If this field is not populated, there are no subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.UpdateContext][google.cloud.aiplatform.v1.MetadataService.UpdateContext].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateContextRequest {
    /// Required. The Context containing updates.
    /// The Context's [Context.name][google.cloud.aiplatform.v1.Context.name] field
    /// is used to identify the Context to be updated. Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/contexts/{context}`
    #[prost(message, optional, tag = "1")]
    pub context: ::core::option::Option<Context>,
    /// Optional. A FieldMask indicating which fields should be updated.
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// If set to true, and the [Context][google.cloud.aiplatform.v1.Context] is
    /// not found, a new [Context][google.cloud.aiplatform.v1.Context] is created.
    #[prost(bool, tag = "3")]
    pub allow_missing: bool,
}
/// Request message for
/// [MetadataService.DeleteContext][google.cloud.aiplatform.v1.MetadataService.DeleteContext].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteContextRequest {
    /// Required. The resource name of the Context to delete.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/contexts/{context}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// The force deletion semantics is still undefined.
    /// Users should not use this field.
    #[prost(bool, tag = "2")]
    pub force: bool,
    /// Optional. The etag of the Context to delete.
    /// If this is provided, it must match the server's etag. Otherwise, the
    /// request will fail with a FAILED_PRECONDITION.
    #[prost(string, tag = "3")]
    pub etag: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.PurgeContexts][google.cloud.aiplatform.v1.MetadataService.PurgeContexts].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PurgeContextsRequest {
    /// Required. The metadata store to purge Contexts from.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. A required filter matching the Contexts to be purged.
    /// E.g., `update_time <= 2020-11-19T11:30:00-04:00`.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// Optional. Flag to indicate to actually perform the purge.
    /// If `force` is set to false, the method will return a sample of
    /// Context names that would be deleted.
    #[prost(bool, tag = "3")]
    pub force: bool,
}
/// Response message for
/// [MetadataService.PurgeContexts][google.cloud.aiplatform.v1.MetadataService.PurgeContexts].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PurgeContextsResponse {
    /// The number of Contexts that this request deleted (or, if `force` is false,
    /// the number of Contexts that will be deleted). This can be an estimate.
    #[prost(int64, tag = "1")]
    pub purge_count: i64,
    /// A sample of the Context names that will be deleted.
    /// Only populated if `force` is set to false. The maximum number of samples is
    /// 100 (it is possible to return fewer).
    #[prost(string, repeated, tag = "2")]
    pub purge_sample: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Details of operations that perform
/// [MetadataService.PurgeContexts][google.cloud.aiplatform.v1.MetadataService.PurgeContexts].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PurgeContextsMetadata {
    /// Operation metadata for purging Contexts.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [MetadataService.AddContextArtifactsAndExecutions][google.cloud.aiplatform.v1.MetadataService.AddContextArtifactsAndExecutions].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AddContextArtifactsAndExecutionsRequest {
    /// Required. The resource name of the Context that the Artifacts and
    /// Executions belong to. Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/contexts/{context}`
    #[prost(string, tag = "1")]
    pub context: ::prost::alloc::string::String,
    /// The resource names of the Artifacts to attribute to the Context.
    ///
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/artifacts/{artifact}`
    #[prost(string, repeated, tag = "2")]
    pub artifacts: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// The resource names of the Executions to associate with the
    /// Context.
    ///
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/executions/{execution}`
    #[prost(string, repeated, tag = "3")]
    pub executions: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Response message for
/// [MetadataService.AddContextArtifactsAndExecutions][google.cloud.aiplatform.v1.MetadataService.AddContextArtifactsAndExecutions].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct AddContextArtifactsAndExecutionsResponse {}
/// Request message for
/// [MetadataService.AddContextChildren][google.cloud.aiplatform.v1.MetadataService.AddContextChildren].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AddContextChildrenRequest {
    /// Required. The resource name of the parent Context.
    ///
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/contexts/{context}`
    #[prost(string, tag = "1")]
    pub context: ::prost::alloc::string::String,
    /// The resource names of the child Contexts.
    #[prost(string, repeated, tag = "2")]
    pub child_contexts: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Response message for
/// [MetadataService.AddContextChildren][google.cloud.aiplatform.v1.MetadataService.AddContextChildren].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct AddContextChildrenResponse {}
/// Request message for
/// [MetadataService.DeleteContextChildrenRequest][].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RemoveContextChildrenRequest {
    /// Required. The resource name of the parent Context.
    ///
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/contexts/{context}`
    #[prost(string, tag = "1")]
    pub context: ::prost::alloc::string::String,
    /// The resource names of the child Contexts.
    #[prost(string, repeated, tag = "2")]
    pub child_contexts: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Response message for
/// [MetadataService.RemoveContextChildren][google.cloud.aiplatform.v1.MetadataService.RemoveContextChildren].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RemoveContextChildrenResponse {}
/// Request message for
/// [MetadataService.QueryContextLineageSubgraph][google.cloud.aiplatform.v1.MetadataService.QueryContextLineageSubgraph].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryContextLineageSubgraphRequest {
    /// Required. The resource name of the Context whose Artifacts and Executions
    /// should be retrieved as a LineageSubgraph.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/contexts/{context}`
    ///
    /// The request may error with FAILED_PRECONDITION if the number of Artifacts,
    /// the number of Executions, or the number of Events that would be returned
    /// for the Context exceeds 1000.
    #[prost(string, tag = "1")]
    pub context: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.CreateExecution][google.cloud.aiplatform.v1.MetadataService.CreateExecution].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateExecutionRequest {
    /// Required. The resource name of the MetadataStore where the Execution should
    /// be created.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The Execution to create.
    #[prost(message, optional, tag = "2")]
    pub execution: ::core::option::Option<Execution>,
    /// The {execution} portion of the resource name with the format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/executions/{execution}`
    /// If not provided, the Execution's ID will be a UUID generated by the
    /// service.
    /// Must be 4-128 characters in length. Valid characters are `/[a-z][0-9]-/`.
    /// Must be unique across all Executions in the parent MetadataStore.
    /// (Otherwise the request will fail with ALREADY_EXISTS, or PERMISSION_DENIED
    /// if the caller can't view the preexisting Execution.)
    #[prost(string, tag = "3")]
    pub execution_id: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.GetExecution][google.cloud.aiplatform.v1.MetadataService.GetExecution].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetExecutionRequest {
    /// Required. The resource name of the Execution to retrieve.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/executions/{execution}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.ListExecutions][google.cloud.aiplatform.v1.MetadataService.ListExecutions].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListExecutionsRequest {
    /// Required. The MetadataStore whose Executions should be listed.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The maximum number of Executions to return. The service may return fewer.
    /// Must be in range 1-1000, inclusive. Defaults to 100.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [MetadataService.ListExecutions][google.cloud.aiplatform.v1.MetadataService.ListExecutions]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other provided parameters must match the call that
    /// provided the page token. (Otherwise the request will fail with an
    /// INVALID_ARGUMENT error.)
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
    /// Filter specifying the boolean condition for the Executions to satisfy in
    /// order to be part of the result set.
    /// The syntax to define filter query is based on <https://google.aip.dev/160.>
    /// Following are the supported set of filters:
    ///
    /// *  **Attribute filtering**:
    ///     For example: `display_name = "test"`.
    ///     Supported fields include: `name`, `display_name`, `state`,
    ///     `schema_title`, `create_time`, and `update_time`.
    ///     Time fields, such as `create_time` and `update_time`, require values
    ///     specified in RFC-3339 format.
    ///     For example: `create_time = "2020-11-19T11:30:00-04:00"`.
    /// *  **Metadata field**:
    ///     To filter on metadata fields use traversal operation as follows:
    ///     `metadata.<field_name>.<type_value>`
    ///     For example: `metadata.field_1.number_value = 10.0`
    ///     In case the field name contains special characters (such as colon), one
    ///     can embed it inside double quote.
    ///     For example: `metadata."field:1".number_value = 10.0`
    /// *  **Context based filtering**:
    ///     To filter Executions based on the contexts to which they belong use
    ///     the function operator with the full resource name:
    ///     `in_context(<context-name>)`.
    ///     For example:
    ///     `in_context("projects/<project_number>/locations/<location>/metadataStores/<metadatastore_name>/contexts/<context-id>")`
    ///
    /// Each of the above supported filters can be combined together using
    /// logical operators (`AND` & `OR`). Maximum nested expression depth allowed
    /// is 5.
    ///
    /// For example: `display_name = "test" AND metadata.field1.bool_value = true`.
    #[prost(string, tag = "4")]
    pub filter: ::prost::alloc::string::String,
    /// How the list of messages is ordered. Specify the values to order by and an
    /// ordering operation. The default sorting order is ascending. To specify
    /// descending order for a field, users append a " desc" suffix; for example:
    /// "foo desc, bar".
    /// Subfields are specified with a `.` character, such as foo.bar.
    /// see <https://google.aip.dev/132#ordering> for more details.
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [MetadataService.ListExecutions][google.cloud.aiplatform.v1.MetadataService.ListExecutions].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListExecutionsResponse {
    /// The Executions retrieved from the MetadataStore.
    #[prost(message, repeated, tag = "1")]
    pub executions: ::prost::alloc::vec::Vec<Execution>,
    /// A token, which can be sent as
    /// [ListExecutionsRequest.page_token][google.cloud.aiplatform.v1.ListExecutionsRequest.page_token]
    /// to retrieve the next page.
    /// If this field is not populated, there are no subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.UpdateExecution][google.cloud.aiplatform.v1.MetadataService.UpdateExecution].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateExecutionRequest {
    /// Required. The Execution containing updates.
    /// The Execution's [Execution.name][google.cloud.aiplatform.v1.Execution.name]
    /// field is used to identify the Execution to be updated. Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/executions/{execution}`
    #[prost(message, optional, tag = "1")]
    pub execution: ::core::option::Option<Execution>,
    /// Optional. A FieldMask indicating which fields should be updated.
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// If set to true, and the [Execution][google.cloud.aiplatform.v1.Execution]
    /// is not found, a new [Execution][google.cloud.aiplatform.v1.Execution] is
    /// created.
    #[prost(bool, tag = "3")]
    pub allow_missing: bool,
}
/// Request message for
/// [MetadataService.DeleteExecution][google.cloud.aiplatform.v1.MetadataService.DeleteExecution].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteExecutionRequest {
    /// Required. The resource name of the Execution to delete.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/executions/{execution}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The etag of the Execution to delete.
    /// If this is provided, it must match the server's etag. Otherwise, the
    /// request will fail with a FAILED_PRECONDITION.
    #[prost(string, tag = "2")]
    pub etag: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.PurgeExecutions][google.cloud.aiplatform.v1.MetadataService.PurgeExecutions].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PurgeExecutionsRequest {
    /// Required. The metadata store to purge Executions from.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. A required filter matching the Executions to be purged.
    /// E.g., `update_time <= 2020-11-19T11:30:00-04:00`.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// Optional. Flag to indicate to actually perform the purge.
    /// If `force` is set to false, the method will return a sample of
    /// Execution names that would be deleted.
    #[prost(bool, tag = "3")]
    pub force: bool,
}
/// Response message for
/// [MetadataService.PurgeExecutions][google.cloud.aiplatform.v1.MetadataService.PurgeExecutions].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PurgeExecutionsResponse {
    /// The number of Executions that this request deleted (or, if `force` is
    /// false, the number of Executions that will be deleted). This can be an
    /// estimate.
    #[prost(int64, tag = "1")]
    pub purge_count: i64,
    /// A sample of the Execution names that will be deleted.
    /// Only populated if `force` is set to false. The maximum number of samples is
    /// 100 (it is possible to return fewer).
    #[prost(string, repeated, tag = "2")]
    pub purge_sample: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Details of operations that perform
/// [MetadataService.PurgeExecutions][google.cloud.aiplatform.v1.MetadataService.PurgeExecutions].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PurgeExecutionsMetadata {
    /// Operation metadata for purging Executions.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [MetadataService.AddExecutionEvents][google.cloud.aiplatform.v1.MetadataService.AddExecutionEvents].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AddExecutionEventsRequest {
    /// Required. The resource name of the Execution that the Events connect
    /// Artifacts with.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/executions/{execution}`
    #[prost(string, tag = "1")]
    pub execution: ::prost::alloc::string::String,
    /// The Events to create and add.
    #[prost(message, repeated, tag = "2")]
    pub events: ::prost::alloc::vec::Vec<Event>,
}
/// Response message for
/// [MetadataService.AddExecutionEvents][google.cloud.aiplatform.v1.MetadataService.AddExecutionEvents].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct AddExecutionEventsResponse {}
/// Request message for
/// [MetadataService.QueryExecutionInputsAndOutputs][google.cloud.aiplatform.v1.MetadataService.QueryExecutionInputsAndOutputs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryExecutionInputsAndOutputsRequest {
    /// Required. The resource name of the Execution whose input and output
    /// Artifacts should be retrieved as a LineageSubgraph. Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/executions/{execution}`
    #[prost(string, tag = "1")]
    pub execution: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.CreateMetadataSchema][google.cloud.aiplatform.v1.MetadataService.CreateMetadataSchema].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateMetadataSchemaRequest {
    /// Required. The resource name of the MetadataStore where the MetadataSchema
    /// should be created. Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The MetadataSchema to create.
    #[prost(message, optional, tag = "2")]
    pub metadata_schema: ::core::option::Option<MetadataSchema>,
    /// The {metadata_schema} portion of the resource name with the format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/metadataSchemas/{metadataschema}`
    /// If not provided, the MetadataStore's ID will be a UUID generated by the
    /// service.
    /// Must be 4-128 characters in length. Valid characters are `/[a-z][0-9]-/`.
    /// Must be unique across all MetadataSchemas in the parent Location.
    /// (Otherwise the request will fail with ALREADY_EXISTS, or PERMISSION_DENIED
    /// if the caller can't view the preexisting MetadataSchema.)
    #[prost(string, tag = "3")]
    pub metadata_schema_id: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.GetMetadataSchema][google.cloud.aiplatform.v1.MetadataService.GetMetadataSchema].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetMetadataSchemaRequest {
    /// Required. The resource name of the MetadataSchema to retrieve.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/metadataSchemas/{metadataschema}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.ListMetadataSchemas][google.cloud.aiplatform.v1.MetadataService.ListMetadataSchemas].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListMetadataSchemasRequest {
    /// Required. The MetadataStore whose MetadataSchemas should be listed.
    /// Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The maximum number of MetadataSchemas to return. The service may return
    /// fewer.
    /// Must be in range 1-1000, inclusive. Defaults to 100.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [MetadataService.ListMetadataSchemas][google.cloud.aiplatform.v1.MetadataService.ListMetadataSchemas]
    /// call. Provide this to retrieve the next page.
    ///
    /// When paginating, all other provided parameters must match the call that
    /// provided the page token. (Otherwise the request will fail with
    /// INVALID_ARGUMENT error.)
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
    /// A query to filter available MetadataSchemas for matching results.
    #[prost(string, tag = "4")]
    pub filter: ::prost::alloc::string::String,
}
/// Response message for
/// [MetadataService.ListMetadataSchemas][google.cloud.aiplatform.v1.MetadataService.ListMetadataSchemas].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListMetadataSchemasResponse {
    /// The MetadataSchemas found for the MetadataStore.
    #[prost(message, repeated, tag = "1")]
    pub metadata_schemas: ::prost::alloc::vec::Vec<MetadataSchema>,
    /// A token, which can be sent as
    /// [ListMetadataSchemasRequest.page_token][google.cloud.aiplatform.v1.ListMetadataSchemasRequest.page_token]
    /// to retrieve the next page. If this field is not populated, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [MetadataService.QueryArtifactLineageSubgraph][google.cloud.aiplatform.v1.MetadataService.QueryArtifactLineageSubgraph].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryArtifactLineageSubgraphRequest {
    /// Required. The resource name of the Artifact whose Lineage needs to be
    /// retrieved as a LineageSubgraph. Format:
    /// `projects/{project}/locations/{location}/metadataStores/{metadatastore}/artifacts/{artifact}`
    ///
    /// The request may error with FAILED_PRECONDITION if the number of Artifacts,
    /// the number of Executions, or the number of Events that would be returned
    /// for the Context exceeds 1000.
    #[prost(string, tag = "1")]
    pub artifact: ::prost::alloc::string::String,
    /// Specifies the size of the lineage graph in terms of number of hops from the
    /// specified artifact.
    /// Negative Value: INVALID_ARGUMENT error is returned
    /// 0: Only input artifact is returned.
    /// No value: Transitive closure is performed to return the complete graph.
    #[prost(int32, tag = "2")]
    pub max_hops: i32,
    /// Filter specifying the boolean condition for the Artifacts to satisfy in
    /// order to be part of the Lineage Subgraph.
    /// The syntax to define filter query is based on <https://google.aip.dev/160.>
    /// The supported set of filters include the following:
    ///
    /// *  **Attribute filtering**:
    ///     For example: `display_name = "test"`
    ///     Supported fields include: `name`, `display_name`, `uri`, `state`,
    ///     `schema_title`, `create_time`, and `update_time`.
    ///     Time fields, such as `create_time` and `update_time`, require values
    ///     specified in RFC-3339 format.
    ///     For example: `create_time = "2020-11-19T11:30:00-04:00"`
    /// *  **Metadata field**:
    ///     To filter on metadata fields use traversal operation as follows:
    ///     `metadata.<field_name>.<type_value>`.
    ///     For example: `metadata.field_1.number_value = 10.0`
    ///     In case the field name contains special characters (such as colon), one
    ///     can embed it inside double quote.
    ///     For example: `metadata."field:1".number_value = 10.0`
    ///
    /// Each of the above supported filter types can be combined together using
    /// logical operators (`AND` & `OR`). Maximum nested expression depth allowed
    /// is 5.
    ///
    /// For example: `display_name = "test" AND metadata.field1.bool_value = true`.
    #[prost(string, tag = "3")]
    pub filter: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod metadata_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// Service for reading and writing metadata entries.
    #[derive(Debug, Clone)]
    pub struct MetadataServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl MetadataServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> MetadataServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> MetadataServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            MetadataServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Initializes a MetadataStore, including allocation of resources.
        pub async fn create_metadata_store(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateMetadataStoreRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/CreateMetadataStore",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "CreateMetadataStore",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Retrieves a specific MetadataStore.
        pub async fn get_metadata_store(
            &mut self,
            request: impl tonic::IntoRequest<super::GetMetadataStoreRequest>,
        ) -> std::result::Result<tonic::Response<super::MetadataStore>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/GetMetadataStore",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "GetMetadataStore",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists MetadataStores for a Location.
        pub async fn list_metadata_stores(
            &mut self,
            request: impl tonic::IntoRequest<super::ListMetadataStoresRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListMetadataStoresResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/ListMetadataStores",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "ListMetadataStores",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a single MetadataStore and all its child resources (Artifacts,
        /// Executions, and Contexts).
        pub async fn delete_metadata_store(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteMetadataStoreRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/DeleteMetadataStore",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "DeleteMetadataStore",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates an Artifact associated with a MetadataStore.
        pub async fn create_artifact(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateArtifactRequest>,
        ) -> std::result::Result<tonic::Response<super::Artifact>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/CreateArtifact",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "CreateArtifact",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Retrieves a specific Artifact.
        pub async fn get_artifact(
            &mut self,
            request: impl tonic::IntoRequest<super::GetArtifactRequest>,
        ) -> std::result::Result<tonic::Response<super::Artifact>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/GetArtifact",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "GetArtifact",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Artifacts in the MetadataStore.
        pub async fn list_artifacts(
            &mut self,
            request: impl tonic::IntoRequest<super::ListArtifactsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListArtifactsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/ListArtifacts",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "ListArtifacts",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a stored Artifact.
        pub async fn update_artifact(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateArtifactRequest>,
        ) -> std::result::Result<tonic::Response<super::Artifact>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/UpdateArtifact",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "UpdateArtifact",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes an Artifact.
        pub async fn delete_artifact(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteArtifactRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/DeleteArtifact",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "DeleteArtifact",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Purges Artifacts.
        pub async fn purge_artifacts(
            &mut self,
            request: impl tonic::IntoRequest<super::PurgeArtifactsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/PurgeArtifacts",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "PurgeArtifacts",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a Context associated with a MetadataStore.
        pub async fn create_context(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateContextRequest>,
        ) -> std::result::Result<tonic::Response<super::Context>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/CreateContext",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "CreateContext",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Retrieves a specific Context.
        pub async fn get_context(
            &mut self,
            request: impl tonic::IntoRequest<super::GetContextRequest>,
        ) -> std::result::Result<tonic::Response<super::Context>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/GetContext",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "GetContext",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Contexts on the MetadataStore.
        pub async fn list_contexts(
            &mut self,
            request: impl tonic::IntoRequest<super::ListContextsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListContextsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/ListContexts",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "ListContexts",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a stored Context.
        pub async fn update_context(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateContextRequest>,
        ) -> std::result::Result<tonic::Response<super::Context>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/UpdateContext",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "UpdateContext",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a stored Context.
        pub async fn delete_context(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteContextRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/DeleteContext",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "DeleteContext",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Purges Contexts.
        pub async fn purge_contexts(
            &mut self,
            request: impl tonic::IntoRequest<super::PurgeContextsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/PurgeContexts",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "PurgeContexts",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Adds a set of Artifacts and Executions to a Context. If any of the
        /// Artifacts or Executions have already been added to a Context, they are
        /// simply skipped.
        pub async fn add_context_artifacts_and_executions(
            &mut self,
            request: impl tonic::IntoRequest<
                super::AddContextArtifactsAndExecutionsRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::AddContextArtifactsAndExecutionsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/AddContextArtifactsAndExecutions",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "AddContextArtifactsAndExecutions",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Adds a set of Contexts as children to a parent Context. If any of the
        /// child Contexts have already been added to the parent Context, they are
        /// simply skipped. If this call would create a cycle or cause any Context to
        /// have more than 10 parents, the request will fail with an INVALID_ARGUMENT
        /// error.
        pub async fn add_context_children(
            &mut self,
            request: impl tonic::IntoRequest<super::AddContextChildrenRequest>,
        ) -> std::result::Result<
            tonic::Response<super::AddContextChildrenResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/AddContextChildren",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "AddContextChildren",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Remove a set of children contexts from a parent Context. If any of the
        /// child Contexts were NOT added to the parent Context, they are
        /// simply skipped.
        pub async fn remove_context_children(
            &mut self,
            request: impl tonic::IntoRequest<super::RemoveContextChildrenRequest>,
        ) -> std::result::Result<
            tonic::Response<super::RemoveContextChildrenResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/RemoveContextChildren",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "RemoveContextChildren",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Retrieves Artifacts and Executions within the specified Context, connected
        /// by Event edges and returned as a LineageSubgraph.
        pub async fn query_context_lineage_subgraph(
            &mut self,
            request: impl tonic::IntoRequest<super::QueryContextLineageSubgraphRequest>,
        ) -> std::result::Result<
            tonic::Response<super::LineageSubgraph>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/QueryContextLineageSubgraph",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "QueryContextLineageSubgraph",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates an Execution associated with a MetadataStore.
        pub async fn create_execution(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateExecutionRequest>,
        ) -> std::result::Result<tonic::Response<super::Execution>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/CreateExecution",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "CreateExecution",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Retrieves a specific Execution.
        pub async fn get_execution(
            &mut self,
            request: impl tonic::IntoRequest<super::GetExecutionRequest>,
        ) -> std::result::Result<tonic::Response<super::Execution>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/GetExecution",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "GetExecution",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Executions in the MetadataStore.
        pub async fn list_executions(
            &mut self,
            request: impl tonic::IntoRequest<super::ListExecutionsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListExecutionsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/ListExecutions",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "ListExecutions",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a stored Execution.
        pub async fn update_execution(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateExecutionRequest>,
        ) -> std::result::Result<tonic::Response<super::Execution>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/UpdateExecution",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "UpdateExecution",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes an Execution.
        pub async fn delete_execution(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteExecutionRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/DeleteExecution",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "DeleteExecution",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Purges Executions.
        pub async fn purge_executions(
            &mut self,
            request: impl tonic::IntoRequest<super::PurgeExecutionsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/PurgeExecutions",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "PurgeExecutions",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Adds Events to the specified Execution. An Event indicates whether an
        /// Artifact was used as an input or output for an Execution. If an Event
        /// already exists between the Execution and the Artifact, the Event is
        /// skipped.
        pub async fn add_execution_events(
            &mut self,
            request: impl tonic::IntoRequest<super::AddExecutionEventsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::AddExecutionEventsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/AddExecutionEvents",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "AddExecutionEvents",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Obtains the set of input and output Artifacts for this Execution, in the
        /// form of LineageSubgraph that also contains the Execution and connecting
        /// Events.
        pub async fn query_execution_inputs_and_outputs(
            &mut self,
            request: impl tonic::IntoRequest<
                super::QueryExecutionInputsAndOutputsRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::LineageSubgraph>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/QueryExecutionInputsAndOutputs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "QueryExecutionInputsAndOutputs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a MetadataSchema.
        pub async fn create_metadata_schema(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateMetadataSchemaRequest>,
        ) -> std::result::Result<tonic::Response<super::MetadataSchema>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/CreateMetadataSchema",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "CreateMetadataSchema",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Retrieves a specific MetadataSchema.
        pub async fn get_metadata_schema(
            &mut self,
            request: impl tonic::IntoRequest<super::GetMetadataSchemaRequest>,
        ) -> std::result::Result<tonic::Response<super::MetadataSchema>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/GetMetadataSchema",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "GetMetadataSchema",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists MetadataSchemas.
        pub async fn list_metadata_schemas(
            &mut self,
            request: impl tonic::IntoRequest<super::ListMetadataSchemasRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListMetadataSchemasResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/ListMetadataSchemas",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "ListMetadataSchemas",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Retrieves lineage of an Artifact represented through Artifacts and
        /// Executions connected by Event edges and returned as a LineageSubgraph.
        pub async fn query_artifact_lineage_subgraph(
            &mut self,
            request: impl tonic::IntoRequest<super::QueryArtifactLineageSubgraphRequest>,
        ) -> std::result::Result<
            tonic::Response<super::LineageSubgraph>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MetadataService/QueryArtifactLineageSubgraph",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MetadataService",
                        "QueryArtifactLineageSubgraph",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Represents one resource that exists in automl.googleapis.com,
/// datalabeling.googleapis.com or ml.googleapis.com.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MigratableResource {
    /// Output only. Timestamp when the last migration attempt on this
    /// MigratableResource started. Will not be set if there's no migration attempt
    /// on this MigratableResource.
    #[prost(message, optional, tag = "5")]
    pub last_migrate_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this MigratableResource was last updated.
    #[prost(message, optional, tag = "6")]
    pub last_update_time: ::core::option::Option<::prost_types::Timestamp>,
    #[prost(oneof = "migratable_resource::Resource", tags = "1, 2, 3, 4")]
    pub resource: ::core::option::Option<migratable_resource::Resource>,
}
/// Nested message and enum types in `MigratableResource`.
pub mod migratable_resource {
    /// Represents one model Version in ml.googleapis.com.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct MlEngineModelVersion {
        /// The ml.googleapis.com endpoint that this model Version currently lives
        /// in.
        /// Example values:
        ///
        /// * ml.googleapis.com
        /// * us-centrall-ml.googleapis.com
        /// * europe-west4-ml.googleapis.com
        /// * asia-east1-ml.googleapis.com
        #[prost(string, tag = "1")]
        pub endpoint: ::prost::alloc::string::String,
        /// Full resource name of ml engine model Version.
        /// Format: `projects/{project}/models/{model}/versions/{version}`.
        #[prost(string, tag = "2")]
        pub version: ::prost::alloc::string::String,
    }
    /// Represents one Model in automl.googleapis.com.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct AutomlModel {
        /// Full resource name of automl Model.
        /// Format:
        /// `projects/{project}/locations/{location}/models/{model}`.
        #[prost(string, tag = "1")]
        pub model: ::prost::alloc::string::String,
        /// The Model's display name in automl.googleapis.com.
        #[prost(string, tag = "3")]
        pub model_display_name: ::prost::alloc::string::String,
    }
    /// Represents one Dataset in automl.googleapis.com.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct AutomlDataset {
        /// Full resource name of automl Dataset.
        /// Format:
        /// `projects/{project}/locations/{location}/datasets/{dataset}`.
        #[prost(string, tag = "1")]
        pub dataset: ::prost::alloc::string::String,
        /// The Dataset's display name in automl.googleapis.com.
        #[prost(string, tag = "4")]
        pub dataset_display_name: ::prost::alloc::string::String,
    }
    /// Represents one Dataset in datalabeling.googleapis.com.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct DataLabelingDataset {
        /// Full resource name of data labeling Dataset.
        /// Format:
        /// `projects/{project}/datasets/{dataset}`.
        #[prost(string, tag = "1")]
        pub dataset: ::prost::alloc::string::String,
        /// The Dataset's display name in datalabeling.googleapis.com.
        #[prost(string, tag = "4")]
        pub dataset_display_name: ::prost::alloc::string::String,
        /// The migratable AnnotatedDataset in datalabeling.googleapis.com belongs to
        /// the data labeling Dataset.
        #[prost(message, repeated, tag = "3")]
        pub data_labeling_annotated_datasets: ::prost::alloc::vec::Vec<
            data_labeling_dataset::DataLabelingAnnotatedDataset,
        >,
    }
    /// Nested message and enum types in `DataLabelingDataset`.
    pub mod data_labeling_dataset {
        /// Represents one AnnotatedDataset in datalabeling.googleapis.com.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct DataLabelingAnnotatedDataset {
            /// Full resource name of data labeling AnnotatedDataset.
            /// Format:
            /// `projects/{project}/datasets/{dataset}/annotatedDatasets/{annotated_dataset}`.
            #[prost(string, tag = "1")]
            pub annotated_dataset: ::prost::alloc::string::String,
            /// The AnnotatedDataset's display name in datalabeling.googleapis.com.
            #[prost(string, tag = "3")]
            pub annotated_dataset_display_name: ::prost::alloc::string::String,
        }
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Resource {
        /// Output only. Represents one Version in ml.googleapis.com.
        #[prost(message, tag = "1")]
        MlEngineModelVersion(MlEngineModelVersion),
        /// Output only. Represents one Model in automl.googleapis.com.
        #[prost(message, tag = "2")]
        AutomlModel(AutomlModel),
        /// Output only. Represents one Dataset in automl.googleapis.com.
        #[prost(message, tag = "3")]
        AutomlDataset(AutomlDataset),
        /// Output only. Represents one Dataset in datalabeling.googleapis.com.
        #[prost(message, tag = "4")]
        DataLabelingDataset(DataLabelingDataset),
    }
}
/// Request message for
/// [MigrationService.SearchMigratableResources][google.cloud.aiplatform.v1.MigrationService.SearchMigratableResources].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchMigratableResourcesRequest {
    /// Required. The location that the migratable resources should be searched
    /// from. It's the Vertex AI location that the resources can be migrated to,
    /// not the resources' original location. Format:
    /// `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard page size.
    /// The default and maximum value is 100.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// The standard page token.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
    /// A filter for your search. You can use the following types of filters:
    ///
    /// *   Resource type filters. The following strings filter for a specific type
    ///      of [MigratableResource][google.cloud.aiplatform.v1.MigratableResource]:
    ///      *   `ml_engine_model_version:*`
    ///      *   `automl_model:*`
    ///      *   `automl_dataset:*`
    ///      *   `data_labeling_dataset:*`
    /// *   "Migrated or not" filters. The following strings filter for resources
    ///      that either have or have not already been migrated:
    ///      *   `last_migrate_time:*` filters for migrated resources.
    ///      *   `NOT last_migrate_time:*` filters for not yet migrated resources.
    #[prost(string, tag = "4")]
    pub filter: ::prost::alloc::string::String,
}
/// Response message for
/// [MigrationService.SearchMigratableResources][google.cloud.aiplatform.v1.MigrationService.SearchMigratableResources].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchMigratableResourcesResponse {
    /// All migratable resources that can be migrated to the
    /// location specified in the request.
    #[prost(message, repeated, tag = "1")]
    pub migratable_resources: ::prost::alloc::vec::Vec<MigratableResource>,
    /// The standard next-page token.
    /// The migratable_resources may not fill page_size in
    /// SearchMigratableResourcesRequest even when there are subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [MigrationService.BatchMigrateResources][google.cloud.aiplatform.v1.MigrationService.BatchMigrateResources].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchMigrateResourcesRequest {
    /// Required. The location of the migrated resource will live in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The request messages specifying the resources to migrate.
    /// They must be in the same location as the destination.
    /// Up to 50 resources can be migrated in one batch.
    #[prost(message, repeated, tag = "2")]
    pub migrate_resource_requests: ::prost::alloc::vec::Vec<MigrateResourceRequest>,
}
/// Config of migrating one resource from automl.googleapis.com,
/// datalabeling.googleapis.com and ml.googleapis.com to Vertex AI.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MigrateResourceRequest {
    #[prost(oneof = "migrate_resource_request::Request", tags = "1, 2, 3, 4")]
    pub request: ::core::option::Option<migrate_resource_request::Request>,
}
/// Nested message and enum types in `MigrateResourceRequest`.
pub mod migrate_resource_request {
    /// Config for migrating version in ml.googleapis.com to Vertex AI's Model.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct MigrateMlEngineModelVersionConfig {
        /// Required. The ml.googleapis.com endpoint that this model version should
        /// be migrated from. Example values:
        ///
        /// * ml.googleapis.com
        ///
        /// * us-centrall-ml.googleapis.com
        ///
        /// * europe-west4-ml.googleapis.com
        ///
        /// * asia-east1-ml.googleapis.com
        #[prost(string, tag = "1")]
        pub endpoint: ::prost::alloc::string::String,
        /// Required. Full resource name of ml engine model version.
        /// Format: `projects/{project}/models/{model}/versions/{version}`.
        #[prost(string, tag = "2")]
        pub model_version: ::prost::alloc::string::String,
        /// Required. Display name of the model in Vertex AI.
        /// System will pick a display name if unspecified.
        #[prost(string, tag = "3")]
        pub model_display_name: ::prost::alloc::string::String,
    }
    /// Config for migrating Model in automl.googleapis.com to Vertex AI's Model.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct MigrateAutomlModelConfig {
        /// Required. Full resource name of automl Model.
        /// Format:
        /// `projects/{project}/locations/{location}/models/{model}`.
        #[prost(string, tag = "1")]
        pub model: ::prost::alloc::string::String,
        /// Optional. Display name of the model in Vertex AI.
        /// System will pick a display name if unspecified.
        #[prost(string, tag = "2")]
        pub model_display_name: ::prost::alloc::string::String,
    }
    /// Config for migrating Dataset in automl.googleapis.com to Vertex AI's
    /// Dataset.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct MigrateAutomlDatasetConfig {
        /// Required. Full resource name of automl Dataset.
        /// Format:
        /// `projects/{project}/locations/{location}/datasets/{dataset}`.
        #[prost(string, tag = "1")]
        pub dataset: ::prost::alloc::string::String,
        /// Required. Display name of the Dataset in Vertex AI.
        /// System will pick a display name if unspecified.
        #[prost(string, tag = "2")]
        pub dataset_display_name: ::prost::alloc::string::String,
    }
    /// Config for migrating Dataset in datalabeling.googleapis.com to Vertex
    /// AI's Dataset.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct MigrateDataLabelingDatasetConfig {
        /// Required. Full resource name of data labeling Dataset.
        /// Format:
        /// `projects/{project}/datasets/{dataset}`.
        #[prost(string, tag = "1")]
        pub dataset: ::prost::alloc::string::String,
        /// Optional. Display name of the Dataset in Vertex AI.
        /// System will pick a display name if unspecified.
        #[prost(string, tag = "2")]
        pub dataset_display_name: ::prost::alloc::string::String,
        /// Optional. Configs for migrating AnnotatedDataset in
        /// datalabeling.googleapis.com to Vertex AI's SavedQuery. The specified
        /// AnnotatedDatasets have to belong to the datalabeling Dataset.
        #[prost(message, repeated, tag = "3")]
        pub migrate_data_labeling_annotated_dataset_configs: ::prost::alloc::vec::Vec<
            migrate_data_labeling_dataset_config::MigrateDataLabelingAnnotatedDatasetConfig,
        >,
    }
    /// Nested message and enum types in `MigrateDataLabelingDatasetConfig`.
    pub mod migrate_data_labeling_dataset_config {
        /// Config for migrating AnnotatedDataset in datalabeling.googleapis.com to
        /// Vertex AI's SavedQuery.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct MigrateDataLabelingAnnotatedDatasetConfig {
            /// Required. Full resource name of data labeling AnnotatedDataset.
            /// Format:
            /// `projects/{project}/datasets/{dataset}/annotatedDatasets/{annotated_dataset}`.
            #[prost(string, tag = "1")]
            pub annotated_dataset: ::prost::alloc::string::String,
        }
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Request {
        /// Config for migrating Version in ml.googleapis.com to Vertex AI's Model.
        #[prost(message, tag = "1")]
        MigrateMlEngineModelVersionConfig(MigrateMlEngineModelVersionConfig),
        /// Config for migrating Model in automl.googleapis.com to Vertex AI's
        /// Model.
        #[prost(message, tag = "2")]
        MigrateAutomlModelConfig(MigrateAutomlModelConfig),
        /// Config for migrating Dataset in automl.googleapis.com to Vertex AI's
        /// Dataset.
        #[prost(message, tag = "3")]
        MigrateAutomlDatasetConfig(MigrateAutomlDatasetConfig),
        /// Config for migrating Dataset in datalabeling.googleapis.com to
        /// Vertex AI's Dataset.
        #[prost(message, tag = "4")]
        MigrateDataLabelingDatasetConfig(MigrateDataLabelingDatasetConfig),
    }
}
/// Response message for
/// [MigrationService.BatchMigrateResources][google.cloud.aiplatform.v1.MigrationService.BatchMigrateResources].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchMigrateResourcesResponse {
    /// Successfully migrated resources.
    #[prost(message, repeated, tag = "1")]
    pub migrate_resource_responses: ::prost::alloc::vec::Vec<MigrateResourceResponse>,
}
/// Describes a successfully migrated resource.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MigrateResourceResponse {
    /// Before migration, the identifier in ml.googleapis.com,
    /// automl.googleapis.com or datalabeling.googleapis.com.
    #[prost(message, optional, tag = "3")]
    pub migratable_resource: ::core::option::Option<MigratableResource>,
    /// After migration, the resource name in Vertex AI.
    #[prost(oneof = "migrate_resource_response::MigratedResource", tags = "1, 2")]
    pub migrated_resource: ::core::option::Option<
        migrate_resource_response::MigratedResource,
    >,
}
/// Nested message and enum types in `MigrateResourceResponse`.
pub mod migrate_resource_response {
    /// After migration, the resource name in Vertex AI.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum MigratedResource {
        /// Migrated Dataset's resource name.
        #[prost(string, tag = "1")]
        Dataset(::prost::alloc::string::String),
        /// Migrated Model's resource name.
        #[prost(string, tag = "2")]
        Model(::prost::alloc::string::String),
    }
}
/// Runtime operation information for
/// [MigrationService.BatchMigrateResources][google.cloud.aiplatform.v1.MigrationService.BatchMigrateResources].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchMigrateResourcesOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// Partial results that reflect the latest migration operation progress.
    #[prost(message, repeated, tag = "2")]
    pub partial_results: ::prost::alloc::vec::Vec<
        batch_migrate_resources_operation_metadata::PartialResult,
    >,
}
/// Nested message and enum types in `BatchMigrateResourcesOperationMetadata`.
pub mod batch_migrate_resources_operation_metadata {
    /// Represents a partial result in batch migration operation for one
    /// [MigrateResourceRequest][google.cloud.aiplatform.v1.MigrateResourceRequest].
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct PartialResult {
        /// It's the same as the value in
        /// [BatchMigrateResourcesRequest.migrate_resource_requests][google.cloud.aiplatform.v1.BatchMigrateResourcesRequest.migrate_resource_requests].
        #[prost(message, optional, tag = "1")]
        pub request: ::core::option::Option<super::MigrateResourceRequest>,
        /// If the resource's migration is ongoing, none of the result will be set.
        /// If the resource's migration is finished, either error or one of the
        /// migrated resource name will be filled.
        #[prost(oneof = "partial_result::Result", tags = "2, 3, 4")]
        pub result: ::core::option::Option<partial_result::Result>,
    }
    /// Nested message and enum types in `PartialResult`.
    pub mod partial_result {
        /// If the resource's migration is ongoing, none of the result will be set.
        /// If the resource's migration is finished, either error or one of the
        /// migrated resource name will be filled.
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum Result {
            /// The error result of the migration request in case of failure.
            #[prost(message, tag = "2")]
            Error(super::super::super::super::super::rpc::Status),
            /// Migrated model resource name.
            #[prost(string, tag = "3")]
            Model(::prost::alloc::string::String),
            /// Migrated dataset resource name.
            #[prost(string, tag = "4")]
            Dataset(::prost::alloc::string::String),
        }
    }
}
/// Generated client implementations.
pub mod migration_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service that migrates resources from automl.googleapis.com,
    /// datalabeling.googleapis.com and ml.googleapis.com to Vertex AI.
    #[derive(Debug, Clone)]
    pub struct MigrationServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl MigrationServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> MigrationServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> MigrationServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            MigrationServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Searches all of the resources in automl.googleapis.com,
        /// datalabeling.googleapis.com and ml.googleapis.com that can be migrated to
        /// Vertex AI's given location.
        pub async fn search_migratable_resources(
            &mut self,
            request: impl tonic::IntoRequest<super::SearchMigratableResourcesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::SearchMigratableResourcesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MigrationService/SearchMigratableResources",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MigrationService",
                        "SearchMigratableResources",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Batch migrates resources from ml.googleapis.com, automl.googleapis.com,
        /// and datalabeling.googleapis.com to Vertex AI.
        pub async fn batch_migrate_resources(
            &mut self,
            request: impl tonic::IntoRequest<super::BatchMigrateResourcesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.MigrationService/BatchMigrateResources",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.MigrationService",
                        "BatchMigrateResources",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// A collection of metrics calculated by comparing Model's predictions on all of
/// the test data against annotations from the test data.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelEvaluation {
    /// Output only. The resource name of the ModelEvaluation.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// The display name of the ModelEvaluation.
    #[prost(string, tag = "10")]
    pub display_name: ::prost::alloc::string::String,
    /// Points to a YAML file stored on Google Cloud Storage describing the
    /// [metrics][google.cloud.aiplatform.v1.ModelEvaluation.metrics] of this
    /// ModelEvaluation. The schema is defined as an OpenAPI 3.0.2 [Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    #[prost(string, tag = "2")]
    pub metrics_schema_uri: ::prost::alloc::string::String,
    /// Evaluation metrics of the Model. The schema of the metrics is stored in
    /// [metrics_schema_uri][google.cloud.aiplatform.v1.ModelEvaluation.metrics_schema_uri]
    #[prost(message, optional, tag = "3")]
    pub metrics: ::core::option::Option<::prost_types::Value>,
    /// Output only. Timestamp when this ModelEvaluation was created.
    #[prost(message, optional, tag = "4")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// All possible
    /// [dimensions][google.cloud.aiplatform.v1.ModelEvaluationSlice.Slice.dimension]
    /// of ModelEvaluationSlices. The dimensions can be used as the filter of the
    /// [ModelService.ListModelEvaluationSlices][google.cloud.aiplatform.v1.ModelService.ListModelEvaluationSlices]
    /// request, in the form of `slice.dimension = <dimension>`.
    #[prost(string, repeated, tag = "5")]
    pub slice_dimensions: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Points to a YAML file stored on Google Cloud Storage describing
    /// [EvaluatedDataItemView.data_item_payload][] and
    /// [EvaluatedAnnotation.data_item_payload][google.cloud.aiplatform.v1.EvaluatedAnnotation.data_item_payload].
    /// The schema is defined as an OpenAPI 3.0.2 [Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    ///
    /// This field is not populated if there are neither EvaluatedDataItemViews nor
    /// EvaluatedAnnotations under this ModelEvaluation.
    #[prost(string, tag = "6")]
    pub data_item_schema_uri: ::prost::alloc::string::String,
    /// Points to a YAML file stored on Google Cloud Storage describing
    /// [EvaluatedDataItemView.predictions][],
    /// [EvaluatedDataItemView.ground_truths][],
    /// [EvaluatedAnnotation.predictions][google.cloud.aiplatform.v1.EvaluatedAnnotation.predictions],
    /// and
    /// [EvaluatedAnnotation.ground_truths][google.cloud.aiplatform.v1.EvaluatedAnnotation.ground_truths].
    /// The schema is defined as an OpenAPI 3.0.2 [Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    ///
    /// This field is not populated if there are neither EvaluatedDataItemViews nor
    /// EvaluatedAnnotations under this ModelEvaluation.
    #[prost(string, tag = "7")]
    pub annotation_schema_uri: ::prost::alloc::string::String,
    /// Aggregated explanation metrics for the Model's prediction output over the
    /// data this ModelEvaluation uses. This field is populated only if the Model
    /// is evaluated with explanations, and only for AutoML tabular Models.
    ///
    #[prost(message, optional, tag = "8")]
    pub model_explanation: ::core::option::Option<ModelExplanation>,
    /// Describes the values of
    /// [ExplanationSpec][google.cloud.aiplatform.v1.ExplanationSpec] that are used
    /// for explaining the predicted values on the evaluated data.
    #[prost(message, repeated, tag = "9")]
    pub explanation_specs: ::prost::alloc::vec::Vec<
        model_evaluation::ModelEvaluationExplanationSpec,
    >,
    /// The metadata of the ModelEvaluation.
    /// For the ModelEvaluation uploaded from Managed Pipeline, metadata contains a
    /// structured value with keys of "pipeline_job_id", "evaluation_dataset_type",
    /// "evaluation_dataset_path", "row_based_metrics_path".
    #[prost(message, optional, tag = "11")]
    pub metadata: ::core::option::Option<::prost_types::Value>,
}
/// Nested message and enum types in `ModelEvaluation`.
pub mod model_evaluation {
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ModelEvaluationExplanationSpec {
        /// Explanation type.
        ///
        /// For AutoML Image Classification models, possible values are:
        ///
        ///    * `image-integrated-gradients`
        ///    * `image-xrai`
        #[prost(string, tag = "1")]
        pub explanation_type: ::prost::alloc::string::String,
        /// Explanation spec details.
        #[prost(message, optional, tag = "2")]
        pub explanation_spec: ::core::option::Option<super::ExplanationSpec>,
    }
}
/// A collection of metrics calculated by comparing Model's predictions on a
/// slice of the test data against ground truth annotations.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelEvaluationSlice {
    /// Output only. The resource name of the ModelEvaluationSlice.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. The slice of the test data that is used to evaluate the Model.
    #[prost(message, optional, tag = "2")]
    pub slice: ::core::option::Option<model_evaluation_slice::Slice>,
    /// Output only. Points to a YAML file stored on Google Cloud Storage
    /// describing the
    /// [metrics][google.cloud.aiplatform.v1.ModelEvaluationSlice.metrics] of this
    /// ModelEvaluationSlice. The schema is defined as an OpenAPI 3.0.2 [Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    #[prost(string, tag = "3")]
    pub metrics_schema_uri: ::prost::alloc::string::String,
    /// Output only. Sliced evaluation metrics of the Model. The schema of the
    /// metrics is stored in
    /// [metrics_schema_uri][google.cloud.aiplatform.v1.ModelEvaluationSlice.metrics_schema_uri]
    #[prost(message, optional, tag = "4")]
    pub metrics: ::core::option::Option<::prost_types::Value>,
    /// Output only. Timestamp when this ModelEvaluationSlice was created.
    #[prost(message, optional, tag = "5")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Aggregated explanation metrics for the Model's prediction
    /// output over the data this ModelEvaluation uses. This field is populated
    /// only if the Model is evaluated with explanations, and only for tabular
    /// Models.
    #[prost(message, optional, tag = "6")]
    pub model_explanation: ::core::option::Option<ModelExplanation>,
}
/// Nested message and enum types in `ModelEvaluationSlice`.
pub mod model_evaluation_slice {
    /// Definition of a slice.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Slice {
        /// Output only. The dimension of the slice.
        /// Well-known dimensions are:
        ///    * `annotationSpec`: This slice is on the test data that has either
        ///      ground truth or prediction with
        ///      [AnnotationSpec.display_name][google.cloud.aiplatform.v1.AnnotationSpec.display_name]
        ///      equals to
        ///      [value][google.cloud.aiplatform.v1.ModelEvaluationSlice.Slice.value].
        ///    * `slice`: This slice is a user customized slice defined by its
        ///      SliceSpec.
        #[prost(string, tag = "1")]
        pub dimension: ::prost::alloc::string::String,
        /// Output only. The value of the dimension in this slice.
        #[prost(string, tag = "2")]
        pub value: ::prost::alloc::string::String,
        /// Output only. Specification for how the data was sliced.
        #[prost(message, optional, tag = "3")]
        pub slice_spec: ::core::option::Option<slice::SliceSpec>,
    }
    /// Nested message and enum types in `Slice`.
    pub mod slice {
        /// Specification for how the data should be sliced.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct SliceSpec {
            /// Mapping configuration for this SliceSpec.
            /// The key is the name of the feature.
            /// By default, the key will be prefixed by "instance" as a dictionary
            /// prefix for Vertex Batch Predictions output format.
            #[prost(map = "string, message", tag = "1")]
            pub configs: ::std::collections::HashMap<
                ::prost::alloc::string::String,
                slice_spec::SliceConfig,
            >,
        }
        /// Nested message and enum types in `SliceSpec`.
        pub mod slice_spec {
            /// Specification message containing the config for this SliceSpec.
            /// When `kind` is selected as `value` and/or `range`, only a single slice
            /// will be computed.
            /// When `all_values` is present, a separate slice will be computed for
            /// each possible label/value for the corresponding key in `config`.
            /// Examples, with feature zip_code with values 12345, 23334, 88888 and
            /// feature country with values "US", "Canada", "Mexico" in the dataset:
            ///
            /// Example 1:
            ///
            ///      {
            ///        "zip_code": { "value": { "float_value": 12345.0 } }
            ///      }
            ///
            /// A single slice for any data with zip_code 12345 in the dataset.
            ///
            /// Example 2:
            ///
            ///      {
            ///        "zip_code": { "range": { "low": 12345, "high": 20000 } }
            ///      }
            ///
            /// A single slice containing data where the zip_codes between 12345 and
            /// 20000 For this example, data with the zip_code of 12345 will be in this
            /// slice.
            ///
            /// Example 3:
            ///
            ///      {
            ///        "zip_code": { "range": { "low": 10000, "high": 20000 } },
            ///        "country": { "value": { "string_value": "US" } }
            ///      }
            ///
            /// A single slice containing data where the zip_codes between 10000 and
            /// 20000 has the country "US". For this example, data with the zip_code of
            /// 12345 and country "US" will be in this slice.
            ///
            /// Example 4:
            ///
            ///      { "country": {"all_values": { "value": true } } }
            ///
            /// Three slices are computed, one for each unique country in the dataset.
            ///
            /// Example 5:
            ///
            ///      {
            ///        "country": { "all_values": { "value": true } },
            ///        "zip_code": { "value": { "float_value": 12345.0 } }
            ///      }
            ///
            /// Three slices are computed, one for each unique country in the dataset
            /// where the zip_code is also 12345. For this example, data with zip_code
            /// 12345 and country "US" will be in one slice, zip_code 12345 and country
            /// "Canada" in another slice, and zip_code 12345 and country "Mexico" in
            /// another slice, totaling 3 slices.
            #[derive(Clone, PartialEq, ::prost::Message)]
            pub struct SliceConfig {
                #[prost(oneof = "slice_config::Kind", tags = "1, 2, 3")]
                pub kind: ::core::option::Option<slice_config::Kind>,
            }
            /// Nested message and enum types in `SliceConfig`.
            pub mod slice_config {
                #[derive(Clone, PartialEq, ::prost::Oneof)]
                pub enum Kind {
                    /// A unique specific value for a given feature.
                    /// Example: `{ "value": { "string_value": "12345" } }`
                    #[prost(message, tag = "1")]
                    Value(super::Value),
                    /// A range of values for a numerical feature.
                    /// Example: `{"range":{"low":10000.0,"high":50000.0}}`
                    /// will capture 12345 and 23334 in the slice.
                    #[prost(message, tag = "2")]
                    Range(super::Range),
                    /// If all_values is set to true, then all possible labels of the keyed
                    /// feature will have another slice computed.
                    /// Example: `{"all_values":{"value":true}}`
                    #[prost(message, tag = "3")]
                    AllValues(bool),
                }
            }
            /// A range of values for slice(s).
            /// `low` is inclusive, `high` is exclusive.
            #[derive(Clone, Copy, PartialEq, ::prost::Message)]
            pub struct Range {
                /// Inclusive low value for the range.
                #[prost(float, tag = "1")]
                pub low: f32,
                /// Exclusive high value for the range.
                #[prost(float, tag = "2")]
                pub high: f32,
            }
            /// Single value that supports strings and floats.
            #[derive(Clone, PartialEq, ::prost::Message)]
            pub struct Value {
                #[prost(oneof = "value::Kind", tags = "1, 2")]
                pub kind: ::core::option::Option<value::Kind>,
            }
            /// Nested message and enum types in `Value`.
            pub mod value {
                #[derive(Clone, PartialEq, ::prost::Oneof)]
                pub enum Kind {
                    /// String type.
                    #[prost(string, tag = "1")]
                    StringValue(::prost::alloc::string::String),
                    /// Float type.
                    #[prost(float, tag = "2")]
                    FloatValue(f32),
                }
            }
        }
    }
}
/// A Model Garden Publisher Model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PublisherModel {
    /// Output only. The resource name of the PublisherModel.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Output only. Immutable. The version ID of the PublisherModel.
    /// A new version is committed when a new model version is uploaded under an
    /// existing model id. It is an auto-incrementing decimal number in string
    /// representation.
    #[prost(string, tag = "2")]
    pub version_id: ::prost::alloc::string::String,
    /// Required. Indicates the open source category of the publisher model.
    #[prost(enumeration = "publisher_model::OpenSourceCategory", tag = "7")]
    pub open_source_category: i32,
    /// Optional. Supported call-to-action options.
    #[prost(message, optional, tag = "19")]
    pub supported_actions: ::core::option::Option<publisher_model::CallToAction>,
    /// Optional. Additional information about the model's Frameworks.
    #[prost(string, repeated, tag = "23")]
    pub frameworks: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. Indicates the launch stage of the model.
    #[prost(enumeration = "publisher_model::LaunchStage", tag = "29")]
    pub launch_stage: i32,
    /// Optional. Indicates the state of the model version.
    #[prost(enumeration = "publisher_model::VersionState", tag = "37")]
    pub version_state: i32,
    /// Optional. Output only. Immutable. Used to indicate this model has a
    /// publisher model and provide the template of the publisher model resource
    /// name.
    #[prost(string, tag = "30")]
    pub publisher_model_template: ::prost::alloc::string::String,
    /// Optional. The schemata that describes formats of the PublisherModel's
    /// predictions and explanations as given and returned via
    /// [PredictionService.Predict][google.cloud.aiplatform.v1.PredictionService.Predict].
    #[prost(message, optional, tag = "31")]
    pub predict_schemata: ::core::option::Option<PredictSchemata>,
}
/// Nested message and enum types in `PublisherModel`.
pub mod publisher_model {
    /// Reference to a resource.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ResourceReference {
        #[prost(oneof = "resource_reference::Reference", tags = "1, 2, 3, 4")]
        pub reference: ::core::option::Option<resource_reference::Reference>,
    }
    /// Nested message and enum types in `ResourceReference`.
    pub mod resource_reference {
        #[derive(Clone, PartialEq, ::prost::Oneof)]
        pub enum Reference {
            /// The URI of the resource.
            #[prost(string, tag = "1")]
            Uri(::prost::alloc::string::String),
            /// The resource name of the Google Cloud resource.
            #[prost(string, tag = "2")]
            ResourceName(::prost::alloc::string::String),
            /// Use case (CUJ) of the resource.
            #[prost(string, tag = "3")]
            UseCase(::prost::alloc::string::String),
            /// Description of the resource.
            #[prost(string, tag = "4")]
            Description(::prost::alloc::string::String),
        }
    }
    /// A named piece of documentation.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Documentation {
        /// Required. E.g., OVERVIEW, USE CASES, DOCUMENTATION, SDK & SAMPLES, JAVA,
        /// NODE.JS, etc..
        #[prost(string, tag = "1")]
        pub title: ::prost::alloc::string::String,
        /// Required. Content of this piece of document (in Markdown format).
        #[prost(string, tag = "2")]
        pub content: ::prost::alloc::string::String,
    }
    /// Actions could take on this Publisher Model.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct CallToAction {
        /// Optional. To view Rest API docs.
        #[prost(message, optional, tag = "1")]
        pub view_rest_api: ::core::option::Option<call_to_action::ViewRestApi>,
        /// Optional. Open notebook of the PublisherModel.
        #[prost(message, optional, tag = "2")]
        pub open_notebook: ::core::option::Option<
            call_to_action::RegionalResourceReferences,
        >,
        /// Optional. Open notebooks of the PublisherModel.
        #[prost(message, optional, tag = "12")]
        pub open_notebooks: ::core::option::Option<call_to_action::OpenNotebooks>,
        /// Optional. Create application using the PublisherModel.
        #[prost(message, optional, tag = "3")]
        pub create_application: ::core::option::Option<
            call_to_action::RegionalResourceReferences,
        >,
        /// Optional. Open fine-tuning pipeline of the PublisherModel.
        #[prost(message, optional, tag = "4")]
        pub open_fine_tuning_pipeline: ::core::option::Option<
            call_to_action::RegionalResourceReferences,
        >,
        /// Optional. Open fine-tuning pipelines of the PublisherModel.
        #[prost(message, optional, tag = "13")]
        pub open_fine_tuning_pipelines: ::core::option::Option<
            call_to_action::OpenFineTuningPipelines,
        >,
        /// Optional. Open prompt-tuning pipeline of the PublisherModel.
        #[prost(message, optional, tag = "5")]
        pub open_prompt_tuning_pipeline: ::core::option::Option<
            call_to_action::RegionalResourceReferences,
        >,
        /// Optional. Open Genie / Playground.
        #[prost(message, optional, tag = "6")]
        pub open_genie: ::core::option::Option<
            call_to_action::RegionalResourceReferences,
        >,
        /// Optional. Deploy the PublisherModel to Vertex Endpoint.
        #[prost(message, optional, tag = "7")]
        pub deploy: ::core::option::Option<call_to_action::Deploy>,
        /// Optional. Deploy PublisherModel to Google Kubernetes Engine.
        #[prost(message, optional, tag = "14")]
        pub deploy_gke: ::core::option::Option<call_to_action::DeployGke>,
        /// Optional. Open in Generation AI Studio.
        #[prost(message, optional, tag = "8")]
        pub open_generation_ai_studio: ::core::option::Option<
            call_to_action::RegionalResourceReferences,
        >,
        /// Optional. Request for access.
        #[prost(message, optional, tag = "9")]
        pub request_access: ::core::option::Option<
            call_to_action::RegionalResourceReferences,
        >,
        /// Optional. Open evaluation pipeline of the PublisherModel.
        #[prost(message, optional, tag = "11")]
        pub open_evaluation_pipeline: ::core::option::Option<
            call_to_action::RegionalResourceReferences,
        >,
    }
    /// Nested message and enum types in `CallToAction`.
    pub mod call_to_action {
        /// The regional resource name or the URI. Key is region, e.g.,
        /// us-central1, europe-west2, global, etc..
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct RegionalResourceReferences {
            /// Required.
            #[prost(map = "string, message", tag = "1")]
            pub references: ::std::collections::HashMap<
                ::prost::alloc::string::String,
                super::ResourceReference,
            >,
            /// Required.
            #[prost(string, tag = "2")]
            pub title: ::prost::alloc::string::String,
            /// Optional. Title of the resource.
            #[prost(string, optional, tag = "3")]
            pub resource_title: ::core::option::Option<::prost::alloc::string::String>,
            /// Optional. Use case (CUJ) of the resource.
            #[prost(string, optional, tag = "4")]
            pub resource_use_case: ::core::option::Option<
                ::prost::alloc::string::String,
            >,
            /// Optional. Description of the resource.
            #[prost(string, optional, tag = "5")]
            pub resource_description: ::core::option::Option<
                ::prost::alloc::string::String,
            >,
        }
        /// Rest API docs.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct ViewRestApi {
            /// Required.
            #[prost(message, repeated, tag = "1")]
            pub documentations: ::prost::alloc::vec::Vec<super::Documentation>,
            /// Required. The title of the view rest API.
            #[prost(string, tag = "2")]
            pub title: ::prost::alloc::string::String,
        }
        /// Open notebooks.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct OpenNotebooks {
            /// Required. Regional resource references to notebooks.
            #[prost(message, repeated, tag = "1")]
            pub notebooks: ::prost::alloc::vec::Vec<RegionalResourceReferences>,
        }
        /// Open fine tuning pipelines.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct OpenFineTuningPipelines {
            /// Required. Regional resource references to fine tuning pipelines.
            #[prost(message, repeated, tag = "1")]
            pub fine_tuning_pipelines: ::prost::alloc::vec::Vec<
                RegionalResourceReferences,
            >,
        }
        /// Model metadata that is needed for UploadModel or
        /// DeployModel/CreateEndpoint requests.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct Deploy {
            /// Optional. Default model display name.
            #[prost(string, tag = "1")]
            pub model_display_name: ::prost::alloc::string::String,
            /// Optional. Large model reference. When this is set, model_artifact_spec
            /// is not needed.
            #[prost(message, optional, tag = "2")]
            pub large_model_reference: ::core::option::Option<
                super::super::LargeModelReference,
            >,
            /// Optional. The specification of the container that is to be used when
            /// deploying this Model in Vertex AI. Not present for Large Models.
            #[prost(message, optional, tag = "3")]
            pub container_spec: ::core::option::Option<super::super::ModelContainerSpec>,
            /// Optional. The path to the directory containing the Model artifact and
            /// any of its supporting files.
            #[prost(string, tag = "4")]
            pub artifact_uri: ::prost::alloc::string::String,
            /// Optional. The name of the deploy task (e.g., "text to image
            /// generation").
            #[prost(string, optional, tag = "10")]
            pub deploy_task_name: ::core::option::Option<::prost::alloc::string::String>,
            /// Optional. Metadata information about this deployment config.
            #[prost(message, optional, tag = "11")]
            pub deploy_metadata: ::core::option::Option<deploy::DeployMetadata>,
            /// Required. The title of the regional resource reference.
            #[prost(string, tag = "8")]
            pub title: ::prost::alloc::string::String,
            /// Optional. The signed URI for ephemeral Cloud Storage access to model
            /// artifact.
            #[prost(string, tag = "9")]
            pub public_artifact_uri: ::prost::alloc::string::String,
            /// The prediction (for example, the machine) resources that the
            /// DeployedModel uses.
            #[prost(oneof = "deploy::PredictionResources", tags = "5, 6, 7")]
            pub prediction_resources: ::core::option::Option<
                deploy::PredictionResources,
            >,
        }
        /// Nested message and enum types in `Deploy`.
        pub mod deploy {
            /// Metadata information about the deployment for managing deployment
            /// config.
            #[derive(Clone, PartialEq, ::prost::Message)]
            pub struct DeployMetadata {
                /// Optional. Labels for the deployment. For managing deployment config
                /// like verifying, source of deployment config, etc.
                #[prost(map = "string, string", tag = "1")]
                pub labels: ::std::collections::HashMap<
                    ::prost::alloc::string::String,
                    ::prost::alloc::string::String,
                >,
                /// Optional. Sample request for deployed endpoint.
                #[prost(string, tag = "2")]
                pub sample_request: ::prost::alloc::string::String,
            }
            /// The prediction (for example, the machine) resources that the
            /// DeployedModel uses.
            #[derive(Clone, PartialEq, ::prost::Oneof)]
            pub enum PredictionResources {
                /// A description of resources that are dedicated to the DeployedModel,
                /// and that need a higher degree of manual configuration.
                #[prost(message, tag = "5")]
                DedicatedResources(super::super::super::DedicatedResources),
                /// A description of resources that to large degree are decided by Vertex
                /// AI, and require only a modest additional configuration.
                #[prost(message, tag = "6")]
                AutomaticResources(super::super::super::AutomaticResources),
                /// The resource name of the shared DeploymentResourcePool to deploy on.
                /// Format:
                /// `projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}`
                #[prost(string, tag = "7")]
                SharedResources(::prost::alloc::string::String),
            }
        }
        /// Configurations for PublisherModel GKE deployment
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct DeployGke {
            /// Optional. GKE deployment configuration in yaml format.
            #[prost(string, repeated, tag = "1")]
            pub gke_yaml_configs: ::prost::alloc::vec::Vec<
                ::prost::alloc::string::String,
            >,
        }
    }
    /// An enum representing the open source category of a PublisherModel.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum OpenSourceCategory {
        /// The open source category is unspecified, which should not be used.
        Unspecified = 0,
        /// Used to indicate the PublisherModel is not open sourced.
        Proprietary = 1,
        /// Used to indicate the PublisherModel is a Google-owned open source model
        /// w/ Google checkpoint.
        GoogleOwnedOssWithGoogleCheckpoint = 2,
        /// Used to indicate the PublisherModel is a 3p-owned open source model w/
        /// Google checkpoint.
        ThirdPartyOwnedOssWithGoogleCheckpoint = 3,
        /// Used to indicate the PublisherModel is a Google-owned pure open source
        /// model.
        GoogleOwnedOss = 4,
        /// Used to indicate the PublisherModel is a 3p-owned pure open source model.
        ThirdPartyOwnedOss = 5,
    }
    impl OpenSourceCategory {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "OPEN_SOURCE_CATEGORY_UNSPECIFIED",
                Self::Proprietary => "PROPRIETARY",
                Self::GoogleOwnedOssWithGoogleCheckpoint => {
                    "GOOGLE_OWNED_OSS_WITH_GOOGLE_CHECKPOINT"
                }
                Self::ThirdPartyOwnedOssWithGoogleCheckpoint => {
                    "THIRD_PARTY_OWNED_OSS_WITH_GOOGLE_CHECKPOINT"
                }
                Self::GoogleOwnedOss => "GOOGLE_OWNED_OSS",
                Self::ThirdPartyOwnedOss => "THIRD_PARTY_OWNED_OSS",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "OPEN_SOURCE_CATEGORY_UNSPECIFIED" => Some(Self::Unspecified),
                "PROPRIETARY" => Some(Self::Proprietary),
                "GOOGLE_OWNED_OSS_WITH_GOOGLE_CHECKPOINT" => {
                    Some(Self::GoogleOwnedOssWithGoogleCheckpoint)
                }
                "THIRD_PARTY_OWNED_OSS_WITH_GOOGLE_CHECKPOINT" => {
                    Some(Self::ThirdPartyOwnedOssWithGoogleCheckpoint)
                }
                "GOOGLE_OWNED_OSS" => Some(Self::GoogleOwnedOss),
                "THIRD_PARTY_OWNED_OSS" => Some(Self::ThirdPartyOwnedOss),
                _ => None,
            }
        }
    }
    /// An enum representing the launch stage of a PublisherModel.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum LaunchStage {
        /// The model launch stage is unspecified.
        Unspecified = 0,
        /// Used to indicate the PublisherModel is at Experimental launch stage,
        /// available to a small set of customers.
        Experimental = 1,
        /// Used to indicate the PublisherModel is at Private Preview launch stage,
        /// only available to a small set of customers, although a larger set of
        /// customers than an Experimental launch. Previews are the first launch
        /// stage used to get feedback from customers.
        PrivatePreview = 2,
        /// Used to indicate the PublisherModel is at Public Preview launch stage,
        /// available to all customers, although not supported for production
        /// workloads.
        PublicPreview = 3,
        /// Used to indicate the PublisherModel is at GA launch stage, available to
        /// all customers and ready for production workload.
        Ga = 4,
    }
    impl LaunchStage {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "LAUNCH_STAGE_UNSPECIFIED",
                Self::Experimental => "EXPERIMENTAL",
                Self::PrivatePreview => "PRIVATE_PREVIEW",
                Self::PublicPreview => "PUBLIC_PREVIEW",
                Self::Ga => "GA",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "LAUNCH_STAGE_UNSPECIFIED" => Some(Self::Unspecified),
                "EXPERIMENTAL" => Some(Self::Experimental),
                "PRIVATE_PREVIEW" => Some(Self::PrivatePreview),
                "PUBLIC_PREVIEW" => Some(Self::PublicPreview),
                "GA" => Some(Self::Ga),
                _ => None,
            }
        }
    }
    /// An enum representing the state of the PublicModelVersion.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum VersionState {
        /// The version state is unspecified.
        Unspecified = 0,
        /// Used to indicate the version is stable.
        Stable = 1,
        /// Used to indicate the version is unstable.
        Unstable = 2,
    }
    impl VersionState {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "VERSION_STATE_UNSPECIFIED",
                Self::Stable => "VERSION_STATE_STABLE",
                Self::Unstable => "VERSION_STATE_UNSTABLE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "VERSION_STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "VERSION_STATE_STABLE" => Some(Self::Stable),
                "VERSION_STATE_UNSTABLE" => Some(Self::Unstable),
                _ => None,
            }
        }
    }
}
/// Request message for
/// [ModelGardenService.GetPublisherModel][google.cloud.aiplatform.v1.ModelGardenService.GetPublisherModel]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetPublisherModelRequest {
    /// Required. The name of the PublisherModel resource.
    /// Format:
    /// `publishers/{publisher}/models/{publisher_model}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The IETF BCP-47 language code representing the language in which
    /// the publisher model's text information should be written in.
    #[prost(string, tag = "2")]
    pub language_code: ::prost::alloc::string::String,
    /// Optional. PublisherModel view specifying which fields to read.
    #[prost(enumeration = "PublisherModelView", tag = "3")]
    pub view: i32,
    /// Optional. Boolean indicates whether the requested model is a Hugging Face
    /// model.
    #[prost(bool, tag = "5")]
    pub is_hugging_face_model: bool,
    /// Optional. Token used to access Hugging Face gated models.
    #[prost(string, tag = "6")]
    pub hugging_face_token: ::prost::alloc::string::String,
}
/// View enumeration of PublisherModel.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum PublisherModelView {
    /// The default / unset value. The API will default to the BASIC view.
    Unspecified = 0,
    /// Include basic metadata about the publisher model, but not the full
    /// contents.
    Basic = 1,
    /// Include everything.
    Full = 2,
    /// Include: VersionId, ModelVersionExternalName, and SupportedActions.
    PublisherModelVersionViewBasic = 3,
}
impl PublisherModelView {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "PUBLISHER_MODEL_VIEW_UNSPECIFIED",
            Self::Basic => "PUBLISHER_MODEL_VIEW_BASIC",
            Self::Full => "PUBLISHER_MODEL_VIEW_FULL",
            Self::PublisherModelVersionViewBasic => "PUBLISHER_MODEL_VERSION_VIEW_BASIC",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "PUBLISHER_MODEL_VIEW_UNSPECIFIED" => Some(Self::Unspecified),
            "PUBLISHER_MODEL_VIEW_BASIC" => Some(Self::Basic),
            "PUBLISHER_MODEL_VIEW_FULL" => Some(Self::Full),
            "PUBLISHER_MODEL_VERSION_VIEW_BASIC" => {
                Some(Self::PublisherModelVersionViewBasic)
            }
            _ => None,
        }
    }
}
/// Generated client implementations.
pub mod model_garden_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// The interface of Model Garden Service.
    #[derive(Debug, Clone)]
    pub struct ModelGardenServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl ModelGardenServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> ModelGardenServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> ModelGardenServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            ModelGardenServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Gets a Model Garden publisher model.
        pub async fn get_publisher_model(
            &mut self,
            request: impl tonic::IntoRequest<super::GetPublisherModelRequest>,
        ) -> std::result::Result<tonic::Response<super::PublisherModel>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelGardenService/GetPublisherModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelGardenService",
                        "GetPublisherModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Request message for
/// [ModelService.UploadModel][google.cloud.aiplatform.v1.ModelService.UploadModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UploadModelRequest {
    /// Required. The resource name of the Location into which to upload the Model.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. The resource name of the model into which to upload the version.
    /// Only specify this field when uploading a new version.
    #[prost(string, tag = "4")]
    pub parent_model: ::prost::alloc::string::String,
    /// Optional. The ID to use for the uploaded Model, which will become the final
    /// component of the model resource name.
    ///
    /// This value may be up to 63 characters, and valid characters are
    /// `\[a-z0-9_-\]`. The first character cannot be a number or hyphen.
    #[prost(string, tag = "5")]
    pub model_id: ::prost::alloc::string::String,
    /// Required. The Model to create.
    #[prost(message, optional, tag = "2")]
    pub model: ::core::option::Option<Model>,
    /// Optional. The user-provided custom service account to use to do the model
    /// upload. If empty, [Vertex AI Service
    /// Agent](<https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents>)
    /// will be used to access resources needed to upload the model. This account
    /// must belong to the target project where the model is uploaded to, i.e., the
    /// project specified in the `parent` field of this request and have necessary
    /// read permissions (to Google Cloud Storage, Artifact Registry, etc.).
    #[prost(string, tag = "6")]
    pub service_account: ::prost::alloc::string::String,
}
/// Details of
/// [ModelService.UploadModel][google.cloud.aiplatform.v1.ModelService.UploadModel]
/// operation.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UploadModelOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Response message of
/// [ModelService.UploadModel][google.cloud.aiplatform.v1.ModelService.UploadModel]
/// operation.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UploadModelResponse {
    /// The name of the uploaded Model resource.
    /// Format: `projects/{project}/locations/{location}/models/{model}`
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Output only. The version ID of the model that is uploaded.
    #[prost(string, tag = "2")]
    pub model_version_id: ::prost::alloc::string::String,
}
/// Request message for
/// [ModelService.GetModel][google.cloud.aiplatform.v1.ModelService.GetModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetModelRequest {
    /// Required. The name of the Model resource.
    /// Format: `projects/{project}/locations/{location}/models/{model}`
    ///
    /// In order to retrieve a specific version of the model, also provide
    /// the version ID or version alias.
    ///    Example: `projects/{project}/locations/{location}/models/{model}@2`
    ///               or
    ///             `projects/{project}/locations/{location}/models/{model}@golden`
    /// If no version ID or alias is specified, the "default" version will be
    /// returned. The "default" version alias is created for the first version of
    /// the model, and can be moved to other versions later on. There will be
    /// exactly one default version.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [ModelService.ListModels][google.cloud.aiplatform.v1.ModelService.ListModels].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsRequest {
    /// Required. The resource name of the Location to list the Models from.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// An expression for filtering the results of the request. For field names
    /// both snake_case and camelCase are supported.
    ///
    ///    * `model` supports = and !=. `model` represents the Model ID,
    ///      i.e. the last segment of the Model's [resource
    ///      name][google.cloud.aiplatform.v1.Model.name].
    ///    * `display_name` supports = and !=
    ///    * `labels` supports general map functions that is:
    ///      * `labels.key=value` - key:value equality
    ///      * `labels.key:* or labels:key - key existence
    ///      * A key including a space must be quoted. `labels."a key"`.
    ///    * `base_model_name` only supports =
    ///
    /// Some examples:
    ///
    ///    * `model=1234`
    ///    * `displayName="myDisplayName"`
    ///    * `labels.myKey="myValue"`
    ///    * `baseModelName="text-bison"`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [ListModelsResponse.next_page_token][google.cloud.aiplatform.v1.ListModelsResponse.next_page_token]
    /// of the previous
    /// [ModelService.ListModels][google.cloud.aiplatform.v1.ModelService.ListModels]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    /// Supported fields:
    ///
    ///    * `display_name`
    ///    * `create_time`
    ///    * `update_time`
    ///
    /// Example: `display_name, create_time desc`.
    #[prost(string, tag = "6")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [ModelService.ListModels][google.cloud.aiplatform.v1.ModelService.ListModels]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsResponse {
    /// List of Models in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub models: ::prost::alloc::vec::Vec<Model>,
    /// A token to retrieve next page of results.
    /// Pass to
    /// [ListModelsRequest.page_token][google.cloud.aiplatform.v1.ListModelsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [ModelService.ListModelVersions][google.cloud.aiplatform.v1.ModelService.ListModelVersions].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelVersionsRequest {
    /// Required. The name of the model to list versions for.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [next_page_token][google.cloud.aiplatform.v1.ListModelVersionsResponse.next_page_token]
    /// of the previous
    /// [ListModelVersions][google.cloud.aiplatform.v1.ModelService.ListModelVersions]
    /// call.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
    /// An expression for filtering the results of the request. For field names
    /// both snake_case and camelCase are supported.
    ///
    ///    * `labels` supports general map functions that is:
    ///      * `labels.key=value` - key:value equality
    ///      * `labels.key:* or labels:key - key existence
    ///      * A key including a space must be quoted. `labels."a key"`.
    ///
    /// Some examples:
    ///
    ///    * `labels.myKey="myValue"`
    #[prost(string, tag = "4")]
    pub filter: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// A comma-separated list of fields to order by, sorted in ascending order.
    /// Use "desc" after a field name for descending.
    /// Supported fields:
    ///
    ///    * `create_time`
    ///    * `update_time`
    ///
    /// Example: `update_time asc, create_time desc`.
    #[prost(string, tag = "6")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [ModelService.ListModelVersions][google.cloud.aiplatform.v1.ModelService.ListModelVersions]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelVersionsResponse {
    /// List of Model versions in the requested page.
    /// In the returned Model name field, version ID instead of regvision tag will
    /// be included.
    #[prost(message, repeated, tag = "1")]
    pub models: ::prost::alloc::vec::Vec<Model>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListModelVersionsRequest.page_token][google.cloud.aiplatform.v1.ListModelVersionsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [ModelService.ListModelVersionCheckpoints][google.cloud.aiplatform.v1.ModelService.ListModelVersionCheckpoints].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelVersionCheckpointsRequest {
    /// Required. The name of the model version to list checkpoints for.
    /// `projects/{project}/locations/{location}/models/{model}@{version}`
    /// Example: `projects/{project}/locations/{location}/models/{model}@2`
    /// or
    /// `projects/{project}/locations/{location}/models/{model}@golden`
    /// If no version ID or alias is specified, the latest version will be
    /// used.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The standard list page size.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// Optional. The standard list page token.
    /// Typically obtained via
    /// [next_page_token][google.cloud.aiplatform.v1.ListModelVersionCheckpointsResponse.next_page_token]
    /// of the previous
    /// [ListModelVersionCheckpoints][google.cloud.aiplatform.v1.ModelService.ListModelVersionCheckpoints]
    /// call.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// A proto representation of a Spanner-stored ModelVersionCheckpoint.
/// The meaning of the fields is equivalent to their in-Spanner counterparts.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ModelVersionCheckpoint {
    /// The ID of the checkpoint.
    #[prost(string, tag = "1")]
    pub checkpoint_id: ::prost::alloc::string::String,
    /// The epoch of the checkpoint.
    #[prost(int64, tag = "2")]
    pub epoch: i64,
    /// The step of the checkpoint.
    #[prost(int64, tag = "3")]
    pub step: i64,
}
/// Response message for
/// [ModelService.ListModelVersionCheckpoints][google.cloud.aiplatform.v1.ModelService.ListModelVersionCheckpoints]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelVersionCheckpointsResponse {
    /// List of Model Version checkpoints.
    #[prost(message, repeated, tag = "1")]
    pub checkpoints: ::prost::alloc::vec::Vec<ModelVersionCheckpoint>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListModelVersionCheckpointsRequest.page_token][google.cloud.aiplatform.v1.ListModelVersionCheckpointsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [ModelService.UpdateModel][google.cloud.aiplatform.v1.ModelService.UpdateModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateModelRequest {
    /// Required. The Model which replaces the resource on the server.
    /// When Model Versioning is enabled, the model.name will be used to determine
    /// whether to update the model or model version.
    /// 1. model.name with the @ value, e.g. models/123@1, refers to a version
    /// specific update.
    /// 2. model.name without the @ value, e.g. models/123, refers to a model
    /// update.
    /// 3. model.name with @-, e.g. models/123@-, refers to a model update.
    /// 4. Supported model fields: display_name, description; supported
    /// version-specific fields: version_description. Labels are supported in both
    /// scenarios. Both the model labels and the version labels are merged when a
    /// model is returned. When updating labels, if the request is for
    /// model-specific update, model label gets updated. Otherwise, version labels
    /// get updated.
    /// 5. A model name or model version name fields update mismatch will cause a
    /// precondition error.
    /// 6. One request cannot update both the model and the version fields. You
    /// must update them separately.
    #[prost(message, optional, tag = "1")]
    pub model: ::core::option::Option<Model>,
    /// Required. The update mask applies to the resource.
    /// For the `FieldMask` definition, see
    /// [google.protobuf.FieldMask][google.protobuf.FieldMask].
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [ModelService.UpdateExplanationDataset][google.cloud.aiplatform.v1.ModelService.UpdateExplanationDataset].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateExplanationDatasetRequest {
    /// Required. The resource name of the Model to update.
    /// Format: `projects/{project}/locations/{location}/models/{model}`
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// The example config containing the location of the dataset.
    #[prost(message, optional, tag = "2")]
    pub examples: ::core::option::Option<Examples>,
}
/// Runtime operation information for
/// [ModelService.UpdateExplanationDataset][google.cloud.aiplatform.v1.ModelService.UpdateExplanationDataset].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateExplanationDatasetOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [ModelService.DeleteModel][google.cloud.aiplatform.v1.ModelService.DeleteModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteModelRequest {
    /// Required. The name of the Model resource to be deleted.
    /// Format: `projects/{project}/locations/{location}/models/{model}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [ModelService.DeleteModelVersion][google.cloud.aiplatform.v1.ModelService.DeleteModelVersion].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteModelVersionRequest {
    /// Required. The name of the model version to be deleted, with a version ID
    /// explicitly included.
    ///
    /// Example: `projects/{project}/locations/{location}/models/{model}@1234`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [ModelService.MergeVersionAliases][google.cloud.aiplatform.v1.ModelService.MergeVersionAliases].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct MergeVersionAliasesRequest {
    /// Required. The name of the model version to merge aliases, with a version ID
    /// explicitly included.
    ///
    /// Example: `projects/{project}/locations/{location}/models/{model}@1234`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The set of version aliases to merge.
    /// The alias should be at most 128 characters, and match
    /// `[a-z][a-zA-Z0-9-]{0,126}\[a-z-0-9\]`.
    /// Add the `-` prefix to an alias means removing that alias from the version.
    /// `-` is NOT counted in the 128 characters. Example: `-golden` means removing
    /// the `golden` alias from the version.
    ///
    /// There is NO ordering in aliases, which means
    /// 1) The aliases returned from GetModel API might not have the exactly same
    /// order from this MergeVersionAliases API. 2) Adding and deleting the same
    /// alias in the request is not recommended, and the 2 operations will be
    /// cancelled out.
    #[prost(string, repeated, tag = "2")]
    pub version_aliases: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Request message for
/// [ModelService.ExportModel][google.cloud.aiplatform.v1.ModelService.ExportModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExportModelRequest {
    /// Required. The resource name of the Model to export.
    /// The resource name may contain version id or version alias to specify the
    /// version, if no version is specified, the default version will be exported.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The desired output location and configuration.
    #[prost(message, optional, tag = "2")]
    pub output_config: ::core::option::Option<export_model_request::OutputConfig>,
}
/// Nested message and enum types in `ExportModelRequest`.
pub mod export_model_request {
    /// Output configuration for the Model export.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct OutputConfig {
        /// The ID of the format in which the Model must be exported. Each Model
        /// lists the [export formats it
        /// supports][google.cloud.aiplatform.v1.Model.supported_export_formats]. If
        /// no value is provided here, then the first from the list of the Model's
        /// supported formats is used by default.
        #[prost(string, tag = "1")]
        pub export_format_id: ::prost::alloc::string::String,
        /// The Cloud Storage location where the Model artifact is to be
        /// written to. Under the directory given as the destination a new one with
        /// name "`model-export-<model-display-name>-<timestamp-of-export-call>`",
        /// where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format,
        /// will be created. Inside, the Model and any of its supporting files
        /// will be written.
        /// This field should only be set when the `exportableContent` field of the
        /// \[Model.supported_export_formats\] object contains `ARTIFACT`.
        #[prost(message, optional, tag = "3")]
        pub artifact_destination: ::core::option::Option<super::GcsDestination>,
        /// The Google Container Registry or Artifact Registry uri where the
        /// Model container image will be copied to.
        /// This field should only be set when the `exportableContent` field of the
        /// \[Model.supported_export_formats\] object contains `IMAGE`.
        #[prost(message, optional, tag = "4")]
        pub image_destination: ::core::option::Option<
            super::ContainerRegistryDestination,
        >,
    }
}
/// Details of
/// [ModelService.ExportModel][google.cloud.aiplatform.v1.ModelService.ExportModel]
/// operation.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExportModelOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// Output only. Information further describing the output of this Model
    /// export.
    #[prost(message, optional, tag = "2")]
    pub output_info: ::core::option::Option<export_model_operation_metadata::OutputInfo>,
}
/// Nested message and enum types in `ExportModelOperationMetadata`.
pub mod export_model_operation_metadata {
    /// Further describes the output of the ExportModel. Supplements
    /// [ExportModelRequest.OutputConfig][google.cloud.aiplatform.v1.ExportModelRequest.OutputConfig].
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct OutputInfo {
        /// Output only. If the Model artifact is being exported to Google Cloud
        /// Storage this is the full path of the directory created, into which the
        /// Model files are being written to.
        #[prost(string, tag = "2")]
        pub artifact_output_uri: ::prost::alloc::string::String,
        /// Output only. If the Model image is being exported to Google Container
        /// Registry or Artifact Registry this is the full path of the image created.
        #[prost(string, tag = "3")]
        pub image_output_uri: ::prost::alloc::string::String,
    }
}
/// Response message of
/// [ModelService.UpdateExplanationDataset][google.cloud.aiplatform.v1.ModelService.UpdateExplanationDataset]
/// operation.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct UpdateExplanationDatasetResponse {}
/// Response message of
/// [ModelService.ExportModel][google.cloud.aiplatform.v1.ModelService.ExportModel]
/// operation.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ExportModelResponse {}
/// Request message for
/// [ModelService.CopyModel][google.cloud.aiplatform.v1.ModelService.CopyModel].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CopyModelRequest {
    /// Required. The resource name of the Location into which to copy the Model.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The resource name of the Model to copy. That Model must be in the
    /// same Project. Format:
    /// `projects/{project}/locations/{location}/models/{model}`
    #[prost(string, tag = "2")]
    pub source_model: ::prost::alloc::string::String,
    /// Customer-managed encryption key options. If this is set,
    /// then the Model copy will be encrypted with the provided encryption key.
    #[prost(message, optional, tag = "3")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// If both fields are unset, a new Model will be created with a generated ID.
    #[prost(oneof = "copy_model_request::DestinationModel", tags = "4, 5")]
    pub destination_model: ::core::option::Option<copy_model_request::DestinationModel>,
}
/// Nested message and enum types in `CopyModelRequest`.
pub mod copy_model_request {
    /// If both fields are unset, a new Model will be created with a generated ID.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum DestinationModel {
        /// Optional. Copy source_model into a new Model with this ID. The ID will
        /// become the final component of the model resource name.
        ///
        /// This value may be up to 63 characters, and valid characters are
        /// `\[a-z0-9_-\]`. The first character cannot be a number or hyphen.
        #[prost(string, tag = "4")]
        ModelId(::prost::alloc::string::String),
        /// Optional. Specify this field to copy source_model into this existing
        /// Model as a new version. Format:
        /// `projects/{project}/locations/{location}/models/{model}`
        #[prost(string, tag = "5")]
        ParentModel(::prost::alloc::string::String),
    }
}
/// Details of
/// [ModelService.CopyModel][google.cloud.aiplatform.v1.ModelService.CopyModel]
/// operation.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CopyModelOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Response message of
/// [ModelService.CopyModel][google.cloud.aiplatform.v1.ModelService.CopyModel]
/// operation.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CopyModelResponse {
    /// The name of the copied Model resource.
    /// Format: `projects/{project}/locations/{location}/models/{model}`
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Output only. The version ID of the model that is copied.
    #[prost(string, tag = "2")]
    pub model_version_id: ::prost::alloc::string::String,
}
/// Request message for
/// [ModelService.ImportModelEvaluation][google.cloud.aiplatform.v1.ModelService.ImportModelEvaluation]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ImportModelEvaluationRequest {
    /// Required. The name of the parent model resource.
    /// Format: `projects/{project}/locations/{location}/models/{model}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. Model evaluation resource to be imported.
    #[prost(message, optional, tag = "2")]
    pub model_evaluation: ::core::option::Option<ModelEvaluation>,
}
/// Request message for
/// [ModelService.BatchImportModelEvaluationSlices][google.cloud.aiplatform.v1.ModelService.BatchImportModelEvaluationSlices]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchImportModelEvaluationSlicesRequest {
    /// Required. The name of the parent ModelEvaluation resource.
    /// Format:
    /// `projects/{project}/locations/{location}/models/{model}/evaluations/{evaluation}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. Model evaluation slice resource to be imported.
    #[prost(message, repeated, tag = "2")]
    pub model_evaluation_slices: ::prost::alloc::vec::Vec<ModelEvaluationSlice>,
}
/// Response message for
/// [ModelService.BatchImportModelEvaluationSlices][google.cloud.aiplatform.v1.ModelService.BatchImportModelEvaluationSlices]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchImportModelEvaluationSlicesResponse {
    /// Output only. List of imported
    /// [ModelEvaluationSlice.name][google.cloud.aiplatform.v1.ModelEvaluationSlice.name].
    #[prost(string, repeated, tag = "1")]
    pub imported_model_evaluation_slices: ::prost::alloc::vec::Vec<
        ::prost::alloc::string::String,
    >,
}
/// Request message for
/// [ModelService.BatchImportEvaluatedAnnotations][google.cloud.aiplatform.v1.ModelService.BatchImportEvaluatedAnnotations]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchImportEvaluatedAnnotationsRequest {
    /// Required. The name of the parent ModelEvaluationSlice resource.
    /// Format:
    /// `projects/{project}/locations/{location}/models/{model}/evaluations/{evaluation}/slices/{slice}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. Evaluated annotations resource to be imported.
    #[prost(message, repeated, tag = "2")]
    pub evaluated_annotations: ::prost::alloc::vec::Vec<EvaluatedAnnotation>,
}
/// Response message for
/// [ModelService.BatchImportEvaluatedAnnotations][google.cloud.aiplatform.v1.ModelService.BatchImportEvaluatedAnnotations]
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct BatchImportEvaluatedAnnotationsResponse {
    /// Output only. Number of EvaluatedAnnotations imported.
    #[prost(int32, tag = "1")]
    pub imported_evaluated_annotations_count: i32,
}
/// Request message for
/// [ModelService.GetModelEvaluation][google.cloud.aiplatform.v1.ModelService.GetModelEvaluation].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetModelEvaluationRequest {
    /// Required. The name of the ModelEvaluation resource.
    /// Format:
    /// `projects/{project}/locations/{location}/models/{model}/evaluations/{evaluation}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [ModelService.ListModelEvaluations][google.cloud.aiplatform.v1.ModelService.ListModelEvaluations].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelEvaluationsRequest {
    /// Required. The resource name of the Model to list the ModelEvaluations from.
    /// Format: `projects/{project}/locations/{location}/models/{model}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [ListModelEvaluationsResponse.next_page_token][google.cloud.aiplatform.v1.ListModelEvaluationsResponse.next_page_token]
    /// of the previous
    /// [ModelService.ListModelEvaluations][google.cloud.aiplatform.v1.ModelService.ListModelEvaluations]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [ModelService.ListModelEvaluations][google.cloud.aiplatform.v1.ModelService.ListModelEvaluations].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelEvaluationsResponse {
    /// List of ModelEvaluations in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub model_evaluations: ::prost::alloc::vec::Vec<ModelEvaluation>,
    /// A token to retrieve next page of results.
    /// Pass to
    /// [ListModelEvaluationsRequest.page_token][google.cloud.aiplatform.v1.ListModelEvaluationsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [ModelService.GetModelEvaluationSlice][google.cloud.aiplatform.v1.ModelService.GetModelEvaluationSlice].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetModelEvaluationSliceRequest {
    /// Required. The name of the ModelEvaluationSlice resource.
    /// Format:
    /// `projects/{project}/locations/{location}/models/{model}/evaluations/{evaluation}/slices/{slice}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [ModelService.ListModelEvaluationSlices][google.cloud.aiplatform.v1.ModelService.ListModelEvaluationSlices].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelEvaluationSlicesRequest {
    /// Required. The resource name of the ModelEvaluation to list the
    /// ModelEvaluationSlices from. Format:
    /// `projects/{project}/locations/{location}/models/{model}/evaluations/{evaluation}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    ///
    ///    * `slice.dimension` - for =.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [ListModelEvaluationSlicesResponse.next_page_token][google.cloud.aiplatform.v1.ListModelEvaluationSlicesResponse.next_page_token]
    /// of the previous
    /// [ModelService.ListModelEvaluationSlices][google.cloud.aiplatform.v1.ModelService.ListModelEvaluationSlices]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [ModelService.ListModelEvaluationSlices][google.cloud.aiplatform.v1.ModelService.ListModelEvaluationSlices].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelEvaluationSlicesResponse {
    /// List of ModelEvaluations in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub model_evaluation_slices: ::prost::alloc::vec::Vec<ModelEvaluationSlice>,
    /// A token to retrieve next page of results.
    /// Pass to
    /// [ListModelEvaluationSlicesRequest.page_token][google.cloud.aiplatform.v1.ListModelEvaluationSlicesRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod model_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for managing Vertex AI's machine learning Models.
    #[derive(Debug, Clone)]
    pub struct ModelServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl ModelServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> ModelServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> ModelServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            ModelServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Uploads a Model artifact into Vertex AI.
        pub async fn upload_model(
            &mut self,
            request: impl tonic::IntoRequest<super::UploadModelRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/UploadModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "UploadModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a Model.
        pub async fn get_model(
            &mut self,
            request: impl tonic::IntoRequest<super::GetModelRequest>,
        ) -> std::result::Result<tonic::Response<super::Model>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/GetModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "GetModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Models in a Location.
        pub async fn list_models(
            &mut self,
            request: impl tonic::IntoRequest<super::ListModelsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListModelsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/ListModels",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "ListModels",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists versions of the specified model.
        pub async fn list_model_versions(
            &mut self,
            request: impl tonic::IntoRequest<super::ListModelVersionsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListModelVersionsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/ListModelVersions",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "ListModelVersions",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists checkpoints of the specified model version.
        pub async fn list_model_version_checkpoints(
            &mut self,
            request: impl tonic::IntoRequest<super::ListModelVersionCheckpointsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListModelVersionCheckpointsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/ListModelVersionCheckpoints",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "ListModelVersionCheckpoints",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a Model.
        pub async fn update_model(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateModelRequest>,
        ) -> std::result::Result<tonic::Response<super::Model>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/UpdateModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "UpdateModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Incrementally update the dataset used for an examples model.
        pub async fn update_explanation_dataset(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateExplanationDatasetRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/UpdateExplanationDataset",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "UpdateExplanationDataset",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a Model.
        ///
        /// A model cannot be deleted if any
        /// [Endpoint][google.cloud.aiplatform.v1.Endpoint] resource has a
        /// [DeployedModel][google.cloud.aiplatform.v1.DeployedModel] based on the
        /// model in its
        /// [deployed_models][google.cloud.aiplatform.v1.Endpoint.deployed_models]
        /// field.
        pub async fn delete_model(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteModelRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/DeleteModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "DeleteModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a Model version.
        ///
        /// Model version can only be deleted if there are no
        /// [DeployedModels][google.cloud.aiplatform.v1.DeployedModel] created from it.
        /// Deleting the only version in the Model is not allowed. Use
        /// [DeleteModel][google.cloud.aiplatform.v1.ModelService.DeleteModel] for
        /// deleting the Model instead.
        pub async fn delete_model_version(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteModelVersionRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/DeleteModelVersion",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "DeleteModelVersion",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Merges a set of aliases for a Model version.
        pub async fn merge_version_aliases(
            &mut self,
            request: impl tonic::IntoRequest<super::MergeVersionAliasesRequest>,
        ) -> std::result::Result<tonic::Response<super::Model>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/MergeVersionAliases",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "MergeVersionAliases",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Exports a trained, exportable Model to a location specified by the
        /// user. A Model is considered to be exportable if it has at least one
        /// [supported export
        /// format][google.cloud.aiplatform.v1.Model.supported_export_formats].
        pub async fn export_model(
            &mut self,
            request: impl tonic::IntoRequest<super::ExportModelRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/ExportModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "ExportModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Copies an already existing Vertex AI Model into the specified Location.
        /// The source Model must exist in the same Project.
        /// When copying custom Models, the users themselves are responsible for
        /// [Model.metadata][google.cloud.aiplatform.v1.Model.metadata] content to be
        /// region-agnostic, as well as making sure that any resources (e.g. files) it
        /// depends on remain accessible.
        pub async fn copy_model(
            &mut self,
            request: impl tonic::IntoRequest<super::CopyModelRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/CopyModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "CopyModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Imports an externally generated ModelEvaluation.
        pub async fn import_model_evaluation(
            &mut self,
            request: impl tonic::IntoRequest<super::ImportModelEvaluationRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ModelEvaluation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/ImportModelEvaluation",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "ImportModelEvaluation",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Imports a list of externally generated ModelEvaluationSlice.
        pub async fn batch_import_model_evaluation_slices(
            &mut self,
            request: impl tonic::IntoRequest<
                super::BatchImportModelEvaluationSlicesRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::BatchImportModelEvaluationSlicesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/BatchImportModelEvaluationSlices",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "BatchImportModelEvaluationSlices",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Imports a list of externally generated EvaluatedAnnotations.
        pub async fn batch_import_evaluated_annotations(
            &mut self,
            request: impl tonic::IntoRequest<
                super::BatchImportEvaluatedAnnotationsRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::BatchImportEvaluatedAnnotationsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/BatchImportEvaluatedAnnotations",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "BatchImportEvaluatedAnnotations",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a ModelEvaluation.
        pub async fn get_model_evaluation(
            &mut self,
            request: impl tonic::IntoRequest<super::GetModelEvaluationRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ModelEvaluation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/GetModelEvaluation",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "GetModelEvaluation",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists ModelEvaluations in a Model.
        pub async fn list_model_evaluations(
            &mut self,
            request: impl tonic::IntoRequest<super::ListModelEvaluationsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListModelEvaluationsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/ListModelEvaluations",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "ListModelEvaluations",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a ModelEvaluationSlice.
        pub async fn get_model_evaluation_slice(
            &mut self,
            request: impl tonic::IntoRequest<super::GetModelEvaluationSliceRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ModelEvaluationSlice>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/GetModelEvaluationSlice",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "GetModelEvaluationSlice",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists ModelEvaluationSlices in a ModelEvaluation.
        pub async fn list_model_evaluation_slices(
            &mut self,
            request: impl tonic::IntoRequest<super::ListModelEvaluationSlicesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListModelEvaluationSlicesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ModelService/ListModelEvaluationSlices",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ModelService",
                        "ListModelEvaluationSlices",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Network spec.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NetworkSpec {
    /// Whether to enable public internet access. Default false.
    #[prost(bool, tag = "1")]
    pub enable_internet_access: bool,
    /// The full name of the Google Compute Engine
    /// [network](<https://cloud.google.com//compute/docs/networks-and-firewalls#networks>)
    #[prost(string, tag = "2")]
    pub network: ::prost::alloc::string::String,
    /// The name of the subnet that this instance is in.
    /// Format:
    /// `projects/{project_id_or_number}/regions/{region}/subnetworks/{subnetwork_id}`
    #[prost(string, tag = "3")]
    pub subnetwork: ::prost::alloc::string::String,
}
/// The euc configuration of NotebookRuntimeTemplate.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct NotebookEucConfig {
    /// Input only. Whether EUC is disabled in this NotebookRuntimeTemplate.
    /// In proto3, the default value of a boolean is false. In this way, by default
    /// EUC will be enabled for NotebookRuntimeTemplate.
    #[prost(bool, tag = "1")]
    pub euc_disabled: bool,
    /// Output only. Whether ActAs check is bypassed for service account attached
    /// to the VM. If false, we need ActAs check for the default Compute Engine
    /// Service account. When a Runtime is created, a VM is allocated using Default
    /// Compute Engine Service Account. Any user requesting to use this Runtime
    /// requires Service Account User (ActAs) permission over this SA. If true,
    /// Runtime owner is using EUC and does not require the above permission as VM
    /// no longer use default Compute Engine SA, but a P4SA.
    #[prost(bool, tag = "2")]
    pub bypass_actas_check: bool,
}
/// NotebookExecutionJob represents an instance of a notebook execution.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NotebookExecutionJob {
    /// Output only. The resource name of this NotebookExecutionJob. Format:
    /// `projects/{project_id}/locations/{location}/notebookExecutionJobs/{job_id}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// The display name of the NotebookExecutionJob. The name can be up to 128
    /// characters long and can consist of any UTF-8 characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Max running time of the execution job in seconds (default 86400s / 24 hrs).
    #[prost(message, optional, tag = "5")]
    pub execution_timeout: ::core::option::Option<::prost_types::Duration>,
    /// Output only. The Schedule resource name if this job is triggered by one.
    /// Format:
    /// `projects/{project_id}/locations/{location}/schedules/{schedule_id}`
    #[prost(string, tag = "6")]
    pub schedule_resource_name: ::prost::alloc::string::String,
    /// Output only. The state of the NotebookExecutionJob.
    #[prost(enumeration = "JobState", tag = "10")]
    pub job_state: i32,
    /// Output only. Populated when the NotebookExecutionJob is completed. When
    /// there is an error during notebook execution, the error details are
    /// populated.
    #[prost(message, optional, tag = "11")]
    pub status: ::core::option::Option<super::super::super::rpc::Status>,
    /// Output only. Timestamp when this NotebookExecutionJob was created.
    #[prost(message, optional, tag = "12")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this NotebookExecutionJob was most recently
    /// updated.
    #[prost(message, optional, tag = "13")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// The labels with user-defined metadata to organize NotebookExecutionJobs.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable.
    #[prost(map = "string, string", tag = "19")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// The name of the kernel to use during notebook execution. If unset, the
    /// default kernel is used.
    #[prost(string, tag = "20")]
    pub kernel_name: ::prost::alloc::string::String,
    /// Customer-managed encryption key spec for the notebook execution job.
    /// This field is auto-populated if the
    /// [NotebookRuntimeTemplate][google.cloud.aiplatform.v1.NotebookRuntimeTemplate]
    /// has an encryption spec.
    #[prost(message, optional, tag = "22")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// The input notebook.
    #[prost(oneof = "notebook_execution_job::NotebookSource", tags = "3, 4, 17")]
    pub notebook_source: ::core::option::Option<notebook_execution_job::NotebookSource>,
    /// The compute config to use for an execution job.
    #[prost(oneof = "notebook_execution_job::EnvironmentSpec", tags = "14, 16")]
    pub environment_spec: ::core::option::Option<
        notebook_execution_job::EnvironmentSpec,
    >,
    /// The location to store the notebook execution result.
    #[prost(oneof = "notebook_execution_job::ExecutionSink", tags = "8")]
    pub execution_sink: ::core::option::Option<notebook_execution_job::ExecutionSink>,
    /// The identity to run the execution as.
    #[prost(oneof = "notebook_execution_job::ExecutionIdentity", tags = "9, 18")]
    pub execution_identity: ::core::option::Option<
        notebook_execution_job::ExecutionIdentity,
    >,
    /// Runtime environment for the notebook execution job. If unspecified, the
    /// default runtime of Colab is used.
    #[prost(oneof = "notebook_execution_job::RuntimeEnvironment", tags = "23")]
    pub runtime_environment: ::core::option::Option<
        notebook_execution_job::RuntimeEnvironment,
    >,
}
/// Nested message and enum types in `NotebookExecutionJob`.
pub mod notebook_execution_job {
    /// The Dataform Repository containing the input notebook.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct DataformRepositorySource {
        /// The resource name of the Dataform Repository. Format:
        /// `projects/{project_id}/locations/{location}/repositories/{repository_id}`
        #[prost(string, tag = "1")]
        pub dataform_repository_resource_name: ::prost::alloc::string::String,
        /// The commit SHA to read repository with. If unset, the file will be read
        /// at HEAD.
        #[prost(string, tag = "2")]
        pub commit_sha: ::prost::alloc::string::String,
    }
    /// The Cloud Storage uri for the input notebook.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct GcsNotebookSource {
        /// The Cloud Storage uri pointing to the ipynb file. Format:
        /// `gs://bucket/notebook_file.ipynb`
        #[prost(string, tag = "1")]
        pub uri: ::prost::alloc::string::String,
        /// The version of the Cloud Storage object to read. If unset, the current
        /// version of the object is read. See
        /// <https://cloud.google.com/storage/docs/metadata#generation-number.>
        #[prost(string, tag = "2")]
        pub generation: ::prost::alloc::string::String,
    }
    /// The content of the input notebook in ipynb format.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct DirectNotebookSource {
        /// The base64-encoded contents of the input notebook file.
        #[prost(bytes = "vec", tag = "1")]
        pub content: ::prost::alloc::vec::Vec<u8>,
    }
    /// Compute configuration to use for an execution job.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct CustomEnvironmentSpec {
        /// The specification of a single machine for the execution job.
        #[prost(message, optional, tag = "1")]
        pub machine_spec: ::core::option::Option<super::MachineSpec>,
        /// The specification of a persistent disk to attach for the execution job.
        #[prost(message, optional, tag = "2")]
        pub persistent_disk_spec: ::core::option::Option<super::PersistentDiskSpec>,
        /// The network configuration to use for the execution job.
        #[prost(message, optional, tag = "3")]
        pub network_spec: ::core::option::Option<super::NetworkSpec>,
    }
    /// Configuration for a Workbench Instances-based environment.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct WorkbenchRuntime {}
    /// The input notebook.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum NotebookSource {
        /// The Dataform Repository pointing to a single file notebook repository.
        #[prost(message, tag = "3")]
        DataformRepositorySource(DataformRepositorySource),
        /// The Cloud Storage url pointing to the ipynb file. Format:
        /// `gs://bucket/notebook_file.ipynb`
        #[prost(message, tag = "4")]
        GcsNotebookSource(GcsNotebookSource),
        /// The contents of an input notebook file.
        #[prost(message, tag = "17")]
        DirectNotebookSource(DirectNotebookSource),
    }
    /// The compute config to use for an execution job.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum EnvironmentSpec {
        /// The NotebookRuntimeTemplate to source compute configuration from.
        #[prost(string, tag = "14")]
        NotebookRuntimeTemplateResourceName(::prost::alloc::string::String),
        /// The custom compute configuration for an execution job.
        #[prost(message, tag = "16")]
        CustomEnvironmentSpec(CustomEnvironmentSpec),
    }
    /// The location to store the notebook execution result.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum ExecutionSink {
        /// The Cloud Storage location to upload the result to. Format:
        /// `gs://bucket-name`
        #[prost(string, tag = "8")]
        GcsOutputUri(::prost::alloc::string::String),
    }
    /// The identity to run the execution as.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum ExecutionIdentity {
        /// The user email to run the execution as. Only supported by Colab runtimes.
        #[prost(string, tag = "9")]
        ExecutionUser(::prost::alloc::string::String),
        /// The service account to run the execution as.
        #[prost(string, tag = "18")]
        ServiceAccount(::prost::alloc::string::String),
    }
    /// Runtime environment for the notebook execution job. If unspecified, the
    /// default runtime of Colab is used.
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum RuntimeEnvironment {
        /// The Workbench runtime configuration to use for the notebook execution.
        #[prost(message, tag = "23")]
        WorkbenchRuntime(WorkbenchRuntime),
    }
}
/// The idle shutdown configuration of NotebookRuntimeTemplate, which contains
/// the idle_timeout as required field.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct NotebookIdleShutdownConfig {
    /// Required. Duration is accurate to the second. In Notebook, Idle Timeout is
    /// accurate to minute so the range of idle_timeout (second) is: 10 * 60 ~ 1440
    /// * 60.
    #[prost(message, optional, tag = "1")]
    pub idle_timeout: ::core::option::Option<::prost_types::Duration>,
    /// Whether Idle Shutdown is disabled in this NotebookRuntimeTemplate.
    #[prost(bool, tag = "2")]
    pub idle_shutdown_disabled: bool,
}
/// Points to a NotebookRuntimeTemplateRef.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NotebookRuntimeTemplateRef {
    /// Immutable. A resource name of the NotebookRuntimeTemplate.
    #[prost(string, tag = "1")]
    pub notebook_runtime_template: ::prost::alloc::string::String,
}
/// A template that specifies runtime configurations such as machine type,
/// runtime version, network configurations, etc.
/// Multiple runtimes can be created from a runtime template.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NotebookRuntimeTemplate {
    /// The resource name of the NotebookRuntimeTemplate.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The display name of the NotebookRuntimeTemplate.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// The description of the NotebookRuntimeTemplate.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// Output only. Deprecated: This field has no behavior. Use
    /// notebook_runtime_type = 'ONE_CLICK' instead.
    ///
    /// The default template to use if not specified.
    #[deprecated]
    #[prost(bool, tag = "4")]
    pub is_default: bool,
    /// Optional. Immutable. The specification of a single machine for the
    /// template.
    #[prost(message, optional, tag = "5")]
    pub machine_spec: ::core::option::Option<MachineSpec>,
    /// Optional. The specification of [persistent
    /// disk][<https://cloud.google.com/compute/docs/disks/persistent-disks]>
    /// attached to the runtime as data disk storage.
    #[prost(message, optional, tag = "8")]
    pub data_persistent_disk_spec: ::core::option::Option<PersistentDiskSpec>,
    /// Optional. Network spec.
    #[prost(message, optional, tag = "12")]
    pub network_spec: ::core::option::Option<NetworkSpec>,
    /// Deprecated: This field is ignored and the "Vertex AI Notebook Service
    /// Account"
    /// (service-PROJECT_NUMBER@gcp-sa-aiplatform-vm.iam.gserviceaccount.com) is
    /// used for the runtime workload identity.
    /// See
    /// <https://cloud.google.com/iam/docs/service-agents#vertex-ai-notebook-service-account>
    /// for more details.
    /// For NotebookExecutionJob, use NotebookExecutionJob.service_account instead.
    ///
    /// The service account that the runtime workload runs as.
    /// You can use any service account within the same project, but you
    /// must have the service account user permission to use the instance.
    ///
    /// If not specified, the [Compute Engine default service
    /// account](<https://cloud.google.com/compute/docs/access/service-accounts#default_service_account>)
    /// is used.
    #[deprecated]
    #[prost(string, tag = "13")]
    pub service_account: ::prost::alloc::string::String,
    /// Used to perform consistent read-modify-write updates. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "14")]
    pub etag: ::prost::alloc::string::String,
    /// The labels with user-defined metadata to organize the
    /// NotebookRuntimeTemplates.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "15")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// The idle shutdown configuration of NotebookRuntimeTemplate. This config
    /// will only be set when idle shutdown is enabled.
    #[prost(message, optional, tag = "17")]
    pub idle_shutdown_config: ::core::option::Option<NotebookIdleShutdownConfig>,
    /// EUC configuration of the NotebookRuntimeTemplate.
    #[prost(message, optional, tag = "18")]
    pub euc_config: ::core::option::Option<NotebookEucConfig>,
    /// Output only. Timestamp when this NotebookRuntimeTemplate was created.
    #[prost(message, optional, tag = "10")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this NotebookRuntimeTemplate was most recently
    /// updated.
    #[prost(message, optional, tag = "11")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. Immutable. The type of the notebook runtime template.
    #[prost(enumeration = "NotebookRuntimeType", tag = "19")]
    pub notebook_runtime_type: i32,
    /// Optional. Immutable. Runtime Shielded VM spec.
    #[prost(message, optional, tag = "20")]
    pub shielded_vm_config: ::core::option::Option<ShieldedVmConfig>,
    /// Optional. The Compute Engine tags to add to runtime (see [Tagging
    /// instances](<https://cloud.google.com/vpc/docs/add-remove-network-tags>)).
    #[prost(string, repeated, tag = "21")]
    pub network_tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Customer-managed encryption key spec for the notebook runtime.
    #[prost(message, optional, tag = "23")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
}
/// A runtime is a virtual machine allocated to a particular user for a
/// particular Notebook file on temporary basis with lifetime limited to 24
/// hours.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct NotebookRuntime {
    /// Output only. The resource name of the NotebookRuntime.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The user email of the NotebookRuntime.
    #[prost(string, tag = "2")]
    pub runtime_user: ::prost::alloc::string::String,
    /// Output only. The pointer to NotebookRuntimeTemplate this NotebookRuntime is
    /// created from.
    #[prost(message, optional, tag = "3")]
    pub notebook_runtime_template_ref: ::core::option::Option<
        NotebookRuntimeTemplateRef,
    >,
    /// Output only. The proxy endpoint used to access the NotebookRuntime.
    #[prost(string, tag = "5")]
    pub proxy_uri: ::prost::alloc::string::String,
    /// Output only. Timestamp when this NotebookRuntime was created.
    #[prost(message, optional, tag = "6")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this NotebookRuntime was most recently updated.
    #[prost(message, optional, tag = "7")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The health state of the NotebookRuntime.
    #[prost(enumeration = "notebook_runtime::HealthState", tag = "8")]
    pub health_state: i32,
    /// Required. The display name of the NotebookRuntime.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "10")]
    pub display_name: ::prost::alloc::string::String,
    /// The description of the NotebookRuntime.
    #[prost(string, tag = "11")]
    pub description: ::prost::alloc::string::String,
    /// Output only. Deprecated: This field is no longer used and the "Vertex AI
    /// Notebook Service Account"
    /// (service-PROJECT_NUMBER@gcp-sa-aiplatform-vm.iam.gserviceaccount.com) is
    /// used for the runtime workload identity.
    /// See
    /// <https://cloud.google.com/iam/docs/service-agents#vertex-ai-notebook-service-account>
    /// for more details.
    ///
    /// The service account that the NotebookRuntime workload runs as.
    #[prost(string, tag = "13")]
    pub service_account: ::prost::alloc::string::String,
    /// Output only. The runtime (instance) state of the NotebookRuntime.
    #[prost(enumeration = "notebook_runtime::RuntimeState", tag = "14")]
    pub runtime_state: i32,
    /// Output only. Whether NotebookRuntime is upgradable.
    #[prost(bool, tag = "15")]
    pub is_upgradable: bool,
    /// The labels with user-defined metadata to organize your
    /// NotebookRuntime.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// No more than 64 user labels can be associated with one NotebookRuntime
    /// (System labels are excluded).
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable. Following system labels exist for NotebookRuntime:
    ///
    /// * "aiplatform.googleapis.com/notebook_runtime_gce_instance_id": output
    /// only, its value is the Compute Engine instance id.
    /// * "aiplatform.googleapis.com/colab_enterprise_entry_service": its value is
    /// either "bigquery" or "vertex"; if absent, it should be "vertex". This is to
    /// describe the entry service, either BigQuery or Vertex.
    #[prost(map = "string, string", tag = "16")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Output only. Timestamp when this NotebookRuntime will be expired:
    /// 1. System Predefined NotebookRuntime: 24 hours after creation. After
    /// expiration, system predifined runtime will be deleted.
    /// 2. User created NotebookRuntime: 6 months after last upgrade. After
    /// expiration, user created runtime will be stopped and allowed for upgrade.
    #[prost(message, optional, tag = "17")]
    pub expiration_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The VM os image version of NotebookRuntime.
    #[prost(string, tag = "18")]
    pub version: ::prost::alloc::string::String,
    /// Output only. The type of the notebook runtime.
    #[prost(enumeration = "NotebookRuntimeType", tag = "19")]
    pub notebook_runtime_type: i32,
    /// Output only. The specification of a single machine used by the notebook
    /// runtime.
    #[prost(message, optional, tag = "20")]
    pub machine_spec: ::core::option::Option<MachineSpec>,
    /// Output only. The specification of [persistent
    /// disk][<https://cloud.google.com/compute/docs/disks/persistent-disks]>
    /// attached to the notebook runtime as data disk storage.
    #[prost(message, optional, tag = "21")]
    pub data_persistent_disk_spec: ::core::option::Option<PersistentDiskSpec>,
    /// Output only. Network spec of the notebook runtime.
    #[prost(message, optional, tag = "22")]
    pub network_spec: ::core::option::Option<NetworkSpec>,
    /// Output only. The idle shutdown configuration of the notebook runtime.
    #[prost(message, optional, tag = "23")]
    pub idle_shutdown_config: ::core::option::Option<NotebookIdleShutdownConfig>,
    /// Output only. EUC configuration of the notebook runtime.
    #[prost(message, optional, tag = "24")]
    pub euc_config: ::core::option::Option<NotebookEucConfig>,
    /// Output only. Runtime Shielded VM spec.
    #[prost(message, optional, tag = "32")]
    pub shielded_vm_config: ::core::option::Option<ShieldedVmConfig>,
    /// Optional. The Compute Engine tags to add to runtime (see [Tagging
    /// instances](<https://cloud.google.com/vpc/docs/add-remove-network-tags>)).
    #[prost(string, repeated, tag = "25")]
    pub network_tags: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Output only. Customer-managed encryption key spec for the notebook runtime.
    #[prost(message, optional, tag = "28")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "29")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "30")]
    pub satisfies_pzi: bool,
}
/// Nested message and enum types in `NotebookRuntime`.
pub mod notebook_runtime {
    /// The substate of the NotebookRuntime to display health information.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum HealthState {
        /// Unspecified health state.
        Unspecified = 0,
        /// NotebookRuntime is in healthy state. Applies to ACTIVE state.
        Healthy = 1,
        /// NotebookRuntime is in unhealthy state. Applies to ACTIVE state.
        Unhealthy = 2,
    }
    impl HealthState {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "HEALTH_STATE_UNSPECIFIED",
                Self::Healthy => "HEALTHY",
                Self::Unhealthy => "UNHEALTHY",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "HEALTH_STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "HEALTHY" => Some(Self::Healthy),
                "UNHEALTHY" => Some(Self::Unhealthy),
                _ => None,
            }
        }
    }
    /// The substate of the NotebookRuntime to display state of runtime.
    /// The resource of NotebookRuntime is in ACTIVE state for these sub state.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum RuntimeState {
        /// Unspecified runtime state.
        Unspecified = 0,
        /// NotebookRuntime is in running state.
        Running = 1,
        /// NotebookRuntime is in starting state.
        BeingStarted = 2,
        /// NotebookRuntime is in stopping state.
        BeingStopped = 3,
        /// NotebookRuntime is in stopped state.
        Stopped = 4,
        /// NotebookRuntime is in upgrading state. It is in the middle of upgrading
        /// process.
        BeingUpgraded = 5,
        /// NotebookRuntime was unable to start/stop properly.
        Error = 100,
        /// NotebookRuntime is in invalid state. Cannot be recovered.
        Invalid = 101,
    }
    impl RuntimeState {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "RUNTIME_STATE_UNSPECIFIED",
                Self::Running => "RUNNING",
                Self::BeingStarted => "BEING_STARTED",
                Self::BeingStopped => "BEING_STOPPED",
                Self::Stopped => "STOPPED",
                Self::BeingUpgraded => "BEING_UPGRADED",
                Self::Error => "ERROR",
                Self::Invalid => "INVALID",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "RUNTIME_STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "RUNNING" => Some(Self::Running),
                "BEING_STARTED" => Some(Self::BeingStarted),
                "BEING_STOPPED" => Some(Self::BeingStopped),
                "STOPPED" => Some(Self::Stopped),
                "BEING_UPGRADED" => Some(Self::BeingUpgraded),
                "ERROR" => Some(Self::Error),
                "INVALID" => Some(Self::Invalid),
                _ => None,
            }
        }
    }
}
/// Represents a notebook runtime type.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum NotebookRuntimeType {
    /// Unspecified notebook runtime type, NotebookRuntimeType will default to
    /// USER_DEFINED.
    Unspecified = 0,
    /// runtime or template with coustomized configurations from user.
    UserDefined = 1,
    /// runtime or template with system defined configurations.
    OneClick = 2,
}
impl NotebookRuntimeType {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "NOTEBOOK_RUNTIME_TYPE_UNSPECIFIED",
            Self::UserDefined => "USER_DEFINED",
            Self::OneClick => "ONE_CLICK",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "NOTEBOOK_RUNTIME_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
            "USER_DEFINED" => Some(Self::UserDefined),
            "ONE_CLICK" => Some(Self::OneClick),
            _ => None,
        }
    }
}
/// Request message for
/// [NotebookService.CreateNotebookRuntimeTemplate][google.cloud.aiplatform.v1.NotebookService.CreateNotebookRuntimeTemplate].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateNotebookRuntimeTemplateRequest {
    /// Required. The resource name of the Location to create the
    /// NotebookRuntimeTemplate. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The NotebookRuntimeTemplate to create.
    #[prost(message, optional, tag = "2")]
    pub notebook_runtime_template: ::core::option::Option<NotebookRuntimeTemplate>,
    /// Optional. User specified ID for the notebook runtime template.
    #[prost(string, tag = "3")]
    pub notebook_runtime_template_id: ::prost::alloc::string::String,
}
/// Metadata information for
/// [NotebookService.CreateNotebookRuntimeTemplate][google.cloud.aiplatform.v1.NotebookService.CreateNotebookRuntimeTemplate].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateNotebookRuntimeTemplateOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [NotebookService.GetNotebookRuntimeTemplate][google.cloud.aiplatform.v1.NotebookService.GetNotebookRuntimeTemplate]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetNotebookRuntimeTemplateRequest {
    /// Required. The name of the NotebookRuntimeTemplate resource.
    /// Format:
    /// `projects/{project}/locations/{location}/notebookRuntimeTemplates/{notebook_runtime_template}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [NotebookService.ListNotebookRuntimeTemplates][google.cloud.aiplatform.v1.NotebookService.ListNotebookRuntimeTemplates].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListNotebookRuntimeTemplatesRequest {
    /// Required. The resource name of the Location from which to list the
    /// NotebookRuntimeTemplates.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. An expression for filtering the results of the request. For field
    /// names both snake_case and camelCase are supported.
    ///
    ///    * `notebookRuntimeTemplate` supports = and !=. `notebookRuntimeTemplate`
    ///      represents the NotebookRuntimeTemplate ID,
    ///      i.e. the last segment of the NotebookRuntimeTemplate's \[resource name\]
    ///      \[google.cloud.aiplatform.v1.NotebookRuntimeTemplate.name\].
    ///    * `display_name` supports = and !=
    ///    * `labels` supports general map functions that is:
    ///      * `labels.key=value` - key:value equality
    ///      * `labels.key:* or labels:key - key existence
    ///      * A key including a space must be quoted. `labels."a key"`.
    ///    * `notebookRuntimeType` supports = and !=. notebookRuntimeType enum:
    ///    \[USER_DEFINED, ONE_CLICK\].
    ///
    /// Some examples:
    ///
    ///    * `notebookRuntimeTemplate=notebookRuntimeTemplate123`
    ///    * `displayName="myDisplayName"`
    ///    * `labels.myKey="myValue"`
    ///    * `notebookRuntimeType=USER_DEFINED`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// Optional. The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// Optional. The standard list page token.
    /// Typically obtained via
    /// [ListNotebookRuntimeTemplatesResponse.next_page_token][google.cloud.aiplatform.v1.ListNotebookRuntimeTemplatesResponse.next_page_token]
    /// of the previous
    /// [NotebookService.ListNotebookRuntimeTemplates][google.cloud.aiplatform.v1.NotebookService.ListNotebookRuntimeTemplates]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Optional. Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// Optional. A comma-separated list of fields to order by, sorted in ascending
    /// order. Use "desc" after a field name for descending. Supported fields:
    ///
    ///    * `display_name`
    ///    * `create_time`
    ///    * `update_time`
    ///
    /// Example: `display_name, create_time desc`.
    #[prost(string, tag = "6")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [NotebookService.ListNotebookRuntimeTemplates][google.cloud.aiplatform.v1.NotebookService.ListNotebookRuntimeTemplates].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListNotebookRuntimeTemplatesResponse {
    /// List of NotebookRuntimeTemplates in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub notebook_runtime_templates: ::prost::alloc::vec::Vec<NotebookRuntimeTemplate>,
    /// A token to retrieve next page of results.
    /// Pass to
    /// [ListNotebookRuntimeTemplatesRequest.page_token][google.cloud.aiplatform.v1.ListNotebookRuntimeTemplatesRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [NotebookService.DeleteNotebookRuntimeTemplate][google.cloud.aiplatform.v1.NotebookService.DeleteNotebookRuntimeTemplate].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteNotebookRuntimeTemplateRequest {
    /// Required. The name of the NotebookRuntimeTemplate resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/notebookRuntimeTemplates/{notebook_runtime_template}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [NotebookService.UpdateNotebookRuntimeTemplate][google.cloud.aiplatform.v1.NotebookService.UpdateNotebookRuntimeTemplate].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateNotebookRuntimeTemplateRequest {
    /// Required. The NotebookRuntimeTemplate to update.
    #[prost(message, optional, tag = "1")]
    pub notebook_runtime_template: ::core::option::Option<NotebookRuntimeTemplate>,
    /// Required. The update mask applies to the resource.
    /// For the `FieldMask` definition, see
    /// [google.protobuf.FieldMask][google.protobuf.FieldMask]. Input format:
    /// `{paths: "${updated_filed}"}` Updatable fields:
    ///
    ///    * `encryption_spec.kms_key_name`
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [NotebookService.AssignNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.AssignNotebookRuntime].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AssignNotebookRuntimeRequest {
    /// Required. The resource name of the Location to get the NotebookRuntime
    /// assignment. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The resource name of the NotebookRuntimeTemplate based on which a
    /// NotebookRuntime will be assigned (reuse or create a new one).
    #[prost(string, tag = "2")]
    pub notebook_runtime_template: ::prost::alloc::string::String,
    /// Required. Provide runtime specific information (e.g. runtime owner,
    /// notebook id) used for NotebookRuntime assignment.
    #[prost(message, optional, tag = "3")]
    pub notebook_runtime: ::core::option::Option<NotebookRuntime>,
    /// Optional. User specified ID for the notebook runtime.
    #[prost(string, tag = "4")]
    pub notebook_runtime_id: ::prost::alloc::string::String,
}
/// Metadata information for
/// [NotebookService.AssignNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.AssignNotebookRuntime].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AssignNotebookRuntimeOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// A human-readable message that shows the intermediate progress details of
    /// NotebookRuntime.
    #[prost(string, tag = "2")]
    pub progress_message: ::prost::alloc::string::String,
}
/// Request message for
/// [NotebookService.GetNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.GetNotebookRuntime]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetNotebookRuntimeRequest {
    /// Required. The name of the NotebookRuntime resource.
    /// Instead of checking whether the name is in valid NotebookRuntime resource
    /// name format, directly throw NotFound exception if there is no such
    /// NotebookRuntime in spanner.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [NotebookService.ListNotebookRuntimes][google.cloud.aiplatform.v1.NotebookService.ListNotebookRuntimes].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListNotebookRuntimesRequest {
    /// Required. The resource name of the Location from which to list the
    /// NotebookRuntimes.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. An expression for filtering the results of the request. For field
    /// names both snake_case and camelCase are supported.
    ///
    ///    * `notebookRuntime` supports = and !=. `notebookRuntime` represents the
    ///      NotebookRuntime ID,
    ///      i.e. the last segment of the NotebookRuntime's \[resource name\]
    ///      \[google.cloud.aiplatform.v1.NotebookRuntime.name\].
    ///    * `displayName` supports = and != and regex.
    ///    * `notebookRuntimeTemplate` supports = and !=. `notebookRuntimeTemplate`
    ///      represents the NotebookRuntimeTemplate ID,
    ///      i.e. the last segment of the NotebookRuntimeTemplate's \[resource name\]
    ///      \[google.cloud.aiplatform.v1.NotebookRuntimeTemplate.name\].
    ///    * `healthState` supports = and !=. healthState enum: [HEALTHY, UNHEALTHY,
    ///    HEALTH_STATE_UNSPECIFIED].
    ///    * `runtimeState` supports = and !=. runtimeState enum:
    ///    [RUNTIME_STATE_UNSPECIFIED, RUNNING, BEING_STARTED, BEING_STOPPED,
    ///    STOPPED, BEING_UPGRADED, ERROR, INVALID].
    ///    * `runtimeUser` supports = and !=.
    ///    * API version is UI only: `uiState` supports = and !=. uiState enum:
    ///    [UI_RESOURCE_STATE_UNSPECIFIED, UI_RESOURCE_STATE_BEING_CREATED,
    ///    UI_RESOURCE_STATE_ACTIVE, UI_RESOURCE_STATE_BEING_DELETED,
    ///    UI_RESOURCE_STATE_CREATION_FAILED].
    ///    * `notebookRuntimeType` supports = and !=. notebookRuntimeType enum:
    ///    \[USER_DEFINED, ONE_CLICK\].
    ///
    /// Some examples:
    ///
    ///    * `notebookRuntime="notebookRuntime123"`
    ///    * `displayName="myDisplayName"` and `displayName=~"myDisplayNameRegex"`
    ///    * `notebookRuntimeTemplate="notebookRuntimeTemplate321"`
    ///    * `healthState=HEALTHY`
    ///    * `runtimeState=RUNNING`
    ///    * `runtimeUser="test@google.com"`
    ///    * `uiState=UI_RESOURCE_STATE_BEING_DELETED`
    ///    * `notebookRuntimeType=USER_DEFINED`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// Optional. The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// Optional. The standard list page token.
    /// Typically obtained via
    /// [ListNotebookRuntimesResponse.next_page_token][google.cloud.aiplatform.v1.ListNotebookRuntimesResponse.next_page_token]
    /// of the previous
    /// [NotebookService.ListNotebookRuntimes][google.cloud.aiplatform.v1.NotebookService.ListNotebookRuntimes]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Optional. Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// Optional. A comma-separated list of fields to order by, sorted in ascending
    /// order. Use "desc" after a field name for descending. Supported fields:
    ///
    ///    * `display_name`
    ///    * `create_time`
    ///    * `update_time`
    ///
    /// Example: `display_name, create_time desc`.
    #[prost(string, tag = "6")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [NotebookService.ListNotebookRuntimes][google.cloud.aiplatform.v1.NotebookService.ListNotebookRuntimes].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListNotebookRuntimesResponse {
    /// List of NotebookRuntimes in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub notebook_runtimes: ::prost::alloc::vec::Vec<NotebookRuntime>,
    /// A token to retrieve next page of results.
    /// Pass to
    /// [ListNotebookRuntimesRequest.page_token][google.cloud.aiplatform.v1.ListNotebookRuntimesRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [NotebookService.DeleteNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.DeleteNotebookRuntime].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteNotebookRuntimeRequest {
    /// Required. The name of the NotebookRuntime resource to be deleted.
    /// Instead of checking whether the name is in valid NotebookRuntime resource
    /// name format, directly throw NotFound exception if there is no such
    /// NotebookRuntime in spanner.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [NotebookService.UpgradeNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.UpgradeNotebookRuntime].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpgradeNotebookRuntimeRequest {
    /// Required. The name of the NotebookRuntime resource to be upgrade.
    /// Instead of checking whether the name is in valid NotebookRuntime resource
    /// name format, directly throw NotFound exception if there is no such
    /// NotebookRuntime in spanner.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Metadata information for
/// [NotebookService.UpgradeNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.UpgradeNotebookRuntime].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpgradeNotebookRuntimeOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// A human-readable message that shows the intermediate progress details of
    /// NotebookRuntime.
    #[prost(string, tag = "2")]
    pub progress_message: ::prost::alloc::string::String,
}
/// Response message for
/// [NotebookService.UpgradeNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.UpgradeNotebookRuntime].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct UpgradeNotebookRuntimeResponse {}
/// Request message for
/// [NotebookService.StartNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.StartNotebookRuntime].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StartNotebookRuntimeRequest {
    /// Required. The name of the NotebookRuntime resource to be started.
    /// Instead of checking whether the name is in valid NotebookRuntime resource
    /// name format, directly throw NotFound exception if there is no such
    /// NotebookRuntime in spanner.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Metadata information for
/// [NotebookService.StartNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.StartNotebookRuntime].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StartNotebookRuntimeOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// A human-readable message that shows the intermediate progress details of
    /// NotebookRuntime.
    #[prost(string, tag = "2")]
    pub progress_message: ::prost::alloc::string::String,
}
/// Response message for
/// [NotebookService.StartNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.StartNotebookRuntime].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct StartNotebookRuntimeResponse {}
/// Request message for
/// [NotebookService.StopNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.StopNotebookRuntime].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StopNotebookRuntimeRequest {
    /// Required. The name of the NotebookRuntime resource to be stopped.
    /// Instead of checking whether the name is in valid NotebookRuntime resource
    /// name format, directly throw NotFound exception if there is no such
    /// NotebookRuntime in spanner.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Metadata information for
/// [NotebookService.StopNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.StopNotebookRuntime].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StopNotebookRuntimeOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Response message for
/// [NotebookService.StopNotebookRuntime][google.cloud.aiplatform.v1.NotebookService.StopNotebookRuntime].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct StopNotebookRuntimeResponse {}
/// Request message for \[NotebookService.CreateNotebookExecutionJob\]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateNotebookExecutionJobRequest {
    /// Required. The resource name of the Location to create the
    /// NotebookExecutionJob. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The NotebookExecutionJob to create.
    #[prost(message, optional, tag = "2")]
    pub notebook_execution_job: ::core::option::Option<NotebookExecutionJob>,
    /// Optional. User specified ID for the NotebookExecutionJob.
    #[prost(string, tag = "3")]
    pub notebook_execution_job_id: ::prost::alloc::string::String,
}
/// Metadata information for
/// [NotebookService.CreateNotebookExecutionJob][google.cloud.aiplatform.v1.NotebookService.CreateNotebookExecutionJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateNotebookExecutionJobOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// A human-readable message that shows the intermediate progress details of
    /// NotebookRuntime.
    #[prost(string, tag = "2")]
    pub progress_message: ::prost::alloc::string::String,
}
/// Request message for \[NotebookService.GetNotebookExecutionJob\]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetNotebookExecutionJobRequest {
    /// Required. The name of the NotebookExecutionJob resource.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The NotebookExecutionJob view. Defaults to BASIC.
    #[prost(enumeration = "NotebookExecutionJobView", tag = "6")]
    pub view: i32,
}
/// Request message for \[NotebookService.ListNotebookExecutionJobs\]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListNotebookExecutionJobsRequest {
    /// Required. The resource name of the Location from which to list the
    /// NotebookExecutionJobs.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. An expression for filtering the results of the request. For field
    /// names both snake_case and camelCase are supported.
    ///
    ///    * `notebookExecutionJob` supports = and !=. `notebookExecutionJob`
    ///    represents the NotebookExecutionJob ID.
    ///    * `displayName` supports = and != and regex.
    ///    * `schedule` supports = and != and regex.
    ///
    /// Some examples:
    ///    * `notebookExecutionJob="123"`
    ///    * `notebookExecutionJob="my-execution-job"`
    ///    * `displayName="myDisplayName"` and `displayName=~"myDisplayNameRegex"`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// Optional. The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// Optional. The standard list page token.
    /// Typically obtained via
    /// [ListNotebookExecutionJobsResponse.next_page_token][google.cloud.aiplatform.v1.ListNotebookExecutionJobsResponse.next_page_token]
    /// of the previous
    /// [NotebookService.ListNotebookExecutionJobs][google.cloud.aiplatform.v1.NotebookService.ListNotebookExecutionJobs]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Optional. A comma-separated list of fields to order by, sorted in ascending
    /// order. Use "desc" after a field name for descending. Supported fields:
    ///
    ///    * `display_name`
    ///    * `create_time`
    ///    * `update_time`
    ///
    /// Example: `display_name, create_time desc`.
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
    /// Optional. The NotebookExecutionJob view. Defaults to BASIC.
    #[prost(enumeration = "NotebookExecutionJobView", tag = "6")]
    pub view: i32,
}
/// Response message for \[NotebookService.CreateNotebookExecutionJob\]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListNotebookExecutionJobsResponse {
    /// List of NotebookExecutionJobs in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub notebook_execution_jobs: ::prost::alloc::vec::Vec<NotebookExecutionJob>,
    /// A token to retrieve next page of results.
    /// Pass to
    /// [ListNotebookExecutionJobsRequest.page_token][google.cloud.aiplatform.v1.ListNotebookExecutionJobsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for \[NotebookService.DeleteNotebookExecutionJob\]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteNotebookExecutionJobRequest {
    /// Required. The name of the NotebookExecutionJob resource to be deleted.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Views for Get/List NotebookExecutionJob
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum NotebookExecutionJobView {
    /// When unspecified, the API defaults to the BASIC view.
    Unspecified = 0,
    /// Includes all fields except for direct notebook inputs.
    Basic = 1,
    /// Includes all fields.
    Full = 2,
}
impl NotebookExecutionJobView {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "NOTEBOOK_EXECUTION_JOB_VIEW_UNSPECIFIED",
            Self::Basic => "NOTEBOOK_EXECUTION_JOB_VIEW_BASIC",
            Self::Full => "NOTEBOOK_EXECUTION_JOB_VIEW_FULL",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "NOTEBOOK_EXECUTION_JOB_VIEW_UNSPECIFIED" => Some(Self::Unspecified),
            "NOTEBOOK_EXECUTION_JOB_VIEW_BASIC" => Some(Self::Basic),
            "NOTEBOOK_EXECUTION_JOB_VIEW_FULL" => Some(Self::Full),
            _ => None,
        }
    }
}
/// Generated client implementations.
pub mod notebook_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// The interface for Vertex Notebook service (a.k.a. Colab on Workbench).
    #[derive(Debug, Clone)]
    pub struct NotebookServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl NotebookServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> NotebookServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> NotebookServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            NotebookServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a NotebookRuntimeTemplate.
        pub async fn create_notebook_runtime_template(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateNotebookRuntimeTemplateRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/CreateNotebookRuntimeTemplate",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "CreateNotebookRuntimeTemplate",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a NotebookRuntimeTemplate.
        pub async fn get_notebook_runtime_template(
            &mut self,
            request: impl tonic::IntoRequest<super::GetNotebookRuntimeTemplateRequest>,
        ) -> std::result::Result<
            tonic::Response<super::NotebookRuntimeTemplate>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/GetNotebookRuntimeTemplate",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "GetNotebookRuntimeTemplate",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists NotebookRuntimeTemplates in a Location.
        pub async fn list_notebook_runtime_templates(
            &mut self,
            request: impl tonic::IntoRequest<super::ListNotebookRuntimeTemplatesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListNotebookRuntimeTemplatesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/ListNotebookRuntimeTemplates",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "ListNotebookRuntimeTemplates",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a NotebookRuntimeTemplate.
        pub async fn delete_notebook_runtime_template(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteNotebookRuntimeTemplateRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/DeleteNotebookRuntimeTemplate",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "DeleteNotebookRuntimeTemplate",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a NotebookRuntimeTemplate.
        pub async fn update_notebook_runtime_template(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateNotebookRuntimeTemplateRequest>,
        ) -> std::result::Result<
            tonic::Response<super::NotebookRuntimeTemplate>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/UpdateNotebookRuntimeTemplate",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "UpdateNotebookRuntimeTemplate",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Assigns a NotebookRuntime to a user for a particular Notebook file. This
        /// method will either returns an existing assignment or generates a new one.
        pub async fn assign_notebook_runtime(
            &mut self,
            request: impl tonic::IntoRequest<super::AssignNotebookRuntimeRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/AssignNotebookRuntime",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "AssignNotebookRuntime",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a NotebookRuntime.
        pub async fn get_notebook_runtime(
            &mut self,
            request: impl tonic::IntoRequest<super::GetNotebookRuntimeRequest>,
        ) -> std::result::Result<
            tonic::Response<super::NotebookRuntime>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/GetNotebookRuntime",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "GetNotebookRuntime",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists NotebookRuntimes in a Location.
        pub async fn list_notebook_runtimes(
            &mut self,
            request: impl tonic::IntoRequest<super::ListNotebookRuntimesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListNotebookRuntimesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/ListNotebookRuntimes",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "ListNotebookRuntimes",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a NotebookRuntime.
        pub async fn delete_notebook_runtime(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteNotebookRuntimeRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/DeleteNotebookRuntime",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "DeleteNotebookRuntime",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Upgrades a NotebookRuntime.
        pub async fn upgrade_notebook_runtime(
            &mut self,
            request: impl tonic::IntoRequest<super::UpgradeNotebookRuntimeRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/UpgradeNotebookRuntime",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "UpgradeNotebookRuntime",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Starts a NotebookRuntime.
        pub async fn start_notebook_runtime(
            &mut self,
            request: impl tonic::IntoRequest<super::StartNotebookRuntimeRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/StartNotebookRuntime",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "StartNotebookRuntime",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Stops a NotebookRuntime.
        pub async fn stop_notebook_runtime(
            &mut self,
            request: impl tonic::IntoRequest<super::StopNotebookRuntimeRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/StopNotebookRuntime",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "StopNotebookRuntime",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a NotebookExecutionJob.
        pub async fn create_notebook_execution_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateNotebookExecutionJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/CreateNotebookExecutionJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "CreateNotebookExecutionJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a NotebookExecutionJob.
        pub async fn get_notebook_execution_job(
            &mut self,
            request: impl tonic::IntoRequest<super::GetNotebookExecutionJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::NotebookExecutionJob>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/GetNotebookExecutionJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "GetNotebookExecutionJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists NotebookExecutionJobs in a Location.
        pub async fn list_notebook_execution_jobs(
            &mut self,
            request: impl tonic::IntoRequest<super::ListNotebookExecutionJobsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListNotebookExecutionJobsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/ListNotebookExecutionJobs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "ListNotebookExecutionJobs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a NotebookExecutionJob.
        pub async fn delete_notebook_execution_job(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteNotebookExecutionJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.NotebookService/DeleteNotebookExecutionJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.NotebookService",
                        "DeleteNotebookExecutionJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Represents long-lasting resources that are dedicated to users to runs custom
/// workloads.
/// A PersistentResource can have multiple node pools and each node
/// pool can have its own machine spec.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PersistentResource {
    /// Immutable. Resource name of a PersistentResource.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. The display name of the PersistentResource.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Required. The spec of the pools of different resources.
    #[prost(message, repeated, tag = "4")]
    pub resource_pools: ::prost::alloc::vec::Vec<ResourcePool>,
    /// Output only. The detailed state of a Study.
    #[prost(enumeration = "persistent_resource::State", tag = "5")]
    pub state: i32,
    /// Output only. Only populated when persistent resource's state is `STOPPING`
    /// or `ERROR`.
    #[prost(message, optional, tag = "6")]
    pub error: ::core::option::Option<super::super::super::rpc::Status>,
    /// Output only. Time when the PersistentResource was created.
    #[prost(message, optional, tag = "7")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the PersistentResource for the first time entered
    /// the `RUNNING` state.
    #[prost(message, optional, tag = "8")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the PersistentResource was most recently updated.
    #[prost(message, optional, tag = "9")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. The labels with user-defined metadata to organize
    /// PersistentResource.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "10")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. The full name of the Compute Engine
    /// [network](/compute/docs/networks-and-firewalls#networks) to peered with
    /// Vertex AI to host the persistent resources.
    /// For example, `projects/12345/global/networks/myVPC`.
    /// [Format](/compute/docs/reference/rest/v1/networks/insert)
    /// is of the form `projects/{project}/global/networks/{network}`.
    /// Where {project} is a project number, as in `12345`, and {network} is a
    /// network name.
    ///
    /// To specify this field, you must have already [configured VPC Network
    /// Peering for Vertex
    /// AI](<https://cloud.google.com/vertex-ai/docs/general/vpc-peering>).
    ///
    /// If this field is left unspecified, the resources aren't peered with any
    /// network.
    #[prost(string, tag = "11")]
    pub network: ::prost::alloc::string::String,
    /// Optional. Customer-managed encryption key spec for a PersistentResource.
    /// If set, this PersistentResource and all sub-resources of this
    /// PersistentResource will be secured by this key.
    #[prost(message, optional, tag = "12")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Optional. Persistent Resource runtime spec.
    /// For example, used for Ray cluster configuration.
    #[prost(message, optional, tag = "13")]
    pub resource_runtime_spec: ::core::option::Option<ResourceRuntimeSpec>,
    /// Output only. Runtime information of the Persistent Resource.
    #[prost(message, optional, tag = "14")]
    pub resource_runtime: ::core::option::Option<ResourceRuntime>,
    /// Optional. A list of names for the reserved IP ranges under the VPC network
    /// that can be used for this persistent resource.
    ///
    /// If set, we will deploy the persistent resource within the provided IP
    /// ranges. Otherwise, the persistent resource is deployed to any IP
    /// ranges under the provided VPC network.
    ///
    /// Example: \['vertex-ai-ip-range'\].
    #[prost(string, repeated, tag = "15")]
    pub reserved_ip_ranges: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Nested message and enum types in `PersistentResource`.
pub mod persistent_resource {
    /// Describes the PersistentResource state.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// Not set.
        Unspecified = 0,
        /// The PROVISIONING state indicates the persistent resources is being
        /// created.
        Provisioning = 1,
        /// The RUNNING state indicates the persistent resource is healthy and fully
        /// usable.
        Running = 3,
        /// The STOPPING state indicates the persistent resource is being deleted.
        Stopping = 4,
        /// The ERROR state indicates the persistent resource may be unusable.
        /// Details can be found in the `error` field.
        Error = 5,
        /// The REBOOTING state indicates the persistent resource is being rebooted
        /// (PR is not available right now but is expected to be ready again later).
        Rebooting = 6,
        /// The UPDATING state indicates the persistent resource is being updated.
        Updating = 7,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STATE_UNSPECIFIED",
                Self::Provisioning => "PROVISIONING",
                Self::Running => "RUNNING",
                Self::Stopping => "STOPPING",
                Self::Error => "ERROR",
                Self::Rebooting => "REBOOTING",
                Self::Updating => "UPDATING",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "PROVISIONING" => Some(Self::Provisioning),
                "RUNNING" => Some(Self::Running),
                "STOPPING" => Some(Self::Stopping),
                "ERROR" => Some(Self::Error),
                "REBOOTING" => Some(Self::Rebooting),
                "UPDATING" => Some(Self::Updating),
                _ => None,
            }
        }
    }
}
/// Represents the spec of a group of resources of the same type,
/// for example machine type, disk, and accelerators, in a PersistentResource.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ResourcePool {
    /// Immutable. The unique ID in a PersistentResource for referring to this
    /// resource pool. User can specify it if necessary. Otherwise, it's generated
    /// automatically.
    #[prost(string, tag = "1")]
    pub id: ::prost::alloc::string::String,
    /// Required. Immutable. The specification of a single machine.
    #[prost(message, optional, tag = "2")]
    pub machine_spec: ::core::option::Option<MachineSpec>,
    /// Optional. The total number of machines to use for this resource pool.
    #[prost(int64, optional, tag = "3")]
    pub replica_count: ::core::option::Option<i64>,
    /// Optional. Disk spec for the machine in this node pool.
    #[prost(message, optional, tag = "4")]
    pub disk_spec: ::core::option::Option<DiskSpec>,
    /// Output only. The number of machines currently in use by training jobs for
    /// this resource pool. Will replace idle_replica_count.
    #[prost(int64, tag = "6")]
    pub used_replica_count: i64,
    /// Optional. Optional spec to configure GKE or Ray-on-Vertex autoscaling
    #[prost(message, optional, tag = "7")]
    pub autoscaling_spec: ::core::option::Option<resource_pool::AutoscalingSpec>,
}
/// Nested message and enum types in `ResourcePool`.
pub mod resource_pool {
    /// The min/max number of replicas allowed if enabling autoscaling
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct AutoscalingSpec {
        /// Optional. min replicas in the node pool,
        /// must be ≤ replica_count and < max_replica_count or will throw error.
        /// For autoscaling enabled Ray-on-Vertex, we allow min_replica_count of a
        /// resource_pool to be 0 to match the OSS Ray
        /// behavior(<https://docs.ray.io/en/latest/cluster/vms/user-guides/configuring-autoscaling.html#cluster-config-parameters>).
        /// As for Persistent Resource, the min_replica_count must be > 0, we added
        /// a corresponding validation inside
        /// CreatePersistentResourceRequestValidator.java.
        #[prost(int64, optional, tag = "1")]
        pub min_replica_count: ::core::option::Option<i64>,
        /// Optional. max replicas in the node pool,
        /// must be ≥ replica_count and > min_replica_count or will throw error
        #[prost(int64, optional, tag = "2")]
        pub max_replica_count: ::core::option::Option<i64>,
    }
}
/// Configuration for the runtime on a PersistentResource instance, including
/// but not limited to:
///
/// * Service accounts used to run the workloads.
/// * Whether to make it a dedicated Ray Cluster.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ResourceRuntimeSpec {
    /// Optional. Configure the use of workload identity on the PersistentResource
    #[prost(message, optional, tag = "2")]
    pub service_account_spec: ::core::option::Option<ServiceAccountSpec>,
    /// Optional. Ray cluster configuration.
    /// Required when creating a dedicated RayCluster on the PersistentResource.
    #[prost(message, optional, tag = "1")]
    pub ray_spec: ::core::option::Option<RaySpec>,
}
/// Configuration information for the Ray cluster.
/// For experimental launch, Ray cluster creation and Persistent
/// cluster creation are 1:1 mapping: We will provision all the nodes within the
/// Persistent cluster as Ray nodes.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RaySpec {
    /// Optional. Default image for user to choose a preferred ML framework
    /// (for example, TensorFlow or Pytorch) by choosing from [Vertex prebuilt
    /// images](<https://cloud.google.com/vertex-ai/docs/training/pre-built-containers>).
    /// Either this or the resource_pool_images is required. Use this field if
    /// you need all the resource pools to have the same Ray image. Otherwise, use
    /// the {@code resource_pool_images} field.
    #[prost(string, tag = "1")]
    pub image_uri: ::prost::alloc::string::String,
    /// Optional. Required if image_uri isn't set. A map of resource_pool_id to
    /// prebuild Ray image if user need to use different images for different
    /// head/worker pools. This map needs to cover all the resource pool ids.
    /// Example:
    /// {
    ///    "ray_head_node_pool": "head image"
    ///    "ray_worker_node_pool1": "worker image"
    ///    "ray_worker_node_pool2": "another worker image"
    /// }
    #[prost(map = "string, string", tag = "6")]
    pub resource_pool_images: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Optional. This will be used to indicate which resource pool will serve as
    /// the Ray head node(the first node within that pool). Will use the machine
    /// from the first workerpool as the head node by default if this field isn't
    /// set.
    #[prost(string, tag = "7")]
    pub head_node_resource_pool_id: ::prost::alloc::string::String,
    /// Optional. Ray metrics configurations.
    #[prost(message, optional, tag = "8")]
    pub ray_metric_spec: ::core::option::Option<RayMetricSpec>,
    /// Optional. OSS Ray logging configurations.
    #[prost(message, optional, tag = "10")]
    pub ray_logs_spec: ::core::option::Option<RayLogsSpec>,
}
/// Persistent Cluster runtime information as output
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ResourceRuntime {
    /// Output only. URIs for user to connect to the Cluster.
    /// Example:
    /// {
    ///    "RAY_HEAD_NODE_INTERNAL_IP": "head-node-IP:10001"
    ///    "RAY_DASHBOARD_URI": "ray-dashboard-address:8888"
    /// }
    #[prost(map = "string, string", tag = "1")]
    pub access_uris: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
/// Configuration for the use of custom service account to run the workloads.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ServiceAccountSpec {
    /// Required. If true, custom user-managed service account is enforced to run
    /// any workloads (for example, Vertex Jobs) on the resource. Otherwise, uses
    /// the [Vertex AI Custom Code Service
    /// Agent](<https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents>).
    #[prost(bool, tag = "1")]
    pub enable_custom_service_account: bool,
    /// Optional. Required when all below conditions are met
    ///   * `enable_custom_service_account` is true;
    ///   * any runtime is specified via `ResourceRuntimeSpec` on creation time,
    ///     for example, Ray
    ///
    /// The users must have `iam.serviceAccounts.actAs` permission on this service
    /// account and then the specified runtime containers will run as it.
    ///
    /// Do not set this field if you want to submit jobs using custom service
    /// account to this PersistentResource after creation, but only specify the
    /// `service_account` inside the job.
    #[prost(string, tag = "2")]
    pub service_account: ::prost::alloc::string::String,
}
/// Configuration for the Ray metrics.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RayMetricSpec {
    /// Optional. Flag to disable the Ray metrics collection.
    #[prost(bool, tag = "1")]
    pub disabled: bool,
}
/// Configuration for the Ray OSS Logs.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RayLogsSpec {
    /// Optional. Flag to disable the export of Ray OSS logs to Cloud Logging.
    #[prost(bool, tag = "1")]
    pub disabled: bool,
}
/// Request message for
/// [PersistentResourceService.CreatePersistentResource][google.cloud.aiplatform.v1.PersistentResourceService.CreatePersistentResource].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreatePersistentResourceRequest {
    /// Required. The resource name of the Location to create the
    /// PersistentResource in. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The PersistentResource to create.
    #[prost(message, optional, tag = "2")]
    pub persistent_resource: ::core::option::Option<PersistentResource>,
    /// Required. The ID to use for the PersistentResource, which become the final
    /// component of the PersistentResource's resource name.
    ///
    /// The maximum length is 63 characters, and valid characters
    /// are `/^[a-z](\[a-z0-9-\]{0,61}\[a-z0-9\])?$/`.
    #[prost(string, tag = "3")]
    pub persistent_resource_id: ::prost::alloc::string::String,
}
/// Details of operations that perform create PersistentResource.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreatePersistentResourceOperationMetadata {
    /// Operation metadata for PersistentResource.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// Progress Message for Create LRO
    #[prost(string, tag = "2")]
    pub progress_message: ::prost::alloc::string::String,
}
/// Details of operations that perform update PersistentResource.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdatePersistentResourceOperationMetadata {
    /// Operation metadata for PersistentResource.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// Progress Message for Update LRO
    #[prost(string, tag = "2")]
    pub progress_message: ::prost::alloc::string::String,
}
/// Details of operations that perform reboot PersistentResource.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RebootPersistentResourceOperationMetadata {
    /// Operation metadata for PersistentResource.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// Progress Message for Reboot LRO
    #[prost(string, tag = "2")]
    pub progress_message: ::prost::alloc::string::String,
}
/// Request message for
/// [PersistentResourceService.GetPersistentResource][google.cloud.aiplatform.v1.PersistentResourceService.GetPersistentResource].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetPersistentResourceRequest {
    /// Required. The name of the PersistentResource resource.
    /// Format:
    /// `projects/{project_id_or_number}/locations/{location_id}/persistentResources/{persistent_resource_id}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [PersistentResourceService.ListPersistentResources][google.cloud.aiplatform.v1.PersistentResourceService.ListPersistentResources].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListPersistentResourcesRequest {
    /// Required. The resource name of the Location to list the PersistentResources
    /// from. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// Optional. The standard list page token.
    /// Typically obtained via
    /// [ListPersistentResourcesResponse.next_page_token][google.cloud.aiplatform.v1.ListPersistentResourcesResponse.next_page_token]
    /// of the previous [PersistentResourceService.ListPersistentResource][] call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response message for
/// [PersistentResourceService.ListPersistentResources][google.cloud.aiplatform.v1.PersistentResourceService.ListPersistentResources]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListPersistentResourcesResponse {
    #[prost(message, repeated, tag = "1")]
    pub persistent_resources: ::prost::alloc::vec::Vec<PersistentResource>,
    /// A token to retrieve next page of results.
    /// Pass to
    /// [ListPersistentResourcesRequest.page_token][google.cloud.aiplatform.v1.ListPersistentResourcesRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [PersistentResourceService.DeletePersistentResource][google.cloud.aiplatform.v1.PersistentResourceService.DeletePersistentResource].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeletePersistentResourceRequest {
    /// Required. The name of the PersistentResource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/persistentResources/{persistent_resource}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for UpdatePersistentResource method.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdatePersistentResourceRequest {
    /// Required. The PersistentResource to update.
    ///
    /// The PersistentResource's `name` field is used to identify the
    /// PersistentResource to update. Format:
    /// `projects/{project}/locations/{location}/persistentResources/{persistent_resource}`
    #[prost(message, optional, tag = "1")]
    pub persistent_resource: ::core::option::Option<PersistentResource>,
    /// Required. Specify the fields to be overwritten in the PersistentResource by
    /// the update method.
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Request message for
/// [PersistentResourceService.RebootPersistentResource][google.cloud.aiplatform.v1.PersistentResourceService.RebootPersistentResource].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RebootPersistentResourceRequest {
    /// Required. The name of the PersistentResource resource.
    /// Format:
    /// `projects/{project_id_or_number}/locations/{location_id}/persistentResources/{persistent_resource_id}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod persistent_resource_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for managing Vertex AI's machine learning PersistentResource.
    #[derive(Debug, Clone)]
    pub struct PersistentResourceServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl PersistentResourceServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> PersistentResourceServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> PersistentResourceServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            PersistentResourceServiceClient::new(
                InterceptedService::new(inner, interceptor),
            )
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a PersistentResource.
        pub async fn create_persistent_resource(
            &mut self,
            request: impl tonic::IntoRequest<super::CreatePersistentResourceRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PersistentResourceService/CreatePersistentResource",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PersistentResourceService",
                        "CreatePersistentResource",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a PersistentResource.
        pub async fn get_persistent_resource(
            &mut self,
            request: impl tonic::IntoRequest<super::GetPersistentResourceRequest>,
        ) -> std::result::Result<
            tonic::Response<super::PersistentResource>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PersistentResourceService/GetPersistentResource",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PersistentResourceService",
                        "GetPersistentResource",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists PersistentResources in a Location.
        pub async fn list_persistent_resources(
            &mut self,
            request: impl tonic::IntoRequest<super::ListPersistentResourcesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListPersistentResourcesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PersistentResourceService/ListPersistentResources",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PersistentResourceService",
                        "ListPersistentResources",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a PersistentResource.
        pub async fn delete_persistent_resource(
            &mut self,
            request: impl tonic::IntoRequest<super::DeletePersistentResourceRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PersistentResourceService/DeletePersistentResource",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PersistentResourceService",
                        "DeletePersistentResource",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a PersistentResource.
        pub async fn update_persistent_resource(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdatePersistentResourceRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PersistentResourceService/UpdatePersistentResource",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PersistentResourceService",
                        "UpdatePersistentResource",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Reboots a PersistentResource.
        pub async fn reboot_persistent_resource(
            &mut self,
            request: impl tonic::IntoRequest<super::RebootPersistentResourceRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PersistentResourceService/RebootPersistentResource",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PersistentResourceService",
                        "RebootPersistentResource",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Represents the failure policy of a pipeline. Currently, the default of a
/// pipeline is that the pipeline will continue to run until no more tasks can be
/// executed, also known as PIPELINE_FAILURE_POLICY_FAIL_SLOW. However, if a
/// pipeline is set to PIPELINE_FAILURE_POLICY_FAIL_FAST, it will stop scheduling
/// any new tasks when a task has failed. Any scheduled tasks will continue to
/// completion.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum PipelineFailurePolicy {
    /// Default value, and follows fail slow behavior.
    Unspecified = 0,
    /// Indicates that the pipeline should continue to run until all possible
    /// tasks have been scheduled and completed.
    FailSlow = 1,
    /// Indicates that the pipeline should stop scheduling new tasks after a task
    /// has failed.
    FailFast = 2,
}
impl PipelineFailurePolicy {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "PIPELINE_FAILURE_POLICY_UNSPECIFIED",
            Self::FailSlow => "PIPELINE_FAILURE_POLICY_FAIL_SLOW",
            Self::FailFast => "PIPELINE_FAILURE_POLICY_FAIL_FAST",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "PIPELINE_FAILURE_POLICY_UNSPECIFIED" => Some(Self::Unspecified),
            "PIPELINE_FAILURE_POLICY_FAIL_SLOW" => Some(Self::FailSlow),
            "PIPELINE_FAILURE_POLICY_FAIL_FAST" => Some(Self::FailFast),
            _ => None,
        }
    }
}
/// Describes the state of a pipeline.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum PipelineState {
    /// The pipeline state is unspecified.
    Unspecified = 0,
    /// The pipeline has been created or resumed, and processing has not yet
    /// begun.
    Queued = 1,
    /// The service is preparing to run the pipeline.
    Pending = 2,
    /// The pipeline is in progress.
    Running = 3,
    /// The pipeline completed successfully.
    Succeeded = 4,
    /// The pipeline failed.
    Failed = 5,
    /// The pipeline is being cancelled. From this state, the pipeline may only go
    /// to either PIPELINE_STATE_SUCCEEDED, PIPELINE_STATE_FAILED or
    /// PIPELINE_STATE_CANCELLED.
    Cancelling = 6,
    /// The pipeline has been cancelled.
    Cancelled = 7,
    /// The pipeline has been stopped, and can be resumed.
    Paused = 8,
}
impl PipelineState {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "PIPELINE_STATE_UNSPECIFIED",
            Self::Queued => "PIPELINE_STATE_QUEUED",
            Self::Pending => "PIPELINE_STATE_PENDING",
            Self::Running => "PIPELINE_STATE_RUNNING",
            Self::Succeeded => "PIPELINE_STATE_SUCCEEDED",
            Self::Failed => "PIPELINE_STATE_FAILED",
            Self::Cancelling => "PIPELINE_STATE_CANCELLING",
            Self::Cancelled => "PIPELINE_STATE_CANCELLED",
            Self::Paused => "PIPELINE_STATE_PAUSED",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "PIPELINE_STATE_UNSPECIFIED" => Some(Self::Unspecified),
            "PIPELINE_STATE_QUEUED" => Some(Self::Queued),
            "PIPELINE_STATE_PENDING" => Some(Self::Pending),
            "PIPELINE_STATE_RUNNING" => Some(Self::Running),
            "PIPELINE_STATE_SUCCEEDED" => Some(Self::Succeeded),
            "PIPELINE_STATE_FAILED" => Some(Self::Failed),
            "PIPELINE_STATE_CANCELLING" => Some(Self::Cancelling),
            "PIPELINE_STATE_CANCELLED" => Some(Self::Cancelled),
            "PIPELINE_STATE_PAUSED" => Some(Self::Paused),
            _ => None,
        }
    }
}
/// Value is the value of the field.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Value {
    #[prost(oneof = "value::Value", tags = "1, 2, 3")]
    pub value: ::core::option::Option<value::Value>,
}
/// Nested message and enum types in `Value`.
pub mod value {
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Value {
        /// An integer value.
        #[prost(int64, tag = "1")]
        IntValue(i64),
        /// A double value.
        #[prost(double, tag = "2")]
        DoubleValue(f64),
        /// A string value.
        #[prost(string, tag = "3")]
        StringValue(::prost::alloc::string::String),
    }
}
/// An instance of a machine learning PipelineJob.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PipelineJob {
    /// Output only. The resource name of the PipelineJob.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// The display name of the Pipeline.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Output only. Pipeline creation time.
    #[prost(message, optional, tag = "3")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Pipeline start time.
    #[prost(message, optional, tag = "4")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Pipeline end time.
    #[prost(message, optional, tag = "5")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this PipelineJob was most recently updated.
    #[prost(message, optional, tag = "6")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// The spec of the pipeline.
    #[prost(message, optional, tag = "7")]
    pub pipeline_spec: ::core::option::Option<::prost_types::Struct>,
    /// Output only. The detailed state of the job.
    #[prost(enumeration = "PipelineState", tag = "8")]
    pub state: i32,
    /// Output only. The details of pipeline run. Not available in the list view.
    #[prost(message, optional, tag = "9")]
    pub job_detail: ::core::option::Option<PipelineJobDetail>,
    /// Output only. The error that occurred during pipeline execution.
    /// Only populated when the pipeline's state is FAILED or CANCELLED.
    #[prost(message, optional, tag = "10")]
    pub error: ::core::option::Option<super::super::super::rpc::Status>,
    /// The labels with user-defined metadata to organize PipelineJob.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    ///
    /// Note there is some reserved label key for Vertex AI Pipelines.
    /// - `vertex-ai-pipelines-run-billing-id`, user set value will get overrided.
    #[prost(map = "string, string", tag = "11")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Runtime config of the pipeline.
    #[prost(message, optional, tag = "12")]
    pub runtime_config: ::core::option::Option<pipeline_job::RuntimeConfig>,
    /// Customer-managed encryption key spec for a pipelineJob. If set, this
    /// PipelineJob and all of its sub-resources will be secured by this key.
    #[prost(message, optional, tag = "16")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// The service account that the pipeline workload runs as.
    /// If not specified, the Compute Engine default service account in the project
    /// will be used.
    /// See
    /// <https://cloud.google.com/compute/docs/access/service-accounts#default_service_account>
    ///
    /// Users starting the pipeline must have the `iam.serviceAccounts.actAs`
    /// permission on this service account.
    #[prost(string, tag = "17")]
    pub service_account: ::prost::alloc::string::String,
    /// The full name of the Compute Engine
    /// [network](/compute/docs/networks-and-firewalls#networks) to which the
    /// Pipeline Job's workload should be peered. For example,
    /// `projects/12345/global/networks/myVPC`.
    /// [Format](/compute/docs/reference/rest/v1/networks/insert)
    /// is of the form `projects/{project}/global/networks/{network}`.
    /// Where {project} is a project number, as in `12345`, and {network} is a
    /// network name.
    ///
    /// Private services access must already be configured for the network.
    /// Pipeline job will apply the network configuration to the Google Cloud
    /// resources being launched, if applied, such as Vertex AI
    /// Training or Dataflow job. If left unspecified, the workload is not peered
    /// with any network.
    #[prost(string, tag = "18")]
    pub network: ::prost::alloc::string::String,
    /// A list of names for the reserved ip ranges under the VPC network
    /// that can be used for this Pipeline Job's workload.
    ///
    /// If set, we will deploy the Pipeline Job's workload within the provided ip
    /// ranges. Otherwise, the job will be deployed to any ip ranges under the
    /// provided VPC network.
    ///
    /// Example: \['vertex-ai-ip-range'\].
    #[prost(string, repeated, tag = "25")]
    pub reserved_ip_ranges: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// A template uri from where the
    /// [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec],
    /// if empty, will be downloaded. Currently, only uri from Vertex Template
    /// Registry & Gallery is supported. Reference to
    /// <https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template.>
    #[prost(string, tag = "19")]
    pub template_uri: ::prost::alloc::string::String,
    /// Output only. Pipeline template metadata. Will fill up fields if
    /// [PipelineJob.template_uri][google.cloud.aiplatform.v1.PipelineJob.template_uri]
    /// is from supported template registry.
    #[prost(message, optional, tag = "20")]
    pub template_metadata: ::core::option::Option<PipelineTemplateMetadata>,
    /// Output only. The schedule resource name.
    /// Only returned if the Pipeline is created by Schedule API.
    #[prost(string, tag = "22")]
    pub schedule_name: ::prost::alloc::string::String,
    /// Optional. Whether to do component level validations before job creation.
    #[prost(bool, tag = "26")]
    pub preflight_validations: bool,
}
/// Nested message and enum types in `PipelineJob`.
pub mod pipeline_job {
    /// The runtime config of a PipelineJob.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct RuntimeConfig {
        /// Deprecated. Use
        /// [RuntimeConfig.parameter_values][google.cloud.aiplatform.v1.PipelineJob.RuntimeConfig.parameter_values]
        /// instead. The runtime parameters of the PipelineJob. The parameters will
        /// be passed into
        /// [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec]
        /// to replace the placeholders at runtime. This field is used by pipelines
        /// built using `PipelineJob.pipeline_spec.schema_version` 2.0.0 or lower,
        /// such as pipelines built using Kubeflow Pipelines SDK 1.8 or lower.
        #[prost(map = "string, message", tag = "1")]
        pub parameters: ::std::collections::HashMap<
            ::prost::alloc::string::String,
            super::Value,
        >,
        /// Required. A path in a Cloud Storage bucket, which will be treated as the
        /// root output directory of the pipeline. It is used by the system to
        /// generate the paths of output artifacts. The artifact paths are generated
        /// with a sub-path pattern `{job_id}/{task_id}/{output_key}` under the
        /// specified output directory. The service account specified in this
        /// pipeline must have the `storage.objects.get` and `storage.objects.create`
        /// permissions for this bucket.
        #[prost(string, tag = "2")]
        pub gcs_output_directory: ::prost::alloc::string::String,
        /// The runtime parameters of the PipelineJob. The parameters will be
        /// passed into
        /// [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec]
        /// to replace the placeholders at runtime. This field is used by pipelines
        /// built using `PipelineJob.pipeline_spec.schema_version` 2.1.0, such as
        /// pipelines built using Kubeflow Pipelines SDK 1.9 or higher and the v2
        /// DSL.
        #[prost(map = "string, message", tag = "3")]
        pub parameter_values: ::std::collections::HashMap<
            ::prost::alloc::string::String,
            ::prost_types::Value,
        >,
        /// Represents the failure policy of a pipeline. Currently, the default of a
        /// pipeline is that the pipeline will continue to run until no more tasks
        /// can be executed, also known as PIPELINE_FAILURE_POLICY_FAIL_SLOW.
        /// However, if a pipeline is set to PIPELINE_FAILURE_POLICY_FAIL_FAST, it
        /// will stop scheduling any new tasks when a task has failed. Any scheduled
        /// tasks will continue to completion.
        #[prost(enumeration = "super::PipelineFailurePolicy", tag = "4")]
        pub failure_policy: i32,
        /// The runtime artifacts of the PipelineJob. The key will be the input
        /// artifact name and the value would be one of the InputArtifact.
        #[prost(map = "string, message", tag = "5")]
        pub input_artifacts: ::std::collections::HashMap<
            ::prost::alloc::string::String,
            runtime_config::InputArtifact,
        >,
    }
    /// Nested message and enum types in `RuntimeConfig`.
    pub mod runtime_config {
        /// The type of an input artifact.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct InputArtifact {
            #[prost(oneof = "input_artifact::Kind", tags = "1")]
            pub kind: ::core::option::Option<input_artifact::Kind>,
        }
        /// Nested message and enum types in `InputArtifact`.
        pub mod input_artifact {
            #[derive(Clone, PartialEq, ::prost::Oneof)]
            pub enum Kind {
                /// Artifact resource id from MLMD. Which is the last portion of an
                /// artifact resource name:
                /// `projects/{project}/locations/{location}/metadataStores/default/artifacts/{artifact_id}`.
                /// The artifact must stay within the same project, location and default
                /// metadatastore as the pipeline.
                #[prost(string, tag = "1")]
                ArtifactId(::prost::alloc::string::String),
            }
        }
    }
}
/// Pipeline template metadata if
/// [PipelineJob.template_uri][google.cloud.aiplatform.v1.PipelineJob.template_uri]
/// is from supported template registry. Currently, the only supported registry
/// is Artifact Registry.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PipelineTemplateMetadata {
    /// The version_name in artifact registry.
    ///
    /// Will always be presented in output if the
    /// [PipelineJob.template_uri][google.cloud.aiplatform.v1.PipelineJob.template_uri]
    /// is from supported template registry.
    ///
    /// Format is "sha256:abcdef123456...".
    #[prost(string, tag = "3")]
    pub version: ::prost::alloc::string::String,
}
/// The runtime detail of PipelineJob.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PipelineJobDetail {
    /// Output only. The context of the pipeline.
    #[prost(message, optional, tag = "1")]
    pub pipeline_context: ::core::option::Option<Context>,
    /// Output only. The context of the current pipeline run.
    #[prost(message, optional, tag = "2")]
    pub pipeline_run_context: ::core::option::Option<Context>,
    /// Output only. The runtime details of the tasks under the pipeline.
    #[prost(message, repeated, tag = "3")]
    pub task_details: ::prost::alloc::vec::Vec<PipelineTaskDetail>,
}
/// The runtime detail of a task execution.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PipelineTaskDetail {
    /// Output only. The system generated ID of the task.
    #[prost(int64, tag = "1")]
    pub task_id: i64,
    /// Output only. The id of the parent task if the task is within a component
    /// scope. Empty if the task is at the root level.
    #[prost(int64, tag = "12")]
    pub parent_task_id: i64,
    /// Output only. The user specified name of the task that is defined in
    /// [pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec].
    #[prost(string, tag = "2")]
    pub task_name: ::prost::alloc::string::String,
    /// Output only. Task create time.
    #[prost(message, optional, tag = "3")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Task start time.
    #[prost(message, optional, tag = "4")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Task end time.
    #[prost(message, optional, tag = "5")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. The detailed execution info.
    #[prost(message, optional, tag = "6")]
    pub executor_detail: ::core::option::Option<PipelineTaskExecutorDetail>,
    /// Output only. State of the task.
    #[prost(enumeration = "pipeline_task_detail::State", tag = "7")]
    pub state: i32,
    /// Output only. The execution metadata of the task.
    #[prost(message, optional, tag = "8")]
    pub execution: ::core::option::Option<Execution>,
    /// Output only. The error that occurred during task execution.
    /// Only populated when the task's state is FAILED or CANCELLED.
    #[prost(message, optional, tag = "9")]
    pub error: ::core::option::Option<super::super::super::rpc::Status>,
    /// Output only. A list of task status. This field keeps a record of task
    /// status evolving over time.
    #[prost(message, repeated, tag = "13")]
    pub pipeline_task_status: ::prost::alloc::vec::Vec<
        pipeline_task_detail::PipelineTaskStatus,
    >,
    /// Output only. The runtime input artifacts of the task.
    #[prost(map = "string, message", tag = "10")]
    pub inputs: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        pipeline_task_detail::ArtifactList,
    >,
    /// Output only. The runtime output artifacts of the task.
    #[prost(map = "string, message", tag = "11")]
    pub outputs: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        pipeline_task_detail::ArtifactList,
    >,
}
/// Nested message and enum types in `PipelineTaskDetail`.
pub mod pipeline_task_detail {
    /// A single record of the task status.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct PipelineTaskStatus {
        /// Output only. Update time of this status.
        #[prost(message, optional, tag = "1")]
        pub update_time: ::core::option::Option<::prost_types::Timestamp>,
        /// Output only. The state of the task.
        #[prost(enumeration = "State", tag = "2")]
        pub state: i32,
        /// Output only. The error that occurred during the state. May be set when
        /// the state is any of the non-final state (PENDING/RUNNING/CANCELLING) or
        /// FAILED state. If the state is FAILED, the error here is final and not
        /// going to be retried. If the state is a non-final state, the error
        /// indicates a system-error being retried.
        #[prost(message, optional, tag = "3")]
        pub error: ::core::option::Option<super::super::super::super::rpc::Status>,
    }
    /// A list of artifact metadata.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ArtifactList {
        /// Output only. A list of artifact metadata.
        #[prost(message, repeated, tag = "1")]
        pub artifacts: ::prost::alloc::vec::Vec<super::Artifact>,
    }
    /// Specifies state of TaskExecution
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// Unspecified.
        Unspecified = 0,
        /// Specifies pending state for the task.
        Pending = 1,
        /// Specifies task is being executed.
        Running = 2,
        /// Specifies task completed successfully.
        Succeeded = 3,
        /// Specifies Task cancel is in pending state.
        CancelPending = 4,
        /// Specifies task is being cancelled.
        Cancelling = 5,
        /// Specifies task was cancelled.
        Cancelled = 6,
        /// Specifies task failed.
        Failed = 7,
        /// Specifies task was skipped due to cache hit.
        Skipped = 8,
        /// Specifies that the task was not triggered because the task's trigger
        /// policy is not satisfied. The trigger policy is specified in the
        /// `condition` field of
        /// [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec].
        NotTriggered = 9,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STATE_UNSPECIFIED",
                Self::Pending => "PENDING",
                Self::Running => "RUNNING",
                Self::Succeeded => "SUCCEEDED",
                Self::CancelPending => "CANCEL_PENDING",
                Self::Cancelling => "CANCELLING",
                Self::Cancelled => "CANCELLED",
                Self::Failed => "FAILED",
                Self::Skipped => "SKIPPED",
                Self::NotTriggered => "NOT_TRIGGERED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "PENDING" => Some(Self::Pending),
                "RUNNING" => Some(Self::Running),
                "SUCCEEDED" => Some(Self::Succeeded),
                "CANCEL_PENDING" => Some(Self::CancelPending),
                "CANCELLING" => Some(Self::Cancelling),
                "CANCELLED" => Some(Self::Cancelled),
                "FAILED" => Some(Self::Failed),
                "SKIPPED" => Some(Self::Skipped),
                "NOT_TRIGGERED" => Some(Self::NotTriggered),
                _ => None,
            }
        }
    }
}
/// The runtime detail of a pipeline executor.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PipelineTaskExecutorDetail {
    #[prost(oneof = "pipeline_task_executor_detail::Details", tags = "1, 2")]
    pub details: ::core::option::Option<pipeline_task_executor_detail::Details>,
}
/// Nested message and enum types in `PipelineTaskExecutorDetail`.
pub mod pipeline_task_executor_detail {
    /// The detail of a container execution. It contains the job names of the
    /// lifecycle of a container execution.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct ContainerDetail {
        /// Output only. The name of the
        /// [CustomJob][google.cloud.aiplatform.v1.CustomJob] for the main container
        /// execution.
        #[prost(string, tag = "1")]
        pub main_job: ::prost::alloc::string::String,
        /// Output only. The name of the
        /// [CustomJob][google.cloud.aiplatform.v1.CustomJob] for the
        /// pre-caching-check container execution. This job will be available if the
        /// [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec]
        /// specifies the `pre_caching_check` hook in the lifecycle events.
        #[prost(string, tag = "2")]
        pub pre_caching_check_job: ::prost::alloc::string::String,
        /// Output only. The names of the previously failed
        /// [CustomJob][google.cloud.aiplatform.v1.CustomJob] for the main container
        /// executions. The list includes the all attempts in chronological order.
        #[prost(string, repeated, tag = "3")]
        pub failed_main_jobs: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        /// Output only. The names of the previously failed
        /// [CustomJob][google.cloud.aiplatform.v1.CustomJob] for the
        /// pre-caching-check container executions. This job will be available if the
        /// [PipelineJob.pipeline_spec][google.cloud.aiplatform.v1.PipelineJob.pipeline_spec]
        /// specifies the `pre_caching_check` hook in the lifecycle events. The list
        /// includes the all attempts in chronological order.
        #[prost(string, repeated, tag = "4")]
        pub failed_pre_caching_check_jobs: ::prost::alloc::vec::Vec<
            ::prost::alloc::string::String,
        >,
    }
    /// The detailed info for a custom job executor.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct CustomJobDetail {
        /// Output only. The name of the
        /// [CustomJob][google.cloud.aiplatform.v1.CustomJob].
        #[prost(string, tag = "1")]
        pub job: ::prost::alloc::string::String,
        /// Output only. The names of the previously failed
        /// [CustomJob][google.cloud.aiplatform.v1.CustomJob]. The list includes the
        /// all attempts in chronological order.
        #[prost(string, repeated, tag = "3")]
        pub failed_jobs: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    }
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Details {
        /// Output only. The detailed info for a container executor.
        #[prost(message, tag = "1")]
        ContainerDetail(ContainerDetail),
        /// Output only. The detailed info for a custom job executor.
        #[prost(message, tag = "2")]
        CustomJobDetail(CustomJobDetail),
    }
}
/// The TrainingPipeline orchestrates tasks associated with training a Model. It
/// always executes the training task, and optionally may also
/// export data from Vertex AI's Dataset which becomes the training input,
/// [upload][google.cloud.aiplatform.v1.ModelService.UploadModel] the Model to
/// Vertex AI, and evaluate the Model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TrainingPipeline {
    /// Output only. Resource name of the TrainingPipeline.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The user-defined name of this TrainingPipeline.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Specifies Vertex AI owned input data that may be used for training the
    /// Model. The TrainingPipeline's
    /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition]
    /// should make clear whether this config is used and if there are any special
    /// requirements on how it should be filled. If nothing about this config is
    /// mentioned in the
    /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition],
    /// then it should be assumed that the TrainingPipeline does not depend on this
    /// configuration.
    #[prost(message, optional, tag = "3")]
    pub input_data_config: ::core::option::Option<InputDataConfig>,
    /// Required. A Google Cloud Storage path to the YAML file that defines the
    /// training task which is responsible for producing the model artifact, and
    /// may also include additional auxiliary work. The definition files that can
    /// be used here are found in
    /// gs://google-cloud-aiplatform/schema/trainingjob/definition/.
    /// Note: The URI given on output will be immutable and probably different,
    /// including the URI scheme, than the one given on input. The output URI will
    /// point to a location where the user only has a read access.
    #[prost(string, tag = "4")]
    pub training_task_definition: ::prost::alloc::string::String,
    /// Required. The training task's parameter(s), as specified in the
    /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition]'s
    /// `inputs`.
    #[prost(message, optional, tag = "5")]
    pub training_task_inputs: ::core::option::Option<::prost_types::Value>,
    /// Output only. The metadata information as specified in the
    /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition]'s
    /// `metadata`. This metadata is an auxiliary runtime and final information
    /// about the training task. While the pipeline is running this information is
    /// populated only at a best effort basis. Only present if the
    /// pipeline's
    /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition]
    /// contains `metadata` object.
    #[prost(message, optional, tag = "6")]
    pub training_task_metadata: ::core::option::Option<::prost_types::Value>,
    /// Describes the Model that may be uploaded (via
    /// [ModelService.UploadModel][google.cloud.aiplatform.v1.ModelService.UploadModel])
    /// by this TrainingPipeline. The TrainingPipeline's
    /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition]
    /// should make clear whether this Model description should be populated, and
    /// if there are any special requirements regarding how it should be filled. If
    /// nothing is mentioned in the
    /// [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition],
    /// then it should be assumed that this field should not be filled and the
    /// training task either uploads the Model without a need of this information,
    /// or that training task does not support uploading a Model as part of the
    /// pipeline. When the Pipeline's state becomes `PIPELINE_STATE_SUCCEEDED` and
    /// the trained Model had been uploaded into Vertex AI, then the
    /// model_to_upload's resource [name][google.cloud.aiplatform.v1.Model.name] is
    /// populated. The Model is always uploaded into the Project and Location in
    /// which this pipeline is.
    #[prost(message, optional, tag = "7")]
    pub model_to_upload: ::core::option::Option<Model>,
    /// Optional. The ID to use for the uploaded Model, which will become the final
    /// component of the model resource name.
    ///
    /// This value may be up to 63 characters, and valid characters are
    /// `\[a-z0-9_-\]`. The first character cannot be a number or hyphen.
    #[prost(string, tag = "22")]
    pub model_id: ::prost::alloc::string::String,
    /// Optional. When specify this field, the `model_to_upload` will not be
    /// uploaded as a new model, instead, it will become a new version of this
    /// `parent_model`.
    #[prost(string, tag = "21")]
    pub parent_model: ::prost::alloc::string::String,
    /// Output only. The detailed state of the pipeline.
    #[prost(enumeration = "PipelineState", tag = "9")]
    pub state: i32,
    /// Output only. Only populated when the pipeline's state is
    /// `PIPELINE_STATE_FAILED` or `PIPELINE_STATE_CANCELLED`.
    #[prost(message, optional, tag = "10")]
    pub error: ::core::option::Option<super::super::super::rpc::Status>,
    /// Output only. Time when the TrainingPipeline was created.
    #[prost(message, optional, tag = "11")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the TrainingPipeline for the first time entered the
    /// `PIPELINE_STATE_RUNNING` state.
    #[prost(message, optional, tag = "12")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the TrainingPipeline entered any of the following
    /// states: `PIPELINE_STATE_SUCCEEDED`, `PIPELINE_STATE_FAILED`,
    /// `PIPELINE_STATE_CANCELLED`.
    #[prost(message, optional, tag = "13")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Time when the TrainingPipeline was most recently updated.
    #[prost(message, optional, tag = "14")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// The labels with user-defined metadata to organize TrainingPipelines.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    #[prost(map = "string, string", tag = "15")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Customer-managed encryption key spec for a TrainingPipeline. If set, this
    /// TrainingPipeline will be secured by this key.
    ///
    /// Note: Model trained by this TrainingPipeline is also secured by this key if
    /// [model_to_upload][google.cloud.aiplatform.v1.TrainingPipeline.encryption_spec]
    /// is not set separately.
    #[prost(message, optional, tag = "18")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
}
/// Specifies Vertex AI owned input data to be used for training, and
/// possibly evaluating, the Model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct InputDataConfig {
    /// Required. The ID of the Dataset in the same Project and Location which data
    /// will be used to train the Model. The Dataset must use schema compatible
    /// with Model being trained, and what is compatible should be described in the
    /// used TrainingPipeline's \[training_task_definition\]
    /// \[google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition\].
    /// For tabular Datasets, all their data is exported to training, to pick
    /// and choose from.
    #[prost(string, tag = "1")]
    pub dataset_id: ::prost::alloc::string::String,
    /// Applicable only to Datasets that have DataItems and Annotations.
    ///
    /// A filter on Annotations of the Dataset. Only Annotations that both
    /// match this filter and belong to DataItems not ignored by the split method
    /// are used in respectively training, validation or test role, depending on
    /// the role of the DataItem they are on (for the auto-assigned that role is
    /// decided by Vertex AI). A filter with same syntax as the one used in
    /// [ListAnnotations][google.cloud.aiplatform.v1.DatasetService.ListAnnotations]
    /// may be used, but note here it filters across all Annotations of the
    /// Dataset, and not just within a single DataItem.
    #[prost(string, tag = "6")]
    pub annotations_filter: ::prost::alloc::string::String,
    /// Applicable only to custom training with Datasets that have DataItems and
    /// Annotations.
    ///
    /// Cloud Storage URI that points to a YAML file describing the annotation
    /// schema. The schema is defined as an OpenAPI 3.0.2 [Schema
    /// Object](<https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject>).
    /// The schema files that can be used here are found in
    /// gs://google-cloud-aiplatform/schema/dataset/annotation/ , note that the
    /// chosen schema must be consistent with
    /// [metadata][google.cloud.aiplatform.v1.Dataset.metadata_schema_uri] of the
    /// Dataset specified by
    /// [dataset_id][google.cloud.aiplatform.v1.InputDataConfig.dataset_id].
    ///
    /// Only Annotations that both match this schema and belong to DataItems not
    /// ignored by the split method are used in respectively training, validation
    /// or test role, depending on the role of the DataItem they are on.
    ///
    /// When used in conjunction with
    /// [annotations_filter][google.cloud.aiplatform.v1.InputDataConfig.annotations_filter],
    /// the Annotations used for training are filtered by both
    /// [annotations_filter][google.cloud.aiplatform.v1.InputDataConfig.annotations_filter]
    /// and
    /// [annotation_schema_uri][google.cloud.aiplatform.v1.InputDataConfig.annotation_schema_uri].
    #[prost(string, tag = "9")]
    pub annotation_schema_uri: ::prost::alloc::string::String,
    /// Only applicable to Datasets that have SavedQueries.
    ///
    /// The ID of a SavedQuery (annotation set) under the Dataset specified by
    /// [dataset_id][google.cloud.aiplatform.v1.InputDataConfig.dataset_id] used
    /// for filtering Annotations for training.
    ///
    /// Only Annotations that are associated with this SavedQuery are used in
    /// respectively training. When used in conjunction with
    /// [annotations_filter][google.cloud.aiplatform.v1.InputDataConfig.annotations_filter],
    /// the Annotations used for training are filtered by both
    /// [saved_query_id][google.cloud.aiplatform.v1.InputDataConfig.saved_query_id]
    /// and
    /// [annotations_filter][google.cloud.aiplatform.v1.InputDataConfig.annotations_filter].
    ///
    /// Only one of
    /// [saved_query_id][google.cloud.aiplatform.v1.InputDataConfig.saved_query_id]
    /// and
    /// [annotation_schema_uri][google.cloud.aiplatform.v1.InputDataConfig.annotation_schema_uri]
    /// should be specified as both of them represent the same thing: problem type.
    #[prost(string, tag = "7")]
    pub saved_query_id: ::prost::alloc::string::String,
    /// Whether to persist the ML use assignment to data item system labels.
    #[prost(bool, tag = "11")]
    pub persist_ml_use_assignment: bool,
    /// The instructions how the input data should be split between the
    /// training, validation and test sets.
    /// If no split type is provided, the
    /// [fraction_split][google.cloud.aiplatform.v1.InputDataConfig.fraction_split]
    /// is used by default.
    #[prost(oneof = "input_data_config::Split", tags = "2, 3, 4, 5, 12")]
    pub split: ::core::option::Option<input_data_config::Split>,
    /// Only applicable to Custom and Hyperparameter Tuning TrainingPipelines.
    ///
    /// The destination of the training data to be written to.
    ///
    /// Supported destination file formats:
    ///    * For non-tabular data: "jsonl".
    ///    * For tabular data: "csv" and "bigquery".
    ///
    /// The following Vertex AI environment variables are passed to containers
    /// or python modules of the training task when this field is set:
    ///
    /// * AIP_DATA_FORMAT : Exported data format.
    /// * AIP_TRAINING_DATA_URI : Sharded exported training data uris.
    /// * AIP_VALIDATION_DATA_URI : Sharded exported validation data uris.
    /// * AIP_TEST_DATA_URI : Sharded exported test data uris.
    #[prost(oneof = "input_data_config::Destination", tags = "8, 10")]
    pub destination: ::core::option::Option<input_data_config::Destination>,
}
/// Nested message and enum types in `InputDataConfig`.
pub mod input_data_config {
    /// The instructions how the input data should be split between the
    /// training, validation and test sets.
    /// If no split type is provided, the
    /// [fraction_split][google.cloud.aiplatform.v1.InputDataConfig.fraction_split]
    /// is used by default.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Split {
        /// Split based on fractions defining the size of each set.
        #[prost(message, tag = "2")]
        FractionSplit(super::FractionSplit),
        /// Split based on the provided filters for each set.
        #[prost(message, tag = "3")]
        FilterSplit(super::FilterSplit),
        /// Supported only for tabular Datasets.
        ///
        /// Split based on a predefined key.
        #[prost(message, tag = "4")]
        PredefinedSplit(super::PredefinedSplit),
        /// Supported only for tabular Datasets.
        ///
        /// Split based on the timestamp of the input data pieces.
        #[prost(message, tag = "5")]
        TimestampSplit(super::TimestampSplit),
        /// Supported only for tabular Datasets.
        ///
        /// Split based on the distribution of the specified column.
        #[prost(message, tag = "12")]
        StratifiedSplit(super::StratifiedSplit),
    }
    /// Only applicable to Custom and Hyperparameter Tuning TrainingPipelines.
    ///
    /// The destination of the training data to be written to.
    ///
    /// Supported destination file formats:
    ///    * For non-tabular data: "jsonl".
    ///    * For tabular data: "csv" and "bigquery".
    ///
    /// The following Vertex AI environment variables are passed to containers
    /// or python modules of the training task when this field is set:
    ///
    /// * AIP_DATA_FORMAT : Exported data format.
    /// * AIP_TRAINING_DATA_URI : Sharded exported training data uris.
    /// * AIP_VALIDATION_DATA_URI : Sharded exported validation data uris.
    /// * AIP_TEST_DATA_URI : Sharded exported test data uris.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Destination {
        /// The Cloud Storage location where the training data is to be
        /// written to. In the given directory a new directory is created with
        /// name:
        /// `dataset-<dataset-id>-<annotation-type>-<timestamp-of-training-call>`
        /// where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
        /// All training input data is written into that directory.
        ///
        /// The Vertex AI environment variables representing Cloud Storage
        /// data URIs are represented in the Cloud Storage wildcard
        /// format to support sharded data. e.g.: "gs://.../training-*.jsonl"
        ///
        /// * AIP_DATA_FORMAT = "jsonl" for non-tabular data, "csv" for tabular data
        /// * AIP_TRAINING_DATA_URI =
        /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/training-*.${AIP_DATA_FORMAT}"
        ///
        /// * AIP_VALIDATION_DATA_URI =
        /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/validation-*.${AIP_DATA_FORMAT}"
        ///
        /// * AIP_TEST_DATA_URI =
        /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/test-*.${AIP_DATA_FORMAT}"
        #[prost(message, tag = "8")]
        GcsDestination(super::GcsDestination),
        /// Only applicable to custom training with tabular Dataset with BigQuery
        /// source.
        ///
        /// The BigQuery project location where the training data is to be written
        /// to. In the given project a new dataset is created with name
        /// `dataset_<dataset-id>_<annotation-type>_<timestamp-of-training-call>`
        /// where timestamp is in YYYY_MM_DDThh_mm_ss_sssZ format. All training
        /// input data is written into that dataset. In the dataset three
        /// tables are created, `training`, `validation` and `test`.
        ///
        /// * AIP_DATA_FORMAT = "bigquery".
        /// * AIP_TRAINING_DATA_URI  =
        /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.training"
        ///
        /// * AIP_VALIDATION_DATA_URI =
        /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.validation"
        ///
        /// * AIP_TEST_DATA_URI =
        /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.test"
        #[prost(message, tag = "10")]
        BigqueryDestination(super::BigQueryDestination),
    }
}
/// Assigns the input data to training, validation, and test sets as per the
/// given fractions. Any of `training_fraction`, `validation_fraction` and
/// `test_fraction` may optionally be provided, they must sum to up to 1. If the
/// provided ones sum to less than 1, the remainder is assigned to sets as
/// decided by Vertex AI. If none of the fractions are set, by default roughly
/// 80% of data is used for training, 10% for validation, and 10% for test.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct FractionSplit {
    /// The fraction of the input data that is to be used to train the Model.
    #[prost(double, tag = "1")]
    pub training_fraction: f64,
    /// The fraction of the input data that is to be used to validate the Model.
    #[prost(double, tag = "2")]
    pub validation_fraction: f64,
    /// The fraction of the input data that is to be used to evaluate the Model.
    #[prost(double, tag = "3")]
    pub test_fraction: f64,
}
/// Assigns input data to training, validation, and test sets based on the given
/// filters, data pieces not matched by any filter are ignored. Currently only
/// supported for Datasets containing DataItems.
/// If any of the filters in this message are to match nothing, then they can be
/// set as '-' (the minus sign).
///
/// Supported only for unstructured Datasets.
///
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FilterSplit {
    /// Required. A filter on DataItems of the Dataset. DataItems that match
    /// this filter are used to train the Model. A filter with same syntax
    /// as the one used in
    /// [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems]
    /// may be used. If a single DataItem is matched by more than one of the
    /// FilterSplit filters, then it is assigned to the first set that applies to
    /// it in the training, validation, test order.
    #[prost(string, tag = "1")]
    pub training_filter: ::prost::alloc::string::String,
    /// Required. A filter on DataItems of the Dataset. DataItems that match
    /// this filter are used to validate the Model. A filter with same syntax
    /// as the one used in
    /// [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems]
    /// may be used. If a single DataItem is matched by more than one of the
    /// FilterSplit filters, then it is assigned to the first set that applies to
    /// it in the training, validation, test order.
    #[prost(string, tag = "2")]
    pub validation_filter: ::prost::alloc::string::String,
    /// Required. A filter on DataItems of the Dataset. DataItems that match
    /// this filter are used to test the Model. A filter with same syntax
    /// as the one used in
    /// [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems]
    /// may be used. If a single DataItem is matched by more than one of the
    /// FilterSplit filters, then it is assigned to the first set that applies to
    /// it in the training, validation, test order.
    #[prost(string, tag = "3")]
    pub test_filter: ::prost::alloc::string::String,
}
/// Assigns input data to training, validation, and test sets based on the
/// value of a provided key.
///
/// Supported only for tabular Datasets.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PredefinedSplit {
    /// Required. The key is a name of one of the Dataset's data columns.
    /// The value of the key (either the label's value or value in the column)
    /// must be one of {`training`, `validation`, `test`}, and it defines to which
    /// set the given piece of data is assigned. If for a piece of data the key
    /// is not present or has an invalid value, that piece is ignored by the
    /// pipeline.
    #[prost(string, tag = "1")]
    pub key: ::prost::alloc::string::String,
}
/// Assigns input data to training, validation, and test sets based on a
/// provided timestamps. The youngest data pieces are assigned to training set,
/// next to validation set, and the oldest to the test set.
///
/// Supported only for tabular Datasets.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TimestampSplit {
    /// The fraction of the input data that is to be used to train the Model.
    #[prost(double, tag = "1")]
    pub training_fraction: f64,
    /// The fraction of the input data that is to be used to validate the Model.
    #[prost(double, tag = "2")]
    pub validation_fraction: f64,
    /// The fraction of the input data that is to be used to evaluate the Model.
    #[prost(double, tag = "3")]
    pub test_fraction: f64,
    /// Required. The key is a name of one of the Dataset's data columns.
    /// The values of the key (the values in the column) must be in RFC 3339
    /// `date-time` format, where `time-offset` = `"Z"`
    /// (e.g. 1985-04-12T23:20:50.52Z). If for a piece of data the key is not
    /// present or has an invalid value, that piece is ignored by the pipeline.
    #[prost(string, tag = "4")]
    pub key: ::prost::alloc::string::String,
}
/// Assigns input data to the training, validation, and test sets so that the
/// distribution of values found in the categorical column (as specified by the
/// `key` field) is mirrored within each split. The fraction values determine
/// the relative sizes of the splits.
///
/// For example, if the specified column has three values, with 50% of the rows
/// having value "A", 25% value "B", and 25% value "C", and the split fractions
/// are specified as 80/10/10, then the training set will constitute 80% of the
/// training data, with about 50% of the training set rows having the value "A"
/// for the specified column, about 25% having the value "B", and about 25%
/// having the value "C".
///
/// Only the top 500 occurring values are used; any values not in the top
/// 500 values are randomly assigned to a split. If less than three rows contain
/// a specific value, those rows are randomly assigned.
///
/// Supported only for tabular Datasets.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StratifiedSplit {
    /// The fraction of the input data that is to be used to train the Model.
    #[prost(double, tag = "1")]
    pub training_fraction: f64,
    /// The fraction of the input data that is to be used to validate the Model.
    #[prost(double, tag = "2")]
    pub validation_fraction: f64,
    /// The fraction of the input data that is to be used to evaluate the Model.
    #[prost(double, tag = "3")]
    pub test_fraction: f64,
    /// Required. The key is a name of one of the Dataset's data columns.
    /// The key provided must be for a categorical column.
    #[prost(string, tag = "4")]
    pub key: ::prost::alloc::string::String,
}
/// Runtime operation information for
/// [PipelineService.BatchCancelPipelineJobs][google.cloud.aiplatform.v1.PipelineService.BatchCancelPipelineJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchCancelPipelineJobsOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [PipelineService.CreateTrainingPipeline][google.cloud.aiplatform.v1.PipelineService.CreateTrainingPipeline].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTrainingPipelineRequest {
    /// Required. The resource name of the Location to create the TrainingPipeline
    /// in. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The TrainingPipeline to create.
    #[prost(message, optional, tag = "2")]
    pub training_pipeline: ::core::option::Option<TrainingPipeline>,
}
/// Request message for
/// [PipelineService.GetTrainingPipeline][google.cloud.aiplatform.v1.PipelineService.GetTrainingPipeline].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetTrainingPipelineRequest {
    /// Required. The name of the TrainingPipeline resource.
    /// Format:
    /// `projects/{project}/locations/{location}/trainingPipelines/{training_pipeline}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [PipelineService.ListTrainingPipelines][google.cloud.aiplatform.v1.PipelineService.ListTrainingPipelines].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTrainingPipelinesRequest {
    /// Required. The resource name of the Location to list the TrainingPipelines
    /// from. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list filter.
    ///
    /// Supported fields:
    ///
    ///    * `display_name` supports `=`, `!=` comparisons, and `:` wildcard.
    ///    * `state` supports `=`, `!=` comparisons.
    ///    * `training_task_definition` `=`, `!=` comparisons, and `:` wildcard.
    ///    * `create_time` supports `=`, `!=`,`<`, `<=`,`>`, `>=` comparisons.
    ///      `create_time` must be in RFC 3339 format.
    ///    * `labels` supports general map functions that is:
    ///      `labels.key=value` - key:value equality
    ///      `labels.key:* - key existence
    ///
    /// Some examples of using the filter are:
    ///
    ///    * `state="PIPELINE_STATE_SUCCEEDED" AND display_name:"my_pipeline_*"`
    ///    * `state!="PIPELINE_STATE_FAILED" OR display_name="my_pipeline"`
    ///    * `NOT display_name="my_pipeline"`
    ///    * `create_time>"2021-05-18T00:00:00Z"`
    ///    * `training_task_definition:"*automl_text_classification*"`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [ListTrainingPipelinesResponse.next_page_token][google.cloud.aiplatform.v1.ListTrainingPipelinesResponse.next_page_token]
    /// of the previous
    /// [PipelineService.ListTrainingPipelines][google.cloud.aiplatform.v1.PipelineService.ListTrainingPipelines]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "5")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [PipelineService.ListTrainingPipelines][google.cloud.aiplatform.v1.PipelineService.ListTrainingPipelines]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTrainingPipelinesResponse {
    /// List of TrainingPipelines in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub training_pipelines: ::prost::alloc::vec::Vec<TrainingPipeline>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListTrainingPipelinesRequest.page_token][google.cloud.aiplatform.v1.ListTrainingPipelinesRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [PipelineService.DeleteTrainingPipeline][google.cloud.aiplatform.v1.PipelineService.DeleteTrainingPipeline].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteTrainingPipelineRequest {
    /// Required. The name of the TrainingPipeline resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/trainingPipelines/{training_pipeline}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [PipelineService.CancelTrainingPipeline][google.cloud.aiplatform.v1.PipelineService.CancelTrainingPipeline].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CancelTrainingPipelineRequest {
    /// Required. The name of the TrainingPipeline to cancel.
    /// Format:
    /// `projects/{project}/locations/{location}/trainingPipelines/{training_pipeline}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [PipelineService.CreatePipelineJob][google.cloud.aiplatform.v1.PipelineService.CreatePipelineJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreatePipelineJobRequest {
    /// Required. The resource name of the Location to create the PipelineJob in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The PipelineJob to create.
    #[prost(message, optional, tag = "2")]
    pub pipeline_job: ::core::option::Option<PipelineJob>,
    /// The ID to use for the PipelineJob, which will become the final component of
    /// the PipelineJob name. If not provided, an ID will be automatically
    /// generated.
    ///
    /// This value should be less than 128 characters, and valid characters
    /// are `/[a-z][0-9]-/`.
    #[prost(string, tag = "3")]
    pub pipeline_job_id: ::prost::alloc::string::String,
}
/// Request message for
/// [PipelineService.GetPipelineJob][google.cloud.aiplatform.v1.PipelineService.GetPipelineJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetPipelineJobRequest {
    /// Required. The name of the PipelineJob resource.
    /// Format:
    /// `projects/{project}/locations/{location}/pipelineJobs/{pipeline_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [PipelineService.ListPipelineJobs][google.cloud.aiplatform.v1.PipelineService.ListPipelineJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListPipelineJobsRequest {
    /// Required. The resource name of the Location to list the PipelineJobs from.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the PipelineJobs that match the filter expression. The following
    /// fields are supported:
    ///
    /// * `pipeline_name`: Supports `=` and `!=` comparisons.
    /// * `display_name`: Supports `=`, `!=` comparisons, and `:` wildcard.
    /// * `pipeline_job_user_id`: Supports `=`, `!=` comparisons, and `:` wildcard.
    ///    for example, can check if pipeline's display_name contains *step* by
    ///    doing display_name:\"*step*\"
    /// * `state`: Supports `=` and `!=` comparisons.
    /// * `create_time`: Supports `=`, `!=`, `<`, `>`, `<=`, and `>=` comparisons.
    ///    Values must be in RFC 3339 format.
    /// * `update_time`: Supports `=`, `!=`, `<`, `>`, `<=`, and `>=` comparisons.
    ///    Values must be in RFC 3339 format.
    /// * `end_time`: Supports `=`, `!=`, `<`, `>`, `<=`, and `>=` comparisons.
    ///    Values must be in RFC 3339 format.
    /// * `labels`: Supports key-value equality and key presence.
    /// * `template_uri`: Supports `=`, `!=` comparisons, and `:` wildcard.
    /// * `template_metadata.version`: Supports `=`, `!=` comparisons, and `:`
    ///    wildcard.
    ///
    /// Filter expressions can be combined together using logical operators
    /// (`AND` & `OR`).
    /// For example: `pipeline_name="test" AND create_time>"2020-05-18T13:30:00Z"`.
    ///
    /// The syntax to define filter expression is based on
    /// <https://google.aip.dev/160.>
    ///
    /// Examples:
    ///
    /// * `create_time>"2021-05-18T00:00:00Z" OR
    ///    update_time>"2020-05-18T00:00:00Z"` PipelineJobs created or updated
    ///    after 2020-05-18 00:00:00 UTC.
    /// * `labels.env = "prod"`
    ///    PipelineJobs with label "env" set to "prod".
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [ListPipelineJobsResponse.next_page_token][google.cloud.aiplatform.v1.ListPipelineJobsResponse.next_page_token]
    /// of the previous
    /// [PipelineService.ListPipelineJobs][google.cloud.aiplatform.v1.PipelineService.ListPipelineJobs]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// A comma-separated list of fields to order by. The default sort order is in
    /// ascending order. Use "desc" after a field name for descending. You can have
    /// multiple order_by fields provided e.g. "create_time desc, end_time",
    /// "end_time, start_time, update_time" For example, using "create_time desc,
    /// end_time" will order results by create time in descending order, and if
    /// there are multiple jobs having the same create time, order them by the end
    /// time in ascending order. if order_by is not specified, it will order by
    /// default order is create time in descending order. Supported fields:
    ///
    ///    * `create_time`
    ///    * `update_time`
    ///    * `end_time`
    ///    * `start_time`
    #[prost(string, tag = "6")]
    pub order_by: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "7")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [PipelineService.ListPipelineJobs][google.cloud.aiplatform.v1.PipelineService.ListPipelineJobs]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListPipelineJobsResponse {
    /// List of PipelineJobs in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub pipeline_jobs: ::prost::alloc::vec::Vec<PipelineJob>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListPipelineJobsRequest.page_token][google.cloud.aiplatform.v1.ListPipelineJobsRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [PipelineService.DeletePipelineJob][google.cloud.aiplatform.v1.PipelineService.DeletePipelineJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeletePipelineJobRequest {
    /// Required. The name of the PipelineJob resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/pipelineJobs/{pipeline_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [PipelineService.BatchDeletePipelineJobs][google.cloud.aiplatform.v1.PipelineService.BatchDeletePipelineJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchDeletePipelineJobsRequest {
    /// Required. The name of the PipelineJobs' parent resource.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The names of the PipelineJobs to delete.
    /// A maximum of 32 PipelineJobs can be deleted in a batch.
    /// Format:
    /// `projects/{project}/locations/{location}/pipelineJobs/{pipelineJob}`
    #[prost(string, repeated, tag = "2")]
    pub names: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Response message for
/// [PipelineService.BatchDeletePipelineJobs][google.cloud.aiplatform.v1.PipelineService.BatchDeletePipelineJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchDeletePipelineJobsResponse {
    /// PipelineJobs deleted.
    #[prost(message, repeated, tag = "1")]
    pub pipeline_jobs: ::prost::alloc::vec::Vec<PipelineJob>,
}
/// Request message for
/// [PipelineService.CancelPipelineJob][google.cloud.aiplatform.v1.PipelineService.CancelPipelineJob].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CancelPipelineJobRequest {
    /// Required. The name of the PipelineJob to cancel.
    /// Format:
    /// `projects/{project}/locations/{location}/pipelineJobs/{pipeline_job}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [PipelineService.BatchCancelPipelineJobs][google.cloud.aiplatform.v1.PipelineService.BatchCancelPipelineJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchCancelPipelineJobsRequest {
    /// Required. The name of the PipelineJobs' parent resource.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The names of the PipelineJobs to cancel.
    /// A maximum of 32 PipelineJobs can be cancelled in a batch.
    /// Format:
    /// `projects/{project}/locations/{location}/pipelineJobs/{pipelineJob}`
    #[prost(string, repeated, tag = "2")]
    pub names: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Response message for
/// [PipelineService.BatchCancelPipelineJobs][google.cloud.aiplatform.v1.PipelineService.BatchCancelPipelineJobs].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchCancelPipelineJobsResponse {
    /// PipelineJobs cancelled.
    #[prost(message, repeated, tag = "1")]
    pub pipeline_jobs: ::prost::alloc::vec::Vec<PipelineJob>,
}
/// Generated client implementations.
pub mod pipeline_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for creating and managing Vertex AI's pipelines. This includes both
    /// `TrainingPipeline` resources (used for AutoML and custom training) and
    /// `PipelineJob` resources (used for Vertex AI Pipelines).
    #[derive(Debug, Clone)]
    pub struct PipelineServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl PipelineServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> PipelineServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> PipelineServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            PipelineServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a TrainingPipeline. A created TrainingPipeline right away will be
        /// attempted to be run.
        pub async fn create_training_pipeline(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateTrainingPipelineRequest>,
        ) -> std::result::Result<
            tonic::Response<super::TrainingPipeline>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PipelineService/CreateTrainingPipeline",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PipelineService",
                        "CreateTrainingPipeline",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a TrainingPipeline.
        pub async fn get_training_pipeline(
            &mut self,
            request: impl tonic::IntoRequest<super::GetTrainingPipelineRequest>,
        ) -> std::result::Result<
            tonic::Response<super::TrainingPipeline>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PipelineService/GetTrainingPipeline",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PipelineService",
                        "GetTrainingPipeline",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists TrainingPipelines in a Location.
        pub async fn list_training_pipelines(
            &mut self,
            request: impl tonic::IntoRequest<super::ListTrainingPipelinesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListTrainingPipelinesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PipelineService/ListTrainingPipelines",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PipelineService",
                        "ListTrainingPipelines",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a TrainingPipeline.
        pub async fn delete_training_pipeline(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteTrainingPipelineRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PipelineService/DeleteTrainingPipeline",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PipelineService",
                        "DeleteTrainingPipeline",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Cancels a TrainingPipeline.
        /// Starts asynchronous cancellation on the TrainingPipeline. The server
        /// makes a best effort to cancel the pipeline, but success is not
        /// guaranteed. Clients can use
        /// [PipelineService.GetTrainingPipeline][google.cloud.aiplatform.v1.PipelineService.GetTrainingPipeline]
        /// or other methods to check whether the cancellation succeeded or whether the
        /// pipeline completed despite cancellation. On successful cancellation,
        /// the TrainingPipeline is not deleted; instead it becomes a pipeline with
        /// a
        /// [TrainingPipeline.error][google.cloud.aiplatform.v1.TrainingPipeline.error]
        /// value with a [google.rpc.Status.code][google.rpc.Status.code] of 1,
        /// corresponding to `Code.CANCELLED`, and
        /// [TrainingPipeline.state][google.cloud.aiplatform.v1.TrainingPipeline.state]
        /// is set to `CANCELLED`.
        pub async fn cancel_training_pipeline(
            &mut self,
            request: impl tonic::IntoRequest<super::CancelTrainingPipelineRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PipelineService/CancelTrainingPipeline",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PipelineService",
                        "CancelTrainingPipeline",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a PipelineJob. A PipelineJob will run immediately when created.
        pub async fn create_pipeline_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CreatePipelineJobRequest>,
        ) -> std::result::Result<tonic::Response<super::PipelineJob>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PipelineService/CreatePipelineJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PipelineService",
                        "CreatePipelineJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a PipelineJob.
        pub async fn get_pipeline_job(
            &mut self,
            request: impl tonic::IntoRequest<super::GetPipelineJobRequest>,
        ) -> std::result::Result<tonic::Response<super::PipelineJob>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PipelineService/GetPipelineJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PipelineService",
                        "GetPipelineJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists PipelineJobs in a Location.
        pub async fn list_pipeline_jobs(
            &mut self,
            request: impl tonic::IntoRequest<super::ListPipelineJobsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListPipelineJobsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PipelineService/ListPipelineJobs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PipelineService",
                        "ListPipelineJobs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a PipelineJob.
        pub async fn delete_pipeline_job(
            &mut self,
            request: impl tonic::IntoRequest<super::DeletePipelineJobRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PipelineService/DeletePipelineJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PipelineService",
                        "DeletePipelineJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Batch deletes PipelineJobs
        /// The Operation is atomic. If it fails, none of the PipelineJobs are deleted.
        /// If it succeeds, all of the PipelineJobs are deleted.
        pub async fn batch_delete_pipeline_jobs(
            &mut self,
            request: impl tonic::IntoRequest<super::BatchDeletePipelineJobsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PipelineService/BatchDeletePipelineJobs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PipelineService",
                        "BatchDeletePipelineJobs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Cancels a PipelineJob.
        /// Starts asynchronous cancellation on the PipelineJob. The server
        /// makes a best effort to cancel the pipeline, but success is not
        /// guaranteed. Clients can use
        /// [PipelineService.GetPipelineJob][google.cloud.aiplatform.v1.PipelineService.GetPipelineJob]
        /// or other methods to check whether the cancellation succeeded or whether the
        /// pipeline completed despite cancellation. On successful cancellation,
        /// the PipelineJob is not deleted; instead it becomes a pipeline with
        /// a [PipelineJob.error][google.cloud.aiplatform.v1.PipelineJob.error] value
        /// with a [google.rpc.Status.code][google.rpc.Status.code] of 1, corresponding
        /// to `Code.CANCELLED`, and
        /// [PipelineJob.state][google.cloud.aiplatform.v1.PipelineJob.state] is set to
        /// `CANCELLED`.
        pub async fn cancel_pipeline_job(
            &mut self,
            request: impl tonic::IntoRequest<super::CancelPipelineJobRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PipelineService/CancelPipelineJob",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PipelineService",
                        "CancelPipelineJob",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Batch cancel PipelineJobs.
        /// Firstly the server will check if all the jobs are in non-terminal states,
        /// and skip the jobs that are already terminated.
        /// If the operation failed, none of the pipeline jobs are cancelled.
        /// The server will poll the states of all the pipeline jobs periodically
        /// to check the cancellation status.
        /// This operation will return an LRO.
        pub async fn batch_cancel_pipeline_jobs(
            &mut self,
            request: impl tonic::IntoRequest<super::BatchCancelPipelineJobsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.PipelineService/BatchCancelPipelineJobs",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.PipelineService",
                        "BatchCancelPipelineJobs",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// ReasoningEngine configurations
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReasoningEngineSpec {
    /// Required. User provided package spec of the ReasoningEngine.
    #[prost(message, optional, tag = "2")]
    pub package_spec: ::core::option::Option<reasoning_engine_spec::PackageSpec>,
    /// Optional. Declarations for object class methods in OpenAPI specification
    /// format.
    #[prost(message, repeated, tag = "3")]
    pub class_methods: ::prost::alloc::vec::Vec<::prost_types::Struct>,
}
/// Nested message and enum types in `ReasoningEngineSpec`.
pub mod reasoning_engine_spec {
    /// User provided package spec like pickled object and package requirements.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct PackageSpec {
        /// Optional. The Cloud Storage URI of the pickled python object.
        #[prost(string, tag = "1")]
        pub pickle_object_gcs_uri: ::prost::alloc::string::String,
        /// Optional. The Cloud Storage URI of the dependency files in tar.gz format.
        #[prost(string, tag = "2")]
        pub dependency_files_gcs_uri: ::prost::alloc::string::String,
        /// Optional. The Cloud Storage URI of the `requirements.txt` file
        #[prost(string, tag = "3")]
        pub requirements_gcs_uri: ::prost::alloc::string::String,
        /// Optional. The Python version. Currently support 3.8, 3.9, 3.10, 3.11.
        /// If not specified, default value is 3.10.
        #[prost(string, tag = "4")]
        pub python_version: ::prost::alloc::string::String,
    }
}
/// ReasoningEngine provides a customizable runtime for models to determine
/// which actions to take and in which order.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReasoningEngine {
    /// Identifier. The resource name of the ReasoningEngine.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The display name of the ReasoningEngine.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Optional. The description of the ReasoningEngine.
    #[prost(string, tag = "7")]
    pub description: ::prost::alloc::string::String,
    /// Required. Configurations of the ReasoningEngine
    #[prost(message, optional, tag = "3")]
    pub spec: ::core::option::Option<ReasoningEngineSpec>,
    /// Output only. Timestamp when this ReasoningEngine was created.
    #[prost(message, optional, tag = "4")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this ReasoningEngine was most recently updated.
    #[prost(message, optional, tag = "5")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. Used to perform consistent read-modify-write updates. If not set,
    /// a blind "overwrite" update happens.
    #[prost(string, tag = "6")]
    pub etag: ::prost::alloc::string::String,
}
/// Request message for [ReasoningEngineExecutionService.Query][].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryReasoningEngineRequest {
    /// Required. The name of the ReasoningEngine resource to use.
    /// Format:
    /// `projects/{project}/locations/{location}/reasoningEngines/{reasoning_engine}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. Input content provided by users in JSON object format. Examples
    /// include text query, function calling parameters, media bytes, etc.
    #[prost(message, optional, tag = "2")]
    pub input: ::core::option::Option<::prost_types::Struct>,
    /// Optional. Class method to be used for the query.
    /// It is optional and defaults to "query" if unspecified.
    #[prost(string, tag = "3")]
    pub class_method: ::prost::alloc::string::String,
}
/// Response message for [ReasoningEngineExecutionService.Query][]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct QueryReasoningEngineResponse {
    /// Response provided by users in JSON object format.
    #[prost(message, optional, tag = "1")]
    pub output: ::core::option::Option<::prost_types::Value>,
}
/// Request message for [ReasoningEngineExecutionService.StreamQuery][].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StreamQueryReasoningEngineRequest {
    /// Required. The name of the ReasoningEngine resource to use.
    /// Format:
    /// `projects/{project}/locations/{location}/reasoningEngines/{reasoning_engine}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. Input content provided by users in JSON object format. Examples
    /// include text query, function calling parameters, media bytes, etc.
    #[prost(message, optional, tag = "2")]
    pub input: ::core::option::Option<::prost_types::Struct>,
    /// Optional. Class method to be used for the stream query.
    /// It is optional and defaults to "stream_query" if unspecified.
    #[prost(string, tag = "3")]
    pub class_method: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod reasoning_engine_execution_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for executing queries on Reasoning Engine.
    #[derive(Debug, Clone)]
    pub struct ReasoningEngineExecutionServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl ReasoningEngineExecutionServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> ReasoningEngineExecutionServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> ReasoningEngineExecutionServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            ReasoningEngineExecutionServiceClient::new(
                InterceptedService::new(inner, interceptor),
            )
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Queries using a reasoning engine.
        pub async fn query_reasoning_engine(
            &mut self,
            request: impl tonic::IntoRequest<super::QueryReasoningEngineRequest>,
        ) -> std::result::Result<
            tonic::Response<super::QueryReasoningEngineResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ReasoningEngineExecutionService/QueryReasoningEngine",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ReasoningEngineExecutionService",
                        "QueryReasoningEngine",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Streams queries using a reasoning engine.
        pub async fn stream_query_reasoning_engine(
            &mut self,
            request: impl tonic::IntoRequest<super::StreamQueryReasoningEngineRequest>,
        ) -> std::result::Result<
            tonic::Response<
                tonic::codec::Streaming<super::super::super::super::api::HttpBody>,
            >,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ReasoningEngineExecutionService/StreamQueryReasoningEngine",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ReasoningEngineExecutionService",
                        "StreamQueryReasoningEngine",
                    ),
                );
            self.inner.server_streaming(req, path, codec).await
        }
    }
}
/// Request message for
/// [ReasoningEngineService.CreateReasoningEngine][google.cloud.aiplatform.v1.ReasoningEngineService.CreateReasoningEngine].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateReasoningEngineRequest {
    /// Required. The resource name of the Location to create the ReasoningEngine
    /// in. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The ReasoningEngine to create.
    #[prost(message, optional, tag = "2")]
    pub reasoning_engine: ::core::option::Option<ReasoningEngine>,
}
/// Details of
/// [ReasoningEngineService.CreateReasoningEngine][google.cloud.aiplatform.v1.ReasoningEngineService.CreateReasoningEngine]
/// operation.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateReasoningEngineOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [ReasoningEngineService.GetReasoningEngine][google.cloud.aiplatform.v1.ReasoningEngineService.GetReasoningEngine].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetReasoningEngineRequest {
    /// Required. The name of the ReasoningEngine resource.
    /// Format:
    /// `projects/{project}/locations/{location}/reasoningEngines/{reasoning_engine}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [ReasoningEngineService.UpdateReasoningEngine][google.cloud.aiplatform.v1.ReasoningEngineService.UpdateReasoningEngine].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateReasoningEngineRequest {
    /// Required. The ReasoningEngine which replaces the resource on the server.
    #[prost(message, optional, tag = "1")]
    pub reasoning_engine: ::core::option::Option<ReasoningEngine>,
    /// Optional. Mask specifying which fields to update.
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Details of
/// [ReasoningEngineService.UpdateReasoningEngine][google.cloud.aiplatform.v1.ReasoningEngineService.UpdateReasoningEngine]
/// operation.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateReasoningEngineOperationMetadata {
    /// The common part of the operation metadata.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [ReasoningEngineService.ListReasoningEngines][google.cloud.aiplatform.v1.ReasoningEngineService.ListReasoningEngines].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListReasoningEnginesRequest {
    /// Required. The resource name of the Location to list the ReasoningEngines
    /// from. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. The standard list filter.
    /// More detail in [AIP-160](<https://google.aip.dev/160>).
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// Optional. The standard list page size.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// Optional. The standard list page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response message for
/// [ReasoningEngineService.ListReasoningEngines][google.cloud.aiplatform.v1.ReasoningEngineService.ListReasoningEngines]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListReasoningEnginesResponse {
    /// List of ReasoningEngines in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub reasoning_engines: ::prost::alloc::vec::Vec<ReasoningEngine>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListReasoningEnginesRequest.page_token][google.cloud.aiplatform.v1.ListReasoningEnginesRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [ReasoningEngineService.DeleteReasoningEngine][google.cloud.aiplatform.v1.ReasoningEngineService.DeleteReasoningEngine].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteReasoningEngineRequest {
    /// Required. The name of the ReasoningEngine resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/reasoningEngines/{reasoning_engine}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod reasoning_engine_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for managing Vertex AI's Reasoning Engines.
    #[derive(Debug, Clone)]
    pub struct ReasoningEngineServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl ReasoningEngineServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> ReasoningEngineServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> ReasoningEngineServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            ReasoningEngineServiceClient::new(
                InterceptedService::new(inner, interceptor),
            )
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a reasoning engine.
        pub async fn create_reasoning_engine(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateReasoningEngineRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ReasoningEngineService/CreateReasoningEngine",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ReasoningEngineService",
                        "CreateReasoningEngine",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a reasoning engine.
        pub async fn get_reasoning_engine(
            &mut self,
            request: impl tonic::IntoRequest<super::GetReasoningEngineRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ReasoningEngine>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ReasoningEngineService/GetReasoningEngine",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ReasoningEngineService",
                        "GetReasoningEngine",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists reasoning engines in a location.
        pub async fn list_reasoning_engines(
            &mut self,
            request: impl tonic::IntoRequest<super::ListReasoningEnginesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListReasoningEnginesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ReasoningEngineService/ListReasoningEngines",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ReasoningEngineService",
                        "ListReasoningEngines",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a reasoning engine.
        pub async fn update_reasoning_engine(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateReasoningEngineRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ReasoningEngineService/UpdateReasoningEngine",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ReasoningEngineService",
                        "UpdateReasoningEngine",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a reasoning engine.
        pub async fn delete_reasoning_engine(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteReasoningEngineRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ReasoningEngineService/DeleteReasoningEngine",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ReasoningEngineService",
                        "DeleteReasoningEngine",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// An instance of a Schedule periodically schedules runs to make API calls based
/// on user specified time specification and API request type.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Schedule {
    /// Immutable. The resource name of the Schedule.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. User provided name of the Schedule.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Optional. Timestamp after which the first run can be scheduled.
    /// Default to Schedule create time if not specified.
    #[prost(message, optional, tag = "3")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. Timestamp after which no new runs can be scheduled.
    /// If specified, The schedule will be completed when either
    /// end_time is reached or when scheduled_run_count >= max_run_count.
    /// If not specified, new runs will keep getting scheduled until this Schedule
    /// is paused or deleted. Already scheduled runs will be allowed to complete.
    /// Unset if not specified.
    #[prost(message, optional, tag = "4")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Optional. Maximum run count of the schedule.
    /// If specified, The schedule will be completed when either
    /// started_run_count >= max_run_count or when end_time is reached.
    /// If not specified, new runs will keep getting scheduled until this Schedule
    /// is paused or deleted. Already scheduled runs will be allowed to complete.
    /// Unset if not specified.
    #[prost(int64, tag = "16")]
    pub max_run_count: i64,
    /// Output only. The number of runs started by this schedule.
    #[prost(int64, tag = "17")]
    pub started_run_count: i64,
    /// Output only. The state of this Schedule.
    #[prost(enumeration = "schedule::State", tag = "5")]
    pub state: i32,
    /// Output only. Timestamp when this Schedule was created.
    #[prost(message, optional, tag = "6")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Schedule was updated.
    #[prost(message, optional, tag = "19")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Schedule should schedule the next run.
    /// Having a next_run_time in the past means the runs are being started
    /// behind schedule.
    #[prost(message, optional, tag = "7")]
    pub next_run_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Schedule was last paused.
    /// Unset if never paused.
    #[prost(message, optional, tag = "8")]
    pub last_pause_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Schedule was last resumed.
    /// Unset if never resumed from pause.
    #[prost(message, optional, tag = "9")]
    pub last_resume_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Required. Maximum number of runs that can be started concurrently for this
    /// Schedule. This is the limit for starting the scheduled requests and not the
    /// execution of the operations/jobs created by the requests (if applicable).
    #[prost(int64, tag = "11")]
    pub max_concurrent_run_count: i64,
    /// Optional. Whether new scheduled runs can be queued when max_concurrent_runs
    /// limit is reached. If set to true, new runs will be queued instead of
    /// skipped. Default to false.
    #[prost(bool, tag = "12")]
    pub allow_queueing: bool,
    /// Output only. Whether to backfill missed runs when the schedule is resumed
    /// from PAUSED state. If set to true, all missed runs will be scheduled. New
    /// runs will be scheduled after the backfill is complete. Default to false.
    #[prost(bool, tag = "13")]
    pub catch_up: bool,
    /// Output only. Response of the last scheduled run.
    /// This is the response for starting the scheduled requests and not the
    /// execution of the operations/jobs created by the requests (if applicable).
    /// Unset if no run has been scheduled yet.
    #[prost(message, optional, tag = "18")]
    pub last_scheduled_run_response: ::core::option::Option<schedule::RunResponse>,
    /// Required.
    /// The time specification to launch scheduled runs.
    #[prost(oneof = "schedule::TimeSpecification", tags = "10")]
    pub time_specification: ::core::option::Option<schedule::TimeSpecification>,
    /// Required.
    /// The API request template to launch the scheduled runs.
    /// User-specified ID is not supported in the request template.
    #[prost(oneof = "schedule::Request", tags = "14, 20")]
    pub request: ::core::option::Option<schedule::Request>,
}
/// Nested message and enum types in `Schedule`.
pub mod schedule {
    /// Status of a scheduled run.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct RunResponse {
        /// The scheduled run time based on the user-specified schedule.
        #[prost(message, optional, tag = "1")]
        pub scheduled_run_time: ::core::option::Option<::prost_types::Timestamp>,
        /// The response of the scheduled run.
        #[prost(string, tag = "2")]
        pub run_response: ::prost::alloc::string::String,
    }
    /// Possible state of the schedule.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// Unspecified.
        Unspecified = 0,
        /// The Schedule is active. Runs are being scheduled on the user-specified
        /// timespec.
        Active = 1,
        /// The schedule is paused. No new runs will be created until the schedule
        /// is resumed. Already started runs will be allowed to complete.
        Paused = 2,
        /// The Schedule is completed. No new runs will be scheduled. Already started
        /// runs will be allowed to complete. Schedules in completed state cannot be
        /// paused or resumed.
        Completed = 3,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STATE_UNSPECIFIED",
                Self::Active => "ACTIVE",
                Self::Paused => "PAUSED",
                Self::Completed => "COMPLETED",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "ACTIVE" => Some(Self::Active),
                "PAUSED" => Some(Self::Paused),
                "COMPLETED" => Some(Self::Completed),
                _ => None,
            }
        }
    }
    /// Required.
    /// The time specification to launch scheduled runs.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum TimeSpecification {
        /// Cron schedule (<https://en.wikipedia.org/wiki/Cron>) to launch scheduled
        /// runs. To explicitly set a timezone to the cron tab, apply a prefix in the
        /// cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}".
        /// The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone
        /// database. For example, "CRON_TZ=America/New_York 1 * * * *", or
        /// "TZ=America/New_York 1 * * * *".
        #[prost(string, tag = "10")]
        Cron(::prost::alloc::string::String),
    }
    /// Required.
    /// The API request template to launch the scheduled runs.
    /// User-specified ID is not supported in the request template.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Request {
        /// Request for
        /// [PipelineService.CreatePipelineJob][google.cloud.aiplatform.v1.PipelineService.CreatePipelineJob].
        /// CreatePipelineJobRequest.parent field is required (format:
        /// projects/{project}/locations/{location}).
        #[prost(message, tag = "14")]
        CreatePipelineJobRequest(super::CreatePipelineJobRequest),
        /// Request for
        /// [NotebookService.CreateNotebookExecutionJob][google.cloud.aiplatform.v1.NotebookService.CreateNotebookExecutionJob].
        #[prost(message, tag = "20")]
        CreateNotebookExecutionJobRequest(super::CreateNotebookExecutionJobRequest),
    }
}
/// Request message for
/// [ScheduleService.CreateSchedule][google.cloud.aiplatform.v1.ScheduleService.CreateSchedule].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateScheduleRequest {
    /// Required. The resource name of the Location to create the Schedule in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The Schedule to create.
    #[prost(message, optional, tag = "2")]
    pub schedule: ::core::option::Option<Schedule>,
}
/// Request message for
/// [ScheduleService.GetSchedule][google.cloud.aiplatform.v1.ScheduleService.GetSchedule].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetScheduleRequest {
    /// Required. The name of the Schedule resource.
    /// Format:
    /// `projects/{project}/locations/{location}/schedules/{schedule}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [ScheduleService.ListSchedules][google.cloud.aiplatform.v1.ScheduleService.ListSchedules].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListSchedulesRequest {
    /// Required. The resource name of the Location to list the Schedules from.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the Schedules that match the filter expression. The following
    /// fields are supported:
    ///
    /// * `display_name`: Supports `=`, `!=` comparisons, and `:` wildcard.
    /// * `state`: Supports `=` and `!=` comparisons.
    /// * `request`: Supports existence of the <request_type> check.
    ///        (e.g. `create_pipeline_job_request:*` --> Schedule has
    ///        create_pipeline_job_request).
    /// * `create_time`: Supports `=`, `!=`, `<`, `>`, `<=`, and `>=` comparisons.
    ///        Values must be in RFC 3339 format.
    /// * `start_time`: Supports `=`, `!=`, `<`, `>`, `<=`, and `>=` comparisons.
    ///        Values must be in RFC 3339 format.
    /// * `end_time`: Supports `=`, `!=`, `<`, `>`, `<=`, `>=` comparisons and `:*`
    ///        existence check. Values must be in RFC 3339 format.
    /// * `next_run_time`: Supports `=`, `!=`, `<`, `>`, `<=`, and `>=`
    ///        comparisons. Values must be in RFC 3339 format.
    ///
    ///
    /// Filter expressions can be combined together using logical operators
    /// (`NOT`, `AND` & `OR`).
    /// The syntax to define filter expression is based on
    /// <https://google.aip.dev/160.>
    ///
    /// Examples:
    ///
    /// * `state="ACTIVE" AND display_name:"my_schedule_*"`
    /// * `NOT display_name="my_schedule"`
    /// * `create_time>"2021-05-18T00:00:00Z"`
    /// * `end_time>"2021-05-18T00:00:00Z" OR NOT end_time:*`
    /// * `create_pipeline_job_request:*`
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The standard list page size.
    /// Default to 100 if not specified.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained via
    /// [ListSchedulesResponse.next_page_token][google.cloud.aiplatform.v1.ListSchedulesResponse.next_page_token]
    /// of the previous
    /// [ScheduleService.ListSchedules][google.cloud.aiplatform.v1.ScheduleService.ListSchedules]
    /// call.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// A comma-separated list of fields to order by. The default sort order is in
    /// ascending order. Use "desc" after a field name for descending. You can have
    /// multiple order_by fields provided.
    ///
    /// For example, using "create_time desc, end_time" will order results by
    /// create time in descending order, and if there are multiple schedules having
    /// the same create time, order them by the end time in ascending order.
    ///
    /// If order_by is not specified, it will order by default with create_time in
    /// descending order.
    ///
    /// Supported fields:
    ///
    ///    * `create_time`
    ///    * `start_time`
    ///    * `end_time`
    ///    * `next_run_time`
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [ScheduleService.ListSchedules][google.cloud.aiplatform.v1.ScheduleService.ListSchedules]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListSchedulesResponse {
    /// List of Schedules in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub schedules: ::prost::alloc::vec::Vec<Schedule>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListSchedulesRequest.page_token][google.cloud.aiplatform.v1.ListSchedulesRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [ScheduleService.DeleteSchedule][google.cloud.aiplatform.v1.ScheduleService.DeleteSchedule].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteScheduleRequest {
    /// Required. The name of the Schedule resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/schedules/{schedule}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [ScheduleService.PauseSchedule][google.cloud.aiplatform.v1.ScheduleService.PauseSchedule].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PauseScheduleRequest {
    /// Required. The name of the Schedule resource to be paused.
    /// Format:
    /// `projects/{project}/locations/{location}/schedules/{schedule}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [ScheduleService.ResumeSchedule][google.cloud.aiplatform.v1.ScheduleService.ResumeSchedule].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ResumeScheduleRequest {
    /// Required. The name of the Schedule resource to be resumed.
    /// Format:
    /// `projects/{project}/locations/{location}/schedules/{schedule}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. Whether to backfill missed runs when the schedule is resumed from
    /// PAUSED state. If set to true, all missed runs will be scheduled. New runs
    /// will be scheduled after the backfill is complete. This will also update
    /// [Schedule.catch_up][google.cloud.aiplatform.v1.Schedule.catch_up] field.
    /// Default to false.
    #[prost(bool, tag = "2")]
    pub catch_up: bool,
}
/// Request message for
/// [ScheduleService.UpdateSchedule][google.cloud.aiplatform.v1.ScheduleService.UpdateSchedule].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateScheduleRequest {
    /// Required. The Schedule which replaces the resource on the server.
    /// The following restrictions will be applied:
    ///
    ///    * The scheduled request type cannot be changed.
    ///    * The non-empty fields cannot be unset.
    ///    * The output_only fields will be ignored if specified.
    #[prost(message, optional, tag = "1")]
    pub schedule: ::core::option::Option<Schedule>,
    /// Required. The update mask applies to the resource. See
    /// [google.protobuf.FieldMask][google.protobuf.FieldMask].
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Generated client implementations.
pub mod schedule_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for creating and managing Vertex AI's Schedule resources to
    /// periodically launch shceudled runs to make API calls.
    #[derive(Debug, Clone)]
    pub struct ScheduleServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl ScheduleServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> ScheduleServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> ScheduleServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            ScheduleServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a Schedule.
        pub async fn create_schedule(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateScheduleRequest>,
        ) -> std::result::Result<tonic::Response<super::Schedule>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ScheduleService/CreateSchedule",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ScheduleService",
                        "CreateSchedule",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a Schedule.
        pub async fn delete_schedule(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteScheduleRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ScheduleService/DeleteSchedule",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ScheduleService",
                        "DeleteSchedule",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a Schedule.
        pub async fn get_schedule(
            &mut self,
            request: impl tonic::IntoRequest<super::GetScheduleRequest>,
        ) -> std::result::Result<tonic::Response<super::Schedule>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ScheduleService/GetSchedule",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ScheduleService",
                        "GetSchedule",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Schedules in a Location.
        pub async fn list_schedules(
            &mut self,
            request: impl tonic::IntoRequest<super::ListSchedulesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListSchedulesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ScheduleService/ListSchedules",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ScheduleService",
                        "ListSchedules",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Pauses a Schedule. Will mark
        /// [Schedule.state][google.cloud.aiplatform.v1.Schedule.state] to 'PAUSED'. If
        /// the schedule is paused, no new runs will be created. Already created runs
        /// will NOT be paused or canceled.
        pub async fn pause_schedule(
            &mut self,
            request: impl tonic::IntoRequest<super::PauseScheduleRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ScheduleService/PauseSchedule",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ScheduleService",
                        "PauseSchedule",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Resumes a paused Schedule to start scheduling new runs. Will mark
        /// [Schedule.state][google.cloud.aiplatform.v1.Schedule.state] to 'ACTIVE'.
        /// Only paused Schedule can be resumed.
        ///
        /// When the Schedule is resumed, new runs will be scheduled starting from the
        /// next execution time after the current time based on the time_specification
        /// in the Schedule. If
        /// [Schedule.catch_up][google.cloud.aiplatform.v1.Schedule.catch_up] is set up
        /// true, all missed runs will be scheduled for backfill first.
        pub async fn resume_schedule(
            &mut self,
            request: impl tonic::IntoRequest<super::ResumeScheduleRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ScheduleService/ResumeSchedule",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ScheduleService",
                        "ResumeSchedule",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates an active or paused Schedule.
        ///
        /// When the Schedule is updated, new runs will be scheduled starting from the
        /// updated next execution time after the update time based on the
        /// time_specification in the updated Schedule. All unstarted runs before the
        /// update time will be skipped while already created runs will NOT be paused
        /// or canceled.
        pub async fn update_schedule(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateScheduleRequest>,
        ) -> std::result::Result<tonic::Response<super::Schedule>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.ScheduleService/UpdateSchedule",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.ScheduleService",
                        "UpdateSchedule",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// SpecialistPool represents customers' own workforce to work on their data
/// labeling jobs. It includes a group of specialist managers and workers.
/// Managers are responsible for managing the workers in this pool as well as
/// customers' data labeling jobs associated with this pool. Customers create
/// specialist pool as well as start data labeling jobs on Cloud, managers and
/// workers handle the jobs using CrowdCompute console.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SpecialistPool {
    /// Required. The resource name of the SpecialistPool.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The user-defined name of the SpecialistPool.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    /// This field should be unique on project-level.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Output only. The number of managers in this SpecialistPool.
    #[prost(int32, tag = "3")]
    pub specialist_managers_count: i32,
    /// The email addresses of the managers in the SpecialistPool.
    #[prost(string, repeated, tag = "4")]
    pub specialist_manager_emails: ::prost::alloc::vec::Vec<
        ::prost::alloc::string::String,
    >,
    /// Output only. The resource name of the pending data labeling jobs.
    #[prost(string, repeated, tag = "5")]
    pub pending_data_labeling_jobs: ::prost::alloc::vec::Vec<
        ::prost::alloc::string::String,
    >,
    /// The email addresses of workers in the SpecialistPool.
    #[prost(string, repeated, tag = "7")]
    pub specialist_worker_emails: ::prost::alloc::vec::Vec<
        ::prost::alloc::string::String,
    >,
}
/// Request message for
/// [SpecialistPoolService.CreateSpecialistPool][google.cloud.aiplatform.v1.SpecialistPoolService.CreateSpecialistPool].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateSpecialistPoolRequest {
    /// Required. The parent Project name for the new SpecialistPool.
    /// The form is `projects/{project}/locations/{location}`.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The SpecialistPool to create.
    #[prost(message, optional, tag = "2")]
    pub specialist_pool: ::core::option::Option<SpecialistPool>,
}
/// Runtime operation information for
/// [SpecialistPoolService.CreateSpecialistPool][google.cloud.aiplatform.v1.SpecialistPoolService.CreateSpecialistPool].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateSpecialistPoolOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [SpecialistPoolService.GetSpecialistPool][google.cloud.aiplatform.v1.SpecialistPoolService.GetSpecialistPool].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetSpecialistPoolRequest {
    /// Required. The name of the SpecialistPool resource.
    /// The form is
    /// `projects/{project}/locations/{location}/specialistPools/{specialist_pool}`.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [SpecialistPoolService.ListSpecialistPools][google.cloud.aiplatform.v1.SpecialistPoolService.ListSpecialistPools].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListSpecialistPoolsRequest {
    /// Required. The name of the SpecialistPool's parent resource.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The standard list page size.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// The standard list page token.
    /// Typically obtained by
    /// [ListSpecialistPoolsResponse.next_page_token][google.cloud.aiplatform.v1.ListSpecialistPoolsResponse.next_page_token]
    /// of the previous
    /// [SpecialistPoolService.ListSpecialistPools][google.cloud.aiplatform.v1.SpecialistPoolService.ListSpecialistPools]
    /// call. Return first page if empty.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
    /// Mask specifying which fields to read. FieldMask represents a set of
    #[prost(message, optional, tag = "4")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [SpecialistPoolService.ListSpecialistPools][google.cloud.aiplatform.v1.SpecialistPoolService.ListSpecialistPools].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListSpecialistPoolsResponse {
    /// A list of SpecialistPools that matches the specified filter in the request.
    #[prost(message, repeated, tag = "1")]
    pub specialist_pools: ::prost::alloc::vec::Vec<SpecialistPool>,
    /// The standard List next-page token.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [SpecialistPoolService.DeleteSpecialistPool][google.cloud.aiplatform.v1.SpecialistPoolService.DeleteSpecialistPool].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteSpecialistPoolRequest {
    /// Required. The resource name of the SpecialistPool to delete. Format:
    /// `projects/{project}/locations/{location}/specialistPools/{specialist_pool}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// If set to true, any specialist managers in this SpecialistPool will also be
    /// deleted. (Otherwise, the request will only work if the SpecialistPool has
    /// no specialist managers.)
    #[prost(bool, tag = "2")]
    pub force: bool,
}
/// Request message for
/// [SpecialistPoolService.UpdateSpecialistPool][google.cloud.aiplatform.v1.SpecialistPoolService.UpdateSpecialistPool].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateSpecialistPoolRequest {
    /// Required. The SpecialistPool which replaces the resource on the server.
    #[prost(message, optional, tag = "1")]
    pub specialist_pool: ::core::option::Option<SpecialistPool>,
    /// Required. The update mask applies to the resource.
    #[prost(message, optional, tag = "2")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Runtime operation metadata for
/// [SpecialistPoolService.UpdateSpecialistPool][google.cloud.aiplatform.v1.SpecialistPoolService.UpdateSpecialistPool].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateSpecialistPoolOperationMetadata {
    /// Output only. The name of the SpecialistPool to which the specialists are
    /// being added. Format:
    /// `projects/{project_id}/locations/{location_id}/specialistPools/{specialist_pool}`
    #[prost(string, tag = "1")]
    pub specialist_pool: ::prost::alloc::string::String,
    /// The operation generic information.
    #[prost(message, optional, tag = "2")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Generated client implementations.
pub mod specialist_pool_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for creating and managing Customer SpecialistPools.
    /// When customers start Data Labeling jobs, they can reuse/create Specialist
    /// Pools to bring their own Specialists to label the data.
    /// Customers can add/remove Managers for the Specialist Pool on Cloud console,
    /// then Managers will get email notifications to manage Specialists and tasks on
    /// CrowdCompute console.
    #[derive(Debug, Clone)]
    pub struct SpecialistPoolServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl SpecialistPoolServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> SpecialistPoolServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> SpecialistPoolServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            SpecialistPoolServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a SpecialistPool.
        pub async fn create_specialist_pool(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateSpecialistPoolRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.SpecialistPoolService/CreateSpecialistPool",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.SpecialistPoolService",
                        "CreateSpecialistPool",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a SpecialistPool.
        pub async fn get_specialist_pool(
            &mut self,
            request: impl tonic::IntoRequest<super::GetSpecialistPoolRequest>,
        ) -> std::result::Result<tonic::Response<super::SpecialistPool>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.SpecialistPoolService/GetSpecialistPool",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.SpecialistPoolService",
                        "GetSpecialistPool",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists SpecialistPools in a Location.
        pub async fn list_specialist_pools(
            &mut self,
            request: impl tonic::IntoRequest<super::ListSpecialistPoolsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListSpecialistPoolsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.SpecialistPoolService/ListSpecialistPools",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.SpecialistPoolService",
                        "ListSpecialistPools",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a SpecialistPool as well as all Specialists in the pool.
        pub async fn delete_specialist_pool(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteSpecialistPoolRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.SpecialistPoolService/DeleteSpecialistPool",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.SpecialistPoolService",
                        "DeleteSpecialistPool",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a SpecialistPool.
        pub async fn update_specialist_pool(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateSpecialistPoolRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.SpecialistPoolService/UpdateSpecialistPool",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.SpecialistPoolService",
                        "UpdateSpecialistPool",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Tensorboard is a physical database that stores users' training metrics.
/// A default Tensorboard is provided in each region of a Google Cloud project.
/// If needed users can also create extra Tensorboards in their projects.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Tensorboard {
    /// Output only. Name of the Tensorboard.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. User provided name of this Tensorboard.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Description of this Tensorboard.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// Customer-managed encryption key spec for a Tensorboard. If set, this
    /// Tensorboard and all sub-resources of this Tensorboard will be secured by
    /// this key.
    #[prost(message, optional, tag = "11")]
    pub encryption_spec: ::core::option::Option<EncryptionSpec>,
    /// Output only. Consumer project Cloud Storage path prefix used to store blob
    /// data, which can either be a bucket or directory. Does not end with a '/'.
    #[prost(string, tag = "10")]
    pub blob_storage_path_prefix: ::prost::alloc::string::String,
    /// Output only. The number of Runs stored in this Tensorboard.
    #[prost(int32, tag = "5")]
    pub run_count: i32,
    /// Output only. Timestamp when this Tensorboard was created.
    #[prost(message, optional, tag = "6")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this Tensorboard was last updated.
    #[prost(message, optional, tag = "7")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// The labels with user-defined metadata to organize your Tensorboards.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// No more than 64 user labels can be associated with one Tensorboard
    /// (System labels are excluded).
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable.
    #[prost(map = "string, string", tag = "8")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Used to perform a consistent read-modify-write updates. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "9")]
    pub etag: ::prost::alloc::string::String,
    /// Used to indicate if the TensorBoard instance is the default one.
    /// Each project & region can have at most one default TensorBoard instance.
    /// Creation of a default TensorBoard instance and updating an existing
    /// TensorBoard instance to be default will mark all other TensorBoard
    /// instances (if any) as non default.
    #[prost(bool, tag = "12")]
    pub is_default: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "13")]
    pub satisfies_pzs: bool,
    /// Output only. Reserved for future use.
    #[prost(bool, tag = "14")]
    pub satisfies_pzi: bool,
}
/// TensorboardTimeSeries maps to times series produced in training runs
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TensorboardTimeSeries {
    /// Output only. Name of the TensorboardTimeSeries.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. User provided name of this TensorboardTimeSeries.
    /// This value should be unique among all TensorboardTimeSeries resources
    /// belonging to the same TensorboardRun resource (parent resource).
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Description of this TensorboardTimeSeries.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// Required. Immutable. Type of TensorboardTimeSeries value.
    #[prost(enumeration = "tensorboard_time_series::ValueType", tag = "4")]
    pub value_type: i32,
    /// Output only. Timestamp when this TensorboardTimeSeries was created.
    #[prost(message, optional, tag = "5")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this TensorboardTimeSeries was last updated.
    #[prost(message, optional, tag = "6")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Used to perform a consistent read-modify-write updates. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "7")]
    pub etag: ::prost::alloc::string::String,
    /// Immutable. Name of the plugin this time series pertain to. Such as Scalar,
    /// Tensor, Blob
    #[prost(string, tag = "8")]
    pub plugin_name: ::prost::alloc::string::String,
    /// Data of the current plugin, with the size limited to 65KB.
    #[prost(bytes = "vec", tag = "9")]
    pub plugin_data: ::prost::alloc::vec::Vec<u8>,
    /// Output only. Scalar, Tensor, or Blob metadata for this
    /// TensorboardTimeSeries.
    #[prost(message, optional, tag = "10")]
    pub metadata: ::core::option::Option<tensorboard_time_series::Metadata>,
}
/// Nested message and enum types in `TensorboardTimeSeries`.
pub mod tensorboard_time_series {
    /// Describes metadata for a TensorboardTimeSeries.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Metadata {
        /// Output only. Max step index of all data points within a
        /// TensorboardTimeSeries.
        #[prost(int64, tag = "1")]
        pub max_step: i64,
        /// Output only. Max wall clock timestamp of all data points within a
        /// TensorboardTimeSeries.
        #[prost(message, optional, tag = "2")]
        pub max_wall_time: ::core::option::Option<::prost_types::Timestamp>,
        /// Output only. The largest blob sequence length (number of blobs) of all
        /// data points in this time series, if its ValueType is BLOB_SEQUENCE.
        #[prost(int64, tag = "3")]
        pub max_blob_sequence_length: i64,
    }
    /// An enum representing the value type of a TensorboardTimeSeries.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum ValueType {
        /// The value type is unspecified.
        Unspecified = 0,
        /// Used for TensorboardTimeSeries that is a list of scalars.
        /// E.g. accuracy of a model over epochs/time.
        Scalar = 1,
        /// Used for TensorboardTimeSeries that is a list of tensors.
        /// E.g. histograms of weights of layer in a model over epoch/time.
        Tensor = 2,
        /// Used for TensorboardTimeSeries that is a list of blob sequences.
        /// E.g. set of sample images with labels over epochs/time.
        BlobSequence = 3,
    }
    impl ValueType {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "VALUE_TYPE_UNSPECIFIED",
                Self::Scalar => "SCALAR",
                Self::Tensor => "TENSOR",
                Self::BlobSequence => "BLOB_SEQUENCE",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "VALUE_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
                "SCALAR" => Some(Self::Scalar),
                "TENSOR" => Some(Self::Tensor),
                "BLOB_SEQUENCE" => Some(Self::BlobSequence),
                _ => None,
            }
        }
    }
}
/// All the data stored in a TensorboardTimeSeries.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TimeSeriesData {
    /// Required. The ID of the TensorboardTimeSeries, which will become the final
    /// component of the TensorboardTimeSeries' resource name
    #[prost(string, tag = "1")]
    pub tensorboard_time_series_id: ::prost::alloc::string::String,
    /// Required. Immutable. The value type of this time series. All the values in
    /// this time series data must match this value type.
    #[prost(enumeration = "tensorboard_time_series::ValueType", tag = "2")]
    pub value_type: i32,
    /// Required. Data points in this time series.
    #[prost(message, repeated, tag = "3")]
    pub values: ::prost::alloc::vec::Vec<TimeSeriesDataPoint>,
}
/// A TensorboardTimeSeries data point.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TimeSeriesDataPoint {
    /// Wall clock timestamp when this data point is generated by the end user.
    #[prost(message, optional, tag = "1")]
    pub wall_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Step index of this data point within the run.
    #[prost(int64, tag = "2")]
    pub step: i64,
    /// Value of this time series data point.
    #[prost(oneof = "time_series_data_point::Value", tags = "3, 4, 5")]
    pub value: ::core::option::Option<time_series_data_point::Value>,
}
/// Nested message and enum types in `TimeSeriesDataPoint`.
pub mod time_series_data_point {
    /// Value of this time series data point.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Value {
        /// A scalar value.
        #[prost(message, tag = "3")]
        Scalar(super::Scalar),
        /// A tensor value.
        #[prost(message, tag = "4")]
        Tensor(super::TensorboardTensor),
        /// A blob sequence value.
        #[prost(message, tag = "5")]
        Blobs(super::TensorboardBlobSequence),
    }
}
/// One point viewable on a scalar metric plot.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct Scalar {
    /// Value of the point at this step / timestamp.
    #[prost(double, tag = "1")]
    pub value: f64,
}
/// One point viewable on a tensor metric plot.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TensorboardTensor {
    /// Required. Serialized form of
    /// <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor.proto>
    #[prost(bytes = "vec", tag = "1")]
    pub value: ::prost::alloc::vec::Vec<u8>,
    /// Optional. Version number of TensorProto used to serialize
    /// [value][google.cloud.aiplatform.v1.TensorboardTensor.value].
    #[prost(int32, tag = "2")]
    pub version_number: i32,
}
/// One point viewable on a blob metric plot, but mostly just a wrapper message
/// to work around repeated fields can't be used directly within `oneof` fields.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TensorboardBlobSequence {
    /// List of blobs contained within the sequence.
    #[prost(message, repeated, tag = "1")]
    pub values: ::prost::alloc::vec::Vec<TensorboardBlob>,
}
/// One blob (e.g, image, graph) viewable on a blob metric plot.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TensorboardBlob {
    /// Output only. A URI safe key uniquely identifying a blob. Can be used to
    /// locate the blob stored in the Cloud Storage bucket of the consumer project.
    #[prost(string, tag = "1")]
    pub id: ::prost::alloc::string::String,
    /// Optional. The bytes of the blob is not present unless it's returned by the
    /// ReadTensorboardBlobData endpoint.
    #[prost(bytes = "vec", tag = "2")]
    pub data: ::prost::alloc::vec::Vec<u8>,
}
/// A TensorboardExperiment is a group of TensorboardRuns, that are typically the
/// results of a training job run, in a Tensorboard.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TensorboardExperiment {
    /// Output only. Name of the TensorboardExperiment.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// User provided name of this TensorboardExperiment.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Description of this TensorboardExperiment.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// Output only. Timestamp when this TensorboardExperiment was created.
    #[prost(message, optional, tag = "4")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this TensorboardExperiment was last updated.
    #[prost(message, optional, tag = "5")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// The labels with user-defined metadata to organize your
    /// TensorboardExperiment.
    ///
    /// Label keys and values cannot be longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// No more than 64 user labels can be associated with one Dataset (System
    /// labels are excluded).
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    /// System reserved label keys are prefixed with `aiplatform.googleapis.com/`
    /// and are immutable. The following system labels exist for each Dataset:
    ///
    /// * `aiplatform.googleapis.com/dataset_metadata_schema`: output only. Its
    ///     value is the
    ///     [metadata_schema's][google.cloud.aiplatform.v1.Dataset.metadata_schema_uri]
    ///     title.
    #[prost(map = "string, string", tag = "6")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Used to perform consistent read-modify-write updates. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "7")]
    pub etag: ::prost::alloc::string::String,
    /// Immutable. Source of the TensorboardExperiment. Example: a custom training
    /// job.
    #[prost(string, tag = "8")]
    pub source: ::prost::alloc::string::String,
}
/// TensorboardRun maps to a specific execution of a training job with a given
/// set of hyperparameter values, model definition, dataset, etc
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TensorboardRun {
    /// Output only. Name of the TensorboardRun.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. User provided name of this TensorboardRun.
    /// This value must be unique among all TensorboardRuns
    /// belonging to the same parent TensorboardExperiment.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Description of this TensorboardRun.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// Output only. Timestamp when this TensorboardRun was created.
    #[prost(message, optional, tag = "6")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this TensorboardRun was last updated.
    #[prost(message, optional, tag = "7")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// The labels with user-defined metadata to organize your TensorboardRuns.
    ///
    /// This field will be used to filter and visualize Runs in the Tensorboard UI.
    /// For example, a Vertex AI training job can set a label
    /// aiplatform.googleapis.com/training_job_id=xxxxx to all the runs created
    /// within that job. An end user can set a label experiment_id=xxxxx for all
    /// the runs produced in a Jupyter notebook. These runs can be grouped by a
    /// label value and visualized together in the Tensorboard UI.
    ///
    /// Label keys and values can be no longer than 64 characters
    /// (Unicode codepoints), can only contain lowercase letters, numeric
    /// characters, underscores and dashes. International characters are allowed.
    /// No more than 64 user labels can be associated with one TensorboardRun
    /// (System labels are excluded).
    ///
    /// See <https://goo.gl/xmQnxf> for more information and examples of labels.
    /// System reserved label keys are prefixed with "aiplatform.googleapis.com/"
    /// and are immutable.
    #[prost(map = "string, string", tag = "8")]
    pub labels: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
    /// Used to perform a consistent read-modify-write updates. If not set, a blind
    /// "overwrite" update happens.
    #[prost(string, tag = "9")]
    pub etag: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.CreateTensorboard][google.cloud.aiplatform.v1.TensorboardService.CreateTensorboard].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTensorboardRequest {
    /// Required. The resource name of the Location to create the Tensorboard in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The Tensorboard to create.
    #[prost(message, optional, tag = "2")]
    pub tensorboard: ::core::option::Option<Tensorboard>,
}
/// Request message for
/// [TensorboardService.GetTensorboard][google.cloud.aiplatform.v1.TensorboardService.GetTensorboard].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetTensorboardRequest {
    /// Required. The name of the Tensorboard resource.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.ListTensorboards][google.cloud.aiplatform.v1.TensorboardService.ListTensorboards].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTensorboardsRequest {
    /// Required. The resource name of the Location to list Tensorboards.
    /// Format:
    /// `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the Tensorboards that match the filter expression.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The maximum number of Tensorboards to return. The service may return
    /// fewer than this value. If unspecified, at most 100 Tensorboards are
    /// returned. The maximum value is 100; values above 100 are coerced to
    /// 100.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [TensorboardService.ListTensorboards][google.cloud.aiplatform.v1.TensorboardService.ListTensorboards]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [TensorboardService.ListTensorboards][google.cloud.aiplatform.v1.TensorboardService.ListTensorboards]
    /// must match the call that provided the page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Field to use to sort the list.
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "6")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [TensorboardService.ListTensorboards][google.cloud.aiplatform.v1.TensorboardService.ListTensorboards].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTensorboardsResponse {
    /// The Tensorboards mathching the request.
    #[prost(message, repeated, tag = "1")]
    pub tensorboards: ::prost::alloc::vec::Vec<Tensorboard>,
    /// A token, which can be sent as
    /// [ListTensorboardsRequest.page_token][google.cloud.aiplatform.v1.ListTensorboardsRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.UpdateTensorboard][google.cloud.aiplatform.v1.TensorboardService.UpdateTensorboard].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateTensorboardRequest {
    /// Required. Field mask is used to specify the fields to be overwritten in the
    /// Tensorboard resource by the update.
    /// The fields specified in the update_mask are relative to the resource, not
    /// the full request. A field is overwritten if it's in the mask. If the
    /// user does not provide a mask then all fields are overwritten if new
    /// values are specified.
    #[prost(message, optional, tag = "1")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// Required. The Tensorboard's `name` field is used to identify the
    /// Tensorboard to be updated. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
    #[prost(message, optional, tag = "2")]
    pub tensorboard: ::core::option::Option<Tensorboard>,
}
/// Request message for
/// [TensorboardService.DeleteTensorboard][google.cloud.aiplatform.v1.TensorboardService.DeleteTensorboard].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteTensorboardRequest {
    /// Required. The name of the Tensorboard to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.ReadTensorboardUsage][google.cloud.aiplatform.v1.TensorboardService.ReadTensorboardUsage].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadTensorboardUsageRequest {
    /// Required. The name of the Tensorboard resource.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
    #[prost(string, tag = "1")]
    pub tensorboard: ::prost::alloc::string::String,
}
/// Response message for
/// [TensorboardService.ReadTensorboardUsage][google.cloud.aiplatform.v1.TensorboardService.ReadTensorboardUsage].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadTensorboardUsageResponse {
    /// Maps year-month (YYYYMM) string to per month usage data.
    #[prost(map = "string, message", tag = "1")]
    pub monthly_usage_data: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        read_tensorboard_usage_response::PerMonthUsageData,
    >,
}
/// Nested message and enum types in `ReadTensorboardUsageResponse`.
pub mod read_tensorboard_usage_response {
    /// Per user usage data.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct PerUserUsageData {
        /// User's username
        #[prost(string, tag = "1")]
        pub username: ::prost::alloc::string::String,
        /// Number of times the user has read data within the Tensorboard.
        #[prost(int64, tag = "2")]
        pub view_count: i64,
    }
    /// Per month usage data
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct PerMonthUsageData {
        /// Usage data for each user in the given month.
        #[prost(message, repeated, tag = "1")]
        pub user_usage_data: ::prost::alloc::vec::Vec<PerUserUsageData>,
    }
}
/// Request message for
/// [TensorboardService.ReadTensorboardSize][google.cloud.aiplatform.v1.TensorboardService.ReadTensorboardSize].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadTensorboardSizeRequest {
    /// Required. The name of the Tensorboard resource.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
    #[prost(string, tag = "1")]
    pub tensorboard: ::prost::alloc::string::String,
}
/// Response message for
/// [TensorboardService.ReadTensorboardSize][google.cloud.aiplatform.v1.TensorboardService.ReadTensorboardSize].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct ReadTensorboardSizeResponse {
    /// Payload storage size for the TensorBoard
    #[prost(int64, tag = "1")]
    pub storage_size_byte: i64,
}
/// Request message for
/// [TensorboardService.CreateTensorboardExperiment][google.cloud.aiplatform.v1.TensorboardService.CreateTensorboardExperiment].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTensorboardExperimentRequest {
    /// Required. The resource name of the Tensorboard to create the
    /// TensorboardExperiment in. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// The TensorboardExperiment to create.
    #[prost(message, optional, tag = "2")]
    pub tensorboard_experiment: ::core::option::Option<TensorboardExperiment>,
    /// Required. The ID to use for the Tensorboard experiment, which becomes the
    /// final component of the Tensorboard experiment's resource name.
    ///
    /// This value should be 1-128 characters, and valid characters
    /// are `/[a-z][0-9]-/`.
    #[prost(string, tag = "3")]
    pub tensorboard_experiment_id: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.GetTensorboardExperiment][google.cloud.aiplatform.v1.TensorboardService.GetTensorboardExperiment].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetTensorboardExperimentRequest {
    /// Required. The name of the TensorboardExperiment resource.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.ListTensorboardExperiments][google.cloud.aiplatform.v1.TensorboardService.ListTensorboardExperiments].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTensorboardExperimentsRequest {
    /// Required. The resource name of the Tensorboard to list
    /// TensorboardExperiments. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the TensorboardExperiments that match the filter expression.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The maximum number of TensorboardExperiments to return. The service may
    /// return fewer than this value. If unspecified, at most 50
    /// TensorboardExperiments are returned. The maximum value is 1000; values
    /// above 1000 are coerced to 1000.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [TensorboardService.ListTensorboardExperiments][google.cloud.aiplatform.v1.TensorboardService.ListTensorboardExperiments]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [TensorboardService.ListTensorboardExperiments][google.cloud.aiplatform.v1.TensorboardService.ListTensorboardExperiments]
    /// must match the call that provided the page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Field to use to sort the list.
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "6")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [TensorboardService.ListTensorboardExperiments][google.cloud.aiplatform.v1.TensorboardService.ListTensorboardExperiments].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTensorboardExperimentsResponse {
    /// The TensorboardExperiments mathching the request.
    #[prost(message, repeated, tag = "1")]
    pub tensorboard_experiments: ::prost::alloc::vec::Vec<TensorboardExperiment>,
    /// A token, which can be sent as
    /// [ListTensorboardExperimentsRequest.page_token][google.cloud.aiplatform.v1.ListTensorboardExperimentsRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.UpdateTensorboardExperiment][google.cloud.aiplatform.v1.TensorboardService.UpdateTensorboardExperiment].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateTensorboardExperimentRequest {
    /// Required. Field mask is used to specify the fields to be overwritten in the
    /// TensorboardExperiment resource by the update.
    /// The fields specified in the update_mask are relative to the resource, not
    /// the full request. A field is overwritten if it's in the mask. If the
    /// user does not provide a mask then all fields are overwritten if new
    /// values are specified.
    #[prost(message, optional, tag = "1")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// Required. The TensorboardExperiment's `name` field is used to identify the
    /// TensorboardExperiment to be updated. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}`
    #[prost(message, optional, tag = "2")]
    pub tensorboard_experiment: ::core::option::Option<TensorboardExperiment>,
}
/// Request message for
/// [TensorboardService.DeleteTensorboardExperiment][google.cloud.aiplatform.v1.TensorboardService.DeleteTensorboardExperiment].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteTensorboardExperimentRequest {
    /// Required. The name of the TensorboardExperiment to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.BatchCreateTensorboardRuns][google.cloud.aiplatform.v1.TensorboardService.BatchCreateTensorboardRuns].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchCreateTensorboardRunsRequest {
    /// Required. The resource name of the TensorboardExperiment to create the
    /// TensorboardRuns in. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}`
    /// The parent field in the CreateTensorboardRunRequest messages must match
    /// this field.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The request message specifying the TensorboardRuns to create.
    /// A maximum of 1000 TensorboardRuns can be created in a batch.
    #[prost(message, repeated, tag = "2")]
    pub requests: ::prost::alloc::vec::Vec<CreateTensorboardRunRequest>,
}
/// Response message for
/// [TensorboardService.BatchCreateTensorboardRuns][google.cloud.aiplatform.v1.TensorboardService.BatchCreateTensorboardRuns].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchCreateTensorboardRunsResponse {
    /// The created TensorboardRuns.
    #[prost(message, repeated, tag = "1")]
    pub tensorboard_runs: ::prost::alloc::vec::Vec<TensorboardRun>,
}
/// Request message for
/// [TensorboardService.CreateTensorboardRun][google.cloud.aiplatform.v1.TensorboardService.CreateTensorboardRun].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTensorboardRunRequest {
    /// Required. The resource name of the TensorboardExperiment to create the
    /// TensorboardRun in. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The TensorboardRun to create.
    #[prost(message, optional, tag = "2")]
    pub tensorboard_run: ::core::option::Option<TensorboardRun>,
    /// Required. The ID to use for the Tensorboard run, which becomes the final
    /// component of the Tensorboard run's resource name.
    ///
    /// This value should be 1-128 characters, and valid characters
    /// are `/[a-z][0-9]-/`.
    #[prost(string, tag = "3")]
    pub tensorboard_run_id: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.GetTensorboardRun][google.cloud.aiplatform.v1.TensorboardService.GetTensorboardRun].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetTensorboardRunRequest {
    /// Required. The name of the TensorboardRun resource.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.ReadTensorboardBlobData][google.cloud.aiplatform.v1.TensorboardService.ReadTensorboardBlobData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadTensorboardBlobDataRequest {
    /// Required. The resource name of the TensorboardTimeSeries to list Blobs.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}/timeSeries/{time_series}`
    #[prost(string, tag = "1")]
    pub time_series: ::prost::alloc::string::String,
    /// IDs of the blobs to read.
    #[prost(string, repeated, tag = "2")]
    pub blob_ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Response message for
/// [TensorboardService.ReadTensorboardBlobData][google.cloud.aiplatform.v1.TensorboardService.ReadTensorboardBlobData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadTensorboardBlobDataResponse {
    /// Blob messages containing blob bytes.
    #[prost(message, repeated, tag = "1")]
    pub blobs: ::prost::alloc::vec::Vec<TensorboardBlob>,
}
/// Request message for
/// [TensorboardService.ListTensorboardRuns][google.cloud.aiplatform.v1.TensorboardService.ListTensorboardRuns].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTensorboardRunsRequest {
    /// Required. The resource name of the TensorboardExperiment to list
    /// TensorboardRuns. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the TensorboardRuns that match the filter expression.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The maximum number of TensorboardRuns to return. The service may return
    /// fewer than this value. If unspecified, at most 50 TensorboardRuns are
    /// returned. The maximum value is 1000; values above 1000 are coerced to
    /// 1000.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [TensorboardService.ListTensorboardRuns][google.cloud.aiplatform.v1.TensorboardService.ListTensorboardRuns]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [TensorboardService.ListTensorboardRuns][google.cloud.aiplatform.v1.TensorboardService.ListTensorboardRuns]
    /// must match the call that provided the page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Field to use to sort the list.
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "6")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [TensorboardService.ListTensorboardRuns][google.cloud.aiplatform.v1.TensorboardService.ListTensorboardRuns].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTensorboardRunsResponse {
    /// The TensorboardRuns mathching the request.
    #[prost(message, repeated, tag = "1")]
    pub tensorboard_runs: ::prost::alloc::vec::Vec<TensorboardRun>,
    /// A token, which can be sent as
    /// [ListTensorboardRunsRequest.page_token][google.cloud.aiplatform.v1.ListTensorboardRunsRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.UpdateTensorboardRun][google.cloud.aiplatform.v1.TensorboardService.UpdateTensorboardRun].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateTensorboardRunRequest {
    /// Required. Field mask is used to specify the fields to be overwritten in the
    /// TensorboardRun resource by the update.
    /// The fields specified in the update_mask are relative to the resource, not
    /// the full request. A field is overwritten if it's in the mask. If the
    /// user does not provide a mask then all fields are overwritten if new
    /// values are specified.
    #[prost(message, optional, tag = "1")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// Required. The TensorboardRun's `name` field is used to identify the
    /// TensorboardRun to be updated. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}`
    #[prost(message, optional, tag = "2")]
    pub tensorboard_run: ::core::option::Option<TensorboardRun>,
}
/// Request message for
/// [TensorboardService.DeleteTensorboardRun][google.cloud.aiplatform.v1.TensorboardService.DeleteTensorboardRun].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteTensorboardRunRequest {
    /// Required. The name of the TensorboardRun to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.BatchCreateTensorboardTimeSeries][google.cloud.aiplatform.v1.TensorboardService.BatchCreateTensorboardTimeSeries].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchCreateTensorboardTimeSeriesRequest {
    /// Required. The resource name of the TensorboardExperiment to create the
    /// TensorboardTimeSeries in.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}`
    /// The TensorboardRuns referenced by the parent fields in the
    /// CreateTensorboardTimeSeriesRequest messages must be sub resources of this
    /// TensorboardExperiment.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The request message specifying the TensorboardTimeSeries to
    /// create. A maximum of 1000 TensorboardTimeSeries can be created in a batch.
    #[prost(message, repeated, tag = "2")]
    pub requests: ::prost::alloc::vec::Vec<CreateTensorboardTimeSeriesRequest>,
}
/// Response message for
/// [TensorboardService.BatchCreateTensorboardTimeSeries][google.cloud.aiplatform.v1.TensorboardService.BatchCreateTensorboardTimeSeries].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchCreateTensorboardTimeSeriesResponse {
    /// The created TensorboardTimeSeries.
    #[prost(message, repeated, tag = "1")]
    pub tensorboard_time_series: ::prost::alloc::vec::Vec<TensorboardTimeSeries>,
}
/// Request message for
/// [TensorboardService.CreateTensorboardTimeSeries][google.cloud.aiplatform.v1.TensorboardService.CreateTensorboardTimeSeries].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTensorboardTimeSeriesRequest {
    /// Required. The resource name of the TensorboardRun to create the
    /// TensorboardTimeSeries in.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. The user specified unique ID to use for the
    /// TensorboardTimeSeries, which becomes the final component of the
    /// TensorboardTimeSeries's resource name. This value should match
    /// "[a-z0-9][a-z0-9-]{0, 127}"
    #[prost(string, tag = "3")]
    pub tensorboard_time_series_id: ::prost::alloc::string::String,
    /// Required. The TensorboardTimeSeries to create.
    #[prost(message, optional, tag = "2")]
    pub tensorboard_time_series: ::core::option::Option<TensorboardTimeSeries>,
}
/// Request message for
/// [TensorboardService.GetTensorboardTimeSeries][google.cloud.aiplatform.v1.TensorboardService.GetTensorboardTimeSeries].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetTensorboardTimeSeriesRequest {
    /// Required. The name of the TensorboardTimeSeries resource.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}/timeSeries/{time_series}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.ListTensorboardTimeSeries][google.cloud.aiplatform.v1.TensorboardService.ListTensorboardTimeSeries].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTensorboardTimeSeriesRequest {
    /// Required. The resource name of the TensorboardRun to list
    /// TensorboardTimeSeries. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Lists the TensorboardTimeSeries that match the filter expression.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The maximum number of TensorboardTimeSeries to return. The service may
    /// return fewer than this value. If unspecified, at most 50
    /// TensorboardTimeSeries are returned. The maximum value is 1000; values
    /// above 1000 are coerced to 1000.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [TensorboardService.ListTensorboardTimeSeries][google.cloud.aiplatform.v1.TensorboardService.ListTensorboardTimeSeries]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [TensorboardService.ListTensorboardTimeSeries][google.cloud.aiplatform.v1.TensorboardService.ListTensorboardTimeSeries]
    /// must match the call that provided the page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Field to use to sort the list.
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
    /// Mask specifying which fields to read.
    #[prost(message, optional, tag = "6")]
    pub read_mask: ::core::option::Option<::prost_types::FieldMask>,
}
/// Response message for
/// [TensorboardService.ListTensorboardTimeSeries][google.cloud.aiplatform.v1.TensorboardService.ListTensorboardTimeSeries].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTensorboardTimeSeriesResponse {
    /// The TensorboardTimeSeries mathching the request.
    #[prost(message, repeated, tag = "1")]
    pub tensorboard_time_series: ::prost::alloc::vec::Vec<TensorboardTimeSeries>,
    /// A token, which can be sent as
    /// [ListTensorboardTimeSeriesRequest.page_token][google.cloud.aiplatform.v1.ListTensorboardTimeSeriesRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.UpdateTensorboardTimeSeries][google.cloud.aiplatform.v1.TensorboardService.UpdateTensorboardTimeSeries].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateTensorboardTimeSeriesRequest {
    /// Required. Field mask is used to specify the fields to be overwritten in the
    /// TensorboardTimeSeries resource by the update.
    /// The fields specified in the update_mask are relative to the resource, not
    /// the full request. A field is overwritten if it's in the mask. If the
    /// user does not provide a mask then all fields are overwritten if new
    /// values are specified.
    #[prost(message, optional, tag = "1")]
    pub update_mask: ::core::option::Option<::prost_types::FieldMask>,
    /// Required. The TensorboardTimeSeries' `name` field is used to identify the
    /// TensorboardTimeSeries to be updated.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}/timeSeries/{time_series}`
    #[prost(message, optional, tag = "2")]
    pub tensorboard_time_series: ::core::option::Option<TensorboardTimeSeries>,
}
/// Request message for
/// [TensorboardService.DeleteTensorboardTimeSeries][google.cloud.aiplatform.v1.TensorboardService.DeleteTensorboardTimeSeries].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteTensorboardTimeSeriesRequest {
    /// Required. The name of the TensorboardTimeSeries to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}/timeSeries/{time_series}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [TensorboardService.BatchReadTensorboardTimeSeriesData][google.cloud.aiplatform.v1.TensorboardService.BatchReadTensorboardTimeSeriesData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchReadTensorboardTimeSeriesDataRequest {
    /// Required. The resource name of the Tensorboard containing
    /// TensorboardTimeSeries to read data from. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}`.
    /// The TensorboardTimeSeries referenced by
    /// [time_series][google.cloud.aiplatform.v1.BatchReadTensorboardTimeSeriesDataRequest.time_series]
    /// must be sub resources of this Tensorboard.
    #[prost(string, tag = "1")]
    pub tensorboard: ::prost::alloc::string::String,
    /// Required. The resource names of the TensorboardTimeSeries to read data
    /// from. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}/timeSeries/{time_series}`
    #[prost(string, repeated, tag = "2")]
    pub time_series: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Response message for
/// [TensorboardService.BatchReadTensorboardTimeSeriesData][google.cloud.aiplatform.v1.TensorboardService.BatchReadTensorboardTimeSeriesData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchReadTensorboardTimeSeriesDataResponse {
    /// The returned time series data.
    #[prost(message, repeated, tag = "1")]
    pub time_series_data: ::prost::alloc::vec::Vec<TimeSeriesData>,
}
/// Request message for
/// [TensorboardService.ReadTensorboardTimeSeriesData][google.cloud.aiplatform.v1.TensorboardService.ReadTensorboardTimeSeriesData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadTensorboardTimeSeriesDataRequest {
    /// Required. The resource name of the TensorboardTimeSeries to read data from.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}/timeSeries/{time_series}`
    #[prost(string, tag = "1")]
    pub tensorboard_time_series: ::prost::alloc::string::String,
    /// The maximum number of TensorboardTimeSeries' data to return.
    ///
    /// This value should be a positive integer.
    /// This value can be set to -1 to return all data.
    #[prost(int32, tag = "2")]
    pub max_data_points: i32,
    /// Reads the TensorboardTimeSeries' data that match the filter expression.
    #[prost(string, tag = "3")]
    pub filter: ::prost::alloc::string::String,
}
/// Response message for
/// [TensorboardService.ReadTensorboardTimeSeriesData][google.cloud.aiplatform.v1.TensorboardService.ReadTensorboardTimeSeriesData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadTensorboardTimeSeriesDataResponse {
    /// The returned time series data.
    #[prost(message, optional, tag = "1")]
    pub time_series_data: ::core::option::Option<TimeSeriesData>,
}
/// Request message for
/// [TensorboardService.WriteTensorboardExperimentData][google.cloud.aiplatform.v1.TensorboardService.WriteTensorboardExperimentData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct WriteTensorboardExperimentDataRequest {
    /// Required. The resource name of the TensorboardExperiment to write data to.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}`
    #[prost(string, tag = "1")]
    pub tensorboard_experiment: ::prost::alloc::string::String,
    /// Required. Requests containing per-run TensorboardTimeSeries data to write.
    #[prost(message, repeated, tag = "2")]
    pub write_run_data_requests: ::prost::alloc::vec::Vec<
        WriteTensorboardRunDataRequest,
    >,
}
/// Response message for
/// [TensorboardService.WriteTensorboardExperimentData][google.cloud.aiplatform.v1.TensorboardService.WriteTensorboardExperimentData].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct WriteTensorboardExperimentDataResponse {}
/// Request message for
/// [TensorboardService.WriteTensorboardRunData][google.cloud.aiplatform.v1.TensorboardService.WriteTensorboardRunData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct WriteTensorboardRunDataRequest {
    /// Required. The resource name of the TensorboardRun to write data to.
    /// Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}`
    #[prost(string, tag = "1")]
    pub tensorboard_run: ::prost::alloc::string::String,
    /// Required. The TensorboardTimeSeries data to write.
    /// Values with in a time series are indexed by their step value.
    /// Repeated writes to the same step will overwrite the existing value for that
    /// step.
    /// The upper limit of data points per write request is 5000.
    #[prost(message, repeated, tag = "2")]
    pub time_series_data: ::prost::alloc::vec::Vec<TimeSeriesData>,
}
/// Response message for
/// [TensorboardService.WriteTensorboardRunData][google.cloud.aiplatform.v1.TensorboardService.WriteTensorboardRunData].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct WriteTensorboardRunDataResponse {}
/// Request message for
/// [TensorboardService.ExportTensorboardTimeSeriesData][google.cloud.aiplatform.v1.TensorboardService.ExportTensorboardTimeSeriesData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExportTensorboardTimeSeriesDataRequest {
    /// Required. The resource name of the TensorboardTimeSeries to export data
    /// from. Format:
    /// `projects/{project}/locations/{location}/tensorboards/{tensorboard}/experiments/{experiment}/runs/{run}/timeSeries/{time_series}`
    #[prost(string, tag = "1")]
    pub tensorboard_time_series: ::prost::alloc::string::String,
    /// Exports the TensorboardTimeSeries' data that match the filter expression.
    #[prost(string, tag = "2")]
    pub filter: ::prost::alloc::string::String,
    /// The maximum number of data points to return per page.
    /// The default page_size is 1000. Values must be between 1 and 10000.
    /// Values above 10000 are coerced to 10000.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
    /// A page token, received from a previous
    /// [ExportTensorboardTimeSeriesData][google.cloud.aiplatform.v1.TensorboardService.ExportTensorboardTimeSeriesData]
    /// call. Provide this to retrieve the subsequent page.
    ///
    /// When paginating, all other parameters provided to
    /// [ExportTensorboardTimeSeriesData][google.cloud.aiplatform.v1.TensorboardService.ExportTensorboardTimeSeriesData]
    /// must match the call that provided the page token.
    #[prost(string, tag = "4")]
    pub page_token: ::prost::alloc::string::String,
    /// Field to use to sort the TensorboardTimeSeries' data.
    /// By default, TensorboardTimeSeries' data is returned in a pseudo random
    /// order.
    #[prost(string, tag = "5")]
    pub order_by: ::prost::alloc::string::String,
}
/// Response message for
/// [TensorboardService.ExportTensorboardTimeSeriesData][google.cloud.aiplatform.v1.TensorboardService.ExportTensorboardTimeSeriesData].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ExportTensorboardTimeSeriesDataResponse {
    /// The returned time series data points.
    #[prost(message, repeated, tag = "1")]
    pub time_series_data_points: ::prost::alloc::vec::Vec<TimeSeriesDataPoint>,
    /// A token, which can be sent as
    /// [page_token][google.cloud.aiplatform.v1.ExportTensorboardTimeSeriesDataRequest.page_token]
    /// to retrieve the next page. If this field is omitted, there are no
    /// subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Details of operations that perform create Tensorboard.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTensorboardOperationMetadata {
    /// Operation metadata for Tensorboard.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Details of operations that perform update Tensorboard.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateTensorboardOperationMetadata {
    /// Operation metadata for Tensorboard.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Generated client implementations.
pub mod tensorboard_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// TensorboardService
    #[derive(Debug, Clone)]
    pub struct TensorboardServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl TensorboardServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> TensorboardServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> TensorboardServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            TensorboardServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a Tensorboard.
        pub async fn create_tensorboard(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateTensorboardRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/CreateTensorboard",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "CreateTensorboard",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a Tensorboard.
        pub async fn get_tensorboard(
            &mut self,
            request: impl tonic::IntoRequest<super::GetTensorboardRequest>,
        ) -> std::result::Result<tonic::Response<super::Tensorboard>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/GetTensorboard",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "GetTensorboard",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a Tensorboard.
        pub async fn update_tensorboard(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateTensorboardRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/UpdateTensorboard",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "UpdateTensorboard",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists Tensorboards in a Location.
        pub async fn list_tensorboards(
            &mut self,
            request: impl tonic::IntoRequest<super::ListTensorboardsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListTensorboardsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/ListTensorboards",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "ListTensorboards",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a Tensorboard.
        pub async fn delete_tensorboard(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteTensorboardRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/DeleteTensorboard",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "DeleteTensorboard",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Returns a list of monthly active users for a given TensorBoard instance.
        pub async fn read_tensorboard_usage(
            &mut self,
            request: impl tonic::IntoRequest<super::ReadTensorboardUsageRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ReadTensorboardUsageResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/ReadTensorboardUsage",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "ReadTensorboardUsage",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Returns the storage size for a given TensorBoard instance.
        pub async fn read_tensorboard_size(
            &mut self,
            request: impl tonic::IntoRequest<super::ReadTensorboardSizeRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ReadTensorboardSizeResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/ReadTensorboardSize",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "ReadTensorboardSize",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a TensorboardExperiment.
        pub async fn create_tensorboard_experiment(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateTensorboardExperimentRequest>,
        ) -> std::result::Result<
            tonic::Response<super::TensorboardExperiment>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/CreateTensorboardExperiment",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "CreateTensorboardExperiment",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a TensorboardExperiment.
        pub async fn get_tensorboard_experiment(
            &mut self,
            request: impl tonic::IntoRequest<super::GetTensorboardExperimentRequest>,
        ) -> std::result::Result<
            tonic::Response<super::TensorboardExperiment>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/GetTensorboardExperiment",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "GetTensorboardExperiment",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a TensorboardExperiment.
        pub async fn update_tensorboard_experiment(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateTensorboardExperimentRequest>,
        ) -> std::result::Result<
            tonic::Response<super::TensorboardExperiment>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/UpdateTensorboardExperiment",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "UpdateTensorboardExperiment",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists TensorboardExperiments in a Location.
        pub async fn list_tensorboard_experiments(
            &mut self,
            request: impl tonic::IntoRequest<super::ListTensorboardExperimentsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListTensorboardExperimentsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/ListTensorboardExperiments",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "ListTensorboardExperiments",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a TensorboardExperiment.
        pub async fn delete_tensorboard_experiment(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteTensorboardExperimentRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/DeleteTensorboardExperiment",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "DeleteTensorboardExperiment",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a TensorboardRun.
        pub async fn create_tensorboard_run(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateTensorboardRunRequest>,
        ) -> std::result::Result<tonic::Response<super::TensorboardRun>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/CreateTensorboardRun",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "CreateTensorboardRun",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Batch create TensorboardRuns.
        pub async fn batch_create_tensorboard_runs(
            &mut self,
            request: impl tonic::IntoRequest<super::BatchCreateTensorboardRunsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::BatchCreateTensorboardRunsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/BatchCreateTensorboardRuns",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "BatchCreateTensorboardRuns",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a TensorboardRun.
        pub async fn get_tensorboard_run(
            &mut self,
            request: impl tonic::IntoRequest<super::GetTensorboardRunRequest>,
        ) -> std::result::Result<tonic::Response<super::TensorboardRun>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/GetTensorboardRun",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "GetTensorboardRun",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a TensorboardRun.
        pub async fn update_tensorboard_run(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateTensorboardRunRequest>,
        ) -> std::result::Result<tonic::Response<super::TensorboardRun>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/UpdateTensorboardRun",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "UpdateTensorboardRun",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists TensorboardRuns in a Location.
        pub async fn list_tensorboard_runs(
            &mut self,
            request: impl tonic::IntoRequest<super::ListTensorboardRunsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListTensorboardRunsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/ListTensorboardRuns",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "ListTensorboardRuns",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a TensorboardRun.
        pub async fn delete_tensorboard_run(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteTensorboardRunRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/DeleteTensorboardRun",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "DeleteTensorboardRun",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Batch create TensorboardTimeSeries that belong to a TensorboardExperiment.
        pub async fn batch_create_tensorboard_time_series(
            &mut self,
            request: impl tonic::IntoRequest<
                super::BatchCreateTensorboardTimeSeriesRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::BatchCreateTensorboardTimeSeriesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/BatchCreateTensorboardTimeSeries",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "BatchCreateTensorboardTimeSeries",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Creates a TensorboardTimeSeries.
        pub async fn create_tensorboard_time_series(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateTensorboardTimeSeriesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::TensorboardTimeSeries>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/CreateTensorboardTimeSeries",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "CreateTensorboardTimeSeries",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a TensorboardTimeSeries.
        pub async fn get_tensorboard_time_series(
            &mut self,
            request: impl tonic::IntoRequest<super::GetTensorboardTimeSeriesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::TensorboardTimeSeries>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/GetTensorboardTimeSeries",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "GetTensorboardTimeSeries",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a TensorboardTimeSeries.
        pub async fn update_tensorboard_time_series(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateTensorboardTimeSeriesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::TensorboardTimeSeries>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/UpdateTensorboardTimeSeries",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "UpdateTensorboardTimeSeries",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists TensorboardTimeSeries in a Location.
        pub async fn list_tensorboard_time_series(
            &mut self,
            request: impl tonic::IntoRequest<super::ListTensorboardTimeSeriesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListTensorboardTimeSeriesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/ListTensorboardTimeSeries",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "ListTensorboardTimeSeries",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a TensorboardTimeSeries.
        pub async fn delete_tensorboard_time_series(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteTensorboardTimeSeriesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/DeleteTensorboardTimeSeries",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "DeleteTensorboardTimeSeries",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Reads multiple TensorboardTimeSeries' data. The data point number limit is
        /// 1000 for scalars, 100 for tensors and blob references. If the number of
        /// data points stored is less than the limit, all data is returned.
        /// Otherwise, the number limit of data points is randomly selected from
        /// this time series and returned.
        pub async fn batch_read_tensorboard_time_series_data(
            &mut self,
            request: impl tonic::IntoRequest<
                super::BatchReadTensorboardTimeSeriesDataRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::BatchReadTensorboardTimeSeriesDataResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/BatchReadTensorboardTimeSeriesData",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "BatchReadTensorboardTimeSeriesData",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Reads a TensorboardTimeSeries' data. By default, if the number of data
        /// points stored is less than 1000, all data is returned. Otherwise, 1000
        /// data points is randomly selected from this time series and returned.
        /// This value can be changed by changing max_data_points, which can't be
        /// greater than 10k.
        pub async fn read_tensorboard_time_series_data(
            &mut self,
            request: impl tonic::IntoRequest<super::ReadTensorboardTimeSeriesDataRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ReadTensorboardTimeSeriesDataResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/ReadTensorboardTimeSeriesData",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "ReadTensorboardTimeSeriesData",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets bytes of TensorboardBlobs.
        /// This is to allow reading blob data stored in consumer project's Cloud
        /// Storage bucket without users having to obtain Cloud Storage access
        /// permission.
        pub async fn read_tensorboard_blob_data(
            &mut self,
            request: impl tonic::IntoRequest<super::ReadTensorboardBlobDataRequest>,
        ) -> std::result::Result<
            tonic::Response<
                tonic::codec::Streaming<super::ReadTensorboardBlobDataResponse>,
            >,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/ReadTensorboardBlobData",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "ReadTensorboardBlobData",
                    ),
                );
            self.inner.server_streaming(req, path, codec).await
        }
        /// Write time series data points of multiple TensorboardTimeSeries in multiple
        /// TensorboardRun's. If any data fail to be ingested, an error is returned.
        pub async fn write_tensorboard_experiment_data(
            &mut self,
            request: impl tonic::IntoRequest<
                super::WriteTensorboardExperimentDataRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::WriteTensorboardExperimentDataResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/WriteTensorboardExperimentData",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "WriteTensorboardExperimentData",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Write time series data points into multiple TensorboardTimeSeries under
        /// a TensorboardRun. If any data fail to be ingested, an error is returned.
        pub async fn write_tensorboard_run_data(
            &mut self,
            request: impl tonic::IntoRequest<super::WriteTensorboardRunDataRequest>,
        ) -> std::result::Result<
            tonic::Response<super::WriteTensorboardRunDataResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/WriteTensorboardRunData",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "WriteTensorboardRunData",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Exports a TensorboardTimeSeries' data. Data is returned in paginated
        /// responses.
        pub async fn export_tensorboard_time_series_data(
            &mut self,
            request: impl tonic::IntoRequest<
                super::ExportTensorboardTimeSeriesDataRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<super::ExportTensorboardTimeSeriesDataResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.TensorboardService/ExportTensorboardTimeSeriesData",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.TensorboardService",
                        "ExportTensorboardTimeSeriesData",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Config for the embedding model to use for RAG.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RagEmbeddingModelConfig {
    /// The model config to use.
    #[prost(oneof = "rag_embedding_model_config::ModelConfig", tags = "1")]
    pub model_config: ::core::option::Option<rag_embedding_model_config::ModelConfig>,
}
/// Nested message and enum types in `RagEmbeddingModelConfig`.
pub mod rag_embedding_model_config {
    /// Config representing a model hosted on Vertex Prediction Endpoint.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct VertexPredictionEndpoint {
        /// Required. The endpoint resource name.
        /// Format:
        /// `projects/{project}/locations/{location}/publishers/{publisher}/models/{model}`
        /// or
        /// `projects/{project}/locations/{location}/endpoints/{endpoint}`
        #[prost(string, tag = "1")]
        pub endpoint: ::prost::alloc::string::String,
        /// Output only. The resource name of the model that is deployed on the
        /// endpoint. Present only when the endpoint is not a publisher model.
        /// Pattern:
        /// `projects/{project}/locations/{location}/models/{model}`
        #[prost(string, tag = "2")]
        pub model: ::prost::alloc::string::String,
        /// Output only. Version ID of the model that is deployed on the endpoint.
        /// Present only when the endpoint is not a publisher model.
        #[prost(string, tag = "3")]
        pub model_version_id: ::prost::alloc::string::String,
    }
    /// The model config to use.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum ModelConfig {
        /// The Vertex AI Prediction Endpoint that either refers to a publisher model
        /// or an endpoint that is hosting a 1P fine-tuned text embedding model.
        /// Endpoints hosting non-1P fine-tuned text embedding models are
        /// currently not supported.
        /// This is used for dense vector search.
        #[prost(message, tag = "1")]
        VertexPredictionEndpoint(VertexPredictionEndpoint),
    }
}
/// Config for the Vector DB to use for RAG.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RagVectorDbConfig {
    /// Authentication config for the chosen Vector DB.
    #[prost(message, optional, tag = "5")]
    pub api_auth: ::core::option::Option<ApiAuth>,
    /// Optional. Immutable. The embedding model config of the Vector DB.
    #[prost(message, optional, tag = "7")]
    pub rag_embedding_model_config: ::core::option::Option<RagEmbeddingModelConfig>,
    /// The config for the Vector DB.
    #[prost(oneof = "rag_vector_db_config::VectorDb", tags = "1, 3, 6")]
    pub vector_db: ::core::option::Option<rag_vector_db_config::VectorDb>,
}
/// Nested message and enum types in `RagVectorDbConfig`.
pub mod rag_vector_db_config {
    /// The config for the default RAG-managed Vector DB.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct RagManagedDb {}
    /// The config for the Pinecone.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Pinecone {
        /// Pinecone index name.
        /// This value cannot be changed after it's set.
        #[prost(string, tag = "1")]
        pub index_name: ::prost::alloc::string::String,
    }
    /// The config for the Vertex Vector Search.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct VertexVectorSearch {
        /// The resource name of the Index Endpoint.
        /// Format:
        /// `projects/{project}/locations/{location}/indexEndpoints/{index_endpoint}`
        #[prost(string, tag = "1")]
        pub index_endpoint: ::prost::alloc::string::String,
        /// The resource name of the Index.
        /// Format:
        /// `projects/{project}/locations/{location}/indexes/{index}`
        #[prost(string, tag = "2")]
        pub index: ::prost::alloc::string::String,
    }
    /// The config for the Vector DB.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum VectorDb {
        /// The config for the RAG-managed Vector DB.
        #[prost(message, tag = "1")]
        RagManagedDb(RagManagedDb),
        /// The config for the Pinecone.
        #[prost(message, tag = "3")]
        Pinecone(Pinecone),
        /// The config for the Vertex Vector Search.
        #[prost(message, tag = "6")]
        VertexVectorSearch(VertexVectorSearch),
    }
}
/// RagFile status.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FileStatus {
    /// Output only. RagFile state.
    #[prost(enumeration = "file_status::State", tag = "1")]
    pub state: i32,
    /// Output only. Only when the `state` field is ERROR.
    #[prost(string, tag = "2")]
    pub error_status: ::prost::alloc::string::String,
}
/// Nested message and enum types in `FileStatus`.
pub mod file_status {
    /// RagFile state.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// RagFile state is unspecified.
        Unspecified = 0,
        /// RagFile resource has been created and indexed successfully.
        Active = 1,
        /// RagFile resource is in a problematic state.
        /// See `error_message` field for details.
        Error = 2,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "STATE_UNSPECIFIED",
                Self::Active => "ACTIVE",
                Self::Error => "ERROR",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "STATE_UNSPECIFIED" => Some(Self::Unspecified),
                "ACTIVE" => Some(Self::Active),
                "ERROR" => Some(Self::Error),
                _ => None,
            }
        }
    }
}
/// RagCorpus status.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CorpusStatus {
    /// Output only. RagCorpus life state.
    #[prost(enumeration = "corpus_status::State", tag = "1")]
    pub state: i32,
    /// Output only. Only when the `state` field is ERROR.
    #[prost(string, tag = "2")]
    pub error_status: ::prost::alloc::string::String,
}
/// Nested message and enum types in `CorpusStatus`.
pub mod corpus_status {
    /// RagCorpus life state.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum State {
        /// This state is not supposed to happen.
        Unknown = 0,
        /// RagCorpus resource entry is initialized, but hasn't done validation.
        Initialized = 1,
        /// RagCorpus is provisioned successfully and is ready to serve.
        Active = 2,
        /// RagCorpus is in a problematic situation.
        /// See `error_message` field for details.
        Error = 3,
    }
    impl State {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unknown => "UNKNOWN",
                Self::Initialized => "INITIALIZED",
                Self::Active => "ACTIVE",
                Self::Error => "ERROR",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "UNKNOWN" => Some(Self::Unknown),
                "INITIALIZED" => Some(Self::Initialized),
                "ACTIVE" => Some(Self::Active),
                "ERROR" => Some(Self::Error),
                _ => None,
            }
        }
    }
}
/// A RagCorpus is a RagFile container and a project can have multiple
/// RagCorpora.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RagCorpus {
    /// Output only. The resource name of the RagCorpus.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The display name of the RagCorpus.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Optional. The description of the RagCorpus.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// Output only. Timestamp when this RagCorpus was created.
    #[prost(message, optional, tag = "4")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this RagCorpus was last updated.
    #[prost(message, optional, tag = "5")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. RagCorpus state.
    #[prost(message, optional, tag = "8")]
    pub corpus_status: ::core::option::Option<CorpusStatus>,
    /// The backend config of the RagCorpus.
    /// It can be data store and/or retrieval engine.
    #[prost(oneof = "rag_corpus::BackendConfig", tags = "9")]
    pub backend_config: ::core::option::Option<rag_corpus::BackendConfig>,
}
/// Nested message and enum types in `RagCorpus`.
pub mod rag_corpus {
    /// The backend config of the RagCorpus.
    /// It can be data store and/or retrieval engine.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum BackendConfig {
        /// Optional. Immutable. The config for the Vector DBs.
        #[prost(message, tag = "9")]
        VectorDbConfig(super::RagVectorDbConfig),
    }
}
/// A RagFile contains user data for chunking, embedding and indexing.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RagFile {
    /// Output only. The resource name of the RagFile.
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The display name of the RagFile.
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
    /// Optional. The description of the RagFile.
    #[prost(string, tag = "3")]
    pub description: ::prost::alloc::string::String,
    /// Output only. Timestamp when this RagFile was created.
    #[prost(message, optional, tag = "6")]
    pub create_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. Timestamp when this RagFile was last updated.
    #[prost(message, optional, tag = "7")]
    pub update_time: ::core::option::Option<::prost_types::Timestamp>,
    /// Output only. State of the RagFile.
    #[prost(message, optional, tag = "13")]
    pub file_status: ::core::option::Option<FileStatus>,
    /// The origin location of the RagFile if it is imported from Google Cloud
    /// Storage or Google Drive.
    #[prost(oneof = "rag_file::RagFileSource", tags = "8, 9, 10, 11, 12, 14")]
    pub rag_file_source: ::core::option::Option<rag_file::RagFileSource>,
}
/// Nested message and enum types in `RagFile`.
pub mod rag_file {
    /// The origin location of the RagFile if it is imported from Google Cloud
    /// Storage or Google Drive.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum RagFileSource {
        /// Output only. Google Cloud Storage location of the RagFile.
        /// It does not support wildcards in the Cloud Storage uri for now.
        #[prost(message, tag = "8")]
        GcsSource(super::GcsSource),
        /// Output only. Google Drive location. Supports importing individual files
        /// as well as Google Drive folders.
        #[prost(message, tag = "9")]
        GoogleDriveSource(super::GoogleDriveSource),
        /// Output only. The RagFile is encapsulated and uploaded in the
        /// UploadRagFile request.
        #[prost(message, tag = "10")]
        DirectUploadSource(super::DirectUploadSource),
        /// The RagFile is imported from a Slack channel.
        #[prost(message, tag = "11")]
        SlackSource(super::SlackSource),
        /// The RagFile is imported from a Jira query.
        #[prost(message, tag = "12")]
        JiraSource(super::JiraSource),
        /// The RagFile is imported from a SharePoint source.
        #[prost(message, tag = "14")]
        SharePointSources(super::SharePointSources),
    }
}
/// Specifies the size and overlap of chunks for RagFiles.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RagFileChunkingConfig {
    /// Specifies the chunking config for RagFiles.
    #[prost(oneof = "rag_file_chunking_config::ChunkingConfig", tags = "3")]
    pub chunking_config: ::core::option::Option<
        rag_file_chunking_config::ChunkingConfig,
    >,
}
/// Nested message and enum types in `RagFileChunkingConfig`.
pub mod rag_file_chunking_config {
    /// Specifies the fixed length chunking config.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct FixedLengthChunking {
        /// The size of the chunks.
        #[prost(int32, tag = "1")]
        pub chunk_size: i32,
        /// The overlap between chunks.
        #[prost(int32, tag = "2")]
        pub chunk_overlap: i32,
    }
    /// Specifies the chunking config for RagFiles.
    #[derive(Clone, Copy, PartialEq, ::prost::Oneof)]
    pub enum ChunkingConfig {
        /// Specifies the fixed length chunking config.
        #[prost(message, tag = "3")]
        FixedLengthChunking(FixedLengthChunking),
    }
}
/// Specifies the transformation config for RagFiles.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RagFileTransformationConfig {
    /// Specifies the chunking config for RagFiles.
    #[prost(message, optional, tag = "1")]
    pub rag_file_chunking_config: ::core::option::Option<RagFileChunkingConfig>,
}
/// Config for uploading RagFile.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct UploadRagFileConfig {
    /// Specifies the transformation config for RagFiles.
    #[prost(message, optional, tag = "3")]
    pub rag_file_transformation_config: ::core::option::Option<
        RagFileTransformationConfig,
    >,
}
/// Config for importing RagFiles.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ImportRagFilesConfig {
    /// Specifies the transformation config for RagFiles.
    #[prost(message, optional, tag = "16")]
    pub rag_file_transformation_config: ::core::option::Option<
        RagFileTransformationConfig,
    >,
    /// Optional. The max number of queries per minute that this job is allowed to
    /// make to the embedding model specified on the corpus. This value is specific
    /// to this job and not shared across other import jobs. Consult the Quotas
    /// page on the project to set an appropriate value here.
    /// If unspecified, a default value of 1,000 QPM would be used.
    #[prost(int32, tag = "5")]
    pub max_embedding_requests_per_min: i32,
    /// The source of the import.
    #[prost(oneof = "import_rag_files_config::ImportSource", tags = "2, 3, 6, 7, 13")]
    pub import_source: ::core::option::Option<import_rag_files_config::ImportSource>,
    /// Optional. If provided, all partial failures are written to the sink.
    /// Deprecated. Prefer to use the `import_result_sink`.
    #[prost(oneof = "import_rag_files_config::PartialFailureSink", tags = "11, 12")]
    pub partial_failure_sink: ::core::option::Option<
        import_rag_files_config::PartialFailureSink,
    >,
}
/// Nested message and enum types in `ImportRagFilesConfig`.
pub mod import_rag_files_config {
    /// The source of the import.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum ImportSource {
        /// Google Cloud Storage location. Supports importing individual files as
        /// well as entire Google Cloud Storage directories. Sample formats:
        /// - `gs://bucket_name/my_directory/object_name/my_file.txt`
        /// - `gs://bucket_name/my_directory`
        #[prost(message, tag = "2")]
        GcsSource(super::GcsSource),
        /// Google Drive location. Supports importing individual files as
        /// well as Google Drive folders.
        #[prost(message, tag = "3")]
        GoogleDriveSource(super::GoogleDriveSource),
        /// Slack channels with their corresponding access tokens.
        #[prost(message, tag = "6")]
        SlackSource(super::SlackSource),
        /// Jira queries with their corresponding authentication.
        #[prost(message, tag = "7")]
        JiraSource(super::JiraSource),
        /// SharePoint sources.
        #[prost(message, tag = "13")]
        SharePointSources(super::SharePointSources),
    }
    /// Optional. If provided, all partial failures are written to the sink.
    /// Deprecated. Prefer to use the `import_result_sink`.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum PartialFailureSink {
        /// The Cloud Storage path to write partial failures to.
        /// Deprecated. Prefer to use `import_result_gcs_sink`.
        #[prost(message, tag = "11")]
        PartialFailureGcsSink(super::GcsDestination),
        /// The BigQuery destination to write partial failures to. It should be a
        /// bigquery table resource name (e.g.
        /// "bq://projectId.bqDatasetId.bqTableId"). The dataset must exist. If the
        /// table does not exist, it will be created with the expected schema. If the
        /// table exists, the schema will be validated and data will be added to this
        /// existing table.
        /// Deprecated. Prefer to use `import_result_bq_sink`.
        #[prost(message, tag = "12")]
        PartialFailureBigquerySink(super::BigQueryDestination),
    }
}
/// Request message for
/// [VertexRagDataService.CreateRagCorpus][google.cloud.aiplatform.v1.VertexRagDataService.CreateRagCorpus].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateRagCorpusRequest {
    /// Required. The resource name of the Location to create the RagCorpus in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The RagCorpus to create.
    #[prost(message, optional, tag = "2")]
    pub rag_corpus: ::core::option::Option<RagCorpus>,
}
/// Request message for
/// [VertexRagDataService.GetRagCorpus][google.cloud.aiplatform.v1.VertexRagDataService.GetRagCorpus]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetRagCorpusRequest {
    /// Required. The name of the RagCorpus resource.
    /// Format:
    /// `projects/{project}/locations/{location}/ragCorpora/{rag_corpus}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [VertexRagDataService.ListRagCorpora][google.cloud.aiplatform.v1.VertexRagDataService.ListRagCorpora].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListRagCorporaRequest {
    /// Required. The resource name of the Location from which to list the
    /// RagCorpora. Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. The standard list page size.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// Optional. The standard list page token.
    /// Typically obtained via
    /// [ListRagCorporaResponse.next_page_token][google.cloud.aiplatform.v1.ListRagCorporaResponse.next_page_token]
    /// of the previous
    /// [VertexRagDataService.ListRagCorpora][google.cloud.aiplatform.v1.VertexRagDataService.ListRagCorpora]
    /// call.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response message for
/// [VertexRagDataService.ListRagCorpora][google.cloud.aiplatform.v1.VertexRagDataService.ListRagCorpora].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListRagCorporaResponse {
    /// List of RagCorpora in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub rag_corpora: ::prost::alloc::vec::Vec<RagCorpus>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListRagCorporaRequest.page_token][google.cloud.aiplatform.v1.ListRagCorporaRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [VertexRagDataService.DeleteRagCorpus][google.cloud.aiplatform.v1.VertexRagDataService.DeleteRagCorpus].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteRagCorpusRequest {
    /// Required. The name of the RagCorpus resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/ragCorpora/{rag_corpus}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. If set to true, any RagFiles in this RagCorpus will also be
    /// deleted. Otherwise, the request will only work if the RagCorpus has no
    /// RagFiles.
    #[prost(bool, tag = "2")]
    pub force: bool,
}
/// Request message for
/// [VertexRagDataService.UploadRagFile][google.cloud.aiplatform.v1.VertexRagDataService.UploadRagFile].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UploadRagFileRequest {
    /// Required. The name of the RagCorpus resource into which to upload the file.
    /// Format:
    /// `projects/{project}/locations/{location}/ragCorpora/{rag_corpus}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The RagFile to upload.
    #[prost(message, optional, tag = "2")]
    pub rag_file: ::core::option::Option<RagFile>,
    /// Required. The config for the RagFiles to be uploaded into the RagCorpus.
    /// [VertexRagDataService.UploadRagFile][google.cloud.aiplatform.v1.VertexRagDataService.UploadRagFile].
    #[prost(message, optional, tag = "5")]
    pub upload_rag_file_config: ::core::option::Option<UploadRagFileConfig>,
}
/// Response message for
/// [VertexRagDataService.UploadRagFile][google.cloud.aiplatform.v1.VertexRagDataService.UploadRagFile].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UploadRagFileResponse {
    /// The result of the upload.
    #[prost(oneof = "upload_rag_file_response::Result", tags = "1, 4")]
    pub result: ::core::option::Option<upload_rag_file_response::Result>,
}
/// Nested message and enum types in `UploadRagFileResponse`.
pub mod upload_rag_file_response {
    /// The result of the upload.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Result {
        /// The RagFile that had been uploaded into the RagCorpus.
        #[prost(message, tag = "1")]
        RagFile(super::RagFile),
        /// The error that occurred while processing the RagFile.
        #[prost(message, tag = "4")]
        Error(super::super::super::super::rpc::Status),
    }
}
/// Request message for
/// [VertexRagDataService.ImportRagFiles][google.cloud.aiplatform.v1.VertexRagDataService.ImportRagFiles].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ImportRagFilesRequest {
    /// Required. The name of the RagCorpus resource into which to import files.
    /// Format:
    /// `projects/{project}/locations/{location}/ragCorpora/{rag_corpus}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The config for the RagFiles to be synced and imported into the
    /// RagCorpus.
    /// [VertexRagDataService.ImportRagFiles][google.cloud.aiplatform.v1.VertexRagDataService.ImportRagFiles].
    #[prost(message, optional, tag = "2")]
    pub import_rag_files_config: ::core::option::Option<ImportRagFilesConfig>,
}
/// Response message for
/// [VertexRagDataService.ImportRagFiles][google.cloud.aiplatform.v1.VertexRagDataService.ImportRagFiles].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ImportRagFilesResponse {
    /// The number of RagFiles that had been imported into the RagCorpus.
    #[prost(int64, tag = "1")]
    pub imported_rag_files_count: i64,
    /// The number of RagFiles that had failed while importing into the RagCorpus.
    #[prost(int64, tag = "2")]
    pub failed_rag_files_count: i64,
    /// The number of RagFiles that was skipped while importing into the RagCorpus.
    #[prost(int64, tag = "3")]
    pub skipped_rag_files_count: i64,
    /// The location into which the partial failures were written.
    #[prost(oneof = "import_rag_files_response::PartialFailureSink", tags = "4, 5")]
    pub partial_failure_sink: ::core::option::Option<
        import_rag_files_response::PartialFailureSink,
    >,
}
/// Nested message and enum types in `ImportRagFilesResponse`.
pub mod import_rag_files_response {
    /// The location into which the partial failures were written.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum PartialFailureSink {
        /// The Google Cloud Storage path into which the partial failures were
        /// written.
        #[prost(string, tag = "4")]
        PartialFailuresGcsPath(::prost::alloc::string::String),
        /// The BigQuery table into which the partial failures were written.
        #[prost(string, tag = "5")]
        PartialFailuresBigqueryTable(::prost::alloc::string::String),
    }
}
/// Request message for
/// [VertexRagDataService.GetRagFile][google.cloud.aiplatform.v1.VertexRagDataService.GetRagFile]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetRagFileRequest {
    /// Required. The name of the RagFile resource.
    /// Format:
    /// `projects/{project}/locations/{location}/ragCorpora/{rag_corpus}/ragFiles/{rag_file}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [VertexRagDataService.ListRagFiles][google.cloud.aiplatform.v1.VertexRagDataService.ListRagFiles].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListRagFilesRequest {
    /// Required. The resource name of the RagCorpus from which to list the
    /// RagFiles. Format:
    /// `projects/{project}/locations/{location}/ragCorpora/{rag_corpus}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. The standard list page size.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// Optional. The standard list page token.
    /// Typically obtained via
    /// [ListRagFilesResponse.next_page_token][google.cloud.aiplatform.v1.ListRagFilesResponse.next_page_token]
    /// of the previous
    /// [VertexRagDataService.ListRagFiles][google.cloud.aiplatform.v1.VertexRagDataService.ListRagFiles]
    /// call.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response message for
/// [VertexRagDataService.ListRagFiles][google.cloud.aiplatform.v1.VertexRagDataService.ListRagFiles].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListRagFilesResponse {
    /// List of RagFiles in the requested page.
    #[prost(message, repeated, tag = "1")]
    pub rag_files: ::prost::alloc::vec::Vec<RagFile>,
    /// A token to retrieve the next page of results.
    /// Pass to
    /// [ListRagFilesRequest.page_token][google.cloud.aiplatform.v1.ListRagFilesRequest.page_token]
    /// to obtain that page.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [VertexRagDataService.DeleteRagFile][google.cloud.aiplatform.v1.VertexRagDataService.DeleteRagFile].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteRagFileRequest {
    /// Required. The name of the RagFile resource to be deleted.
    /// Format:
    /// `projects/{project}/locations/{location}/ragCorpora/{rag_corpus}/ragFiles/{rag_file}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Runtime operation information for
/// [VertexRagDataService.CreateRagCorpus][google.cloud.aiplatform.v1.VertexRagDataService.CreateRagCorpus].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateRagCorpusOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Request message for
/// [VertexRagDataService.UpdateRagCorpus][google.cloud.aiplatform.v1.VertexRagDataService.UpdateRagCorpus].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateRagCorpusRequest {
    /// Required. The RagCorpus which replaces the resource on the server.
    #[prost(message, optional, tag = "1")]
    pub rag_corpus: ::core::option::Option<RagCorpus>,
}
/// Runtime operation information for
/// [VertexRagDataService.UpdateRagCorpus][google.cloud.aiplatform.v1.VertexRagDataService.UpdateRagCorpus].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct UpdateRagCorpusOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
}
/// Runtime operation information for
/// [VertexRagDataService.ImportRagFiles][google.cloud.aiplatform.v1.VertexRagDataService.ImportRagFiles].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ImportRagFilesOperationMetadata {
    /// The operation generic information.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// The resource ID of RagCorpus that this operation is executed on.
    #[prost(int64, tag = "2")]
    pub rag_corpus_id: i64,
    /// Output only. The config that was passed in the ImportRagFilesRequest.
    #[prost(message, optional, tag = "3")]
    pub import_rag_files_config: ::core::option::Option<ImportRagFilesConfig>,
    /// The progress percentage of the operation. Value is in the range \[0, 100\].
    /// This percentage is calculated as follows:
    ///     progress_percentage = 100 * (successes + failures + skips) / total
    #[prost(int32, tag = "4")]
    pub progress_percentage: i32,
}
/// Generated client implementations.
pub mod vertex_rag_data_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for managing user data for RAG.
    #[derive(Debug, Clone)]
    pub struct VertexRagDataServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl VertexRagDataServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> VertexRagDataServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> VertexRagDataServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            VertexRagDataServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a RagCorpus.
        pub async fn create_rag_corpus(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateRagCorpusRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagDataService/CreateRagCorpus",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagDataService",
                        "CreateRagCorpus",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Updates a RagCorpus.
        pub async fn update_rag_corpus(
            &mut self,
            request: impl tonic::IntoRequest<super::UpdateRagCorpusRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagDataService/UpdateRagCorpus",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagDataService",
                        "UpdateRagCorpus",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a RagCorpus.
        pub async fn get_rag_corpus(
            &mut self,
            request: impl tonic::IntoRequest<super::GetRagCorpusRequest>,
        ) -> std::result::Result<tonic::Response<super::RagCorpus>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagDataService/GetRagCorpus",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagDataService",
                        "GetRagCorpus",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists RagCorpora in a Location.
        pub async fn list_rag_corpora(
            &mut self,
            request: impl tonic::IntoRequest<super::ListRagCorporaRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListRagCorporaResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagDataService/ListRagCorpora",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagDataService",
                        "ListRagCorpora",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a RagCorpus.
        pub async fn delete_rag_corpus(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteRagCorpusRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagDataService/DeleteRagCorpus",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagDataService",
                        "DeleteRagCorpus",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Upload a file into a RagCorpus.
        pub async fn upload_rag_file(
            &mut self,
            request: impl tonic::IntoRequest<super::UploadRagFileRequest>,
        ) -> std::result::Result<
            tonic::Response<super::UploadRagFileResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagDataService/UploadRagFile",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagDataService",
                        "UploadRagFile",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Import files from Google Cloud Storage or Google Drive into a RagCorpus.
        pub async fn import_rag_files(
            &mut self,
            request: impl tonic::IntoRequest<super::ImportRagFilesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagDataService/ImportRagFiles",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagDataService",
                        "ImportRagFiles",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a RagFile.
        pub async fn get_rag_file(
            &mut self,
            request: impl tonic::IntoRequest<super::GetRagFileRequest>,
        ) -> std::result::Result<tonic::Response<super::RagFile>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagDataService/GetRagFile",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagDataService",
                        "GetRagFile",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists RagFiles in a RagCorpus.
        pub async fn list_rag_files(
            &mut self,
            request: impl tonic::IntoRequest<super::ListRagFilesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListRagFilesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagDataService/ListRagFiles",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagDataService",
                        "ListRagFiles",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a RagFile.
        pub async fn delete_rag_file(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteRagFileRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagDataService/DeleteRagFile",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagDataService",
                        "DeleteRagFile",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// A query to retrieve relevant contexts.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RagQuery {
    /// Optional. The retrieval config for the query.
    #[prost(message, optional, tag = "6")]
    pub rag_retrieval_config: ::core::option::Option<RagRetrievalConfig>,
    /// The query to retrieve contexts.
    /// Currently only text query is supported.
    #[prost(oneof = "rag_query::Query", tags = "1")]
    pub query: ::core::option::Option<rag_query::Query>,
}
/// Nested message and enum types in `RagQuery`.
pub mod rag_query {
    /// The query to retrieve contexts.
    /// Currently only text query is supported.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Query {
        /// Optional. The query in text format to get relevant contexts.
        #[prost(string, tag = "1")]
        Text(::prost::alloc::string::String),
    }
}
/// Request message for
/// [VertexRagService.RetrieveContexts][google.cloud.aiplatform.v1.VertexRagService.RetrieveContexts].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RetrieveContextsRequest {
    /// Required. The resource name of the Location from which to retrieve
    /// RagContexts. The users must have permission to make a call in the project.
    /// Format:
    /// `projects/{project}/locations/{location}`.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. Single RAG retrieve query.
    #[prost(message, optional, tag = "3")]
    pub query: ::core::option::Option<RagQuery>,
    /// Data Source to retrieve contexts.
    #[prost(oneof = "retrieve_contexts_request::DataSource", tags = "2")]
    pub data_source: ::core::option::Option<retrieve_contexts_request::DataSource>,
}
/// Nested message and enum types in `RetrieveContextsRequest`.
pub mod retrieve_contexts_request {
    /// The data source for Vertex RagStore.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct VertexRagStore {
        /// Optional. The representation of the rag source. It can be used to specify
        /// corpus only or ragfiles. Currently only support one corpus or multiple
        /// files from one corpus. In the future we may open up multiple corpora
        /// support.
        #[prost(message, repeated, tag = "3")]
        pub rag_resources: ::prost::alloc::vec::Vec<vertex_rag_store::RagResource>,
        /// Optional. Only return contexts with vector distance smaller than the
        /// threshold.
        #[deprecated]
        #[prost(double, optional, tag = "2")]
        pub vector_distance_threshold: ::core::option::Option<f64>,
    }
    /// Nested message and enum types in `VertexRagStore`.
    pub mod vertex_rag_store {
        /// The definition of the Rag resource.
        #[derive(Clone, PartialEq, ::prost::Message)]
        pub struct RagResource {
            /// Optional. RagCorpora resource name.
            /// Format:
            /// `projects/{project}/locations/{location}/ragCorpora/{rag_corpus}`
            #[prost(string, tag = "1")]
            pub rag_corpus: ::prost::alloc::string::String,
            /// Optional. rag_file_id. The files should be in the same rag_corpus set
            /// in rag_corpus field.
            #[prost(string, repeated, tag = "2")]
            pub rag_file_ids: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
        }
    }
    /// Data Source to retrieve contexts.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum DataSource {
        /// The data source for Vertex RagStore.
        #[prost(message, tag = "2")]
        VertexRagStore(VertexRagStore),
    }
}
/// Relevant contexts for one query.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RagContexts {
    /// All its contexts.
    #[prost(message, repeated, tag = "1")]
    pub contexts: ::prost::alloc::vec::Vec<rag_contexts::Context>,
}
/// Nested message and enum types in `RagContexts`.
pub mod rag_contexts {
    /// A context of the query.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Context {
        /// If the file is imported from Cloud Storage or Google Drive, source_uri
        /// will be original file URI in Cloud Storage or Google Drive; if file is
        /// uploaded, source_uri will be file display name.
        #[prost(string, tag = "1")]
        pub source_uri: ::prost::alloc::string::String,
        /// The file display name.
        #[prost(string, tag = "5")]
        pub source_display_name: ::prost::alloc::string::String,
        /// The text chunk.
        #[prost(string, tag = "2")]
        pub text: ::prost::alloc::string::String,
        /// According to the underlying Vector DB and the selected metric type, the
        /// score can be either the distance or the similarity between the query and
        /// the context and its range depends on the metric type.
        ///
        /// For example, if the metric type is COSINE_DISTANCE, it represents the
        /// distance between the query and the context. The larger the distance, the
        /// less relevant the context is to the query. The range is \[0, 2\], while 0
        /// means the most relevant and 2 means the least relevant.
        #[prost(double, optional, tag = "6")]
        pub score: ::core::option::Option<f64>,
    }
}
/// Response message for
/// [VertexRagService.RetrieveContexts][google.cloud.aiplatform.v1.VertexRagService.RetrieveContexts].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct RetrieveContextsResponse {
    /// The contexts of the query.
    #[prost(message, optional, tag = "1")]
    pub contexts: ::core::option::Option<RagContexts>,
}
/// Request message for AugmentPrompt.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AugmentPromptRequest {
    /// Required. The resource name of the Location from which to augment prompt.
    /// The users must have permission to make a call in the project.
    /// Format:
    /// `projects/{project}/locations/{location}`.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. Input content to augment, only text format is supported for now.
    #[prost(message, repeated, tag = "2")]
    pub contents: ::prost::alloc::vec::Vec<Content>,
    /// Optional. Metadata of the backend deployed model.
    #[prost(message, optional, tag = "3")]
    pub model: ::core::option::Option<augment_prompt_request::Model>,
    /// The data source for retrieving contexts.
    #[prost(oneof = "augment_prompt_request::DataSource", tags = "4")]
    pub data_source: ::core::option::Option<augment_prompt_request::DataSource>,
}
/// Nested message and enum types in `AugmentPromptRequest`.
pub mod augment_prompt_request {
    /// Metadata of the backend deployed model.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Model {
        /// Optional. The model that the user will send the augmented prompt for
        /// content generation.
        #[prost(string, tag = "1")]
        pub model: ::prost::alloc::string::String,
        /// Optional. The model version of the backend deployed model.
        #[prost(string, tag = "2")]
        pub model_version: ::prost::alloc::string::String,
    }
    /// The data source for retrieving contexts.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum DataSource {
        /// Optional. Retrieves contexts from the Vertex RagStore.
        #[prost(message, tag = "4")]
        VertexRagStore(super::VertexRagStore),
    }
}
/// Response message for AugmentPrompt.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AugmentPromptResponse {
    /// Augmented prompt, only text format is supported for now.
    #[prost(message, repeated, tag = "1")]
    pub augmented_prompt: ::prost::alloc::vec::Vec<Content>,
    /// Retrieved facts from RAG data sources.
    #[prost(message, repeated, tag = "2")]
    pub facts: ::prost::alloc::vec::Vec<Fact>,
}
/// Request message for CorroborateContent.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CorroborateContentRequest {
    /// Required. The resource name of the Location from which to corroborate text.
    /// The users must have permission to make a call in the project.
    /// Format:
    /// `projects/{project}/locations/{location}`.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. Input content to corroborate, only text format is supported for
    /// now.
    #[prost(message, optional, tag = "2")]
    pub content: ::core::option::Option<Content>,
    /// Optional. Facts used to generate the text can also be used to corroborate
    /// the text.
    #[prost(message, repeated, tag = "3")]
    pub facts: ::prost::alloc::vec::Vec<Fact>,
    /// Optional. Parameters that can be set to override default settings per
    /// request.
    #[prost(message, optional, tag = "4")]
    pub parameters: ::core::option::Option<corroborate_content_request::Parameters>,
}
/// Nested message and enum types in `CorroborateContentRequest`.
pub mod corroborate_content_request {
    /// Parameters that can be overrided per request.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct Parameters {
        /// Optional. Only return claims with citation score larger than the
        /// threshold.
        #[prost(double, tag = "1")]
        pub citation_threshold: f64,
    }
}
/// Response message for CorroborateContent.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CorroborateContentResponse {
    /// Confidence score of corroborating content. Value is \[0,1\] with 1 is the
    /// most confidence.
    #[prost(float, optional, tag = "1")]
    pub corroboration_score: ::core::option::Option<f32>,
    /// Claims that are extracted from the input content and facts that support the
    /// claims.
    #[prost(message, repeated, tag = "2")]
    pub claims: ::prost::alloc::vec::Vec<Claim>,
}
/// The fact used in grounding.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Fact {
    /// Query that is used to retrieve this fact.
    #[prost(string, optional, tag = "1")]
    pub query: ::core::option::Option<::prost::alloc::string::String>,
    /// If present, it refers to the title of this fact.
    #[prost(string, optional, tag = "2")]
    pub title: ::core::option::Option<::prost::alloc::string::String>,
    /// If present, this uri links to the source of the fact.
    #[prost(string, optional, tag = "3")]
    pub uri: ::core::option::Option<::prost::alloc::string::String>,
    /// If present, the summary/snippet of the fact.
    #[prost(string, optional, tag = "4")]
    pub summary: ::core::option::Option<::prost::alloc::string::String>,
    /// If present, the distance between the query vector and this fact vector.
    #[deprecated]
    #[prost(double, optional, tag = "5")]
    pub vector_distance: ::core::option::Option<f64>,
    /// If present, according to the underlying Vector DB and the selected metric
    /// type, the score can be either the distance or the similarity between the
    /// query and the fact and its range depends on the metric type.
    ///
    /// For example, if the metric type is COSINE_DISTANCE, it represents the
    /// distance between the query and the fact. The larger the distance, the less
    /// relevant the fact is to the query. The range is \[0, 2\], while 0 means the
    /// most relevant and 2 means the least relevant.
    #[prost(double, optional, tag = "6")]
    pub score: ::core::option::Option<f64>,
}
/// Claim that is extracted from the input text and facts that support it.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Claim {
    /// Index in the input text where the claim starts (inclusive).
    #[prost(int32, optional, tag = "1")]
    pub start_index: ::core::option::Option<i32>,
    /// Index in the input text where the claim ends (exclusive).
    #[prost(int32, optional, tag = "2")]
    pub end_index: ::core::option::Option<i32>,
    /// Indexes of the facts supporting this claim.
    #[prost(int32, repeated, tag = "3")]
    pub fact_indexes: ::prost::alloc::vec::Vec<i32>,
    /// Confidence score of this corroboration.
    #[prost(float, optional, tag = "4")]
    pub score: ::core::option::Option<f32>,
}
/// Generated client implementations.
pub mod vertex_rag_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// A service for retrieving relevant contexts.
    #[derive(Debug, Clone)]
    pub struct VertexRagServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl VertexRagServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> VertexRagServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> VertexRagServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            VertexRagServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Retrieves relevant contexts for a query.
        pub async fn retrieve_contexts(
            &mut self,
            request: impl tonic::IntoRequest<super::RetrieveContextsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::RetrieveContextsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagService/RetrieveContexts",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagService",
                        "RetrieveContexts",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Given an input prompt, it returns augmented prompt from vertex rag store
        ///  to guide LLM towards generating grounded responses.
        pub async fn augment_prompt(
            &mut self,
            request: impl tonic::IntoRequest<super::AugmentPromptRequest>,
        ) -> std::result::Result<
            tonic::Response<super::AugmentPromptResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagService/AugmentPrompt",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagService",
                        "AugmentPrompt",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Given an input text, it returns a score that evaluates the factuality of
        /// the text. It also extracts and returns claims from the text and provides
        /// supporting facts.
        pub async fn corroborate_content(
            &mut self,
            request: impl tonic::IntoRequest<super::CorroborateContentRequest>,
        ) -> std::result::Result<
            tonic::Response<super::CorroborateContentResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VertexRagService/CorroborateContent",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VertexRagService",
                        "CorroborateContent",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Request message for
/// [VizierService.GetStudy][google.cloud.aiplatform.v1.VizierService.GetStudy].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetStudyRequest {
    /// Required. The name of the Study resource.
    /// Format: `projects/{project}/locations/{location}/studies/{study}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [VizierService.CreateStudy][google.cloud.aiplatform.v1.VizierService.CreateStudy].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateStudyRequest {
    /// Required. The resource name of the Location to create the CustomJob in.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The Study configuration used to create the Study.
    #[prost(message, optional, tag = "2")]
    pub study: ::core::option::Option<Study>,
}
/// Request message for
/// [VizierService.ListStudies][google.cloud.aiplatform.v1.VizierService.ListStudies].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListStudiesRequest {
    /// Required. The resource name of the Location to list the Study from.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. A page token to request the next page of results.
    /// If unspecified, there are no subsequent pages.
    #[prost(string, tag = "2")]
    pub page_token: ::prost::alloc::string::String,
    /// Optional. The maximum number of studies to return per "page" of results.
    /// If unspecified, service will pick an appropriate default.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
}
/// Response message for
/// [VizierService.ListStudies][google.cloud.aiplatform.v1.VizierService.ListStudies].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListStudiesResponse {
    /// The studies associated with the project.
    #[prost(message, repeated, tag = "1")]
    pub studies: ::prost::alloc::vec::Vec<Study>,
    /// Passes this token as the `page_token` field of the request for a
    /// subsequent call.
    /// If this field is omitted, there are no subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [VizierService.DeleteStudy][google.cloud.aiplatform.v1.VizierService.DeleteStudy].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteStudyRequest {
    /// Required. The name of the Study resource to be deleted.
    /// Format: `projects/{project}/locations/{location}/studies/{study}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [VizierService.LookupStudy][google.cloud.aiplatform.v1.VizierService.LookupStudy].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct LookupStudyRequest {
    /// Required. The resource name of the Location to get the Study from.
    /// Format: `projects/{project}/locations/{location}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The user-defined display name of the Study
    #[prost(string, tag = "2")]
    pub display_name: ::prost::alloc::string::String,
}
/// Request message for
/// [VizierService.SuggestTrials][google.cloud.aiplatform.v1.VizierService.SuggestTrials].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SuggestTrialsRequest {
    /// Required. The project and location that the Study belongs to.
    /// Format: `projects/{project}/locations/{location}/studies/{study}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The number of suggestions requested. It must be positive.
    #[prost(int32, tag = "2")]
    pub suggestion_count: i32,
    /// Required. The identifier of the client that is requesting the suggestion.
    ///
    /// If multiple SuggestTrialsRequests have the same `client_id`,
    /// the service will return the identical suggested Trial if the Trial is
    /// pending, and provide a new Trial if the last suggested Trial was completed.
    #[prost(string, tag = "3")]
    pub client_id: ::prost::alloc::string::String,
    /// Optional. This allows you to specify the "context" for a Trial; a context
    /// is a slice (a subspace) of the search space.
    ///
    /// Typical uses for contexts:
    /// 1) You are using Vizier to tune a server for best performance, but there's
    ///    a strong weekly cycle.  The context specifies the day-of-week.
    ///    This allows Tuesday to generalize from Wednesday without assuming that
    ///    everything is identical.
    /// 2) Imagine you're optimizing some medical treatment for people.
    ///    As they walk in the door, you know certain facts about them
    ///    (e.g. sex, weight, height, blood-pressure).  Put that information in the
    ///    context, and Vizier will adapt its suggestions to the patient.
    /// 3) You want to do a fair A/B test efficiently.  Specify the "A" and "B"
    ///    conditions as contexts, and Vizier will generalize between "A" and "B"
    ///    conditions.  If they are similar, this will allow Vizier to converge
    ///    to the optimum faster than if "A" and "B" were separate Studies.
    ///    NOTE: You can also enter contexts as REQUESTED Trials, e.g. via the
    ///    CreateTrial() RPC; that's the asynchronous option where you don't need a
    ///    close association between contexts and suggestions.
    ///
    /// NOTE: All the Parameters you set in a context MUST be defined in the
    ///    Study.
    /// NOTE: You must supply 0 or $suggestion_count contexts.
    ///    If you don't supply any contexts, Vizier will make suggestions
    ///    from the full search space specified in the StudySpec; if you supply
    ///    a full set of context, each suggestion will match the corresponding
    ///    context.
    /// NOTE: A Context with no features set matches anything, and allows
    ///    suggestions from the full search space.
    /// NOTE: Contexts MUST lie within the search space specified in the
    ///    StudySpec.  It's an error if they don't.
    /// NOTE: Contexts preferentially match ACTIVE then REQUESTED trials before
    ///    new suggestions are generated.
    /// NOTE: Generation of suggestions involves a match between a Context and
    ///    (optionally) a REQUESTED trial; if that match is not fully specified, a
    ///    suggestion will be geneated in the merged subspace.
    #[prost(message, repeated, tag = "4")]
    pub contexts: ::prost::alloc::vec::Vec<TrialContext>,
}
/// Response message for
/// [VizierService.SuggestTrials][google.cloud.aiplatform.v1.VizierService.SuggestTrials].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SuggestTrialsResponse {
    /// A list of Trials.
    #[prost(message, repeated, tag = "1")]
    pub trials: ::prost::alloc::vec::Vec<Trial>,
    /// The state of the Study.
    #[prost(enumeration = "study::State", tag = "2")]
    pub study_state: i32,
    /// The time at which the operation was started.
    #[prost(message, optional, tag = "3")]
    pub start_time: ::core::option::Option<::prost_types::Timestamp>,
    /// The time at which operation processing completed.
    #[prost(message, optional, tag = "4")]
    pub end_time: ::core::option::Option<::prost_types::Timestamp>,
}
/// Details of operations that perform Trials suggestion.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SuggestTrialsMetadata {
    /// Operation metadata for suggesting Trials.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// The identifier of the client that is requesting the suggestion.
    ///
    /// If multiple SuggestTrialsRequests have the same `client_id`,
    /// the service will return the identical suggested Trial if the Trial is
    /// pending, and provide a new Trial if the last suggested Trial was completed.
    #[prost(string, tag = "2")]
    pub client_id: ::prost::alloc::string::String,
}
/// Request message for
/// [VizierService.CreateTrial][google.cloud.aiplatform.v1.VizierService.CreateTrial].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CreateTrialRequest {
    /// Required. The resource name of the Study to create the Trial in.
    /// Format: `projects/{project}/locations/{location}/studies/{study}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Required. The Trial to create.
    #[prost(message, optional, tag = "2")]
    pub trial: ::core::option::Option<Trial>,
}
/// Request message for
/// [VizierService.GetTrial][google.cloud.aiplatform.v1.VizierService.GetTrial].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetTrialRequest {
    /// Required. The name of the Trial resource.
    /// Format:
    /// `projects/{project}/locations/{location}/studies/{study}/trials/{trial}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [VizierService.ListTrials][google.cloud.aiplatform.v1.VizierService.ListTrials].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTrialsRequest {
    /// Required. The resource name of the Study to list the Trial from.
    /// Format: `projects/{project}/locations/{location}/studies/{study}`
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
    /// Optional. A page token to request the next page of results.
    /// If unspecified, there are no subsequent pages.
    #[prost(string, tag = "2")]
    pub page_token: ::prost::alloc::string::String,
    /// Optional. The number of Trials to retrieve per "page" of results.
    /// If unspecified, the service will pick an appropriate default.
    #[prost(int32, tag = "3")]
    pub page_size: i32,
}
/// Response message for
/// [VizierService.ListTrials][google.cloud.aiplatform.v1.VizierService.ListTrials].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListTrialsResponse {
    /// The Trials associated with the Study.
    #[prost(message, repeated, tag = "1")]
    pub trials: ::prost::alloc::vec::Vec<Trial>,
    /// Pass this token as the `page_token` field of the request for a
    /// subsequent call.
    /// If this field is omitted, there are no subsequent pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// [VizierService.AddTrialMeasurement][google.cloud.aiplatform.v1.VizierService.AddTrialMeasurement].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct AddTrialMeasurementRequest {
    /// Required. The name of the trial to add measurement.
    /// Format:
    /// `projects/{project}/locations/{location}/studies/{study}/trials/{trial}`
    #[prost(string, tag = "1")]
    pub trial_name: ::prost::alloc::string::String,
    /// Required. The measurement to be added to a Trial.
    #[prost(message, optional, tag = "3")]
    pub measurement: ::core::option::Option<Measurement>,
}
/// Request message for
/// [VizierService.CompleteTrial][google.cloud.aiplatform.v1.VizierService.CompleteTrial].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CompleteTrialRequest {
    /// Required. The Trial's name.
    /// Format:
    /// `projects/{project}/locations/{location}/studies/{study}/trials/{trial}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Optional. If provided, it will be used as the completed Trial's
    /// final_measurement; Otherwise, the service will auto-select a
    /// previously reported measurement as the final-measurement
    #[prost(message, optional, tag = "2")]
    pub final_measurement: ::core::option::Option<Measurement>,
    /// Optional. True if the Trial cannot be run with the given Parameter, and
    /// final_measurement will be ignored.
    #[prost(bool, tag = "3")]
    pub trial_infeasible: bool,
    /// Optional. A human readable reason why the trial was infeasible. This should
    /// only be provided if `trial_infeasible` is true.
    #[prost(string, tag = "4")]
    pub infeasible_reason: ::prost::alloc::string::String,
}
/// Request message for
/// [VizierService.DeleteTrial][google.cloud.aiplatform.v1.VizierService.DeleteTrial].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct DeleteTrialRequest {
    /// Required. The Trial's name.
    /// Format:
    /// `projects/{project}/locations/{location}/studies/{study}/trials/{trial}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [VizierService.CheckTrialEarlyStoppingState][google.cloud.aiplatform.v1.VizierService.CheckTrialEarlyStoppingState].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CheckTrialEarlyStoppingStateRequest {
    /// Required. The Trial's name.
    /// Format:
    /// `projects/{project}/locations/{location}/studies/{study}/trials/{trial}`
    #[prost(string, tag = "1")]
    pub trial_name: ::prost::alloc::string::String,
}
/// Response message for
/// [VizierService.CheckTrialEarlyStoppingState][google.cloud.aiplatform.v1.VizierService.CheckTrialEarlyStoppingState].
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct CheckTrialEarlyStoppingStateResponse {
    /// True if the Trial should stop.
    #[prost(bool, tag = "1")]
    pub should_stop: bool,
}
/// This message will be placed in the metadata field of a
/// google.longrunning.Operation associated with a CheckTrialEarlyStoppingState
/// request.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CheckTrialEarlyStoppingStateMetatdata {
    /// Operation metadata for suggesting Trials.
    #[prost(message, optional, tag = "1")]
    pub generic_metadata: ::core::option::Option<GenericOperationMetadata>,
    /// The name of the Study that the Trial belongs to.
    #[prost(string, tag = "2")]
    pub study: ::prost::alloc::string::String,
    /// The Trial name.
    #[prost(string, tag = "3")]
    pub trial: ::prost::alloc::string::String,
}
/// Request message for
/// [VizierService.StopTrial][google.cloud.aiplatform.v1.VizierService.StopTrial].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct StopTrialRequest {
    /// Required. The Trial's name.
    /// Format:
    /// `projects/{project}/locations/{location}/studies/{study}/trials/{trial}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request message for
/// [VizierService.ListOptimalTrials][google.cloud.aiplatform.v1.VizierService.ListOptimalTrials].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListOptimalTrialsRequest {
    /// Required. The name of the Study that the optimal Trial belongs to.
    #[prost(string, tag = "1")]
    pub parent: ::prost::alloc::string::String,
}
/// Response message for
/// [VizierService.ListOptimalTrials][google.cloud.aiplatform.v1.VizierService.ListOptimalTrials].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListOptimalTrialsResponse {
    /// The pareto-optimal Trials for multiple objective Study or the
    /// optimal trial for single objective Study. The definition of
    /// pareto-optimal can be checked in wiki page.
    /// <https://en.wikipedia.org/wiki/Pareto_efficiency>
    #[prost(message, repeated, tag = "1")]
    pub optimal_trials: ::prost::alloc::vec::Vec<Trial>,
}
/// Generated client implementations.
pub mod vizier_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// Vertex AI Vizier API.
    ///
    /// Vertex AI Vizier is a service to solve blackbox optimization problems,
    /// such as tuning machine learning hyperparameters and searching over deep
    /// learning architectures.
    #[derive(Debug, Clone)]
    pub struct VizierServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl VizierServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> VizierServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> VizierServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            VizierServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Creates a Study. A resource name will be generated after creation of the
        /// Study.
        pub async fn create_study(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateStudyRequest>,
        ) -> std::result::Result<tonic::Response<super::Study>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/CreateStudy",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "CreateStudy",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a Study by name.
        pub async fn get_study(
            &mut self,
            request: impl tonic::IntoRequest<super::GetStudyRequest>,
        ) -> std::result::Result<tonic::Response<super::Study>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/GetStudy",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "GetStudy",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists all the studies in a region for an associated project.
        pub async fn list_studies(
            &mut self,
            request: impl tonic::IntoRequest<super::ListStudiesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListStudiesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/ListStudies",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "ListStudies",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a Study.
        pub async fn delete_study(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteStudyRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/DeleteStudy",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "DeleteStudy",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Looks a study up using the user-defined display_name field instead of the
        /// fully qualified resource name.
        pub async fn lookup_study(
            &mut self,
            request: impl tonic::IntoRequest<super::LookupStudyRequest>,
        ) -> std::result::Result<tonic::Response<super::Study>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/LookupStudy",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "LookupStudy",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Adds one or more Trials to a Study, with parameter values
        /// suggested by Vertex AI Vizier. Returns a long-running
        /// operation associated with the generation of Trial suggestions.
        /// When this long-running operation succeeds, it will contain
        /// a
        /// [SuggestTrialsResponse][google.cloud.aiplatform.v1.SuggestTrialsResponse].
        pub async fn suggest_trials(
            &mut self,
            request: impl tonic::IntoRequest<super::SuggestTrialsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/SuggestTrials",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "SuggestTrials",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Adds a user provided Trial to a Study.
        pub async fn create_trial(
            &mut self,
            request: impl tonic::IntoRequest<super::CreateTrialRequest>,
        ) -> std::result::Result<tonic::Response<super::Trial>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/CreateTrial",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "CreateTrial",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Gets a Trial.
        pub async fn get_trial(
            &mut self,
            request: impl tonic::IntoRequest<super::GetTrialRequest>,
        ) -> std::result::Result<tonic::Response<super::Trial>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/GetTrial",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "GetTrial",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists the Trials associated with a Study.
        pub async fn list_trials(
            &mut self,
            request: impl tonic::IntoRequest<super::ListTrialsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListTrialsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/ListTrials",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "ListTrials",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Adds a measurement of the objective metrics to a Trial. This measurement
        /// is assumed to have been taken before the Trial is complete.
        pub async fn add_trial_measurement(
            &mut self,
            request: impl tonic::IntoRequest<super::AddTrialMeasurementRequest>,
        ) -> std::result::Result<tonic::Response<super::Trial>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/AddTrialMeasurement",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "AddTrialMeasurement",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Marks a Trial as complete.
        pub async fn complete_trial(
            &mut self,
            request: impl tonic::IntoRequest<super::CompleteTrialRequest>,
        ) -> std::result::Result<tonic::Response<super::Trial>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/CompleteTrial",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "CompleteTrial",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Deletes a Trial.
        pub async fn delete_trial(
            &mut self,
            request: impl tonic::IntoRequest<super::DeleteTrialRequest>,
        ) -> std::result::Result<tonic::Response<()>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/DeleteTrial",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "DeleteTrial",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Checks  whether a Trial should stop or not. Returns a
        /// long-running operation. When the operation is successful,
        /// it will contain a
        /// [CheckTrialEarlyStoppingStateResponse][google.cloud.aiplatform.v1.CheckTrialEarlyStoppingStateResponse].
        pub async fn check_trial_early_stopping_state(
            &mut self,
            request: impl tonic::IntoRequest<super::CheckTrialEarlyStoppingStateRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::super::super::longrunning::Operation>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/CheckTrialEarlyStoppingState",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "CheckTrialEarlyStoppingState",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Stops a Trial.
        pub async fn stop_trial(
            &mut self,
            request: impl tonic::IntoRequest<super::StopTrialRequest>,
        ) -> std::result::Result<tonic::Response<super::Trial>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/StopTrial",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "StopTrial",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists the pareto-optimal Trials for multi-objective Study or the
        /// optimal Trials for single-objective Study. The definition of
        /// pareto-optimal can be checked in wiki page.
        /// https://en.wikipedia.org/wiki/Pareto_efficiency
        pub async fn list_optimal_trials(
            &mut self,
            request: impl tonic::IntoRequest<super::ListOptimalTrialsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListOptimalTrialsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.aiplatform.v1.VizierService/ListOptimalTrials",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.aiplatform.v1.VizierService",
                        "ListOptimalTrials",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
