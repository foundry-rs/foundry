// This file is @generated by prost-build.
/// A collection of source attributions for a piece of content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CitationMetadata {
    /// Citations to sources for a specific response.
    #[prost(message, repeated, tag = "1")]
    pub citation_sources: ::prost::alloc::vec::Vec<CitationSource>,
}
/// A citation to a source for a portion of a specific response.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CitationSource {
    /// Optional. Start of segment of the response that is attributed to this
    /// source.
    ///
    /// Index indicates the start of the segment, measured in bytes.
    #[prost(int32, optional, tag = "1")]
    pub start_index: ::core::option::Option<i32>,
    /// Optional. End of the attributed segment, exclusive.
    #[prost(int32, optional, tag = "2")]
    pub end_index: ::core::option::Option<i32>,
    /// Optional. URI that is attributed as a source for a portion of the text.
    #[prost(string, optional, tag = "3")]
    pub uri: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. License for the GitHub project that is attributed as a source for
    /// segment.
    ///
    /// License info is required for code citations.
    #[prost(string, optional, tag = "4")]
    pub license: ::core::option::Option<::prost::alloc::string::String>,
}
/// The base structured datatype containing multi-part content of a message.
///
/// A `Content` includes a `role` field designating the producer of the `Content`
/// and a `parts` field containing multi-part data that contains the content of
/// the message turn.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Content {
    /// Ordered `Parts` that constitute a single message. Parts may have different
    /// MIME types.
    #[prost(message, repeated, tag = "1")]
    pub parts: ::prost::alloc::vec::Vec<Part>,
    /// Optional. The producer of the content. Must be either 'user' or 'model'.
    ///
    /// Useful to set for multi-turn conversations, otherwise can be left blank
    /// or unset.
    #[prost(string, tag = "2")]
    pub role: ::prost::alloc::string::String,
}
/// A datatype containing media that is part of a multi-part `Content` message.
///
/// A `Part` consists of data which has an associated datatype. A `Part` can only
/// contain one of the accepted types in `Part.data`.
///
/// A `Part` must have a fixed IANA MIME type identifying the type and subtype
/// of the media if the `inline_data` field is filled with raw bytes.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Part {
    #[prost(oneof = "part::Data", tags = "2, 3")]
    pub data: ::core::option::Option<part::Data>,
}
/// Nested message and enum types in `Part`.
pub mod part {
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum Data {
        /// Inline text.
        #[prost(string, tag = "2")]
        Text(::prost::alloc::string::String),
        /// Inline media bytes.
        #[prost(message, tag = "3")]
        InlineData(super::Blob),
    }
}
/// Raw media bytes.
///
/// Text should not be sent as raw bytes, use the 'text' field.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Blob {
    /// The IANA standard MIME type of the source data.
    /// Examples:
    ///    - image/png
    ///    - image/jpeg
    /// If an unsupported MIME type is provided, an error will be returned. For a
    /// complete list of supported types, see [Supported file
    /// formats](<https://ai.google.dev/gemini-api/docs/prompting_with_media#supported_file_formats>).
    #[prost(string, tag = "1")]
    pub mime_type: ::prost::alloc::string::String,
    /// Raw bytes for media formats.
    #[prost(bytes = "vec", tag = "2")]
    pub data: ::prost::alloc::vec::Vec<u8>,
}
/// Safety rating for a piece of content.
///
/// The safety rating contains the category of harm and the
/// harm probability level in that category for a piece of content.
/// Content is classified for safety across a number of
/// harm categories and the probability of the harm classification is included
/// here.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetyRating {
    /// Required. The category for this rating.
    #[prost(enumeration = "HarmCategory", tag = "3")]
    pub category: i32,
    /// Required. The probability of harm for this content.
    #[prost(enumeration = "safety_rating::HarmProbability", tag = "4")]
    pub probability: i32,
    /// Was this content blocked because of this rating?
    #[prost(bool, tag = "5")]
    pub blocked: bool,
}
/// Nested message and enum types in `SafetyRating`.
pub mod safety_rating {
    /// The probability that a piece of content is harmful.
    ///
    /// The classification system gives the probability of the content being
    /// unsafe. This does not indicate the severity of harm for a piece of content.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum HarmProbability {
        /// Probability is unspecified.
        Unspecified = 0,
        /// Content has a negligible chance of being unsafe.
        Negligible = 1,
        /// Content has a low chance of being unsafe.
        Low = 2,
        /// Content has a medium chance of being unsafe.
        Medium = 3,
        /// Content has a high chance of being unsafe.
        High = 4,
    }
    impl HarmProbability {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "HARM_PROBABILITY_UNSPECIFIED",
                Self::Negligible => "NEGLIGIBLE",
                Self::Low => "LOW",
                Self::Medium => "MEDIUM",
                Self::High => "HIGH",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "HARM_PROBABILITY_UNSPECIFIED" => Some(Self::Unspecified),
                "NEGLIGIBLE" => Some(Self::Negligible),
                "LOW" => Some(Self::Low),
                "MEDIUM" => Some(Self::Medium),
                "HIGH" => Some(Self::High),
                _ => None,
            }
        }
    }
}
/// Safety setting, affecting the safety-blocking behavior.
///
/// Passing a safety setting for a category changes the allowed probability that
/// content is blocked.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct SafetySetting {
    /// Required. The category for this setting.
    #[prost(enumeration = "HarmCategory", tag = "3")]
    pub category: i32,
    /// Required. Controls the probability threshold at which harm is blocked.
    #[prost(enumeration = "safety_setting::HarmBlockThreshold", tag = "4")]
    pub threshold: i32,
}
/// Nested message and enum types in `SafetySetting`.
pub mod safety_setting {
    /// Block at and beyond a specified harm probability.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum HarmBlockThreshold {
        /// Threshold is unspecified.
        Unspecified = 0,
        /// Content with NEGLIGIBLE will be allowed.
        BlockLowAndAbove = 1,
        /// Content with NEGLIGIBLE and LOW will be allowed.
        BlockMediumAndAbove = 2,
        /// Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.
        BlockOnlyHigh = 3,
        /// All content will be allowed.
        BlockNone = 4,
        /// Turn off the safety filter.
        Off = 5,
    }
    impl HarmBlockThreshold {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "HARM_BLOCK_THRESHOLD_UNSPECIFIED",
                Self::BlockLowAndAbove => "BLOCK_LOW_AND_ABOVE",
                Self::BlockMediumAndAbove => "BLOCK_MEDIUM_AND_ABOVE",
                Self::BlockOnlyHigh => "BLOCK_ONLY_HIGH",
                Self::BlockNone => "BLOCK_NONE",
                Self::Off => "OFF",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "HARM_BLOCK_THRESHOLD_UNSPECIFIED" => Some(Self::Unspecified),
                "BLOCK_LOW_AND_ABOVE" => Some(Self::BlockLowAndAbove),
                "BLOCK_MEDIUM_AND_ABOVE" => Some(Self::BlockMediumAndAbove),
                "BLOCK_ONLY_HIGH" => Some(Self::BlockOnlyHigh),
                "BLOCK_NONE" => Some(Self::BlockNone),
                "OFF" => Some(Self::Off),
                _ => None,
            }
        }
    }
}
/// The category of a rating.
///
/// These categories cover various kinds of harms that developers
/// may wish to adjust.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum HarmCategory {
    /// Category is unspecified.
    Unspecified = 0,
    /// **PaLM** - Negative or harmful comments targeting identity and/or protected
    /// attribute.
    Derogatory = 1,
    /// **PaLM** - Content that is rude, disrespectful, or profane.
    Toxicity = 2,
    /// **PaLM** - Describes scenarios depicting violence against an individual or
    /// group, or general descriptions of gore.
    Violence = 3,
    /// **PaLM** - Contains references to sexual acts or other lewd content.
    Sexual = 4,
    /// **PaLM** - Promotes unchecked medical advice.
    Medical = 5,
    /// **PaLM** - Dangerous content that promotes, facilitates, or encourages
    /// harmful acts.
    Dangerous = 6,
    /// **Gemini** - Harassment content.
    Harassment = 7,
    /// **Gemini** - Hate speech and content.
    HateSpeech = 8,
    /// **Gemini** - Sexually explicit content.
    SexuallyExplicit = 9,
    /// **Gemini** - Dangerous content.
    DangerousContent = 10,
    /// **Gemini** - Content that may be used to harm civic integrity.
    CivicIntegrity = 11,
}
impl HarmCategory {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "HARM_CATEGORY_UNSPECIFIED",
            Self::Derogatory => "HARM_CATEGORY_DEROGATORY",
            Self::Toxicity => "HARM_CATEGORY_TOXICITY",
            Self::Violence => "HARM_CATEGORY_VIOLENCE",
            Self::Sexual => "HARM_CATEGORY_SEXUAL",
            Self::Medical => "HARM_CATEGORY_MEDICAL",
            Self::Dangerous => "HARM_CATEGORY_DANGEROUS",
            Self::Harassment => "HARM_CATEGORY_HARASSMENT",
            Self::HateSpeech => "HARM_CATEGORY_HATE_SPEECH",
            Self::SexuallyExplicit => "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            Self::DangerousContent => "HARM_CATEGORY_DANGEROUS_CONTENT",
            Self::CivicIntegrity => "HARM_CATEGORY_CIVIC_INTEGRITY",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "HARM_CATEGORY_UNSPECIFIED" => Some(Self::Unspecified),
            "HARM_CATEGORY_DEROGATORY" => Some(Self::Derogatory),
            "HARM_CATEGORY_TOXICITY" => Some(Self::Toxicity),
            "HARM_CATEGORY_VIOLENCE" => Some(Self::Violence),
            "HARM_CATEGORY_SEXUAL" => Some(Self::Sexual),
            "HARM_CATEGORY_MEDICAL" => Some(Self::Medical),
            "HARM_CATEGORY_DANGEROUS" => Some(Self::Dangerous),
            "HARM_CATEGORY_HARASSMENT" => Some(Self::Harassment),
            "HARM_CATEGORY_HATE_SPEECH" => Some(Self::HateSpeech),
            "HARM_CATEGORY_SEXUALLY_EXPLICIT" => Some(Self::SexuallyExplicit),
            "HARM_CATEGORY_DANGEROUS_CONTENT" => Some(Self::DangerousContent),
            "HARM_CATEGORY_CIVIC_INTEGRITY" => Some(Self::CivicIntegrity),
            _ => None,
        }
    }
}
/// Request to generate a completion from the model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateContentRequest {
    /// Required. The name of the `Model` to use for generating the completion.
    ///
    /// Format: `models/{model}`.
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Required. The content of the current conversation with the model.
    ///
    /// For single-turn queries, this is a single instance. For multi-turn queries
    /// like [chat](<https://ai.google.dev/gemini-api/docs/text-generation#chat>),
    /// this is a repeated field that contains the conversation history and the
    /// latest request.
    #[prost(message, repeated, tag = "2")]
    pub contents: ::prost::alloc::vec::Vec<Content>,
    /// Optional. A list of unique `SafetySetting` instances for blocking unsafe
    /// content.
    ///
    /// This will be enforced on the `GenerateContentRequest.contents` and
    /// `GenerateContentResponse.candidates`. There should not be more than one
    /// setting for each `SafetyCategory` type. The API will block any contents and
    /// responses that fail to meet the thresholds set by these settings. This list
    /// overrides the default settings for each `SafetyCategory` specified in the
    /// safety_settings. If there is no `SafetySetting` for a given
    /// `SafetyCategory` provided in the list, the API will use the default safety
    /// setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
    /// HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
    /// HARM_CATEGORY_HARASSMENT, HARM_CATEGORY_CIVIC_INTEGRITY are supported.
    /// Refer to the [guide](<https://ai.google.dev/gemini-api/docs/safety-settings>)
    /// for detailed information on available safety settings. Also refer to the
    /// [Safety guidance](<https://ai.google.dev/gemini-api/docs/safety-guidance>) to
    /// learn how to incorporate safety considerations in your AI applications.
    #[prost(message, repeated, tag = "3")]
    pub safety_settings: ::prost::alloc::vec::Vec<SafetySetting>,
    /// Optional. Configuration options for model generation and outputs.
    #[prost(message, optional, tag = "4")]
    pub generation_config: ::core::option::Option<GenerationConfig>,
}
/// Configuration options for model generation and outputs. Not all parameters
/// are configurable for every model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerationConfig {
    /// Optional. Number of generated responses to return.
    ///
    /// Currently, this value can only be set to 1. If unset, this will default
    /// to 1.
    #[prost(int32, optional, tag = "1")]
    pub candidate_count: ::core::option::Option<i32>,
    /// Optional. The set of character sequences (up to 5) that will stop output
    /// generation. If specified, the API will stop at the first appearance of a
    /// `stop_sequence`. The stop sequence will not be included as part of the
    /// response.
    #[prost(string, repeated, tag = "2")]
    pub stop_sequences: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Optional. The maximum number of tokens to include in a response candidate.
    ///
    /// Note: The default value varies by model, see the `Model.output_token_limit`
    /// attribute of the `Model` returned from the `getModel` function.
    #[prost(int32, optional, tag = "4")]
    pub max_output_tokens: ::core::option::Option<i32>,
    /// Optional. Controls the randomness of the output.
    ///
    /// Note: The default value varies by model, see the `Model.temperature`
    /// attribute of the `Model` returned from the `getModel` function.
    ///
    /// Values can range from \[0.0, 2.0\].
    #[prost(float, optional, tag = "5")]
    pub temperature: ::core::option::Option<f32>,
    /// Optional. The maximum cumulative probability of tokens to consider when
    /// sampling.
    ///
    /// The model uses combined Top-k and Top-p (nucleus) sampling.
    ///
    /// Tokens are sorted based on their assigned probabilities so that only the
    /// most likely tokens are considered. Top-k sampling directly limits the
    /// maximum number of tokens to consider, while Nucleus sampling limits the
    /// number of tokens based on the cumulative probability.
    ///
    /// Note: The default value varies by `Model` and is specified by
    /// the`Model.top_p` attribute returned from the `getModel` function. An empty
    /// `top_k` attribute indicates that the model doesn't apply top-k sampling
    /// and doesn't allow setting `top_k` on requests.
    #[prost(float, optional, tag = "6")]
    pub top_p: ::core::option::Option<f32>,
    /// Optional. The maximum number of tokens to consider when sampling.
    ///
    /// Gemini models use Top-p (nucleus) sampling or a combination of Top-k and
    /// nucleus sampling. Top-k sampling considers the set of `top_k` most probable
    /// tokens. Models running with nucleus sampling don't allow top_k setting.
    ///
    /// Note: The default value varies by `Model` and is specified by
    /// the`Model.top_p` attribute returned from the `getModel` function. An empty
    /// `top_k` attribute indicates that the model doesn't apply top-k sampling
    /// and doesn't allow setting `top_k` on requests.
    #[prost(int32, optional, tag = "7")]
    pub top_k: ::core::option::Option<i32>,
    /// Optional. Presence penalty applied to the next token's logprobs if the
    /// token has already been seen in the response.
    ///
    /// This penalty is binary on/off and not dependant on the number of times the
    /// token is used (after the first). Use
    /// [frequency_penalty][google.ai.generativelanguage.v1.GenerationConfig.frequency_penalty]
    /// for a penalty that increases with each use.
    ///
    /// A positive penalty will discourage the use of tokens that have already
    /// been used in the response, increasing the vocabulary.
    ///
    /// A negative penalty will encourage the use of tokens that have already been
    /// used in the response, decreasing the vocabulary.
    #[prost(float, optional, tag = "15")]
    pub presence_penalty: ::core::option::Option<f32>,
    /// Optional. Frequency penalty applied to the next token's logprobs,
    /// multiplied by the number of times each token has been seen in the respponse
    /// so far.
    ///
    /// A positive penalty will discourage the use of tokens that have already
    /// been used, proportional to the number of times the token has been used:
    /// The more a token is used, the more dificult it is for the model to use
    /// that token again increasing the vocabulary of responses.
    ///
    /// Caution: A _negative_ penalty will encourage the model to reuse tokens
    /// proportional to the number of times the token has been used. Small
    /// negative values will reduce the vocabulary of a response. Larger negative
    /// values will cause the model to start repeating a common token  until it
    /// hits the
    /// [max_output_tokens][google.ai.generativelanguage.v1.GenerationConfig.max_output_tokens]
    /// limit.
    #[prost(float, optional, tag = "16")]
    pub frequency_penalty: ::core::option::Option<f32>,
    /// Optional. If true, export the logprobs results in response.
    #[prost(bool, optional, tag = "17")]
    pub response_logprobs: ::core::option::Option<bool>,
    /// Optional. Only valid if
    /// [response_logprobs=True][google.ai.generativelanguage.v1.GenerationConfig.response_logprobs].
    /// This sets the number of top logprobs to return at each decoding step in the
    /// [Candidate.logprobs_result][google.ai.generativelanguage.v1.Candidate.logprobs_result].
    #[prost(int32, optional, tag = "18")]
    pub logprobs: ::core::option::Option<i32>,
    /// Optional. Enables enhanced civic answers. It may not be available for all
    /// models.
    #[prost(bool, optional, tag = "19")]
    pub enable_enhanced_civic_answers: ::core::option::Option<bool>,
}
/// Response from the model supporting multiple candidate responses.
///
/// Safety ratings and content filtering are reported for both
/// prompt in `GenerateContentResponse.prompt_feedback` and for each candidate
/// in `finish_reason` and in `safety_ratings`. The API:
///   - Returns either all requested candidates or none of them
///   - Returns no candidates at all only if there was something wrong with the
///     prompt (check `prompt_feedback`)
///   - Reports feedback on each candidate in `finish_reason` and
///     `safety_ratings`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateContentResponse {
    /// Candidate responses from the model.
    #[prost(message, repeated, tag = "1")]
    pub candidates: ::prost::alloc::vec::Vec<Candidate>,
    /// Returns the prompt's feedback related to the content filters.
    #[prost(message, optional, tag = "2")]
    pub prompt_feedback: ::core::option::Option<
        generate_content_response::PromptFeedback,
    >,
    /// Output only. Metadata on the generation requests' token usage.
    #[prost(message, optional, tag = "3")]
    pub usage_metadata: ::core::option::Option<generate_content_response::UsageMetadata>,
    /// Output only. The model version used to generate the response.
    #[prost(string, tag = "4")]
    pub model_version: ::prost::alloc::string::String,
}
/// Nested message and enum types in `GenerateContentResponse`.
pub mod generate_content_response {
    /// A set of the feedback metadata the prompt specified in
    /// `GenerateContentRequest.content`.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct PromptFeedback {
        /// Optional. If set, the prompt was blocked and no candidates are returned.
        /// Rephrase the prompt.
        #[prost(enumeration = "prompt_feedback::BlockReason", tag = "1")]
        pub block_reason: i32,
        /// Ratings for safety of the prompt.
        /// There is at most one rating per category.
        #[prost(message, repeated, tag = "2")]
        pub safety_ratings: ::prost::alloc::vec::Vec<super::SafetyRating>,
    }
    /// Nested message and enum types in `PromptFeedback`.
    pub mod prompt_feedback {
        /// Specifies the reason why the prompt was blocked.
        #[derive(
            Clone,
            Copy,
            Debug,
            PartialEq,
            Eq,
            Hash,
            PartialOrd,
            Ord,
            ::prost::Enumeration
        )]
        #[repr(i32)]
        pub enum BlockReason {
            /// Default value. This value is unused.
            Unspecified = 0,
            /// Prompt was blocked due to safety reasons. Inspect `safety_ratings`
            /// to understand which safety category blocked it.
            Safety = 1,
            /// Prompt was blocked due to unknown reasons.
            Other = 2,
            /// Prompt was blocked due to the terms which are included from the
            /// terminology blocklist.
            Blocklist = 3,
            /// Prompt was blocked due to prohibited content.
            ProhibitedContent = 4,
            /// Candidates blocked due to unsafe image generation content.
            ImageSafety = 5,
        }
        impl BlockReason {
            /// String value of the enum field names used in the ProtoBuf definition.
            ///
            /// The values are not transformed in any way and thus are considered stable
            /// (if the ProtoBuf definition does not change) and safe for programmatic use.
            pub fn as_str_name(&self) -> &'static str {
                match self {
                    Self::Unspecified => "BLOCK_REASON_UNSPECIFIED",
                    Self::Safety => "SAFETY",
                    Self::Other => "OTHER",
                    Self::Blocklist => "BLOCKLIST",
                    Self::ProhibitedContent => "PROHIBITED_CONTENT",
                    Self::ImageSafety => "IMAGE_SAFETY",
                }
            }
            /// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
                match value {
                    "BLOCK_REASON_UNSPECIFIED" => Some(Self::Unspecified),
                    "SAFETY" => Some(Self::Safety),
                    "OTHER" => Some(Self::Other),
                    "BLOCKLIST" => Some(Self::Blocklist),
                    "PROHIBITED_CONTENT" => Some(Self::ProhibitedContent),
                    "IMAGE_SAFETY" => Some(Self::ImageSafety),
                    _ => None,
                }
            }
        }
    }
    /// Metadata on the generation request's token usage.
    #[derive(Clone, Copy, PartialEq, ::prost::Message)]
    pub struct UsageMetadata {
        /// Number of tokens in the prompt. When `cached_content` is set, this is
        /// still the total effective prompt size meaning this includes the number of
        /// tokens in the cached content.
        #[prost(int32, tag = "1")]
        pub prompt_token_count: i32,
        /// Total number of tokens across all the generated response candidates.
        #[prost(int32, tag = "2")]
        pub candidates_token_count: i32,
        /// Total token count for the generation request (prompt + response
        /// candidates).
        #[prost(int32, tag = "3")]
        pub total_token_count: i32,
    }
}
/// A response candidate generated from the model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Candidate {
    /// Output only. Index of the candidate in the list of response candidates.
    #[prost(int32, optional, tag = "3")]
    pub index: ::core::option::Option<i32>,
    /// Output only. Generated content returned from the model.
    #[prost(message, optional, tag = "1")]
    pub content: ::core::option::Option<Content>,
    /// Optional. Output only. The reason why the model stopped generating tokens.
    ///
    /// If empty, the model has not stopped generating tokens.
    #[prost(enumeration = "candidate::FinishReason", tag = "2")]
    pub finish_reason: i32,
    /// List of ratings for the safety of a response candidate.
    ///
    /// There is at most one rating per category.
    #[prost(message, repeated, tag = "5")]
    pub safety_ratings: ::prost::alloc::vec::Vec<SafetyRating>,
    /// Output only. Citation information for model-generated candidate.
    ///
    /// This field may be populated with recitation information for any text
    /// included in the `content`. These are passages that are "recited" from
    /// copyrighted material in the foundational LLM's training data.
    #[prost(message, optional, tag = "6")]
    pub citation_metadata: ::core::option::Option<CitationMetadata>,
    /// Output only. Token count for this candidate.
    #[prost(int32, tag = "7")]
    pub token_count: i32,
    /// Output only. Grounding metadata for the candidate.
    ///
    /// This field is populated for `GenerateContent` calls.
    #[prost(message, optional, tag = "9")]
    pub grounding_metadata: ::core::option::Option<GroundingMetadata>,
    /// Output only. Average log probability score of the candidate.
    #[prost(double, tag = "10")]
    pub avg_logprobs: f64,
    /// Output only. Log-likelihood scores for the response tokens and top tokens
    #[prost(message, optional, tag = "11")]
    pub logprobs_result: ::core::option::Option<LogprobsResult>,
}
/// Nested message and enum types in `Candidate`.
pub mod candidate {
    /// Defines the reason why the model stopped generating tokens.
    #[derive(
        Clone,
        Copy,
        Debug,
        PartialEq,
        Eq,
        Hash,
        PartialOrd,
        Ord,
        ::prost::Enumeration
    )]
    #[repr(i32)]
    pub enum FinishReason {
        /// Default value. This value is unused.
        Unspecified = 0,
        /// Natural stop point of the model or provided stop sequence.
        Stop = 1,
        /// The maximum number of tokens as specified in the request was reached.
        MaxTokens = 2,
        /// The response candidate content was flagged for safety reasons.
        Safety = 3,
        /// The response candidate content was flagged for recitation reasons.
        Recitation = 4,
        /// The response candidate content was flagged for using an unsupported
        /// language.
        Language = 6,
        /// Unknown reason.
        Other = 5,
        /// Token generation stopped because the content contains forbidden terms.
        Blocklist = 7,
        /// Token generation stopped for potentially containing prohibited content.
        ProhibitedContent = 8,
        /// Token generation stopped because the content potentially contains
        /// Sensitive Personally Identifiable Information (SPII).
        Spii = 9,
        /// The function call generated by the model is invalid.
        MalformedFunctionCall = 10,
        /// Token generation stopped because generated images contain safety
        /// violations.
        ImageSafety = 11,
    }
    impl FinishReason {
        /// String value of the enum field names used in the ProtoBuf definition.
        ///
        /// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
            match self {
                Self::Unspecified => "FINISH_REASON_UNSPECIFIED",
                Self::Stop => "STOP",
                Self::MaxTokens => "MAX_TOKENS",
                Self::Safety => "SAFETY",
                Self::Recitation => "RECITATION",
                Self::Language => "LANGUAGE",
                Self::Other => "OTHER",
                Self::Blocklist => "BLOCKLIST",
                Self::ProhibitedContent => "PROHIBITED_CONTENT",
                Self::Spii => "SPII",
                Self::MalformedFunctionCall => "MALFORMED_FUNCTION_CALL",
                Self::ImageSafety => "IMAGE_SAFETY",
            }
        }
        /// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
            match value {
                "FINISH_REASON_UNSPECIFIED" => Some(Self::Unspecified),
                "STOP" => Some(Self::Stop),
                "MAX_TOKENS" => Some(Self::MaxTokens),
                "SAFETY" => Some(Self::Safety),
                "RECITATION" => Some(Self::Recitation),
                "LANGUAGE" => Some(Self::Language),
                "OTHER" => Some(Self::Other),
                "BLOCKLIST" => Some(Self::Blocklist),
                "PROHIBITED_CONTENT" => Some(Self::ProhibitedContent),
                "SPII" => Some(Self::Spii),
                "MALFORMED_FUNCTION_CALL" => Some(Self::MalformedFunctionCall),
                "IMAGE_SAFETY" => Some(Self::ImageSafety),
                _ => None,
            }
        }
    }
}
/// Logprobs Result
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct LogprobsResult {
    /// Length = total number of decoding steps.
    #[prost(message, repeated, tag = "1")]
    pub top_candidates: ::prost::alloc::vec::Vec<logprobs_result::TopCandidates>,
    /// Length = total number of decoding steps.
    /// The chosen candidates may or may not be in top_candidates.
    #[prost(message, repeated, tag = "2")]
    pub chosen_candidates: ::prost::alloc::vec::Vec<logprobs_result::Candidate>,
}
/// Nested message and enum types in `LogprobsResult`.
pub mod logprobs_result {
    /// Candidate for the logprobs token and score.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Candidate {
        /// The candidate’s token string value.
        #[prost(string, optional, tag = "1")]
        pub token: ::core::option::Option<::prost::alloc::string::String>,
        /// The candidate’s token id value.
        #[prost(int32, optional, tag = "3")]
        pub token_id: ::core::option::Option<i32>,
        /// The candidate's log probability.
        #[prost(float, optional, tag = "2")]
        pub log_probability: ::core::option::Option<f32>,
    }
    /// Candidates with top log probabilities at each decoding step.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct TopCandidates {
        /// Sorted by log probability in descending order.
        #[prost(message, repeated, tag = "1")]
        pub candidates: ::prost::alloc::vec::Vec<Candidate>,
    }
}
/// Metadata related to retrieval in the grounding flow.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct RetrievalMetadata {
    /// Optional. Score indicating how likely information from google search could
    /// help answer the prompt. The score is in the range \[0, 1\], where 0 is the
    /// least likely and 1 is the most likely. This score is only populated when
    /// google search grounding and dynamic retrieval is enabled. It will be
    /// compared to the threshold to determine whether to trigger google search.
    #[prost(float, tag = "2")]
    pub google_search_dynamic_retrieval_score: f32,
}
/// Metadata returned to client when grounding is enabled.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundingMetadata {
    /// Optional. Google search entry for the following-up web searches.
    #[prost(message, optional, tag = "1")]
    pub search_entry_point: ::core::option::Option<SearchEntryPoint>,
    /// List of supporting references retrieved from specified grounding source.
    #[prost(message, repeated, tag = "2")]
    pub grounding_chunks: ::prost::alloc::vec::Vec<GroundingChunk>,
    /// List of grounding support.
    #[prost(message, repeated, tag = "3")]
    pub grounding_supports: ::prost::alloc::vec::Vec<GroundingSupport>,
    /// Metadata related to retrieval in the grounding flow.
    #[prost(message, optional, tag = "4")]
    pub retrieval_metadata: ::core::option::Option<RetrievalMetadata>,
    /// Web search queries for the following-up web search.
    #[prost(string, repeated, tag = "5")]
    pub web_search_queries: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
}
/// Google search entry point.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SearchEntryPoint {
    /// Optional. Web content snippet that can be embedded in a web page or an app
    /// webview.
    #[prost(string, tag = "1")]
    pub rendered_content: ::prost::alloc::string::String,
    /// Optional. Base64 encoded JSON representing array of <search term, search
    /// url> tuple.
    #[prost(bytes = "vec", tag = "2")]
    pub sdk_blob: ::prost::alloc::vec::Vec<u8>,
}
/// Grounding chunk.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundingChunk {
    /// Chunk type.
    #[prost(oneof = "grounding_chunk::ChunkType", tags = "1")]
    pub chunk_type: ::core::option::Option<grounding_chunk::ChunkType>,
}
/// Nested message and enum types in `GroundingChunk`.
pub mod grounding_chunk {
    /// Chunk from the web.
    #[derive(Clone, PartialEq, ::prost::Message)]
    pub struct Web {
        /// URI reference of the chunk.
        #[prost(string, optional, tag = "1")]
        pub uri: ::core::option::Option<::prost::alloc::string::String>,
        /// Title of the chunk.
        #[prost(string, optional, tag = "2")]
        pub title: ::core::option::Option<::prost::alloc::string::String>,
    }
    /// Chunk type.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
    pub enum ChunkType {
        /// Grounding chunk from the web.
        #[prost(message, tag = "1")]
        Web(Web),
    }
}
/// Segment of the content.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Segment {
    /// Output only. The index of a Part object within its parent Content object.
    #[prost(int32, tag = "1")]
    pub part_index: i32,
    /// Output only. Start index in the given Part, measured in bytes. Offset from
    /// the start of the Part, inclusive, starting at zero.
    #[prost(int32, tag = "2")]
    pub start_index: i32,
    /// Output only. End index in the given Part, measured in bytes. Offset from
    /// the start of the Part, exclusive, starting at zero.
    #[prost(int32, tag = "3")]
    pub end_index: i32,
    /// Output only. The text corresponding to the segment from the response.
    #[prost(string, tag = "4")]
    pub text: ::prost::alloc::string::String,
}
/// Grounding support.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GroundingSupport {
    /// Segment of the content this support belongs to.
    #[prost(message, optional, tag = "1")]
    pub segment: ::core::option::Option<Segment>,
    /// A list of indices (into 'grounding_chunk') specifying the
    /// citations associated with the claim. For instance \[1,3,4\] means
    /// that grounding_chunk\[1\], grounding_chunk\[3\],
    /// grounding_chunk\[4\] are the retrieved content attributed to the claim.
    #[prost(int32, repeated, tag = "2")]
    pub grounding_chunk_indices: ::prost::alloc::vec::Vec<i32>,
    /// Confidence score of the support references. Ranges from 0 to 1. 1 is the
    /// most confident. This list must have the same size as the
    /// grounding_chunk_indices.
    #[prost(float, repeated, tag = "3")]
    pub confidence_scores: ::prost::alloc::vec::Vec<f32>,
}
/// Request containing the `Content` for the model to embed.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EmbedContentRequest {
    /// Required. The model's resource name. This serves as an ID for the Model to
    /// use.
    ///
    /// This name should match a model name returned by the `ListModels` method.
    ///
    /// Format: `models/{model}`
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Required. The content to embed. Only the `parts.text` fields will be
    /// counted.
    #[prost(message, optional, tag = "2")]
    pub content: ::core::option::Option<Content>,
    /// Optional. Optional task type for which the embeddings will be used. Can
    /// only be set for `models/embedding-001`.
    #[prost(enumeration = "TaskType", optional, tag = "3")]
    pub task_type: ::core::option::Option<i32>,
    /// Optional. An optional title for the text. Only applicable when TaskType is
    /// `RETRIEVAL_DOCUMENT`.
    ///
    /// Note: Specifying a `title` for `RETRIEVAL_DOCUMENT` provides better quality
    /// embeddings for retrieval.
    #[prost(string, optional, tag = "4")]
    pub title: ::core::option::Option<::prost::alloc::string::String>,
    /// Optional. Optional reduced dimension for the output embedding. If set,
    /// excessive values in the output embedding are truncated from the end.
    /// Supported by newer models since 2024 only. You cannot set this value if
    /// using the earlier model (`models/embedding-001`).
    #[prost(int32, optional, tag = "5")]
    pub output_dimensionality: ::core::option::Option<i32>,
}
/// A list of floats representing an embedding.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ContentEmbedding {
    /// The embedding values.
    #[prost(float, repeated, tag = "1")]
    pub values: ::prost::alloc::vec::Vec<f32>,
}
/// The response to an `EmbedContentRequest`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EmbedContentResponse {
    /// Output only. The embedding generated from the input content.
    #[prost(message, optional, tag = "1")]
    pub embedding: ::core::option::Option<ContentEmbedding>,
}
/// Batch request to get embeddings from the model for a list of prompts.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchEmbedContentsRequest {
    /// Required. The model's resource name. This serves as an ID for the Model to
    /// use.
    ///
    /// This name should match a model name returned by the `ListModels` method.
    ///
    /// Format: `models/{model}`
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Required. Embed requests for the batch. The model in each of these requests
    /// must match the model specified `BatchEmbedContentsRequest.model`.
    #[prost(message, repeated, tag = "2")]
    pub requests: ::prost::alloc::vec::Vec<EmbedContentRequest>,
}
/// The response to a `BatchEmbedContentsRequest`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct BatchEmbedContentsResponse {
    /// Output only. The embeddings for each request, in the same order as provided
    /// in the batch request.
    #[prost(message, repeated, tag = "1")]
    pub embeddings: ::prost::alloc::vec::Vec<ContentEmbedding>,
}
/// Counts the number of tokens in the `prompt` sent to a model.
///
/// Models may tokenize text differently, so each model may return a different
/// `token_count`.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CountTokensRequest {
    /// Required. The model's resource name. This serves as an ID for the Model to
    /// use.
    ///
    /// This name should match a model name returned by the `ListModels` method.
    ///
    /// Format: `models/{model}`
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Optional. The input given to the model as a prompt. This field is ignored
    /// when `generate_content_request` is set.
    #[prost(message, repeated, tag = "2")]
    pub contents: ::prost::alloc::vec::Vec<Content>,
    /// Optional. The overall input given to the `Model`. This includes the prompt
    /// as well as other model steering information like [system
    /// instructions](<https://ai.google.dev/gemini-api/docs/system-instructions>),
    /// and/or function declarations for [function
    /// calling](<https://ai.google.dev/gemini-api/docs/function-calling>).
    /// `Model`s/`Content`s and `generate_content_request`s are mutually
    /// exclusive. You can either send `Model` + `Content`s or a
    /// `generate_content_request`, but never both.
    #[prost(message, optional, tag = "3")]
    pub generate_content_request: ::core::option::Option<GenerateContentRequest>,
}
/// A response from `CountTokens`.
///
/// It returns the model's `token_count` for the `prompt`.
#[derive(Clone, Copy, PartialEq, ::prost::Message)]
pub struct CountTokensResponse {
    /// The number of tokens that the `Model` tokenizes the `prompt` into. Always
    /// non-negative.
    #[prost(int32, tag = "1")]
    pub total_tokens: i32,
}
/// Type of task for which the embedding will be used.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
#[repr(i32)]
pub enum TaskType {
    /// Unset value, which will default to one of the other enum values.
    Unspecified = 0,
    /// Specifies the given text is a query in a search/retrieval setting.
    RetrievalQuery = 1,
    /// Specifies the given text is a document from the corpus being searched.
    RetrievalDocument = 2,
    /// Specifies the given text will be used for STS.
    SemanticSimilarity = 3,
    /// Specifies that the given text will be classified.
    Classification = 4,
    /// Specifies that the embeddings will be used for clustering.
    Clustering = 5,
    /// Specifies that the given text will be used for question answering.
    QuestionAnswering = 6,
    /// Specifies that the given text will be used for fact verification.
    FactVerification = 7,
}
impl TaskType {
    /// String value of the enum field names used in the ProtoBuf definition.
    ///
    /// The values are not transformed in any way and thus are considered stable
    /// (if the ProtoBuf definition does not change) and safe for programmatic use.
    pub fn as_str_name(&self) -> &'static str {
        match self {
            Self::Unspecified => "TASK_TYPE_UNSPECIFIED",
            Self::RetrievalQuery => "RETRIEVAL_QUERY",
            Self::RetrievalDocument => "RETRIEVAL_DOCUMENT",
            Self::SemanticSimilarity => "SEMANTIC_SIMILARITY",
            Self::Classification => "CLASSIFICATION",
            Self::Clustering => "CLUSTERING",
            Self::QuestionAnswering => "QUESTION_ANSWERING",
            Self::FactVerification => "FACT_VERIFICATION",
        }
    }
    /// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
        match value {
            "TASK_TYPE_UNSPECIFIED" => Some(Self::Unspecified),
            "RETRIEVAL_QUERY" => Some(Self::RetrievalQuery),
            "RETRIEVAL_DOCUMENT" => Some(Self::RetrievalDocument),
            "SEMANTIC_SIMILARITY" => Some(Self::SemanticSimilarity),
            "CLASSIFICATION" => Some(Self::Classification),
            "CLUSTERING" => Some(Self::Clustering),
            "QUESTION_ANSWERING" => Some(Self::QuestionAnswering),
            "FACT_VERIFICATION" => Some(Self::FactVerification),
            _ => None,
        }
    }
}
/// Generated client implementations.
pub mod generative_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// API for using Large Models that generate multimodal content and have
    /// additional capabilities beyond text generation.
    #[derive(Debug, Clone)]
    pub struct GenerativeServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl GenerativeServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> GenerativeServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> GenerativeServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            GenerativeServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Generates a model response given an input `GenerateContentRequest`.
        /// Refer to the [text generation
        /// guide](https://ai.google.dev/gemini-api/docs/text-generation) for detailed
        /// usage information. Input capabilities differ between models, including
        /// tuned models. Refer to the [model
        /// guide](https://ai.google.dev/gemini-api/docs/models/gemini) and [tuning
        /// guide](https://ai.google.dev/gemini-api/docs/model-tuning) for details.
        pub async fn generate_content(
            &mut self,
            request: impl tonic::IntoRequest<super::GenerateContentRequest>,
        ) -> std::result::Result<
            tonic::Response<super::GenerateContentResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.ai.generativelanguage.v1.GenerativeService/GenerateContent",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.ai.generativelanguage.v1.GenerativeService",
                        "GenerateContent",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Generates a [streamed
        /// response](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#generate-a-text-stream)
        /// from the model given an input `GenerateContentRequest`.
        pub async fn stream_generate_content(
            &mut self,
            request: impl tonic::IntoRequest<super::GenerateContentRequest>,
        ) -> std::result::Result<
            tonic::Response<tonic::codec::Streaming<super::GenerateContentResponse>>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.ai.generativelanguage.v1.GenerativeService/StreamGenerateContent",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.ai.generativelanguage.v1.GenerativeService",
                        "StreamGenerateContent",
                    ),
                );
            self.inner.server_streaming(req, path, codec).await
        }
        /// Generates a text embedding vector from the input `Content` using the
        /// specified [Gemini Embedding
        /// model](https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding).
        pub async fn embed_content(
            &mut self,
            request: impl tonic::IntoRequest<super::EmbedContentRequest>,
        ) -> std::result::Result<
            tonic::Response<super::EmbedContentResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.ai.generativelanguage.v1.GenerativeService/EmbedContent",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.ai.generativelanguage.v1.GenerativeService",
                        "EmbedContent",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Generates multiple embedding vectors from the input `Content` which
        /// consists of a batch of strings represented as `EmbedContentRequest`
        /// objects.
        pub async fn batch_embed_contents(
            &mut self,
            request: impl tonic::IntoRequest<super::BatchEmbedContentsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::BatchEmbedContentsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.ai.generativelanguage.v1.GenerativeService/BatchEmbedContents",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.ai.generativelanguage.v1.GenerativeService",
                        "BatchEmbedContents",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Runs a model's tokenizer on input `Content` and returns the token count.
        /// Refer to the [tokens guide](https://ai.google.dev/gemini-api/docs/tokens)
        /// to learn more about tokens.
        pub async fn count_tokens(
            &mut self,
            request: impl tonic::IntoRequest<super::CountTokensRequest>,
        ) -> std::result::Result<
            tonic::Response<super::CountTokensResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.ai.generativelanguage.v1.GenerativeService/CountTokens",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.ai.generativelanguage.v1.GenerativeService",
                        "CountTokens",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Information about a Generative Language Model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Model {
    /// Required. The resource name of the `Model`. Refer to [Model
    /// variants](<https://ai.google.dev/gemini-api/docs/models/gemini#model-variations>)
    /// for all allowed values.
    ///
    /// Format: `models/{model}` with a `{model}` naming convention of:
    ///
    /// * "{base_model_id}-{version}"
    ///
    /// Examples:
    ///
    /// * `models/gemini-1.5-flash-001`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
    /// Required. The name of the base model, pass this to the generation request.
    ///
    /// Examples:
    ///
    /// * `gemini-1.5-flash`
    #[prost(string, tag = "2")]
    pub base_model_id: ::prost::alloc::string::String,
    /// Required. The version number of the model.
    ///
    /// This represents the major version (`1.0` or `1.5`)
    #[prost(string, tag = "3")]
    pub version: ::prost::alloc::string::String,
    /// The human-readable name of the model. E.g. "Gemini 1.5 Flash".
    ///
    /// The name can be up to 128 characters long and can consist of any UTF-8
    /// characters.
    #[prost(string, tag = "4")]
    pub display_name: ::prost::alloc::string::String,
    /// A short description of the model.
    #[prost(string, tag = "5")]
    pub description: ::prost::alloc::string::String,
    /// Maximum number of input tokens allowed for this model.
    #[prost(int32, tag = "6")]
    pub input_token_limit: i32,
    /// Maximum number of output tokens available for this model.
    #[prost(int32, tag = "7")]
    pub output_token_limit: i32,
    /// The model's supported generation methods.
    ///
    /// The corresponding API method names are defined as Pascal case
    /// strings, such as `generateMessage` and `generateContent`.
    #[prost(string, repeated, tag = "8")]
    pub supported_generation_methods: ::prost::alloc::vec::Vec<
        ::prost::alloc::string::String,
    >,
    /// Controls the randomness of the output.
    ///
    /// Values can range over `\[0.0,max_temperature\]`, inclusive. A higher value
    /// will produce responses that are more varied, while a value closer to `0.0`
    /// will typically result in less surprising responses from the model.
    /// This value specifies default to be used by the backend while making the
    /// call to the model.
    #[prost(float, optional, tag = "9")]
    pub temperature: ::core::option::Option<f32>,
    /// The maximum temperature this model can use.
    #[prost(float, optional, tag = "13")]
    pub max_temperature: ::core::option::Option<f32>,
    /// For [Nucleus
    /// sampling](<https://ai.google.dev/gemini-api/docs/prompting-strategies#top-p>).
    ///
    /// Nucleus sampling considers the smallest set of tokens whose probability
    /// sum is at least `top_p`.
    /// This value specifies default to be used by the backend while making the
    /// call to the model.
    #[prost(float, optional, tag = "10")]
    pub top_p: ::core::option::Option<f32>,
    /// For Top-k sampling.
    ///
    /// Top-k sampling considers the set of `top_k` most probable tokens.
    /// This value specifies default to be used by the backend while making the
    /// call to the model.
    /// If empty, indicates the model doesn't use top-k sampling, and `top_k` isn't
    /// allowed as a generation parameter.
    #[prost(int32, optional, tag = "11")]
    pub top_k: ::core::option::Option<i32>,
}
/// Request for getting information about a specific Model.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GetModelRequest {
    /// Required. The resource name of the model.
    ///
    /// This name should match a model name returned by the `ListModels` method.
    ///
    /// Format: `models/{model}`
    #[prost(string, tag = "1")]
    pub name: ::prost::alloc::string::String,
}
/// Request for listing all Models.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsRequest {
    /// The maximum number of `Models` to return (per page).
    ///
    /// If unspecified, 50 models will be returned per page.
    /// This method returns at most 1000 models per page, even if you pass a larger
    /// page_size.
    #[prost(int32, tag = "2")]
    pub page_size: i32,
    /// A page token, received from a previous `ListModels` call.
    ///
    /// Provide the `page_token` returned by one request as an argument to the next
    /// request to retrieve the next page.
    ///
    /// When paginating, all other parameters provided to `ListModels` must match
    /// the call that provided the page token.
    #[prost(string, tag = "3")]
    pub page_token: ::prost::alloc::string::String,
}
/// Response from `ListModel` containing a paginated list of Models.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ListModelsResponse {
    /// The returned Models.
    #[prost(message, repeated, tag = "1")]
    pub models: ::prost::alloc::vec::Vec<Model>,
    /// A token, which can be sent as `page_token` to retrieve the next page.
    ///
    /// If this field is omitted, there are no more pages.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod model_service_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// Provides methods for getting metadata information about Generative Models.
    #[derive(Debug, Clone)]
    pub struct ModelServiceClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl ModelServiceClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> ModelServiceClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::BoxBody>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> ModelServiceClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::BoxBody>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::BoxBody>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            ModelServiceClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Gets information about a specific `Model` such as its version number, token
        /// limits,
        /// [parameters](https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters)
        /// and other metadata. Refer to the [Gemini models
        /// guide](https://ai.google.dev/gemini-api/docs/models/gemini) for detailed
        /// model information.
        pub async fn get_model(
            &mut self,
            request: impl tonic::IntoRequest<super::GetModelRequest>,
        ) -> std::result::Result<tonic::Response<super::Model>, tonic::Status> {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.ai.generativelanguage.v1.ModelService/GetModel",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.ai.generativelanguage.v1.ModelService",
                        "GetModel",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Lists the [`Model`s](https://ai.google.dev/gemini-api/docs/models/gemini)
        /// available through the Gemini API.
        pub async fn list_models(
            &mut self,
            request: impl tonic::IntoRequest<super::ListModelsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::ListModelsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.ai.generativelanguage.v1.ModelService/ListModels",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.ai.generativelanguage.v1.ModelService",
                        "ListModels",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
